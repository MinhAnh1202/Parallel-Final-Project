{"cells":[{"cell_type":"markdown","source":["# CPU"],"metadata":{"id":"yML7sBgZAWFg"}},{"cell_type":"markdown","source":["## 1) Huấn luyện Autoencoder:"],"metadata":{"id":"SDOf21TsAdH1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvsCf0PFdQ07","outputId":"ccbe37ff-301a-46fb-da6a-53b5f2e4cf31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing cpu_layers.c\n"]}],"source":["%%writefile cpu_layers.c\n","#include \"cpu_layers.h\"\n","\n","void Relu(float* input, int N, float* output) {\n","    for (int i = 0; i < N; i++) {\n","        output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n","    }\n","}\n","\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width)\n","{\n","    // // Tính toán kích thước output\n","    // int H_out = (input_height + 2 * padding - kernel_height) / stride + 1;\n","    // int W_out = (input_width + 2 * padding - kernel_width) / stride + 1;\n","    // int output_size = filter_count * H_out * W_out;\n","    // output = (float*)malloc(output_size * sizeof(float));\n","    // if (output == NULL) {\n","    //     fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","    //     return;\n","    // }\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua chiều cao output\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            // Lặp qua chiều rộng output\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float sum = 0.0f;\n","                // Lặp qua kênh đầu vào (c_in)\n","                for (int c_in = 0; c_in < input_channels; c_in++) {\n","                    // Lặp qua kernel height\n","                    for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                        // Lặp qua kernel width\n","                        for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                            // Vị trí input tương ứng\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float val = 0.0f;\n","                            // Kiểm tra zero padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int channel_size = input_width * input_height;\n","                                val = input[c_in * channel_size + h_in * input_width + w_in];\n","                            }\n","\n","                            int weight_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            k_h * kernel_width + k_w;\n","\n","                            sum += val * kernel[weight_idx];\n","                        }\n","                    }\n","                }\n","                sum += biases[c_out];  // Thêm bias\n","                int output_idx = h_out * output_width + w_out + c_out * output_width * output_height;\n","                output[output_idx] = sum;\n","            }\n","        }\n","    }\n","}\n","\n","\n","void MaxPool2D_Forward(float* input, int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width) {\n","    // int H_out = (input_height - filter_height) / stride + 1;\n","    // int W_out = (input_width - filter_width) / stride + 1;\n","\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float max_val = -FLT_MAX;\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = c * plane_size_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                        }\n","                    }\n","                }\n","                int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                output[output_idx] = max_val;\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","int scale_factor, int filter_count, float* output, int output_height, int output_width) {\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float val = input[c * plane_size_in + h_in * input_width + w_in];\n","                for (int sh = 0; sh < scale_factor; sh++) { // Gấp đôi hàng\n","                    for (int sw = 0; sw < scale_factor; sw++) { // Gấp đôi cột\n","                        int h_out = h_in * scale_factor + sh;\n","                        int w_out = w_in * scale_factor + sw;\n","                        int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                        output[output_idx] = val;\n","                    }\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","float MSE(float* input, float* output, int size) {\n","    float sum = 0.0f;\n","    for (int i = 0; i < size; i++) {\n","        sum += (output[i] - input[i]) * (output[i] - input[i]);\n","    }\n","    return sum / size;\n","}\n","\n","void Relu_Backward(float* d_output, float* input,int N) {\n","    for (int i = 0; i < N; i++) {\n","        d_output[i] = input[i] > 0.0f ? d_output[i] : 0.0f;\n","    }\n","}\n","\n","void MSE_Gradient(float* input, float* output, int size, float* d_output) {\n","    float sum = 0.0f;\n","    float factor = 2.0f / size;\n","    for (int i = 0; i < size; i++) {\n","        d_output[i] = factor * (output[i] - input[i]);\n","    }\n","}\n","\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* d_input)\n","{\n","    // Chỉ gán vị trí giá trị max của input ban đầu là gradient của lớp tiếp theo (d_output), còn lại là 0\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) { // Khởi tạo gradient của input ban đầu là 0\n","        d_input[i] = 0.0f;\n","    }\n","\n","    for (int c = 0; c < filter_count; c++) {\n","\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","\n","                float max_val = -FLT_MAX;\n","                int max_input_idx = -1;\n","\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = channel_offset_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                            max_input_idx = input_idx;\n","                        }\n","                    }\n","                }\n","                //Lấy gradient từ output\n","                if (max_input_idx != -1) {\n","                    int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                    d_input[max_input_idx] += d_output[output_idx];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width) {\n","\n","    int plane_size_in = d_input_height * d_input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) {\n","        d_input[i] = 0.0f;\n","    }\n","    for (int c = 0; c < filter_count; c++) {\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < d_input_height; h_in++) {\n","            for (int w_in = 0; w_in < d_input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                int h_start_out = h_in * scale_factor;\n","                int w_start_out = w_in * scale_factor;\n","                for (int sh = 0; sh < scale_factor; sh++) {\n","                    for (int sw = 0; sw < scale_factor; sw++) {\n","                        int h_out = h_start_out + sh;\n","                        int w_out = w_start_out + sw;\n","                        if (h_out < d_output_height && w_out < d_output_width) {\n","                            int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                            sum_gradient += d_output[output_idx];\n","                        }\n","                    }\n","                }\n","                int input_idx = channel_offset_in + h_in * d_input_width + w_in;\n","                d_input[input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input) {\n","    // Thực hiện tích chập giữa dE/dO và kernel (xoay 180 độ) để tính dE/dI\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh input (kênh output gradient)\n","    for (int c_in = 0; c_in < input_channels; c_in++) {\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                // Lặp qua kênh output (số lượng filter)\n","                for (int c_out = 0; c_out < filter_count; c_out++) {\n","                    // Lặp qua kernel (xoay 180 độ)\n","                    for (int kh = 0; kh < kernel_height; kh++) {\n","                        for (int kw = 0; kw < kernel_width; kw++) {\n","                            int h_out = h_in - kh + padding;\n","                            int w_out = w_in - kw + padding;\n","                            float d_output_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_out >= 0 && h_out < d_output_height && w_out >= 0 && w_out < d_output_width) {\n","                                int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                                d_output_val = d_output[d_output_idx];\n","                            }\n","                            // Tính chỉ số kernel (xoay 180 độ)\n","                            int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            (kernel_height - 1 - kh) * kernel_width + (kernel_width - 1 - kw);\n","\n","                            sum_gradient += d_output_val * kernel[kernel_idx];\n","                        }\n","                    }\n","                }\n","                int d_input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                d_input[d_input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights) {\n","    // Thực hiện tích chập giữa dE/dO và input để tính dE/dW\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua kênh đầu vào\n","        for (int c_in = 0; c_in < input_channels; c_in++) {\n","            // Lặp qua kích thước kernel\n","            for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                    float sum_gradient = 0.0f;\n","                    // Lặp qua output grid (d_output) để tích lũy\n","                    for (int h_out = 0; h_out < d_output_height; h_out++) {\n","                        for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float input_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                                input_val = input[input_idx];\n","                            }\n","                            int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                            float d_output_val = d_output[d_output_idx];\n","                            // Tính gradient\n","                            sum_gradient += input_val * d_output_val;\n","                        }\n","                    }\n","                    int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                    k_h * kernel_width + k_w;\n","                    d_weights[kernel_idx] += sum_gradient;\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height,\n","    int filter_count, float* d_biases) {\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua từng filter\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        float sum_gradient = 0.0f;\n","        // Lặp qua từng vị trí trong output\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                // Tính chỉ số trong mảng d_output\n","                int output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                // Cộng dồn gradient từ d_output\n","                sum_gradient += d_output[output_idx];\n","            }\n","        }\n","        d_biases[c_out] += sum_gradient;\n","    }\n","}\n","\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params) {\n","    for (int i = 0; i < N_params; i++) {\n","        weights[i] -= (learning_rate * d_weights[i]);\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"929dCgq1dZu3","outputId":"c80f69ca-06aa-4241-da56-273f4a5cb5c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing cpu_layers.h\n"]}],"source":["%%writefile cpu_layers.h\n","#pragma once\n","#include <stdio.h>\n","#include <float.h>\n","\n","void Relu(float* input, int N, float* output);\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void MaxPool2D_Forward(float* input, int input_width, int input_height,\n","    int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","    int scale_factor, int filter_count,\n","    float* output, int output_height, int output_width);\n","float MSE(float* input, float* output, int size);\n","void Relu_Backward(float* d_output, float* input,int N);\n","void MSE_Gradient(float* input, float* output, int size, float* d_output);\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width);\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights);\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height, int filter_count, float* d_biases);\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZDAw3eKdhov","outputId":"ee125922-9db8-47ca-88f7-e6dd63216453"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing cpu_autoencoder.h\n"]}],"source":["%%writefile cpu_autoencoder.h\n","#pragma once\n","#include \"cpu_layers.h\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include <math.h>\n","#include <stdint.h>\n","#include <time.h>\n","#include <string.h>\n","// Định nghĩa kích thước Kernel/Stride/Padding\n","#define KERNEL_SIZE 3\n","#define POOL_SIZE 2\n","#define UPSAMPLE_SIZE 2\n","#define CONV_PADDING 1\n","#define CONV_STRIDE 1\n","#define POOL_STRIDE 2\n","\n","typedef struct {\n","    int batch_size;\n","    double learning_rate;\n","    // kích thước input\n","    int input_height;       // 32\n","    int input_width;        // 32\n","    int input_channels;     // 3\n","    // weight, bias và gradient của từng lớp Conv2D\n","    float* w1; float* b1; float* d_w1; float* d_b1;\n","    float* w2; float* b2; float* d_w2; float* d_b2;\n","    float* w3; float* b3; float* d_w3; float* d_b3;\n","    float* w4; float* b4; float* d_w4; float* d_b4;\n","    float* w5; float* b5; float* d_w5; float* d_b5;\n","\n","    float* batch_input;\n","    float* final_output;\n","    float* loss_gradient;\n","    // ouput và gradient của từng lớp Conv2D/MaxPool/UpSample\n","    float* conv1_output;   float* d_conv1_output;\n","    float* pool1_output;   float* d_pool1_output;\n","    float* conv2_output;   float* d_conv2_output;\n","    float* pool2_output;   float* d_pool2_output; // LATENT SPACE\n","    float* conv3_output;   float* d_conv3_output;\n","    float* upsample1_output; float* d_upsample1_output;\n","    float* conv4_output;   float* d_conv4_output;\n","    float* upsample2_output; float* d_upsample2_output;\n","} CPUAutoEncoder;\n","\n","void random_initialize(float* array, int size, float min, float max);\n","void initialize_autoencoder(CPUAutoEncoder* autoencoder, int batch_size, double learning_rate);\n","void free_autoencoder(CPUAutoEncoder* autoencoder);\n","void forward_autoencoder(CPUAutoEncoder* autoencoder);\n","void backward_autoencoder(CPUAutoEncoder* autoencoder);\n","void update_autoencoder_parameters(CPUAutoEncoder* autoencoder);\n","void save_weights(CPUAutoEncoder* autoencoder, const char* filename);\n","void cpu_extract_features(CPUAutoEncoder* autoencoder, float* input_data, int num_images, float* features_output);\n","void cpu_load_weights(CPUAutoEncoder* autoencoder, const char* filename);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4y5aXh1idtX2","outputId":"2c654c3a-99d8-4ab3-9f8e-292204ecafcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing cpu_autoencoder.c\n"]}],"source":["%%writefile cpu_autoencoder.c\n","#include \"cpu_autoencoder.h\"\n","\n","// Hàm khởi tạo mảng trọng số với giá trị ngẫu nhiên trong khoảng [min, max]\n","void random_initialize(float* array, int size, float min, float max) {\n","    for (int i = 0; i < size; i++) {\n","        float scale = (float)rand() / (float)RAND_MAX;\n","        array[i] = min + scale * (max - min);\n","    }\n","}\n","\n","void zero_initialize(float* array, int size) {\n","    for (int i = 0; i < size; i++) {\n","        array[i] = 0.0f;\n","    }\n","}\n","\n","\n","void initialize_conv_layer(float** w, float** b, float** dw, float** db, int C_in, int C_out) {\n","    int size_W = C_out * C_in * KERNEL_SIZE * KERNEL_SIZE;\n","    *w = (float*)malloc(size_W * sizeof(float));\n","    *b = (float*)malloc(C_out * sizeof(float));\n","    *dw = (float*)malloc(size_W * sizeof(float));\n","    *db = (float*)malloc(C_out * sizeof(float));\n","\n","    random_initialize(*w, size_W, -0.05f, 0.05f);\n","    random_initialize(*b, C_out, -0.05f, 0.05f);\n","    zero_initialize(*dw, size_W);\n","    zero_initialize(*db, C_out);\n","}\n","\n","float* allocate_buffer(int batch_size, int H, int W, int C) {\n","    int size = batch_size * H * W * C;\n","    return (float*)malloc(size * sizeof(float));\n","}\n","\n","\n","void initialize_autoencoder(CPUAutoEncoder* autoencoder, int batch_size, double learning_rate) {\n","    // Tham số chung\n","    autoencoder->batch_size = batch_size;\n","    autoencoder->learning_rate = learning_rate;\n","    autoencoder->input_height = 32;\n","    autoencoder->input_width = 32;\n","    autoencoder->input_channels = 3;\n","\n","    // Output channels của các lớp\n","    int C_in = 3, C1 = 256, C2 = 128, C3 = 128, C4 = 256, C5 = 3;\n","    // Kích thước không gian (Pixel/kênh)\n","    int P1 = 32 * 32, P2 = 16 * 16, P3 = 8 * 8;\n","    // Khởi tạo trọng số, bias và gradient cho từng lớp Conv2D\n","    initialize_conv_layer(&autoencoder->w1, &autoencoder->b1, &autoencoder->d_w1, &autoencoder->d_b1, C_in, C1);\n","    initialize_conv_layer(&autoencoder->w2, &autoencoder->b2, &autoencoder->d_w2, &autoencoder->d_b2, C1, C2);\n","    initialize_conv_layer(&autoencoder->w3, &autoencoder->b3, &autoencoder->d_w3, &autoencoder->d_b3, C2, C3);\n","    initialize_conv_layer(&autoencoder->w4, &autoencoder->b4, &autoencoder->d_w4, &autoencoder->d_b4, C3, C4);\n","    initialize_conv_layer(&autoencoder->w5, &autoencoder->b5, &autoencoder->d_w5, &autoencoder->d_b5, C4, C5);\n","    // Khởi tạo Buffers cho activations và gradients\n","    int input_height = 32, input_width = 32;\n","    autoencoder->batch_input = allocate_buffer(batch_size, input_height, input_width, C_in);\n","    autoencoder->final_output = allocate_buffer(batch_size, input_height, input_width, C5); // Output size (32x32x3)\n","    autoencoder->loss_gradient = allocate_buffer(batch_size, input_height, input_width, C5);\n","    // Layer 1 (Conv1): 32x32x256\n","    autoencoder->conv1_output = allocate_buffer(batch_size, input_height, input_width, C1);\n","    autoencoder->d_conv1_output = allocate_buffer(batch_size, input_height, input_width, C1);\n","    // Layer 2 (Pool1): 16x16x256\n","    int H2 = 16, W2 = 16;\n","    autoencoder->pool1_output = allocate_buffer(batch_size, H2, W2, C1);\n","    autoencoder->d_pool1_output = allocate_buffer(batch_size, H2, W2, C1);\n","    // Layer 3 (Conv2): 16x16x128\n","    autoencoder->conv2_output = allocate_buffer(batch_size, H2, W2, C2);\n","    autoencoder->d_conv2_output = allocate_buffer(batch_size, H2, W2, C2);\n","    // Layer 4 (Pool2 - Latent): 8x8x128\n","    int H3 = 8, W3 = 8;\n","    autoencoder->pool2_output = allocate_buffer(batch_size, H3, W3, C2);\n","    autoencoder->d_pool2_output = allocate_buffer(batch_size, H3, W3, C2);\n","    // Layer 5 (Conv3): 8x8x128\n","    autoencoder->conv3_output = allocate_buffer(batch_size, H3, W3, C3);\n","    autoencoder->d_conv3_output = allocate_buffer(batch_size, H3, W3, C3);\n","    // Layer 6 (UpSample1): 16x16x128\n","    autoencoder->upsample1_output = allocate_buffer(batch_size, H2, W2, C3);\n","    autoencoder->d_upsample1_output = allocate_buffer(batch_size, H2, W2, C3);\n","    // Layer 7 (Conv4): 16x16x256\n","    autoencoder->conv4_output = allocate_buffer(batch_size, H2, W2, C4);\n","    autoencoder->d_conv4_output = allocate_buffer(batch_size, H2, W2, C4);\n","    // Layer 8 (UpSample2): 32x32x256\n","    autoencoder->upsample2_output = allocate_buffer(batch_size, input_height, input_width, C4);\n","    autoencoder->d_upsample2_output = allocate_buffer(batch_size, input_height, input_width, C4);\n","}\n","\n","void free_autoencoder(CPUAutoEncoder* autoencoder) {\n","    // Giải phóng trọng số và gradient\n","    free(autoencoder->w1); free(autoencoder->b1); free(autoencoder->d_w1); free(autoencoder->d_b1);\n","    free(autoencoder->w2); free(autoencoder->b2); free(autoencoder->d_w2); free(autoencoder->d_b2);\n","    free(autoencoder->w3); free(autoencoder->b3); free(autoencoder->d_w3); free(autoencoder->d_b3);\n","    free(autoencoder->w4); free(autoencoder->b4); free(autoencoder->d_w4); free(autoencoder->d_b4);\n","    free(autoencoder->w5); free(autoencoder->b5); free(autoencoder->d_w5); free(autoencoder->d_b5);\n","\n","    // Giải phóng buffers activation/gradient\n","    free(autoencoder->batch_input);\n","    free(autoencoder->final_output);\n","    free(autoencoder->loss_gradient);\n","    free(autoencoder->conv1_output); free(autoencoder->d_conv1_output);\n","    free(autoencoder->pool1_output); free(autoencoder->d_pool1_output);\n","    free(autoencoder->conv2_output); free(autoencoder->d_conv2_output);\n","    free(autoencoder->pool2_output); free(autoencoder->d_pool2_output);\n","    free(autoencoder->conv3_output); free(autoencoder->d_conv3_output);\n","    free(autoencoder->upsample1_output); free(autoencoder->d_upsample1_output);\n","    free(autoencoder->conv4_output); free(autoencoder->d_conv4_output);\n","    free(autoencoder->upsample2_output); free(autoencoder->d_upsample2_output);\n","}\n","\n","// Forward\n","void forward_autoencoder(CPUAutoEncoder* autoencoder) {\n","    int bs = autoencoder->batch_size;\n","\n","    // Kích thước activation của 1 ảnh tại các lớp\n","    int size_input = 32 * 32 * 3;\n","    int size_L1 = 32 * 32 * 256;\n","    int size_L2 = 16 * 16 * 256;\n","    int size_L3 = 16 * 16 * 128;\n","    int size_L4 = 8 * 8 * 128; // Latent\n","    // Decoder sizes\n","    int size_L5 = 8 * 8 * 128;\n","    int size_L6 = 16 * 16 * 128;\n","    int size_L7 = 16 * 16 * 256;\n","    int size_L8 = 32 * 32 * 256;\n","    int size_Out = 32 * 32 * 3;\n","    for (int b = 0; b < bs; b++) {\n","        // Tính offset con trỏ cho ảnh thứ b\n","        float* ptr_input = autoencoder->batch_input + b * size_input;\n","        float* ptr_L1 = autoencoder->conv1_output + b * size_L1;\n","        float* ptr_L2 = autoencoder->pool1_output + b * size_L2;\n","        float* ptr_L3 = autoencoder->conv2_output + b * size_L3;\n","        float* ptr_L4 = autoencoder->pool2_output + b * size_L4;\n","        float* ptr_L5 = autoencoder->conv3_output + b * size_L5;\n","        float* ptr_L6 = autoencoder->upsample1_output + b * size_L6;\n","        float* ptr_L7 = autoencoder->conv4_output + b * size_L7;\n","        float* ptr_L8 = autoencoder->upsample2_output + b * size_L8;\n","        float* ptr_Out = autoencoder->final_output + b * size_Out;\n","        // --- ENCODER ---\n","        // L1: Conv1 + ReLU\n","        Conv2D_Forward(ptr_input, 32, 32, 3, autoencoder->w1, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b1, CONV_PADDING, CONV_STRIDE, 256, ptr_L1, 32, 32);\n","        Relu(ptr_L1, size_L1, ptr_L1);\n","\n","        // L2: Pool1\n","        MaxPool2D_Forward(ptr_L1, 32, 32, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 256, ptr_L2, 16, 16);\n","\n","        // L3: Conv2 + ReLU\n","        Conv2D_Forward(ptr_L2, 16, 16, 256, autoencoder->w2, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b2, CONV_PADDING, CONV_STRIDE, 128, ptr_L3, 16, 16);\n","        Relu(ptr_L3, size_L3, ptr_L3);\n","\n","        // L4: Pool2 (Latent)\n","        MaxPool2D_Forward(ptr_L3, 16, 16, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 128, ptr_L4, 8, 8);\n","\n","        // --- DECODER ---\n","        // L5: Conv3 + ReLU\n","        Conv2D_Forward(ptr_L4, 8, 8, 128, autoencoder->w3, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b3, CONV_PADDING, CONV_STRIDE, 128, ptr_L5, 8, 8);\n","        Relu(ptr_L5, size_L5, ptr_L5);\n","\n","        // L6: UpSample1\n","        UpSample2D_Forward(ptr_L5, 8, 8, UPSAMPLE_SIZE, 128, ptr_L6, 16, 16);\n","\n","        // L7: Conv4 + ReLU\n","        Conv2D_Forward(ptr_L6, 16, 16, 128, autoencoder->w4, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b4, CONV_PADDING, CONV_STRIDE, 256, ptr_L7, 16, 16);\n","        Relu(ptr_L7, size_L7, ptr_L7);\n","\n","        // L8: UpSample2\n","        UpSample2D_Forward(ptr_L7, 16, 16, UPSAMPLE_SIZE, 256, ptr_L8, 32, 32);\n","\n","        // L9: Conv5 (Output)\n","        Conv2D_Forward(ptr_L8, 32, 32, 256, autoencoder->w5, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b5, CONV_PADDING, CONV_STRIDE, 3, ptr_Out, 32, 32);\n","    }\n","}\n","\n","\n","// Backward\n","void backward_autoencoder(CPUAutoEncoder* autoencoder) {\n","    int bs = autoencoder->batch_size;\n","    int total_elements = bs * 32 * 32 * 3;\n","    MSE_Gradient(autoencoder->batch_input, autoencoder->final_output, total_elements, autoencoder->loss_gradient);\n","    // Khởi tạo gradient về 0 trước khi cộng dồn\n","    zero_initialize(autoencoder->d_w1, 256*3*3*3); zero_initialize(autoencoder->d_b1, 256);\n","    zero_initialize(autoencoder->d_w2, 128*256*3*3); zero_initialize(autoencoder->d_b2, 128);\n","    zero_initialize(autoencoder->d_w3, 128*128*3*3); zero_initialize(autoencoder->d_b3, 128);\n","    zero_initialize(autoencoder->d_w4, 256*128*3*3); zero_initialize(autoencoder->d_b4, 256);\n","    zero_initialize(autoencoder->d_w5, 3*256*3*3); zero_initialize(autoencoder->d_b5, 3);\n","    // Kích thước 1 ảnh tại các lớp (như Forward)\n","    int size_Out = 32*32*3;\n","    int size_L8 = 32*32*256;\n","    int size_L7 = 16*16*256;\n","    int size_L6 = 16*16*128;\n","    int size_L5 = 8*8*128;\n","    int size_L4 = 8*8*128;\n","    int size_L3 = 16*16*128;\n","    int size_L2 = 16*16*256;\n","    int size_L1 = 32*32*256;\n","    int size_In = 32*32*3;\n","\n","    for (int b = 0; b < bs; b++) {\n","        // Offset pointers\n","        float* ptr_dOut = autoencoder->loss_gradient + b * size_Out;\n","        float* ptr_Upsample2_Out = autoencoder->upsample2_output + b * size_L8;\n","        float* ptr_d_Upsample2_Out = autoencoder->d_upsample2_output + b * size_L8;\n","        float* ptr_d_Conv4_Out = autoencoder->d_conv4_output + b * size_L7;\n","        float* ptr_Upsample1_Out = autoencoder->upsample1_output + b * size_L6;\n","        float* ptr_d_Upsample1_Out = autoencoder->d_upsample1_output + b * size_L6;\n","        float* ptr_d_Conv3_Out = autoencoder->d_conv3_output + b * size_L5;\n","        float* ptr_Pool2_Out = autoencoder->pool2_output + b * size_L4;\n","        float* ptr_d_Pool2_Out = autoencoder->d_pool2_output + b * size_L4;\n","        float* ptr_Conv2_Out = autoencoder->conv2_output + b * size_L3;\n","        float* ptr_d_Conv2_Out = autoencoder->d_conv2_output + b * size_L3;\n","        float* ptr_Pool1_Out = autoencoder->pool1_output + b * size_L2;\n","        float* ptr_d_Pool1_Out = autoencoder->d_pool1_output + b * size_L2;\n","        float* ptr_Conv1_Out = autoencoder->conv1_output + b * size_L1;\n","        float* ptr_d_Conv1_Out = autoencoder->d_conv1_output + b * size_L1;\n","        float* ptr_Input = autoencoder->batch_input + b * size_In;\n","\n","        // === L9 (Conv5) ===\n","        // dW5, dB5\n","        Conv2D_Backward_Kernel(ptr_dOut, 32, 32, ptr_Upsample2_Out, 32, 32, 256, 3, 3, 1, 1, 3, autoencoder->d_w5);\n","        Conv2D_Backward_Biases(ptr_dOut, 32, 32, 3, autoencoder->d_b5);\n","        // dInput cho L8\n","        Conv2D_Backward_Input(ptr_dOut, 32, 32, autoencoder->w5, 3, 3, 32, 32, 256, 1, 1, 3, ptr_d_Upsample2_Out);\n","\n","        // === L8 (Upsample2) ===\n","        UpSample2D_Backward(ptr_d_Upsample2_Out, 32, 32, UPSAMPLE_SIZE, 256, autoencoder->d_conv4_output + b * size_L7, 16, 16);\n","\n","        // === L7 (Conv4) ===\n","        // ReLU Backward\n","        Relu_Backward(ptr_d_Conv4_Out, autoencoder->conv4_output + b * size_L7, 16*16*256);\n","        Conv2D_Backward_Kernel(ptr_d_Conv4_Out, 16, 16, ptr_Upsample1_Out, 16, 16, 128, 3, 3, 1, 1, 256, autoencoder->d_w4);\n","        Conv2D_Backward_Biases(ptr_d_Conv4_Out, 16, 16, 256, autoencoder->d_b4);\n","        Conv2D_Backward_Input(ptr_d_Conv4_Out, 16, 16, autoencoder->w4, 3, 3, 16, 16, 128, 1, 1, 256, ptr_d_Upsample1_Out);\n","\n","        // === L6 (Upsample1) ===\n","        UpSample2D_Backward(ptr_d_Upsample1_Out, 16, 16, UPSAMPLE_SIZE, 128, ptr_d_Conv3_Out, 8, 8);\n","\n","        // === L5 (Conv3) ===\n","        Relu_Backward(ptr_d_Conv3_Out, autoencoder->conv3_output + b * size_L5, 8*8*128);\n","        Conv2D_Backward_Kernel(ptr_d_Conv3_Out, 8, 8, ptr_Pool2_Out, 8, 8, 128, 3, 3, 1, 1, 128, autoencoder->d_w3);\n","        Conv2D_Backward_Biases(ptr_d_Conv3_Out, 8, 8, 128, autoencoder->d_b3);\n","        Conv2D_Backward_Input(ptr_d_Conv3_Out, 8, 8, autoencoder->w3, 3, 3, 8, 8, 128, 1, 1, 128, ptr_d_Pool2_Out);\n","\n","        // === L4 (Pool2) ===\n","        MaxPool2D_Backward(ptr_d_Pool2_Out, 8, 8, ptr_Conv2_Out, 16, 16, 2, 2, 2, 128, ptr_d_Conv2_Out);\n","\n","        // === L3 (Conv2) ===\n","        Relu_Backward(ptr_d_Conv2_Out, ptr_Conv2_Out, 16*16*128);\n","        Conv2D_Backward_Kernel(ptr_d_Conv2_Out, 16, 16, ptr_Pool1_Out, 16, 16, 256, 3, 3, 1, 1, 128, autoencoder->d_w2);\n","        Conv2D_Backward_Biases(ptr_d_Conv2_Out, 16, 16, 128, autoencoder->d_b2);\n","        Conv2D_Backward_Input(ptr_d_Conv2_Out, 16, 16, autoencoder->w2, 3, 3, 16, 16, 256, 1, 1, 128, ptr_d_Pool1_Out);\n","\n","        // === L2 (Pool1) ===\n","        MaxPool2D_Backward(ptr_d_Pool1_Out, 16, 16, ptr_Conv1_Out, 32, 32, 2, 2, 2, 256, ptr_d_Conv1_Out);\n","\n","        // === L1 (Conv1) ===\n","        Relu_Backward(ptr_d_Conv1_Out, ptr_Conv1_Out, 32*32*256);\n","        Conv2D_Backward_Kernel(ptr_d_Conv1_Out, 32, 32, ptr_Input, 32, 32, 3, 3, 3, 1, 1, 256, autoencoder->d_w1);\n","        Conv2D_Backward_Biases(ptr_d_Conv1_Out, 32, 32, 256, autoencoder->d_b1);\n","    }\n","}\n","\n","void update_autoencoder_parameters(CPUAutoEncoder* autoencoder) {\n","    // Cập nhật tất cả 5 lớp Conv: W += -learning_rate * dW\n","    int size_W1 = 256 * 3 * 3 * 3;\n","    SGD_Update(autoencoder->w1, autoencoder->d_w1, autoencoder->learning_rate, size_W1);\n","    SGD_Update(autoencoder->b1, autoencoder->d_b1, autoencoder->learning_rate, 256);\n","    int size_W2 = 128 * 256 * 3 * 3;\n","    SGD_Update(autoencoder->w2, autoencoder->d_w2, autoencoder->learning_rate, size_W2);\n","    SGD_Update(autoencoder->b2, autoencoder->d_b2, autoencoder->learning_rate, 128);\n","    int size_W3  = 128 * 128 * 3 * 3;\n","    SGD_Update(autoencoder->w3, autoencoder->d_w3, autoencoder->learning_rate, size_W3);\n","    SGD_Update(autoencoder->b3, autoencoder->d_b3, autoencoder->learning_rate, 128);\n","    int size_W4 = 256 * 128 * 3 * 3;\n","    SGD_Update(autoencoder->w4, autoencoder->d_w4, autoencoder->learning_rate, size_W4);\n","    SGD_Update(autoencoder->b4, autoencoder->d_b4, autoencoder->learning_rate, 256);\n","    int size_W5 = 3 * 256 * 3 * 3;\n","    SGD_Update(autoencoder->w5, autoencoder->d_w5, autoencoder->learning_rate, size_W5);\n","    SGD_Update(autoencoder->b5, autoencoder->d_b5, autoencoder->learning_rate, 3);\n","}\n","\n","void save_weights(CPUAutoEncoder* autoencoder, const char* filename) {\n","    FILE* file = fopen(filename, \"wb\");\n","    if (file == NULL) {\n","        printf(\"Error opening file for writing weights.\\n\");\n","        return;\n","    }\n","    // Lưu trọng số và bias của từng lớp Conv2D\n","    fwrite(autoencoder->w1, sizeof(float), 256*3*3*3, file);\n","    fwrite(autoencoder->b1, sizeof(float), 256, file);\n","    fwrite(autoencoder->w2, sizeof(float), 128*256*3*3, file);\n","    fwrite(autoencoder->b2, sizeof(float), 128, file);\n","    fwrite(autoencoder->w3, sizeof(float), 128*128*3*3, file);\n","    fwrite(autoencoder->b3, sizeof(float), 128, file);\n","    fwrite(autoencoder->w4, sizeof(float), 256*128*3*3, file);\n","    fwrite(autoencoder->b4, sizeof(float), 256, file);\n","    fwrite(autoencoder->w5, sizeof(float), 3*256*3*3, file);\n","    fwrite(autoencoder->b5, sizeof(float), 3, file);\n","    fclose(file);\n","}\n","\n","\n","void cpu_extract_features(CPUAutoEncoder* autoencoder, float* input_data, int num_images, float* features_output) {\n","    // Chỉ có forward pass của encoder\n","    // Kích thước các lớp\n","    int size_input = 32 * 32 * 3;\n","    int size_L1 = 32 * 32 * 256;\n","    int size_L2 = 16 * 16 * 256;\n","    int size_L3 = 16 * 16 * 128;\n","    int size_L4 = 8 * 8 * 128;\n","    for (int i = 0; i < num_images; i++) {\n","        float* ptr_input = input_data + i * size_input;\n","        float* ptr_feature_dst = features_output + i * size_L4;\n","        float* ptr_L1 = autoencoder->conv1_output;\n","        float* ptr_L2 = autoencoder->pool1_output;\n","        float* ptr_L3 = autoencoder->conv2_output;\n","        float* ptr_L4 = autoencoder->pool2_output;\n","        //  ENCODER FORWARD PASS\n","        // L1: Conv1 + ReLU\n","        Conv2D_Forward(ptr_input, 32, 32, 3, autoencoder->w1, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b1, CONV_PADDING, CONV_STRIDE, 256, ptr_L1, 32, 32);\n","        Relu(ptr_L1, size_L1, ptr_L1);\n","        // L2: Pool1\n","        MaxPool2D_Forward(ptr_L1, 32, 32, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 256, ptr_L2, 16, 16);\n","        // L3: Conv2 + ReLU\n","        Conv2D_Forward(ptr_L2, 16, 16, 256, autoencoder->w2, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b2, CONV_PADDING, CONV_STRIDE, 128, ptr_L3, 16, 16);\n","        Relu(ptr_L3, size_L3, ptr_L3);\n","        // L4: Pool2 (Latent Space)\n","        MaxPool2D_Forward(ptr_L3, 16, 16, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 128, ptr_L4, 8, 8);\n","\n","        // Sao chép kết quả vào mảng output tổng\n","        memcpy(ptr_feature_dst, ptr_L4, size_L4 * sizeof(float));\n","    }\n","}\n","\n","void cpu_load_weights(CPUAutoEncoder* autoencoder, const char* filename) {\n","    FILE* file = fopen(filename, \"rb\");\n","    if (file == NULL) {\n","        printf(\"Error opening file for reading weights.\\n\");\n","        return;\n","    }\n","    // Đọc trọng số và bias của từng lớp Conv2D\n","    fread(autoencoder->w1, sizeof(float), 256*3*3*3, file);\n","    fread(autoencoder->b1, sizeof(float), 256, file);\n","    fread(autoencoder->w2, sizeof(float), 128*256*3*3, file);\n","    fread(autoencoder->b2, sizeof(float), 128, file);\n","    fread(autoencoder->w3, sizeof(float), 128*128*3*3, file);\n","    fread(autoencoder->b3, sizeof(float), 128, file);\n","    fread(autoencoder->w4, sizeof(float), 256*128*3*3, file);\n","    fread(autoencoder->b4, sizeof(float), 256, file);\n","    fread(autoencoder->w5, sizeof(float), 3*256*3*3, file);\n","    fread(autoencoder->b5, sizeof(float), 3, file);\n","    fclose(file);\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND_Smnn1d8GS","outputId":"06bf1b1b-b2e9-4e35-9010-74616510eb72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing load_data.h\n"]}],"source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlE9ZxjUeFcy","outputId":"d268b1c6-9cbc-44f7-b47c-66149636c36c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing load_data.c\n"]}],"source":["%%writefile load_data.c\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","    // for (int i = 0; i < 2; i++) {\n","    //     printf(\"Label: %d\\n\", data->test_labels[i]);\n","    //     for (int j = 0; j < IMG_SIZE; j++) {\n","    //         printf(\"%f \", data->test_images[i*IMG_SIZE + j]);\n","    //     }\n","    // }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp-p2v8LeM0a","outputId":"8f420677-30b6-485e-f9ec-84dcfa610e74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing main.c\n"]}],"source":["%%writefile main.c\n","#include \"load_data.h\"\n","#include \"cpu_autoencoder.h\"\n","#include <time.h>\n","#include <sys/resource.h>\n","#include <stdint.h>\n","\n","void print_memory_usage() {\n","    struct rusage usage;\n","    if (getrusage(RUSAGE_SELF, &usage) == 0) {\n","        double memory_usage_mb = usage.ru_maxrss / 1024.0;\n","        double memory_usage_gb = memory_usage_mb / 1024.0;\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB (%.4f GB)\\n\", memory_usage_mb, memory_usage_gb);\n","    } else {\n","        printf(\"[SYSTEM] Error checking memory usage.\\n\");\n","    }\n","}\n","\n","// void save_summary(double total_time, double final_loss, FILE* file) {\n","//     fprintf(file, \"\\n*** Training Summary ***\\n\");\n","//     fprintf(file, \"Total training time: %.2f seconds.\\n\", total_time);\n","//     fprintf(file, \"Final reconstruction loss: %f\\n\", final_loss);\n","//     fclose(file);\n","// }\n","\n","uint8_t float_to_pixel(float val) {\n","    if (val < 0.0f) val = 0.0f;\n","    if (val > 1.0f) val = 1.0f;\n","    return (uint8_t)(val * 255.0f);\n","}\n","\n","void save_image_pnm(const char* filename, float* planar_data, int width, int height) {\n","    FILE* f = fopen(filename, \"wb\");\n","    if (!f) {\n","        printf(\"Error opening file %s for writing\\n\", filename);\n","        return;\n","    }\n","\n","    // Header PNM: P6 format (binary)\n","    fprintf(f, \"P6\\n%d %d\\n255\\n\", width, height);\n","\n","    int plane_size = width * height;\n","    uint8_t* pixel_buffer = (uint8_t*)malloc(width * height * 3 * sizeof(uint8_t));\n","    if (!pixel_buffer) {\n","        printf(\"Error allocating pixel buffer\\n\");\n","        fclose(f);\n","        return;\n","    }\n","\n","    // Convert float data to uint8_t and interleave RGB channels\n","    for (int h = 0; h < height; h++) {\n","        for (int w = 0; w < width; w++) {\n","            int pixel_idx = (h * width + w) * 3;\n","            int data_idx = h * width + w;\n","\n","            pixel_buffer[pixel_idx] = float_to_pixel(planar_data[data_idx]);\n","            pixel_buffer[pixel_idx + 1] = float_to_pixel(planar_data[plane_size + data_idx]);\n","            pixel_buffer[pixel_idx + 2] = float_to_pixel(planar_data[2 * plane_size + data_idx]);\n","        }\n","    }\n","\n","    // Write all pixel data at once\n","    fwrite(pixel_buffer, 1, width * height * 3, f);\n","    fclose(f);\n","    free(pixel_buffer);\n","}\n","\n","void sample_reconstructions(CPUAutoEncoder* ae, Cifar10* data, int num_samples) {\n","    printf(\"\\n*** Sampling Reconstructed Images ***\\n\");\n","    int batch_size = ae->batch_size;\n","\n","    float* sample_batch = (float*)malloc(batch_size * 32 * 32 * 3 * sizeof(float));\n","    if (!sample_batch) {\n","        printf(\"Error allocating sample batch\\n\");\n","        return;\n","    }\n","\n","    memcpy(ae->batch_input, data->test_images, batch_size * 32 * 32 * 3 * sizeof(float));\n","    forward_autoencoder(ae);\n","    char filename[64];\n","    int img_size = 32 * 32 * 3;\n","\n","    for (int i = 0; i < num_samples; i++) {\n","        // Save original image\n","        snprintf(filename, sizeof(filename), \"sample_%d_original.pnm\", i);\n","        save_image_pnm(filename, ae->batch_input + i * img_size, 32, 32);\n","\n","        // Save reconstructed image\n","        snprintf(filename, sizeof(filename), \"sample_%d_reconstructed.pnm\", i);\n","        save_image_pnm(filename, ae->final_output + i * img_size, 32, 32);\n","\n","        printf(\"Saved pair %d: %s vs %s\\n\", i, \"original\", \"reconstructed\");\n","    }\n","\n","    free(sample_batch);\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","    //Load Data\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","    printf(\"Data loaded and normalized.\\n\");\n","\n","    // Hyperparameters\n","    int train_subset_size = 1000;\n","    int batch_size = 32; // Can be changed\n","    int num_epochs = 20; // Can be changed\n","    int num_batches = train_subset_size / batch_size;\n","    float learning_rate = 0.001;\n","    float* batch_images = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    double total_time = 0.0;\n","    double final_loss = 0.0;\n","    // Initialize AutoEncoder\n","    CPUAutoEncoder autoencoder;\n","    initialize_autoencoder(&autoencoder, batch_size, learning_rate);\n","    printf(\"Autoencoder initialized (batch_size=%d, learning_rate=%f)\\n\", batch_size, learning_rate);\n","    printf(\"Start training...\\n\");\n","    // Training Loop\n","    for (int epoch = 0; epoch < num_epochs; epoch++) {\n","        // Shuffle the training indices at the beginning of each epoch\n","        clock_t start_time = clock();\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","        printf(\"Epoch %d/%d\\n\", epoch + 1, num_epochs);\n","        for (int batch_id = 0; batch_id < num_batches; batch_id++) {\n","            // Get the current batch data from the shuffled array\n","            get_next_batch(&data, batch_size, batch_id, batch_images);\n","            // forward + backward autoencoder on batch_images\n","            // copy into autoencoder input buffer\n","            for (int i = 0; i < batch_size * IMG_SIZE; i++) {\n","                autoencoder.batch_input[i] = batch_images[i];\n","            }\n","            // Training process\n","            forward_autoencoder(&autoencoder);\n","            // Calculate loss for display\n","            float current_loss = MSE(autoencoder.batch_input, autoencoder.final_output, batch_size * IMG_SIZE);\n","            epoch_loss += current_loss;\n","            backward_autoencoder(&autoencoder);\n","            update_autoencoder_parameters(&autoencoder);\n","            if ((batch_id + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\", epoch + 1, batch_id + 1, num_batches, current_loss);\n","            }\n","        }\n","        clock_t end_time = clock();\n","        double epoch_time = (double)(end_time - start_time) / CLOCKS_PER_SEC;\n","        total_time += epoch_time;\n","        printf(\"==> Epoch %d finished. Avg Loss: %f, time: %.2f seconds\\n\", epoch + 1, epoch_loss / num_batches, epoch_time);\n","        if(epoch + 1 == 20) final_loss = epoch_loss / num_batches;\n","    }\n","\n","    printf(\"\\n*** Training Summary ***\\n\");\n","    printf(\"Total training time: %.2f seconds.\\n\", total_time);\n","    printf(\"Final reconstruction loss: %f\\n\", final_loss);\n","    print_memory_usage();\n","\n","    //Save weights after training\n","    save_weights(&autoencoder, \"autoencoder_weights_cpu.bin\");\n","    //Save 5 pairs of original and reconstructed images\n","    //sample_reconstructions(&autoencoder, &data, 5);\n","    // Free memory\n","    free_autoencoder(&autoencoder);\n","    free(batch_images);\n","    free_cifar10(&data);\n","\n","    return 0;\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqXzjsgcebHi","outputId":"3b9f3783-b781-432d-c99f-321a12e85dca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing extract_svm_features_cpu.c\n"]}],"source":["%%writefile extract_svm_features_cpu.c\n","// # include <cstdio>\n","// # include <cstdlib>\n","// # include <cstring>\n","// # include <cuda_runtime.h>\n","\n","\n","  #include \"load_data.h\"\n","  //#include \"gpu_autoencoder.h\"\n","  #include \"cpu_autoencoder.h\"\n","\n","\n","#define AE_LATENT_DIM 128 * 8 * 8\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    //if (argc < 3) {\n","        //fprintf(stderr,\n","                //\"Usage: %s <path_to_cifar-10-batches-bin> <ae_weights.bin>\\n\",\n","                //argv[0]);\n","        //return 1;\n","    //}\n","    //const char* data_dir    = argv[1];\n","    //const char* weight_file = argv[2];\n","    char* weight_file_cpu = \"autoencoder_weights_cpu.bin\";\n","    float learning_rate = 0.001;\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    //GPUAutoencoder ae;\n","    //gpu_autoencoder_init(&ae, batch_size);\n","    //gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","    CPUAutoEncoder autoencoder;\n","    initialize_autoencoder(&autoencoder, batch_size, learning_rate);\n","    cpu_load_weights(&autoencoder, weight_file_cpu);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = 10000;//TRAIN_NUM; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        //gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","        cpu_extract_features(&autoencoder, h_batch, cur_bs, h_latent);\n","\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = 2000;//TEST_NUM; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        //gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","        cpu_extract_features(&autoencoder, h_batch, cur_bs, h_latent);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    //gpu_autoencoder_free(&ae);\n","    free_autoencoder(&autoencoder);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQ4Ryj3nGEk7"},"outputs":[],"source":["!gcc main.c load_data.c cpu_layers.c cpu_autoencoder.c -o run_model -lm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYKhfq9SGH56","outputId":"553e6824-e18f-49cc-e351-d097e58a9b3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["CIFAR-10 loaded successfully\n","Data loaded and normalized.\n","Autoencoder initialized (batch_size=32, learning_rate=0.001000)\n","Start training...\n","Epoch 1/20\n","==> Epoch 1 finished. Avg Loss: 0.279725, time: 2200.85 seconds\n","Epoch 2/20\n","==> Epoch 2 finished. Avg Loss: 0.212086, time: 2150.78 seconds\n","Epoch 3/20\n","==> Epoch 3 finished. Avg Loss: 0.159211, time: 2146.80 seconds\n","Epoch 4/20\n","==> Epoch 4 finished. Avg Loss: 0.110326, time: 2170.06 seconds\n","Epoch 5/20\n","==> Epoch 5 finished. Avg Loss: 0.079942, time: 2154.88 seconds\n","Epoch 6/20\n","==> Epoch 6 finished. Avg Loss: 0.065304, time: 2164.51 seconds\n","Epoch 7/20\n","==> Epoch 7 finished. Avg Loss: 0.060133, time: 2167.37 seconds\n","Epoch 8/20\n","==> Epoch 8 finished. Avg Loss: 0.057733, time: 2181.04 seconds\n","Epoch 9/20\n","==> Epoch 9 finished. Avg Loss: 0.056533, time: 2160.87 seconds\n","Epoch 10/20\n","==> Epoch 10 finished. Avg Loss: 0.056988, time: 2162.17 seconds\n","Epoch 11/20\n","==> Epoch 11 finished. Avg Loss: 0.055740, time: 2166.18 seconds\n","Epoch 12/20\n","==> Epoch 12 finished. Avg Loss: 0.055807, time: 2148.40 seconds\n","Epoch 13/20\n","==> Epoch 13 finished. Avg Loss: 0.056124, time: 2174.88 seconds\n","Epoch 14/20\n","==> Epoch 14 finished. Avg Loss: 0.054417, time: 2186.78 seconds\n","Epoch 15/20\n","==> Epoch 15 finished. Avg Loss: 0.054435, time: 2167.06 seconds\n","Epoch 16/20\n","==> Epoch 16 finished. Avg Loss: 0.054497, time: 2186.77 seconds\n","Epoch 17/20\n","==> Epoch 17 finished. Avg Loss: 0.055318, time: 2223.58 seconds\n","Epoch 18/20\n","==> Epoch 18 finished. Avg Loss: 0.053215, time: 2207.93 seconds\n","Epoch 19/20\n","==> Epoch 19 finished. Avg Loss: 0.054353, time: 2192.42 seconds\n","Epoch 20/20\n","==> Epoch 20 finished. Avg Loss: 0.052954, time: 2187.25 seconds\n","\n","*** Training Summary ***\n","Total training time: 43500.57 seconds.\n","Final reconstruction loss: 0.052954\n","[SYSTEM] Memory Usage: 858704.00 MB (838.5781 GB)\n"]}],"source":["!./run_model"]},{"cell_type":"markdown","source":["## 2) Trích xuất đặc trưng:"],"metadata":{"id":"TLbO2O5TAsM5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rm8Xbxo14L-Y"},"outputs":[],"source":["!gcc extract_svm_features_cpu.c load_data.c cpu_layers.c cpu_autoencoder.c -o run_cpu -lm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nQIaQ_L4Opv","outputId":"3ede9b2f-0678-4aab-a5e3-e00dd2584cc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/157 done\n","[SVM][TRAIN] Batch 2/157 done\n","[SVM][TRAIN] Batch 3/157 done\n","[SVM][TRAIN] Batch 4/157 done\n","[SVM][TRAIN] Batch 5/157 done\n","[SVM][TRAIN] Batch 6/157 done\n","[SVM][TRAIN] Batch 7/157 done\n","[SVM][TRAIN] Batch 8/157 done\n","[SVM][TRAIN] Batch 9/157 done\n","[SVM][TRAIN] Batch 10/157 done\n","[SVM][TRAIN] Batch 11/157 done\n","[SVM][TRAIN] Batch 12/157 done\n","[SVM][TRAIN] Batch 13/157 done\n","[SVM][TRAIN] Batch 14/157 done\n","[SVM][TRAIN] Batch 15/157 done\n","[SVM][TRAIN] Batch 16/157 done\n","[SVM][TRAIN] Batch 17/157 done\n","[SVM][TRAIN] Batch 18/157 done\n","[SVM][TRAIN] Batch 19/157 done\n","[SVM][TRAIN] Batch 20/157 done\n","[SVM][TRAIN] Batch 21/157 done\n","[SVM][TRAIN] Batch 22/157 done\n","[SVM][TRAIN] Batch 23/157 done\n","[SVM][TRAIN] Batch 24/157 done\n","[SVM][TRAIN] Batch 25/157 done\n","[SVM][TRAIN] Batch 26/157 done\n","[SVM][TRAIN] Batch 27/157 done\n","[SVM][TRAIN] Batch 28/157 done\n","[SVM][TRAIN] Batch 29/157 done\n","[SVM][TRAIN] Batch 30/157 done\n","[SVM][TRAIN] Batch 31/157 done\n","[SVM][TRAIN] Batch 32/157 done\n","[SVM][TRAIN] Batch 33/157 done\n","[SVM][TRAIN] Batch 34/157 done\n","[SVM][TRAIN] Batch 35/157 done\n","[SVM][TRAIN] Batch 36/157 done\n","[SVM][TRAIN] Batch 37/157 done\n","[SVM][TRAIN] Batch 38/157 done\n","[SVM][TRAIN] Batch 39/157 done\n","[SVM][TRAIN] Batch 40/157 done\n","[SVM][TRAIN] Batch 41/157 done\n","[SVM][TRAIN] Batch 42/157 done\n","[SVM][TRAIN] Batch 43/157 done\n","[SVM][TRAIN] Batch 44/157 done\n","[SVM][TRAIN] Batch 45/157 done\n","[SVM][TRAIN] Batch 46/157 done\n","[SVM][TRAIN] Batch 47/157 done\n","[SVM][TRAIN] Batch 48/157 done\n","[SVM][TRAIN] Batch 49/157 done\n","[SVM][TRAIN] Batch 50/157 done\n","[SVM][TRAIN] Batch 51/157 done\n","[SVM][TRAIN] Batch 52/157 done\n","[SVM][TRAIN] Batch 53/157 done\n","[SVM][TRAIN] Batch 54/157 done\n","[SVM][TRAIN] Batch 55/157 done\n","[SVM][TRAIN] Batch 56/157 done\n","[SVM][TRAIN] Batch 57/157 done\n","[SVM][TRAIN] Batch 58/157 done\n","[SVM][TRAIN] Batch 59/157 done\n","[SVM][TRAIN] Batch 60/157 done\n","[SVM][TRAIN] Batch 61/157 done\n","[SVM][TRAIN] Batch 62/157 done\n","[SVM][TRAIN] Batch 63/157 done\n","[SVM][TRAIN] Batch 64/157 done\n","[SVM][TRAIN] Batch 65/157 done\n","[SVM][TRAIN] Batch 66/157 done\n","[SVM][TRAIN] Batch 67/157 done\n","[SVM][TRAIN] Batch 68/157 done\n","[SVM][TRAIN] Batch 69/157 done\n","[SVM][TRAIN] Batch 70/157 done\n","[SVM][TRAIN] Batch 71/157 done\n","[SVM][TRAIN] Batch 72/157 done\n","[SVM][TRAIN] Batch 73/157 done\n","[SVM][TRAIN] Batch 74/157 done\n","[SVM][TRAIN] Batch 75/157 done\n","[SVM][TRAIN] Batch 76/157 done\n","[SVM][TRAIN] Batch 77/157 done\n","[SVM][TRAIN] Batch 78/157 done\n","[SVM][TRAIN] Batch 79/157 done\n","[SVM][TRAIN] Batch 80/157 done\n","[SVM][TRAIN] Batch 81/157 done\n","[SVM][TRAIN] Batch 82/157 done\n","[SVM][TRAIN] Batch 83/157 done\n","[SVM][TRAIN] Batch 84/157 done\n","[SVM][TRAIN] Batch 85/157 done\n","[SVM][TRAIN] Batch 86/157 done\n","[SVM][TRAIN] Batch 87/157 done\n","[SVM][TRAIN] Batch 88/157 done\n","[SVM][TRAIN] Batch 89/157 done\n","[SVM][TRAIN] Batch 90/157 done\n","[SVM][TRAIN] Batch 91/157 done\n","[SVM][TRAIN] Batch 92/157 done\n","[SVM][TRAIN] Batch 93/157 done\n","[SVM][TRAIN] Batch 94/157 done\n","[SVM][TRAIN] Batch 95/157 done\n","[SVM][TRAIN] Batch 96/157 done\n","[SVM][TRAIN] Batch 97/157 done\n","[SVM][TRAIN] Batch 98/157 done\n","[SVM][TRAIN] Batch 99/157 done\n","[SVM][TRAIN] Batch 100/157 done\n","[SVM][TRAIN] Batch 101/157 done\n","[SVM][TRAIN] Batch 102/157 done\n","[SVM][TRAIN] Batch 103/157 done\n","[SVM][TRAIN] Batch 104/157 done\n","[SVM][TRAIN] Batch 105/157 done\n","[SVM][TRAIN] Batch 106/157 done\n","[SVM][TRAIN] Batch 107/157 done\n","[SVM][TRAIN] Batch 108/157 done\n","[SVM][TRAIN] Batch 109/157 done\n","[SVM][TRAIN] Batch 110/157 done\n","[SVM][TRAIN] Batch 111/157 done\n","[SVM][TRAIN] Batch 112/157 done\n","[SVM][TRAIN] Batch 113/157 done\n","[SVM][TRAIN] Batch 114/157 done\n","[SVM][TRAIN] Batch 115/157 done\n","[SVM][TRAIN] Batch 116/157 done\n","[SVM][TRAIN] Batch 117/157 done\n","[SVM][TRAIN] Batch 118/157 done\n","[SVM][TRAIN] Batch 119/157 done\n","[SVM][TRAIN] Batch 120/157 done\n","[SVM][TRAIN] Batch 121/157 done\n","[SVM][TRAIN] Batch 122/157 done\n","[SVM][TRAIN] Batch 123/157 done\n","[SVM][TRAIN] Batch 124/157 done\n","[SVM][TRAIN] Batch 125/157 done\n","[SVM][TRAIN] Batch 126/157 done\n","[SVM][TRAIN] Batch 127/157 done\n","[SVM][TRAIN] Batch 128/157 done\n","[SVM][TRAIN] Batch 129/157 done\n","[SVM][TRAIN] Batch 130/157 done\n","[SVM][TRAIN] Batch 131/157 done\n","[SVM][TRAIN] Batch 132/157 done\n","[SVM][TRAIN] Batch 133/157 done\n","[SVM][TRAIN] Batch 134/157 done\n","[SVM][TRAIN] Batch 135/157 done\n","[SVM][TRAIN] Batch 136/157 done\n","[SVM][TRAIN] Batch 137/157 done\n","[SVM][TRAIN] Batch 138/157 done\n","[SVM][TRAIN] Batch 139/157 done\n","[SVM][TRAIN] Batch 140/157 done\n","[SVM][TRAIN] Batch 141/157 done\n","[SVM][TRAIN] Batch 142/157 done\n","[SVM][TRAIN] Batch 143/157 done\n","[SVM][TRAIN] Batch 144/157 done\n","[SVM][TRAIN] Batch 145/157 done\n","[SVM][TRAIN] Batch 146/157 done\n","[SVM][TRAIN] Batch 147/157 done\n","[SVM][TRAIN] Batch 148/157 done\n","[SVM][TRAIN] Batch 149/157 done\n","[SVM][TRAIN] Batch 150/157 done\n","[SVM][TRAIN] Batch 151/157 done\n","[SVM][TRAIN] Batch 152/157 done\n","[SVM][TRAIN] Batch 153/157 done\n","[SVM][TRAIN] Batch 154/157 done\n","[SVM][TRAIN] Batch 155/157 done\n","[SVM][TRAIN] Batch 156/157 done\n","[SVM][TRAIN] Batch 157/157 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/32 done\n","[SVM][TEST] Batch 2/32 done\n","[SVM][TEST] Batch 3/32 done\n","[SVM][TEST] Batch 4/32 done\n","[SVM][TEST] Batch 5/32 done\n","[SVM][TEST] Batch 6/32 done\n","[SVM][TEST] Batch 7/32 done\n","[SVM][TEST] Batch 8/32 done\n","[SVM][TEST] Batch 9/32 done\n","[SVM][TEST] Batch 10/32 done\n","[SVM][TEST] Batch 11/32 done\n","[SVM][TEST] Batch 12/32 done\n","[SVM][TEST] Batch 13/32 done\n","[SVM][TEST] Batch 14/32 done\n","[SVM][TEST] Batch 15/32 done\n","[SVM][TEST] Batch 16/32 done\n","[SVM][TEST] Batch 17/32 done\n","[SVM][TEST] Batch 18/32 done\n","[SVM][TEST] Batch 19/32 done\n","[SVM][TEST] Batch 20/32 done\n","[SVM][TEST] Batch 21/32 done\n","[SVM][TEST] Batch 22/32 done\n","[SVM][TEST] Batch 23/32 done\n","[SVM][TEST] Batch 24/32 done\n","[SVM][TEST] Batch 25/32 done\n","[SVM][TEST] Batch 26/32 done\n","[SVM][TEST] Batch 27/32 done\n","[SVM][TEST] Batch 28/32 done\n","[SVM][TEST] Batch 29/32 done\n","[SVM][TEST] Batch 30/32 done\n","[SVM][TEST] Batch 31/32 done\n","[SVM][TEST] Batch 32/32 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}],"source":["!./run_cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsQdXgn6xlDg","outputId":"e524e6df-294f-450f-81b6-0600c314e34f"},"outputs":[{"name":"stdout","output_type":"stream","text":["6 1:0.0758321 2:0.0997156 3:0.101006 4:0.104555 5:0.0996163 6:0.105693 7:0.115024 8:0.135557 9:0.0746598 10:0.0581133 11:0.073651 12:0.0568446 13:0.0886734 14:0.0670331 15:0.0723226 16:0.0824314 17:0.0888202 18:0.0697416 19:0.0611363 20:0.143325 21:0.139629 22:0.1219 23:0.0904831 24:0.0907371 25:0.0822361 26:0.0578712 27:0.145694 28:0.184246 29:0.185922 30:0.210267 31:0.160294 32:0.0933218 33:0.0639415 34:0.177734 35:0.194098 36:0.139713 37:0.10564 38:0.175907 39:0.104334 40:0.105946 41:0.0629558 42:0.0996863 43:0.090156 44:0.113609 45:0.0742682 46:0.0619334 47:0.0254723 48:0.11474 49:0.0910471 50:0.066766 51:0.0664103 52:0.0678658 53:0.0645771 54:0.0592995 55:0.0716544 56:0.113293 57:0.0856906 58:0.103317 59:0.0793105 60:0.0740893 61:0.0749404 62:0.0585442 63:0.0584144 64:0.106119 65:0.119777 66:0.148933 67:0.153089 68:0.148225 69:0.14583 70:0.159303 71:0.16475 72:0.14545 73:0.140957 74:0.150326 75:0.149316 76:0.134958 77:0.178343 78:0.156475 79:0.160586 80:0.133052 81:0.176826 82:0.159745 83:0.15889 84:0.168934 85:0.178484 86:0.173413 87:0.16 88:0.146064 89:0.174118 90:0.144715 91:0.206207 92:0.282293 93:0.278783 94:0.253351 95:0.210109 96:0.150264 97:0.147908 98:0.247744 99:0.249432 100:0.285134 101:0.271944 102:0.284972 103:0.234564 104:0.158634 105:0.170278 106:0.228044 107:0.2705 108:0.214904 109:0.205331 110:0.216515 111:0.201405 112:0.184617 113:0.200455 114:0.197563 115:0.206191 116:0.226555 117:0.185578 118:0.171671 119:0.177762 120:0.19684 121:0.217795 122:0.219702 123:0.185318 124:0.171202 125:0.170497 126:0.176187 127:0.169507 128:0.213797 129:0.0562372 130:0.0797403 131:0.0608403 132:0.0614734 133:0.0673946 134:0.0790016 135:0.0799663 136:0.069828 137:0.0643417 138:0.0367668 139:0.0430163 140:0.0345222 141:0.0483867 142:0.045326 143:0.0388506 144:0.0252573 145:0.0728489 146:0.0463436 147:0.0396147 148:0.118768 149:0.0472739 150:0.0591407 151:0.0378133 152:0.0392597 153:0.0667568 154:0.0501617 155:0.0996144 156:0.116106 157:0.0899915 158:0.0728411 159:0.0535759 160:0.0380004 161:0.0598785 162:0.156565 163:0.120704 164:0.0968654 165:0.0653048 166:0.0894651 167:0.00854401 168:0.0347762 169:0.0601473 170:0.119085 171:0.0788183 172:0.0401871 173:0.0747829 174:0.0941341 175:0.0315385 176:0.0453294 177:0.0682363 178:0.0713312 179:0.0768947 180:0.026979 181:0.0416961 182:0.0450345 183:0.0827056 184:0.0913672 185:0.0715243 186:0.065616 187:0.0597513 188:0.0536385 189:0.0639629 190:0.0476185 191:0.0778668 192:0.00257104 193:0 194:0 195:0 196:0 197:0 198:0 199:0.00748331 200:0 201:0 202:0 203:0 204:0 205:0 206:0 207:0 208:0 209:0 210:0 211:0 212:0 213:0 214:0 215:0 216:0 217:0 218:0 219:0 220:0 221:0 222:0 223:0 224:0 225:0 226:0 227:0 228:0 229:0 230:0 231:0 232:0 233:0 234:0 235:0 236:0 237:0 238:0 239:0 240:0 241:0 242:0 243:0 244:0 245:0 246:0 247:0 248:0 249:0 250:0 251:0 252:0 253:0 254:0 255:0 256:0 257:0 258:0 259:0 260:0 261:0 262:0 263:0 264:0 265:0 266:0 267:0 268:0 269:0 270:0 271:0 272:0 273:0 274:0 275:0 276:0 277:0 278:0 279:0 280:0 281:0 282:0 283:0 284:0 285:0 286:0 287:0 288:0 289:0 290:0 291:0 292:0 293:0 294:0 295:0 296:0 297:0 298:0 299:0 300:0 301:0 302:0 303:0 304:0 305:0 306:0 307:0 308:0 309:0 310:0 311:0 312:0 313:0 314:0 315:0 316:0 317:0 318:0 319:0 320:0 321:0.0374181 322:0.0444606 323:0.0469482 324:0.044287 325:0.0444022 326:0.047025 327:0.0473597 328:0.0417883 329:0.00163723 330:0.00388369 331:0 332:0.0137715 333:0.0175765 334:0.00361851 335:0.0104644 336:0 337:0.000984225 338:0.00517335 339:0.0305994 340:0.036179 341:0.00456505 342:0.00718367 343:0 344:0 345:0.0112052 346:0.0373212 347:0.00879386 348:0.0219975 349:0 350:0 351:0 352:0 353:0.0231103 354:0.0282177 355:0.00772665 356:0.0132072 357:0.00956697 358:0.0160401 359:0 360:0.0147084 361:0.00725075 362:0.0246117 363:0.0342809 364:0.0023281 365:0.00336115 366:0.0189293 367:0.00801399 368:0.00641251 369:0.024852 370:0.00542042 371:0.00701876 372:0 373:0.00124116 374:0.0103038 375:0.0177396 376:0 377:0.0336872 378:0.0360828 379:0.0236804 380:0.0126469 381:0.00517482 382:0.010124 383:0.0224126 384:0.0355857 385:0.0238657 386:0.0261465 387:0.0361341 388:0.0260214 389:0.0264823 390:0.0243121 391:0.0323459 392:0.0383277 393:0.00955269 394:0.00424827 395:0.0141533 396:0.0272995 397:0.00903106 398:0.00595628 399:0.0128961 400:0.0251462 401:0.00696795 402:0.00360566 403:0.0307995 404:0 405:0.00976981 406:0.00525948 407:0 408:0.0231709 409:0 410:0.024291 411:0 412:0.00721484 413:0 414:0.0100946 415:0.0237671 416:0.0154619 417:0.0139315 418:0.0304666 419:0.0105327 420:0.00885346 421:0.00368118 422:0 423:0.000968685 424:0.0110391 425:0 426:0.00880797 427:0.0187766 428:0 429:0 430:0.00660197 431:0.0306204 432:0.0126634 433:0.00522892 434:0 435:0 436:0.0084516 437:0 438:0 439:0.0062544 440:0.0670365 441:0.0133633 442:0.0246428 443:0.0239289 444:0.0165619 445:0.0173907 446:0.0109042 447:0.0076192 448:0.0569904 449:0 450:0 451:0 452:0 453:0 454:0 455:0 456:0 457:0 458:0 459:0 460:0 461:0 462:0 463:0 464:0 465:0 466:0 467:0 468:0 469:0 470:0 471:0 472:0 473:0 474:0 475:0 476:0 477:0 478:0 479:0 480:0 481:0 482:0 483:0 484:0 485:0 486:0 487:0 488:0 489:0 490:0 491:0 492:0 493:0 494:0 495:0 496:0 497:0 498:0 499:0 500:0 501:0 502:0 503:0 504:0 505:0 506:0 507:0 508:0 509:0 510:0 511:0 512:0 513:0 514:0 515:0 516:0.00474385 517:0 518:0 519:0.00257393 520:0.0484519 521:0 522:0 523:0 524:0.000817124 525:0 526:0 527:0 528:0.00691464 529:0.000978032 530:0 531:0 532:0 533:0 534:0 535:0 536:0 537:0.00401053 538:0 539:0 540:0 541:0 542:0 543:0 544:0 545:0 546:0 547:0 548:0.0245103 549:0.0335733 550:0 551:0 552:0.000382285 553:0 554:0.0315804 555:0.0112549 556:0 557:0 558:0.0736069 559:0.0356402 560:0 561:0 562:0 563:0 564:0.015644 565:0 566:0 567:0 568:0.0163424 569:0.0609845 570:0.061109 571:0.0384073 572:0.032383 573:0.0391564 574:0.0321645 575:0.0242695 576:0.0650151 577:0 578:0 579:0 580:0 581:0 582:0 583:0 584:0 585:0 586:0 587:0 588:0 589:0 590:0 591:0 592:0 593:0 594:0 595:0 596:0 597:0 598:0 599:0 600:0.00241258 601:0 602:0 603:0 604:0 605:0 606:0 607:0 608:0 609:0 610:0 611:0 612:0 613:0 614:0 615:0 616:0 617:0 618:0 619:0 620:0 621:0 622:0 623:0 624:0 625:0 626:0 627:0 628:0 629:0 630:0 631:0 632:0.0214962 633:0 634:0 635:0 636:0 637:0 638:0 639:0 640:0.00967734 641:0.0385151 642:0.0190195 643:0.00865647 644:0.00624475 645:0.00330723 646:0.00462448 647:0.00644227 648:0 649:0.034947 650:0.0210266 651:0.0224358 652:0.0219353 653:0.0256591 654:0.0259649 655:0.0169931 656:0.0366586 657:0.0124121 658:0.00633185 659:0.0452397 660:0.0540718 661:0.038048 662:0.0358721 663:0.0234571 664:0.000259435 665:0.0028702 666:0.045513 667:0.0252022 668:0 669:0 670:0.0121163 671:0.03089 672:0.00256894 673:0.00951806 674:0.050077 675:0.00893517 676:0 677:0 678:0 679:0 680:0.0120318 681:0.0158914 682:0 683:0 684:0 685:0.00623453 686:0 687:0.0406982 688:0.0022908 689:0.0198109 690:0.0125826 691:0.0047038 692:0 693:0.00490636 694:0.0120655 695:0.0245684 696:0.000839505 697:0.0251034 698:0 699:0 700:0 701:0 702:0 703:0.0148948 704:0 705:0.0700238 706:0.0416759 707:0.0307878 708:0.0291794 709:0.0290002 710:0.0301566 711:0.0282631 712:0.0289261 713:0.0606399 714:0.00481347 715:0 716:0.0499741 717:0.00880682 718:0.0120066 719:0.00234418 720:0 721:0.0177964 722:0.00133431 723:0.0291952 724:0.0265104 725:0 726:0 727:0 728:0.0174413 729:0 730:0.0662653 731:0.0672521 732:0.00224129 733:0 734:0 735:0 736:0.0048119 737:0.0206447 738:0.0838486 739:0.000614215 740:0 741:0.00296517 742:0.0129894 743:0 744:0.0238302 745:0.0315818 746:0.0553561 747:0.00179579 748:0 749:0 750:0 751:0.0127701 752:0.0476571 753:0 754:0.00460443 755:0.000825427 756:0 757:0.00353697 758:0 759:0.0334492 760:0 761:0.102758 762:0.0777697 763:0.0837687 764:0.0804038 765:0.0763549 766:0.0702667 767:0.0877479 768:0.0208149 769:0.0464486 770:0.0437992 771:0.0317617 772:0.0306234 773:0.0177217 774:0.0320827 775:0.034282 776:0.0258788 777:0.0392696 778:0.0341573 779:0.0349731 780:0.0434938 781:0.0467626 782:0.0512233 783:0.0336601 784:0.034563 785:0.0281563 786:0.0302002 787:0.0412335 788:0.0903448 789:0.0805508 790:0.0560691 791:0.0508914 792:0.039343 793:0.0403169 794:0.076866 795:0.0969239 796:0.0977194 797:0.0546108 798:0.0395701 799:0.0479146 800:0.0475875 801:0.0465269 802:0.104509 803:0.0727622 804:0.067254 805:0.0592655 806:0.0822029 807:0.0192587 808:0.0401494 809:0.0550647 810:0.0858795 811:0.0823722 812:0.0809253 813:0.0691048 814:0.0732458 815:0.0792134 816:0.103661 817:0.0541672 818:0.0607894 819:0.0621251 820:0.0490035 821:0.0501941 822:0.0423439 823:0.0664163 824:0.0797039 825:0.0150202 826:0.0270623 827:0.0364348 828:0.0304934 829:0.0244276 830:0.0195934 831:0.0444545 832:0.0170975 833:0.0203815 834:0.0114344 835:0.0133068 836:0.0118428 837:0.0110031 838:0.0200415 839:0.0237869 840:0.00147965 841:0.0311313 842:0.0185296 843:0.0107543 844:0.0218177 845:0.016287 846:0.0194438 847:0.02193 848:0.0222572 849:0.0373705 850:0.0194919 851:0.0479108 852:0.0524818 853:0.0823175 854:0.0575153 855:0.0417505 856:0.0271777 857:0.0400898 858:0.0503957 859:0.0858301 860:0.100507 861:0.0963932 862:0.0863499 863:0.0445341 864:0.0271682 865:0.0410913 866:0.0920265 867:0.0893282 868:0.0909508 869:0.0904357 870:0.131251 871:0.0723615 872:0.0385829 873:0.0339412 874:0.0829249 875:0.117291 876:0.104057 877:0.0555962 878:0.0743504 879:0.0542931 880:0.0683381 881:0.0473874 882:0.0410521 883:0.0534989 884:0.0489531 885:0.0385894 886:0.0280371 887:0.0521608 888:0.0785567 889:0.068151 890:0.0605757 891:0.0500549 892:0.045987 893:0.0333226 894:0.0260302 895:0.0546182 896:0.0462152 897:0.0145648 898:0.00360401 899:0.0108018 900:0.015458 901:0.0248075 902:0.0284635 903:0.00920172 904:0.0214743 905:0.00750961 906:0.0138396 907:0.0140957 908:0.0217127 909:0.0308998 910:0.0219125 911:0.0261327 912:0.0172246 913:0.021926 914:0.024735 915:0.0141258 916:0.0259202 917:0.0207682 918:0.0361076 919:0.0150513 920:0.0158025 921:0.0240142 922:0.0341357 923:0.0248025 924:0.0513782 925:0.0550431 926:0.0575096 927:0.0235824 928:0.022276 929:0.0275818 930:0.0361419 931:0.0409617 932:0.0201335 933:0.0424227 934:0.0530144 935:0.0555974 936:0.0258625 937:0.0167693 938:0 939:0.0399761 940:0.040701 941:0.0360057 942:0.0330676 943:0.0424683 944:0.0299228 945:0.0124059 946:0.00686852 947:0.01241 948:0.034866 949:0.019286 950:0.0223933 951:0.0295884 952:0.0312574 953:0.0413245 954:0.0303418 955:0.0426968 956:0.0459075 957:0.0476356 958:0.0433273 959:0.0376018 960:0.038438 961:0.032857 962:0.0604817 963:0.0726754 964:0.0711606 965:0.0620111 966:0.0720905 967:0.0786115 968:0.0985933 969:0.0455418 970:0.0563432 971:0.0627171 972:0.0597197 973:0.066754 974:0.0799189 975:0.084852 976:0.0718273 977:0.102153 978:0.0856358 979:0.0669038 980:0.059302 981:0.100288 982:0.086161 983:0.0825291 984:0.0670493 985:0.106482 986:0.0798773 987:0.147519 988:0.202051 989:0.241379 990:0.227349 991:0.177003 992:0.0719505 993:0.0825597 994:0.143301 995:0.229609 996:0.238768 997:0.266389 998:0.222554 999:0.219264 1000:0.0895581 1001:0.0739166 1002:0.136414 1003:0.191717 1004:0.154177 1005:0.12518 1006:0.156761 1007:0.173845 1008:0.0838652 1009:0.0746263 1010:0.0864888 1011:0.152816 1012:0.161204 1013:0.112119 1014:0.0822743 1015:0.0971144 1016:0.118284 1017:0.171767 1018:0.15193 1019:0.132113 1020:0.124089 1021:0.121168 1022:0.119414 1023:0.114295 1024:0.14739 1025:0.0687161 1026:0.110971 1027:0.125714 1028:0.11446 1029:0.11624 1030:0.125166 1031:0.120699 1032:0.101947 1033:0.0874322 1034:0.0939116 1035:0.0885606 1036:0.114403 1037:0.0979851 1038:0.110978 1039:0.113182 1040:0.0957717 1041:0.115995 1042:0.0961656 1043:0.142291 1044:0.120392 1045:0.120047 1046:0.125143 1047:0.102806 1048:0.0905341 1049:0.121593 1050:0.108723 1051:0.121343 1052:0.159811 1053:0.157439 1054:0.17914 1055:0.124125 1056:0.102049 1057:0.131071 1058:0.105672 1059:0.165285 1060:0.147006 1061:0.189608 1062:0.191084 1063:0.167327 1064:0.125429 1065:0.108054 1066:0.128773 1067:0.196212 1068:0.151486 1069:0.12187 1070:0.140011 1071:0.201522 1072:0.112838 1073:0.141415 1074:0.111533 1075:0.127846 1076:0.154216 1077:0.126894 1078:0.109567 1079:0.108258 1080:0.0997557 1081:0.16589 1082:0.130544 1083:0.114065 1084:0.109981 1085:0.105716 1086:0.0995449 1087:0.106124 1088:0.115046 1089:0.0875442 1090:0.108225 1091:0.104806 1092:0.110634 1093:0.110377 1094:0.118796 1095:0.113326 1096:0.11535 1097:0.0914093 1098:0.0937306 1099:0.0994573 1100:0.115002 1101:0.0990327 1102:0.116516 1103:0.0970056 1104:0.0902856 1105:0.115583 1106:0.111335 1107:0.107423 1108:0.120631 1109:0.0994674 1110:0.0840403 1111:0.100216 1112:0.0968935 1113:0.113677 1114:0.108426 1115:0.138767 1116:0.177228 1117:0.161425 1118:0.15437 1119:0.102775 1120:0.0942219 1121:0.124096 1122:0.170064 1123:0.170381 1124:0.158739 1125:0.179225 1126:0.183492 1127:0.133398 1128:0.10082 1129:0.113544 1130:0.165033 1131:0.16058 1132:0.119406 1133:0.124784 1134:0.14689 1135:0.128403 1136:0.103793 1137:0.105563 1138:0.120488 1139:0.144391 1140:0.13732 1141:0.110446 1142:0.106208 1143:0.12468 1144:0.132589 1145:0.168413 1146:0.172088 1147:0.151163 1148:0.143852 1149:0.139336 1150:0.135298 1151:0.144485 1152:0.136185 1153:0 1154:0 1155:0 1156:0 1157:0 1158:0 1159:0 1160:0 1161:0 1162:0 1163:0 1164:0 1165:0.0249022 1166:0 1167:0 1168:0 1169:0 1170:0 1171:0 1172:0.0454575 1173:0.0348099 1174:0.0231109 1175:0.00358004 1176:0 1177:0 1178:0.00812849 1179:0.0553227 1180:0.0286183 1181:0.0479599 1182:0.0359748 1183:0.0419661 1184:0 1185:0 1186:0.0261858 1187:0.0494469 1188:0.0366044 1189:0 1190:0.0212692 1191:0.0233603 1192:0 1193:0 1194:0.00582434 1195:0.0227314 1196:0.0271388 1197:0.00885254 1198:0.00034431 1199:0 1200:0 1201:0 1202:0 1203:0.000247359 1204:0 1205:0 1206:0 1207:0 1208:0 1209:0 1210:0 1211:0.00189208 1212:0 1213:0 1214:0 1215:0 1216:0 1217:0.0901262 1218:0.1187 1219:0.12229 1220:0.119011 1221:0.12064 1222:0.123233 1223:0.13945 1224:0.136287 1225:0.111081 1226:0.117323 1227:0.133075 1228:0.11193 1229:0.134914 1230:0.132208 1231:0.126301 1232:0.104047 1233:0.134429 1234:0.142405 1235:0.110082 1236:0.112813 1237:0.133811 1238:0.13159 1239:0.126223 1240:0.114844 1241:0.143246 1242:0.13448 1243:0.1541 1244:0.172458 1245:0.185133 1246:0.159818 1247:0.159626 1248:0.122896 1249:0.115559 1250:0.162414 1251:0.208145 1252:0.237287 1253:0.212265 1254:0.194599 1255:0.169813 1256:0.120309 1257:0.14535 1258:0.202088 1259:0.166279 1260:0.146042 1261:0.183126 1262:0.208007 1263:0.161643 1264:0.123252 1265:0.147994 1266:0.147571 1267:0.193116 1268:0.150604 1269:0.135107 1270:0.129958 1271:0.142451 1272:0.124603 1273:0.175053 1274:0.162426 1275:0.157783 1276:0.161385 1277:0.155233 1278:0.148894 1279:0.154824 1280:0.14734 1281:0.0236272 1282:0 1283:0 1284:0 1285:0 1286:0 1287:0 1288:0 1289:0 1290:0 1291:0 1292:0 1293:0 1294:0 1295:0 1296:0 1297:0 1298:0 1299:0 1300:0 1301:0 1302:0 1303:0 1304:0 1305:0 1306:0 1307:0 1308:0 1309:0 1310:0 1311:0 1312:0 1313:0 1314:0 1315:0 1316:0 1317:0 1318:0 1319:0 1320:0 1321:0 1322:0 1323:0 1324:0 1325:0 1326:0 1327:0 1328:0 1329:0 1330:0 1331:0 1332:0 1333:0 1334:0 1335:0 1336:0 1337:0 1338:0 1339:0 1340:0 1341:0 1342:0 1343:0 1344:0 1345:0.0636733 1346:0.0931375 1347:0.103219 1348:0.0999353 1349:0.0973016 1350:0.100132 1351:0.108915 1352:0.10955 1353:0.060312 1354:0.0710749 1355:0.0689261 1356:0.0815129 1357:0.076382 1358:0.091077 1359:0.0828389 1360:0.0626623 1361:0.0760329 1362:0.0795103 1363:0.0795058 1364:0.0675374 1365:0.0872611 1366:0.0699204 1367:0.0884511 1368:0.0836137 1369:0.0737228 1370:0.0794752 1371:0.106123 1372:0.111244 1373:0.114375 1374:0.113877 1375:0.0917646 1376:0.0733538 1377:0.0930157 1378:0.096528 1379:0.108059 1380:0.14806 1381:0.162453 1382:0.140039 1383:0.137209 1384:0.0907422 1385:0.0886111 1386:0.129026 1387:0.143404 1388:0.114245 1389:0.133138 1390:0.126817 1391:0.117268 1392:0.0882735 1393:0.0814535 1394:0.0955664 1395:0.116963 1396:0.119021 1397:0.0944999 1398:0.103981 1399:0.0932728 1400:0.0903056 1401:0.133335 1402:0.144096 1403:0.112677 1404:0.0954134 1405:0.0966799 1406:0.0891258 1407:0.0900099 1408:0.0992178 1409:0 1410:0.00901632 1411:0.00159359 1412:0.000331812 1413:0 1414:0 1415:0.00219326 1416:0 1417:0.00500244 1418:0.00118528 1419:0.00345571 1420:0 1421:0 1422:0 1423:0 1424:0.00458956 1425:0.0129417 1426:0 1427:0 1428:0 1429:0.00684445 1430:0.00207784 1431:0.015208 1432:0 1433:0.0182563 1434:0 1435:0 1436:0 1437:0 1438:0.00217181 1439:0.0307819 1440:0.00374388 1441:0 1442:0 1443:0.00263753 1444:0 1445:0 1446:0 1447:0.0402217 1448:4.62756e-05 1449:0.00113677 1450:0 1451:0 1452:0.0151927 1453:0.0181961 1454:0.00252923 1455:0.00677656 1456:0 1457:0.0265234 1458:0.00447052 1459:0 1460:0.00508361 1461:0 1462:0.00538339 1463:0 1464:0.0136241 1465:0.000830173 1466:0 1467:0 1468:0 1469:0 1470:0 1471:0 1472:0.0070704 1473:0 1474:0 1475:0 1476:0 1477:0 1478:0 1479:0 1480:0 1481:0 1482:0 1483:0 1484:0 1485:0.0174012 1486:0 1487:0 1488:0.0127198 1489:0 1490:0 1491:0.0379604 1492:0.0220487 1493:0 1494:0.0073538 1495:0.00218726 1496:0.0112762 1497:0 1498:0.0203258 1499:0.030932 1500:0 1501:0 1502:0.00960504 1503:0 1504:0.0158893 1505:0 1506:0.0313427 1507:0 1508:0 1509:0 1510:0 1511:0.00682262 1512:0.0326743 1513:0 1514:0 1515:0 1516:0 1517:0 1518:0 1519:0.00336484 1520:0.0247393 1521:0 1522:0 1523:0 1524:0 1525:0 1526:0 1527:0 1528:0 1529:0 1530:0 1531:0 1532:0 1533:0 1534:0.00317757 1535:0 1536:0.022042 1537:0.032552 1538:0.0274142 1539:0.0247614 1540:0.0301351 1541:0.0302585 1542:0.0252217 1543:0.0234511 1544:0.0771422 1545:0.0232987 1546:0.0136463 1547:0.016097 1548:0.00176457 1549:0.0212117 1550:0 1551:0.0110344 1552:0.0678566 1553:0.00882387 1554:0.00274878 1555:0.0205869 1556:0.0087506 1557:0.0110035 1558:0.0145703 1559:0.00711605 1560:0.0784037 1561:0.0102825 1562:0.00454738 1563:0.0056044 1564:0 1565:0 1566:0 1567:0.00619174 1568:0.0723422 1569:0.015609 1570:0 1571:0 1572:0 1573:0 1574:0 1575:0 1576:0.0882833 1577:0.0189508 1578:0 1579:0 1580:0 1581:0 1582:0 1583:0.0076066 1584:0.103075 1585:0.0323387 1586:0 1587:0 1588:0 1589:0 1590:0.00432204 1591:0 1592:0.100723 1593:0.0613932 1594:0.0788166 1595:0.0681315 1596:0.0589315 1597:0.051591 1598:0.0543879 1599:0.0636532 1600:0.0776073 1601:0 1602:0 1603:0 1604:0 1605:0 1606:0 1607:0 1608:0 1609:0 1610:0 1611:0 1612:0 1613:0 1614:0 1615:0 1616:0 1617:0 1618:0 1619:0 1620:0 1621:0 1622:0 1623:0 1624:0 1625:0 1626:0 1627:0 1628:0 1629:0 1630:0 1631:0 1632:0 1633:0 1634:0 1635:0 1636:0 1637:0 1638:0 1639:0 1640:0 1641:0 1642:0 1643:0 1644:0 1645:0 1646:0 1647:0 1648:0 1649:0 1650:0 1651:0 1652:0 1653:0 1654:0 1655:0 1656:0 1657:0 1658:0 1659:0 1660:0 1661:0 1662:0 1663:0 1664:0 1665:0.0865375 1666:0.11675 1667:0.127171 1668:0.124049 1669:0.125531 1670:0.147202 1671:0.138082 1672:0.13094 1673:0.118567 1674:0.14604 1675:0.127968 1676:0.142852 1677:0.158982 1678:0.133517 1679:0.149844 1680:0.137369 1681:0.142851 1682:0.141243 1683:0.19606 1684:0.218942 1685:0.21931 1686:0.221335 1687:0.143781 1688:0.146246 1689:0.146168 1690:0.196106 1691:0.204177 1692:0.245351 1693:0.246008 1694:0.254874 1695:0.221079 1696:0.138853 1697:0.148395 1698:0.212487 1699:0.201661 1700:0.240836 1701:0.214688 1702:0.258338 1703:0.235811 1704:0.158598 1705:0.158029 1706:0.164038 1707:0.236581 1708:0.235028 1709:0.19667 1710:0.169861 1711:0.202853 1712:0.172907 1713:0.190042 1714:0.16774 1715:0.171986 1716:0.199064 1717:0.177825 1718:0.158721 1719:0.16845 1720:0.15832 1721:0.238245 1722:0.226532 1723:0.174845 1724:0.175695 1725:0.171736 1726:0.173287 1727:0.185721 1728:0.188966 1729:0.0145485 1730:0.0293183 1731:0.0229344 1732:0.0215229 1733:0.0266087 1734:0.023541 1735:0.0233075 1736:0.0435977 1737:0 1738:0 1739:0 1740:0 1741:0 1742:0 1743:0 1744:0.0232663 1745:0 1746:0 1747:0 1748:0 1749:0 1750:0 1751:0 1752:0.030748 1753:0 1754:0 1755:0 1756:0 1757:0 1758:0 1759:0.00493153 1760:0.0352526 1761:0 1762:0 1763:0 1764:0 1765:0 1766:0 1767:0 1768:0.0322777 1769:0 1770:0 1771:0 1772:0 1773:0 1774:0 1775:0 1776:0.0365664 1777:0 1778:0 1779:0 1780:0 1781:0 1782:0 1783:0 1784:0.0610545 1785:0 1786:0 1787:0 1788:0 1789:0 1790:0 1791:0 1792:0.0300424 1793:0.0304606 1794:0.00228501 1795:0 1796:0 1797:0 1798:0 1799:0.00141785 1800:0.0225634 1801:0.00345446 1802:0 1803:0 1804:0 1805:0 1806:0 1807:0 1808:0 1809:0.0139219 1810:0 1811:0 1812:0 1813:0 1814:0 1815:0 1816:0.0042745 1817:0.00987244 1818:0 1819:0 1820:0 1821:0.0159882 1822:0 1823:0 1824:0 1825:0.00641641 1826:0.0126816 1827:0 1828:0 1829:0 1830:0.0150321 1831:0 1832:0 1833:0.0023391 1834:0.0152512 1835:0 1836:0 1837:0 1838:0 1839:0 1840:6.90073e-05 1841:0.00544836 1842:0 1843:0 1844:0 1845:0 1846:0 1847:0 1848:0 1849:0.0373495 1850:0.061688 1851:0.0541068 1852:0.0419297 1853:0.0399975 1854:0.0404509 1855:0.037291 1856:0.0671256 1857:0.0360837 1858:0.0541425 1859:0.0572417 1860:0.052802 1861:0.0530716 1862:0.0583125 1863:0.064872 1864:0.0506067 1865:0.0438371 1866:0.0446475 1867:0.0442432 1868:0.0483701 1869:0.0737928 1870:0.0642284 1871:0.0553784 1872:0.0377337 1873:0.0495247 1874:0.0566168 1875:0.0757195 1876:0.077875 1877:0.05847 1878:0.0670954 1879:0.0434669 1880:0.0429365 1881:0.0580286 1882:0.0941267 1883:0.101078 1884:0.146421 1885:0.150102 1886:0.136991 1887:0.0945731 1888:0.0414691 1889:0.0694465 1890:0.142909 1891:0.134887 1892:0.137172 1893:0.172239 1894:0.205663 1895:0.111917 1896:0.0574599 1897:0.0704985 1898:0.111359 1899:0.156926 1900:0.120727 1901:0.0936624 1902:0.115001 1903:0.112421 1904:0.0641391 1905:0.0535418 1906:0.0804749 1907:0.109915 1908:0.110222 1909:0.0883193 1910:0.0711404 1911:0.0760119 1912:0.0702744 1913:0.0836092 1914:0.0781723 1915:0.0692921 1916:0.0617591 1917:0.0575286 1918:0.053038 1919:0.0782147 1920:0.0512499 1921:0.0960569 1922:0.0966289 1923:0.110869 1924:0.109495 1925:0.107615 1926:0.103976 1927:0.113091 1928:0.120168 1929:0.0420363 1930:0.00526851 1931:0.00479064 1932:0.00826716 1933:0.0200809 1934:0.0291669 1935:0.00649548 1936:0.0490916 1937:0.0393742 1938:0.008617 1939:0.0590585 1940:0.0608786 1941:0.0229989 1942:0.0370622 1943:0.0524868 1944:0.0334172 1945:0.0368119 1946:0.0381415 1947:0.0487262 1948:0.0535777 1949:0.0498706 1950:0.0265701 1951:0.0325247 1952:0.0126627 1953:0.0261623 1954:0.0687546 1955:0.0291398 1956:0.0214799 1957:0 1958:0 1959:0 1960:0.022544 1961:0.0280178 1962:0.00355056 1963:0 1964:0 1965:0 1966:0 1967:0.00137494 1968:0.0191759 1969:0.0304994 1970:0 1971:0 1972:0 1973:0 1974:0 1975:0.00400988 1976:0.0125213 1977:0.0109052 1978:0.0107457 1979:0.00270618 1980:0 1981:0.00397455 1982:0.000754263 1983:0 1984:0.0149195 1985:0 1986:0 1987:0 1988:0 1989:0 1990:0 1991:0 1992:0 1993:0 1994:0 1995:0 1996:0 1997:0 1998:0 1999:0 2000:0 2001:0 2002:0 2003:0 2004:0 2005:0 2006:0 2007:0 2008:0 2009:0 2010:0 2011:0 2012:0 2013:0 2014:0 2015:0 2016:0 2017:0 2018:0 2019:0 2020:0 2021:0 2022:0 2023:0 2024:0 2025:0 2026:0 2027:0 2028:0 2029:0 2030:0 2031:0 2032:0 2033:0 2034:0 2035:0 2036:0 2037:0 2038:0 2039:0 2040:0 2041:0 2042:0 2043:0 2044:0 2045:0 2046:0 2047:0 2048:0 2049:0.102049 2050:0.133654 2051:0.137904 2052:0.133964 2053:0.134975 2054:0.146026 2055:0.15165 2056:0.158567 2057:0.130799 2058:0.147544 2059:0.152729 2060:0.156265 2061:0.171601 2062:0.163162 2063:0.168681 2064:0.154663 2065:0.172045 2066:0.164908 2067:0.178127 2068:0.176956 2069:0.192728 2070:0.181405 2071:0.161246 2072:0.166154 2073:0.180368 2074:0.15255 2075:0.203602 2076:0.328174 2077:0.333262 2078:0.315041 2079:0.231047 2080:0.172476 2081:0.159669 2082:0.256792 2083:0.285085 2084:0.312882 2085:0.328441 2086:0.337986 2087:0.282606 2088:0.177403 2089:0.153018 2090:0.260445 2091:0.302578 2092:0.244261 2093:0.203543 2094:0.246306 2095:0.253181 2096:0.195702 2097:0.156715 2098:0.188192 2099:0.222675 2100:0.243978 2101:0.188174 2102:0.17326 2103:0.181557 2104:0.210908 2105:0.188569 2106:0.208646 2107:0.172428 2108:0.170037 2109:0.172194 2110:0.162674 2111:0.159038 2112:0.210987 2113:0 2114:0 2115:0 2116:0 2117:0 2118:0 2119:0 2120:0 2121:0 2122:0 2123:0 2124:0 2125:0 2126:0 2127:0 2128:0 2129:0 2130:0 2131:0 2132:0 2133:0 2134:0 2135:0 2136:0 2137:0 2138:0 2139:0 2140:0 2141:0 2142:0 2143:0 2144:0 2145:0 2146:0 2147:0 2148:0 2149:0.0238519 2150:0.00576483 2151:0.0273866 2152:0 2153:0 2154:0 2155:0 2156:0 2157:0 2158:0 2159:0.0366242 2160:0 2161:0 2162:0 2163:0 2164:0.00371445 2165:0 2166:0 2167:0 2168:0 2169:0.0875708 2170:0.0499125 2171:0.0497832 2172:0.0465618 2173:0.0418089 2174:0.0414679 2175:0.0602913 2176:0.068373 2177:0.10627 2178:0.140747 2179:0.132335 2180:0.132048 2181:0.134078 2182:0.136581 2183:0.136917 2184:0.138833 2185:0.0871566 2186:0.0721617 2187:0.0601977 2188:0.0533763 2189:0.0733323 2190:0.0586677 2191:0.056692 2192:0.0728266 2193:0.0696791 2194:0.0619169 2195:0.0759996 2196:0.100767 2197:0.0836954 2198:0.0914628 2199:0.0616362 2200:0.0732657 2201:0.0631693 2202:0.0433632 2203:0.0808008 2204:0.11576 2205:0.0767414 2206:0.0763064 2207:0.059327 2208:0.0717505 2209:0.0726722 2210:0.0839259 2211:0.0997713 2212:0.0263583 2213:0.00365244 2214:0.0324486 2215:0.0240456 2216:0.0650212 2217:0.0692881 2218:0.0600631 2219:0.0207227 2220:0.0384139 2221:0.042917 2222:0.041033 2223:0.0616526 2224:0.086314 2225:0.0994695 2226:0.0619666 2227:0.0303241 2228:0.0322269 2229:0.0534326 2230:0.0677678 2231:0.0678124 2232:0.0925313 2233:0.0986385 2234:0.0873698 2235:0.0625982 2236:0.0511066 2237:0.0627424 2238:0.0571902 2239:0.0555736 2240:0.0582599 2241:0.150777 2242:0.228278 2243:0.230447 2244:0.223683 2245:0.222684 2246:0.24943 2247:0.260479 2248:0.249323 2249:0.209887 2250:0.206633 2251:0.206423 2252:0.19976 2253:0.245886 2254:0.22576 2255:0.230874 2256:0.199832 2257:0.267037 2258:0.240622 2259:0.220439 2260:0.278619 2261:0.307412 2262:0.286343 2263:0.231443 2264:0.212521 2265:0.274686 2266:0.247095 2267:0.369072 2268:0.472374 2269:0.488285 2270:0.435444 2271:0.393203 2272:0.225168 2273:0.245704 2274:0.357454 2275:0.450826 2276:0.503841 2277:0.532054 2278:0.491965 2279:0.447685 2280:0.216289 2281:0.25928 2282:0.407289 2283:0.458397 2284:0.427641 2285:0.327532 2286:0.399831 2287:0.332854 2288:0.236738 2289:0.283964 2290:0.282717 2291:0.347277 2292:0.361744 2293:0.273773 2294:0.247611 2295:0.252643 2296:0.289142 2297:0.270755 2298:0.269885 2299:0.26794 2300:0.251191 2301:0.242329 2302:0.240735 2303:0.264098 2304:0.280452 2305:0.0723678 2306:0.107719 2307:0.111275 2308:0.110904 2309:0.11566 2310:0.118617 2311:0.113928 2312:0.116247 2313:0.0904895 2314:0.111156 2315:0.101197 2316:0.117407 2317:0.0969763 2318:0.11953 2319:0.113648 2320:0.109873 2321:0.116138 2322:0.117504 2323:0.147255 2324:0.149879 2325:0.168868 2326:0.143855 2327:0.121738 2328:0.125982 2329:0.110607 2330:0.184463 2331:0.174517 2332:0.239883 2333:0.230383 2334:0.23868 2335:0.132588 2336:0.127439 2337:0.133353 2338:0.195605 2339:0.213757 2340:0.181219 2341:0.252433 2342:0.282005 2343:0.188664 2344:0.148861 2345:0.12971 2346:0.168598 2347:0.243649 2348:0.165996 2349:0.165944 2350:0.183932 2351:0.19064 2352:0.183056 2353:0.136616 2354:0.143196 2355:0.168053 2356:0.169739 2357:0.140863 2358:0.12301 2359:0.170306 2360:0.14927 2361:0.160712 2362:0.170161 2363:0.150552 2364:0.130722 2365:0.126399 2366:0.118766 2367:0.142978 2368:0.122132 2369:0 2370:0 2371:0 2372:0 2373:0 2374:0 2375:0 2376:0 2377:0 2378:0 2379:0 2380:0.0186522 2381:0.00790889 2382:0.00272604 2383:0 2384:0 2385:0 2386:0.00522694 2387:0.0115965 2388:0.0487347 2389:0.0297424 2390:0.0340466 2391:0.00201853 2392:0 2393:0 2394:0.0440029 2395:0.0491136 2396:0.0575527 2397:0.0420778 2398:0.0491625 2399:0 2400:0 2401:0 2402:0.0700559 2403:0.0187284 2404:0.0365558 2405:0.000240687 2406:0.0355439 2407:0 2408:0.000540502 2409:0.00162602 2410:0.0347162 2411:0.0111904 2412:0 2413:0.00710883 2414:0 2415:0 2416:0.00551061 2417:0 2418:0 2419:0.00217292 2420:0 2421:0 2422:0.00230726 2423:0.0231763 2424:0.0189732 2425:0.0377865 2426:0.0249683 2427:0.0175988 2428:0.0136349 2429:0.00949922 2430:0 2431:0.0252853 2432:0.00138315 2433:0.00424893 2434:0 2435:0 2436:0 2437:0 2438:0 2439:0 2440:0.0485043 2441:0 2442:0 2443:0 2444:0.00430669 2445:0.00210337 2446:0 2447:0 2448:0.0371499 2449:0 2450:0 2451:0.0101197 2452:0 2453:0 2454:0 2455:0 2456:0.0338847 2457:0.00325606 2458:0 2459:0 2460:0 2461:0 2462:0 2463:0 2464:0.0375643 2465:0.00847732 2466:0 2467:0 2468:0 2469:0 2470:0 2471:0 2472:0.0341872 2473:0 2474:0 2475:0 2476:0 2477:0 2478:0 2479:0 2480:0.0148324 2481:0 2482:0 2483:0 2484:0 2485:0 2486:0 2487:0 2488:0.037317 2489:0.0399839 2490:0.0490869 2491:0.0514689 2492:0.0482332 2493:0.0480592 2494:0.0566195 2495:0.0571722 2496:0.101279 2497:0 2498:0 2499:0 2500:0 2501:0 2502:0 2503:0 2504:0 2505:0 2506:0 2507:0 2508:0 2509:0 2510:0 2511:0 2512:0 2513:0 2514:0 2515:0 2516:0 2517:0 2518:0 2519:0 2520:0 2521:0 2522:0 2523:0 2524:0 2525:0 2526:0 2527:0 2528:0 2529:0 2530:0 2531:0 2532:0 2533:0 2534:0 2535:0 2536:0 2537:0 2538:0 2539:0 2540:0 2541:0 2542:0 2543:0 2544:0 2545:0 2546:0 2547:0 2548:0 2549:0 2550:0 2551:0 2552:0 2553:0 2554:0 2555:0 2556:0 2557:0 2558:0 2559:0 2560:0 2561:0.00439791 2562:0 2563:0 2564:0 2565:0 2566:0 2567:0 2568:0.0103426 2569:0 2570:0 2571:0 2572:0 2573:0 2574:0 2575:0 2576:0 2577:0 2578:0 2579:0 2580:0 2581:0 2582:0 2583:0 2584:0 2585:0 2586:0 2587:0 2588:0 2589:0 2590:0 2591:0 2592:0 2593:0 2594:0 2595:0 2596:0 2597:0 2598:0 2599:0 2600:0 2601:0 2602:0 2603:0 2604:0 2605:0 2606:0 2607:0 2608:0 2609:0 2610:0 2611:0 2612:0 2613:0 2614:0 2615:0 2616:0 2617:0.00587183 2618:0 2619:0 2620:0 2621:0 2622:0 2623:0 2624:0.00744657 2625:0 2626:0 2627:0 2628:0 2629:0 2630:0 2631:0 2632:0 2633:0 2634:0 2635:0 2636:0 2637:0 2638:0 2639:0 2640:0 2641:0 2642:0 2643:0 2644:0 2645:0 2646:0 2647:0 2648:0 2649:0 2650:0 2651:0 2652:0 2653:0 2654:0 2655:0 2656:0 2657:0 2658:0 2659:0 2660:0 2661:0 2662:0 2663:0 2664:0 2665:0 2666:0 2667:0 2668:0 2669:0 2670:0 2671:0 2672:0 2673:0 2674:0 2675:0 2676:0 2677:0 2678:0 2679:0 2680:0 2681:0 2682:0 2683:0 2684:0 2685:0.000214653 2686:0.00210491 2687:0.00959727 2688:0 2689:0.0405934 2690:0.03354 2691:0.0197494 2692:0.0246153 2693:0.0242749 2694:0.0205132 2695:0.0173198 2696:0.0338914 2697:0.00946074 2698:0.00441903 2699:0.0199688 2700:0.0187485 2701:0.0262726 2702:0.0166255 2703:0.00404971 2704:0.0361928 2705:0.00862697 2706:0.0136345 2707:0.0127075 2708:0.00417875 2709:0 2710:0.0101943 2711:0.0112757 2712:0.0095351 2713:0.00344951 2714:0.0120316 2715:0 2716:0 2717:0 2718:0 2719:0.0225689 2720:0.0119427 2721:0.00843712 2722:0 2723:0 2724:0 2725:0 2726:0 2727:0.009969 2728:0.0224633 2729:0.00836094 2730:0 2731:0 2732:0.00635466 2733:0.000294928 2734:0.00166509 2735:0.0063834 2736:0.0204056 2737:0.0057741 2738:0 2739:0 2740:0 2741:0.00470368 2742:0.00643606 2743:0.00699411 2744:0.000348896 2745:0.0294464 2746:0.0309927 2747:0.0255916 2748:0.0281406 2749:0.0297826 2750:0.0292321 2751:0.0316146 2752:0.0323518 2753:0 2754:0 2755:0 2756:0 2757:0 2758:0 2759:0 2760:0 2761:0 2762:0 2763:0 2764:0 2765:0 2766:0 2767:0 2768:0 2769:0 2770:0 2771:0 2772:0 2773:0 2774:0 2775:0 2776:0 2777:0 2778:0 2779:0 2780:0 2781:0 2782:0 2783:0 2784:0 2785:0 2786:0 2787:0 2788:0 2789:0 2790:0 2791:0 2792:0 2793:0 2794:0 2795:0 2796:0 2797:0 2798:0 2799:0 2800:0 2801:0 2802:0 2803:0 2804:0 2805:0 2806:0 2807:0 2808:0 2809:0 2810:0 2811:0.00772632 2812:0.00880727 2813:0.0130702 2814:0.0210194 2815:0.00681281 2816:0.0223941 2817:0.0506334 2818:0.0700124 2819:0.0747158 2820:0.0674944 2821:0.0620056 2822:0.0661889 2823:0.0803944 2824:0.0748666 2825:0.0738827 2826:0.0497842 2827:0.0457793 2828:0.0598216 2829:0.0597764 2830:0.0890612 2831:0.0631034 2832:0.0490111 2833:0.120279 2834:0.0662976 2835:0.073884 2836:0.0720726 2837:0.100257 2838:0.0716643 2839:0.0806751 2840:0.0596424 2841:0.117309 2842:0.074213 2843:0.124566 2844:0.155842 2845:0.118702 2846:0.135292 2847:0.0692064 2848:0.059121 2849:0.110777 2850:0.130229 2851:0.148044 2852:0.166176 2853:0.143543 2854:0.15147 2855:0.0564573 2856:0.0798909 2857:0.097823 2858:0.183273 2859:0.148712 2860:0.0885683 2861:0.0680031 2862:0.145165 2863:0.072839 2864:0.0573365 2865:0.115045 2866:0.0789816 2867:0.116144 2868:0.109182 2869:0.0652364 2870:0.0639276 2871:0.0630036 2872:0.0983583 2873:0.155446 2874:0.134728 2875:0.116129 2876:0.112924 2877:0.117831 2878:0.0965245 2879:0.0881475 2880:0.14482 2881:0.0264329 2882:0.0164121 2883:0 2884:0 2885:0 2886:0 2887:0 2888:0 2889:0 2890:0 2891:0 2892:0 2893:0 2894:0 2895:0 2896:0 2897:0.0071414 2898:0 2899:0 2900:0 2901:0 2902:0 2903:0 2904:0 2905:0.0123424 2906:0 2907:0 2908:0 2909:0 2910:0 2911:0 2912:0 2913:0.00858049 2914:0 2915:0 2916:0 2917:0 2918:0 2919:0 2920:0.00439348 2921:0.00301546 2922:0 2923:0 2924:0 2925:0 2926:0 2927:0 2928:0.00262821 2929:0.000997839 2930:0 2931:0 2932:0 2933:0 2934:0 2935:0 2936:0 2937:0.0329696 2938:0.0167874 2939:0.014924 2940:0.0152278 2941:0.0124618 2942:0.0121843 2943:0.0106038 2944:0.0200389 2945:0 2946:0 2947:0 2948:0 2949:0 2950:0 2951:0 2952:0 2953:0 2954:0 2955:0 2956:0 2957:0 2958:0 2959:0 2960:0 2961:0 2962:0 2963:0 2964:0 2965:0 2966:0 2967:0 2968:0 2969:0 2970:0 2971:0 2972:0 2973:0 2974:0 2975:0 2976:0 2977:0 2978:0 2979:0 2980:0 2981:0 2982:0 2983:0 2984:0 2985:0 2986:0 2987:0 2988:0 2989:0 2990:0 2991:0 2992:0 2993:0 2994:0 2995:0 2996:0 2997:0 2998:0 2999:0 3000:0 3001:0 3002:0 3003:0 3004:0 3005:0 3006:0 3007:0 3008:0.0218902 3009:0.0714838 3010:0.0921413 3011:0.103151 3012:0.0981003 3013:0.096081 3014:0.102543 3015:0.106161 3016:0.110885 3017:0.0784314 3018:0.0722705 3019:0.0652356 3020:0.0718785 3021:0.0731956 3022:0.0808284 3023:0.0771802 3024:0.0712121 3025:0.097252 3026:0.0811146 3027:0.0945302 3028:0.123815 3029:0.118317 3030:0.118017 3031:0.0824348 3032:0.0740788 3033:0.0901243 3034:0.10932 3035:0.179164 3036:0.213039 3037:0.204588 3038:0.207428 3039:0.160026 3040:0.0766424 3041:0.0884893 3042:0.159795 3043:0.176886 3044:0.176521 3045:0.1802 3046:0.174952 3047:0.135597 3048:0.0794384 3049:0.0899343 3050:0.133284 3051:0.123609 3052:0.132112 3053:0.0861832 3054:0.099446 3055:0.105038 3056:0.103838 3057:0.105687 3058:0.0935807 3059:0.114105 3060:0.115355 3061:0.0902138 3062:0.0853983 3063:0.094666 3064:0.108778 3065:0.118911 3066:0.128686 3067:0.108097 3068:0.0975269 3069:0.098265 3070:0.0930089 3071:0.11314 3072:0.108225 3073:0.0524452 3074:0.0577254 3075:0.0475242 3076:0.051819 3077:0.05312 3078:0.0541269 3079:0.0487278 3080:0.0506838 3081:0.0487299 3082:0.0508685 3083:0.048598 3084:0.0419397 3085:0.0596704 3086:0.0391449 3087:0.049616 3088:0.0718566 3089:0.0585413 3090:0.0479041 3091:0.0567887 3092:0.0657589 3093:0.0868827 3094:0.0684748 3095:0.0596536 3096:0.0586355 3097:0.0558886 3098:0.0511612 3099:0.075912 3100:0.0470457 3101:0.0558171 3102:0.0643436 3103:0.053868 3104:0.0594972 3105:0.0585093 3106:0.0565552 3107:0.0374777 3108:0.0150648 3109:0.0284063 3110:0.0493261 3111:0.0423882 3112:0.0671885 3113:0.0578745 3114:0.0283519 3115:0.058731 3116:0.0575858 3117:0.0763202 3118:0.0691603 3119:0.0870532 3120:0.067713 3121:0.0711799 3122:0.040505 3123:0.0281044 3124:0.056197 3125:0.0452651 3126:0.0488539 3127:0.0639848 3128:0.0651529 3129:0.0805369 3130:0.0417973 3131:0.0395496 3132:0.0500266 3133:0.0377178 3134:0.0325062 3135:0.0217872 3136:0.0619642 3137:0.0341021 3138:0.0577894 3139:0.0594702 3140:0.0588283 3141:0.0566683 3142:0.0632658 3143:0.0595037 3144:0.11099 3145:0.00578025 3146:0 3147:0 3148:0 3149:0.0197661 3150:0 3151:0 3152:0.0831894 3153:0 3154:0 3155:0 3156:0.038528 3157:0.00608978 3158:0.0410167 3159:0 3160:0.0706588 3161:0 3162:0 3163:0.00685163 3164:0.000528309 3165:0.00635388 3166:0 3167:0.0168729 3168:0.0630692 3169:0 3170:0 3171:0 3172:0 3173:0 3174:0 3175:0 3176:0.0687872 3177:0 3178:0 3179:0 3180:0 3181:0 3182:0 3183:0 3184:0.0946377 3185:0 3186:0 3187:0 3188:0 3189:0 3190:0 3191:0 3192:0.0737389 3193:0 3194:0 3195:0 3196:0 3197:0 3198:0 3199:0 3200:0.0305634 3201:0 3202:0 3203:0 3204:0 3205:0 3206:0 3207:0 3208:0 3209:0 3210:0 3211:0 3212:0 3213:0 3214:0 3215:0 3216:0 3217:0.0143572 3218:0 3219:0 3220:0 3221:0 3222:0 3223:0 3224:0 3225:0.0158285 3226:0 3227:0.00710469 3228:0.020726 3229:0.00652568 3230:0 3231:0 3232:0 3233:0.0100138 3234:0.00236669 3235:0 3236:0.0292934 3237:0.0593236 3238:0.0378013 3239:0 3240:0 3241:0.00239813 3242:0.0360806 3243:0.0547754 3244:0 3245:0.00923284 3246:0.041601 3247:0.0105652 3248:0 3249:0.0202215 3250:0 3251:0.0311024 3252:0.0206765 3253:0 3254:0 3255:0 3256:0 3257:0.0601074 3258:0.0627453 3259:0.0568688 3260:0.0435718 3261:0.0409998 3262:0.0387803 3263:0.0454193 3264:0.0426722 3265:0.06331 3266:0.0512857 3267:0.0627273 3268:0.0661452 3269:0.0629323 3270:0.0590037 3271:0.0629228 3272:0.0857579 3273:0.0559735 3274:0.0613651 3275:0.0763051 3276:0.0612788 3277:0.0774248 3278:0.0638002 3279:0.0714444 3280:0.0930264 3281:0.0689527 3282:0.0673373 3283:0.0706768 3284:0.073441 3285:0.076023 3286:0.0798661 3287:0.0722397 3288:0.0788411 3289:0.0646223 3290:0.0697277 3291:0.0664008 3292:0.0524378 3293:0.051233 3294:0.0581449 3295:0.0833697 3296:0.0844257 3297:0.0695621 3298:0.0694822 3299:0.0394058 3300:0.0623218 3301:0.0327213 3302:0.0431197 3303:0.0941493 3304:0.0810446 3305:0.0549263 3306:0.0265015 3307:0.0578168 3308:0.0704855 3309:0.0817758 3310:0.0651512 3311:0.0735978 3312:0.0869448 3313:0.047928 3314:0.0432599 3315:0.0445951 3316:0.0613638 3317:0.0618975 3318:0.0645134 3319:0.0745011 3320:0.0950728 3321:0.0482319 3322:0.0458308 3323:0.0658369 3324:0.0649869 3325:0.0610776 3326:0.0661595 3327:0.0772201 3328:0.0984238 3329:0.0445797 3330:0.0786936 3331:0.0723887 3332:0.0675517 3333:0.0826378 3334:0.0874248 3335:0.0968329 3336:0.0487497 3337:0.0693233 3338:0.0697079 3339:0.0642238 3340:0.0867203 3341:0.0800232 3342:0.0860087 3343:0.074235 3344:0.0506626 3345:0.0682287 3346:0.0900364 3347:0.0782395 3348:0.0901831 3349:0.0929127 3350:0.0883569 3351:0.0917998 3352:0.0673199 3353:0.0611448 3354:0.0876134 3355:0.162854 3356:0.185436 3357:0.169971 3358:0.167918 3359:0.114988 3360:0.0753296 3361:0.0870117 3362:0.112278 3363:0.155297 3364:0.176527 3365:0.174821 3366:0.184553 3367:0.138959 3368:0.110579 3369:0.0840303 3370:0.140568 3371:0.145898 3372:0.113047 3373:0.112968 3374:0.150713 3375:0.101282 3376:0.106114 3377:0.0872976 3378:0.105871 3379:0.12666 3380:0.109284 3381:0.0979531 3382:0.0950269 3383:0.122456 3384:0.0891805 3385:0.106851 3386:0.0901269 3387:0.0860831 3388:0.0813062 3389:0.086868 3390:0.0852858 3391:0.115569 3392:0.0647837 3393:0 3394:0 3395:0 3396:0 3397:0 3398:0 3399:0 3400:0 3401:0 3402:0 3403:0 3404:0 3405:0 3406:0 3407:0 3408:0 3409:0 3410:0 3411:0 3412:0 3413:0 3414:0 3415:0 3416:0 3417:0 3418:0 3419:0 3420:0 3421:0 3422:0 3423:0 3424:0 3425:0 3426:0 3427:0 3428:0 3429:0 3430:0 3431:0 3432:0 3433:0 3434:0 3435:0 3436:0 3437:0 3438:0 3439:0 3440:0 3441:0 3442:0 3443:0 3444:0 3445:0 3446:0 3447:0 3448:0 3449:0 3450:0 3451:0 3452:0 3453:0 3454:0 3455:0 3456:0 3457:0.0540544 3458:0.0749401 3459:0.0794362 3460:0.0801546 3461:0.07484 3462:0.0691177 3463:0.0850037 3464:0.0884705 3465:0.0649961 3466:0.0506371 3467:0.0522363 3468:0.0672586 3469:0.0551817 3470:0.076901 3471:0.0620386 3472:0.0536619 3473:0.0604079 3474:0.0638522 3475:0.0471579 3476:0.0538805 3477:0.052714 3478:0.0516214 3479:0.0674805 3480:0.0562905 3481:0.0686471 3482:0.0633606 3483:0.059157 3484:0.105556 3485:0.0930508 3486:0.0848293 3487:0.0755326 3488:0.0635232 3489:0.0580934 3490:0.0920516 3491:0.136282 3492:0.147211 3493:0.158233 3494:0.140988 3495:0.0820581 3496:0.0773322 3497:0.0744722 3498:0.134952 3499:0.106072 3500:0.0737869 3501:0.0879029 3502:0.1209 3503:0.0833502 3504:0.0516321 3505:0.0658545 3506:0.0685474 3507:0.101015 3508:0.0906397 3509:0.0707775 3510:0.0706762 3511:0.088143 3512:0.0512711 3513:0.0761041 3514:0.0630748 3515:0.0642539 3516:0.0730313 3517:0.0727204 3518:0.0692568 3519:0.100287 3520:0.0721806 3521:0 3522:0 3523:0 3524:0 3525:0 3526:0 3527:0 3528:0.00549764 3529:0 3530:0 3531:0 3532:0 3533:0 3534:0 3535:0 3536:0.00410156 3537:0 3538:0 3539:0 3540:0 3541:0 3542:0 3543:0 3544:0 3545:0 3546:0 3547:0 3548:0 3549:0 3550:0 3551:0 3552:0 3553:0 3554:0 3555:0 3556:0 3557:0 3558:0 3559:0 3560:0 3561:0 3562:0 3563:0 3564:0 3565:0 3566:0 3567:0 3568:0 3569:0 3570:0 3571:0 3572:0 3573:0 3574:0 3575:0 3576:0 3577:0 3578:0 3579:0 3580:0 3581:0 3582:0 3583:0 3584:0.033628 3585:0 3586:0 3587:0 3588:0 3589:0 3590:0 3591:0 3592:0 3593:0.000588277 3594:0 3595:0 3596:0 3597:0 3598:0 3599:0 3600:0 3601:0.0144019 3602:0 3603:0 3604:0 3605:0 3606:0 3607:0 3608:0 3609:0.00474249 3610:0 3611:0 3612:0 3613:0 3614:0 3615:0 3616:0 3617:0.00738397 3618:0 3619:0 3620:0 3621:0 3622:0 3623:0 3624:0 3625:0.00118898 3626:0 3627:0 3628:0 3629:0 3630:0 3631:0 3632:0.00850773 3633:0.00447956 3634:0 3635:0 3636:0 3637:0 3638:0 3639:0 3640:0 3641:0.0111294 3642:0 3643:0 3644:0 3645:0 3646:0 3647:0 3648:0 3649:0 3650:0 3651:0.00350884 3652:0.00218835 3653:0.000465011 3654:0.00108679 3655:0 3656:0.00177482 3657:0 3658:0 3659:0 3660:0 3661:0 3662:0 3663:0 3664:0 3665:0 3666:0 3667:0 3668:0 3669:0 3670:0.00108825 3671:0 3672:0.00907357 3673:0 3674:0 3675:0 3676:0 3677:0 3678:0.00228472 3679:0 3680:0.00560135 3681:0 3682:0 3683:0 3684:0 3685:0 3686:0 3687:0 3688:0 3689:0 3690:0 3691:0 3692:0 3693:0 3694:0 3695:0 3696:0.0126956 3697:0 3698:0 3699:0 3700:0 3701:0 3702:0 3703:0 3704:0.0455383 3705:0 3706:0 3707:0 3708:0 3709:0 3710:0 3711:0 3712:0.00777215 3713:0 3714:0 3715:0 3716:0 3717:0 3718:0 3719:0 3720:0 3721:0 3722:0 3723:0 3724:0 3725:0 3726:0 3727:0 3728:0 3729:0 3730:0 3731:0 3732:0 3733:0 3734:0 3735:0 3736:0 3737:0 3738:0 3739:0 3740:0 3741:0 3742:0 3743:0 3744:0 3745:0 3746:0 3747:0 3748:0 3749:0 3750:0 3751:0 3752:0 3753:0 3754:0 3755:0 3756:0 3757:0 3758:0 3759:0 3760:0 3761:0 3762:0 3763:0 3764:0 3765:0 3766:0 3767:0 3768:0 3769:0 3770:0 3771:0 3772:0 3773:0 3774:0 3775:0 3776:0 3777:0.101402 3778:0.114085 3779:0.111147 3780:0.111488 3781:0.109022 3782:0.124241 3783:0.127116 3784:0.110749 3785:0.097312 3786:0.104225 3787:0.10062 3788:0.11722 3789:0.116903 3790:0.114745 3791:0.11346 3792:0.106549 3793:0.107076 3794:0.112565 3795:0.114053 3796:0.128583 3797:0.115824 3798:0.118314 3799:0.113965 3800:0.11232 3801:0.11 3802:0.108284 3803:0.159325 3804:0.172956 3805:0.175539 3806:0.182988 3807:0.128014 3808:0.113663 3809:0.115515 3810:0.154972 3811:0.147758 3812:0.209794 3813:0.195998 3814:0.169693 3815:0.137931 3816:0.12216 3817:0.122437 3818:0.180093 3819:0.160506 3820:0.152117 3821:0.163389 3822:0.187267 3823:0.133209 3824:0.134831 3825:0.112955 3826:0.133322 3827:0.169439 3828:0.132668 3829:0.125769 3830:0.114675 3831:0.138547 3832:0.141654 3833:0.134669 3834:0.129554 3835:0.125096 3836:0.129716 3837:0.127211 3838:0.131756 3839:0.142794 3840:0.095666 3841:0 3842:0 3843:0 3844:0 3845:0 3846:0 3847:0 3848:0 3849:0 3850:0 3851:0 3852:0 3853:0 3854:0 3855:0 3856:0 3857:0 3858:0 3859:0 3860:0 3861:0 3862:0 3863:0 3864:0 3865:0 3866:0 3867:0 3868:0 3869:0 3870:0 3871:0 3872:0 3873:0 3874:0 3875:0 3876:0 3877:0 3878:0 3879:0 3880:0 3881:0 3882:0 3883:0 3884:0 3885:0 3886:0 3887:0 3888:0 3889:0 3890:0 3891:0 3892:0 3893:0 3894:0 3895:0 3896:0 3897:0 3898:0 3899:0 3900:0 3901:0 3902:0 3903:0 3904:0 3905:0.0436803 3906:0.0556349 3907:0.0658323 3908:0.0635811 3909:0.0622092 3910:0.0653892 3911:0.0671142 3912:0.075132 3913:0.0226919 3914:0.0214149 3915:0.0263902 3916:0.0244097 3917:0.0216023 3918:0.0135213 3919:0.0199412 3920:0.040214 3921:0.0264599 3922:0.02037 3923:0.0208385 3924:0.0286612 3925:0.0255335 3926:0.0334671 3927:0.0242781 3928:0.0281231 3929:0.0305774 3930:0.0245219 3931:0.0394746 3932:0.0452287 3933:0.0288069 3934:0.0366651 3935:0.0121651 3936:0.0276251 3937:0.0337519 3938:0.0327289 3939:0.0201205 3940:0 3941:0 3942:0 3943:0.01698 3944:0.0384848 3945:0.0268831 3946:0.000513282 3947:0.00465276 3948:0.0108459 3949:0.00491782 3950:0 3951:0.0394721 3952:0.0263891 3953:0.0148372 3954:0.00175384 3955:0 3956:0.0130491 3957:0.00698269 3958:0.0128073 3959:0.0144471 3960:0.0210733 3961:0.0634096 3962:0.0597127 3963:0.0638077 3964:0.0627126 3965:0.0590203 3966:0.051348 3967:0.053302 3968:0.0827574 3969:0.0582348 3970:0.0884624 3971:0.100983 3972:0.0969791 3973:0.100108 3974:0.0950397 3975:0.106758 3976:0.130004 3977:0.0462546 3978:0.0633457 3979:0.0825364 3980:0.0749888 3981:0.0946574 3982:0.08364 3983:0.0867311 3984:0.117174 3985:0.0555064 3986:0.0799085 3987:0.0829888 3988:0.0627561 3989:0.086908 3990:0.0852078 3991:0.0805753 3992:0.132788 3993:0.0574406 3994:0.0942807 3995:0.0934133 3996:0.0974128 3997:0.114231 3998:0.0922545 3999:0.101759 4000:0.134958 4001:0.0514756 4002:0.0733122 4003:0.140069 4004:0.158029 4005:0.156856 4006:0.147286 4007:0.133336 4008:0.13077 4009:0.0478341 4010:0.139813 4011:0.148933 4012:0.140886 4013:0.114145 4014:0.140001 4015:0.135299 4016:0.143532 4017:0.0425707 4018:0.0790089 4019:0.115358 4020:0.108431 4021:0.086671 4022:0.0801778 4023:0.0780178 4024:0.170519 4025:0.0444448 4026:0.110359 4027:0.109419 4028:0.102836 4029:0.0906554 4030:0.0866316 4031:0.0975187 4032:0.164217 4033:0.0238394 4034:0.00424888 4035:0 4036:0 4037:0 4038:0 4039:0 4040:0 4041:0.0192377 4042:0 4043:0 4044:0 4045:0 4046:0 4047:0 4048:0 4049:0.0239997 4050:0 4051:0 4052:0 4053:0 4054:0 4055:0 4056:0 4057:0.0284154 4058:0 4059:0 4060:0 4061:0 4062:0 4063:0 4064:0 4065:0.0276827 4066:0.00029387 4067:0 4068:0 4069:0 4070:0.00917204 4071:0 4072:0 4073:0.0292707 4074:0.00379502 4075:0 4076:0 4077:0 4078:0.0115251 4079:0 4080:0 4081:0.0235664 4082:0 4083:0 4084:0 4085:0 4086:0 4087:0 4088:0 4089:0.0758082 4090:0.0765667 4091:0.0762517 4092:0.064104 4093:0.066587 4094:0.0668309 4095:0.0864239 4096:0.067901 4097:0.129156 4098:0.157703 4099:0.167863 4100:0.158992 4101:0.160907 4102:0.165999 4103:0.169741 4104:0.158068 4105:0.175188 4106:0.175952 4107:0.155547 4108:0.166431 4109:0.176611 4110:0.171979 4111:0.174293 4112:0.153506 4113:0.196792 4114:0.18171 4115:0.198668 4116:0.21956 4117:0.246545 4118:0.231692 4119:0.189382 4120:0.166118 4121:0.187489 4122:0.197373 4123:0.278686 4124:0.286135 4125:0.317597 4126:0.305214 4127:0.252524 4128:0.171151 4129:0.196546 4130:0.268693 4131:0.283273 4132:0.277111 4133:0.298261 4134:0.303987 4135:0.258762 4136:0.171943 4137:0.209103 4138:0.251911 4139:0.279311 4140:0.238505 4141:0.211504 4142:0.223739 4143:0.22557 4144:0.18415 4145:0.224796 4146:0.193288 4147:0.228844 4148:0.224578 4149:0.203888 4150:0.192307 4151:0.206488 4152:0.17944 4153:0.245999 4154:0.206401 4155:0.184267 4156:0.185992 4157:0.184259 4158:0.185683 4159:0.194517 4160:0.150264 4161:0.0325367 4162:0 4163:0 4164:0 4165:0 4166:0 4167:0 4168:0 4169:0.0228849 4170:0 4171:0 4172:0 4173:0 4174:0 4175:0 4176:0 4177:0.0231394 4178:0 4179:0 4180:0 4181:0 4182:0 4183:0 4184:0 4185:0.0207977 4186:0 4187:0 4188:0 4189:0 4190:0 4191:0 4192:0 4193:0.0204781 4194:0 4195:0 4196:0 4197:0 4198:0 4199:0 4200:0 4201:0.0207966 4202:0.014001 4203:0 4204:0 4205:0 4206:0 4207:0 4208:0 4209:0.0088705 4210:0 4211:0 4212:0 4213:0 4214:0 4215:0 4216:0 4217:0.0338545 4218:0 4219:0 4220:0 4221:0 4222:0 4223:0 4224:0 4225:0 4226:0 4227:0 4228:0 4229:0 4230:0 4231:0 4232:0 4233:0 4234:0 4235:0 4236:0 4237:0 4238:0 4239:0 4240:0 4241:0 4242:0 4243:0 4244:0 4245:0 4246:0 4247:0 4248:0 4249:0 4250:0 4251:0 4252:0 4253:0 4254:0 4255:0 4256:0 4257:0 4258:0 4259:0 4260:0 4261:0 4262:0 4263:0 4264:0 4265:0 4266:0 4267:0 4268:0 4269:0 4270:0 4271:0 4272:0 4273:0 4274:0 4275:0 4276:0 4277:0 4278:0 4279:0 4280:0 4281:0 4282:0 4283:0 4284:0 4285:0 4286:0 4287:0 4288:0 4289:0 4290:0.00164879 4291:0.00682222 4292:0.00801078 4293:0.0046831 4294:0 4295:0.00964129 4296:0.0150681 4297:0 4298:0.00375415 4299:0.00347906 4300:0.00682582 4301:0.0121342 4302:0 4303:0.0153387 4304:0.00169086 4305:0.0121822 4306:0.0106509 4307:0.016049 4308:0.00268791 4309:0.0232129 4310:0.0108428 4311:0.0218013 4312:0.00278644 4313:0.0114386 4314:0.01203 4315:0.00332598 4316:0.0111652 4317:0.00684258 4318:0.0322127 4319:0.0389962 4320:0.01845 4321:0.00905711 4322:0.00669143 4323:0.021163 4324:0.0131521 4325:0 4326:0.0267415 4327:0.0504986 4328:0.0159819 4329:0.0109377 4330:0.0240044 4331:0.0184325 4332:0.0266974 4333:0.0407656 4334:0.0337649 4335:0.0102921 4336:0.015144 4337:0.0560048 4338:0.035116 4339:0.0306129 4340:0.0217574 4341:0.017438 4342:0.0226838 4343:0.0242036 4344:0.0291089 4345:0.0730781 4346:0.0537116 4347:0.0410116 4348:0.0407025 4349:0.0257712 4350:0.020668 4351:0.00305296 4352:0.0454315 4353:0.000599102 4354:0 4355:0 4356:0 4357:0 4358:0 4359:0 4360:0 4361:0 4362:0 4363:0 4364:0 4365:0 4366:0 4367:0 4368:0 4369:0 4370:0 4371:0 4372:0 4373:0 4374:0 4375:0 4376:0 4377:0 4378:0 4379:0 4380:0 4381:0 4382:0 4383:0 4384:0 4385:0 4386:0 4387:0 4388:0 4389:0 4390:0 4391:0 4392:0 4393:0 4394:0 4395:0 4396:0 4397:0 4398:0 4399:0 4400:0 4401:0 4402:0 4403:0 4404:0 4405:0 4406:0 4407:0 4408:0 4409:0 4410:0 4411:0 4412:0 4413:0 4414:0 4415:0 4416:0 4417:0.114583 4418:0.13781 4419:0.140982 4420:0.141949 4421:0.141349 4422:0.149275 4423:0.139758 4424:0.154362 4425:0.133514 4426:0.124577 4427:0.121148 4428:0.130302 4429:0.170719 4430:0.12674 4431:0.141928 4432:0.120567 4433:0.159725 4434:0.134235 4435:0.14327 4436:0.186054 4437:0.167451 4438:0.168271 4439:0.144954 4440:0.12511 4441:0.140988 4442:0.156032 4443:0.18302 4444:0.183919 4445:0.213932 4446:0.197789 4447:0.191276 4448:0.127073 4449:0.138913 4450:0.172713 4451:0.190929 4452:0.160717 4453:0.163199 4454:0.159498 4455:0.175191 4456:0.129613 4457:0.138898 4458:0.14064 4459:0.172985 4460:0.190087 4461:0.157295 4462:0.140826 4463:0.137691 4464:0.11983 4465:0.145072 4466:0.120502 4467:0.128101 4468:0.127559 4469:0.129589 4470:0.136719 4471:0.142309 4472:0.132401 4473:0.138225 4474:0.11567 4475:0.125495 4476:0.135152 4477:0.120704 4478:0.118101 4479:0.125838 4480:0.144036 4481:0 4482:0 4483:0 4484:0 4485:0 4486:0 4487:0 4488:0 4489:0 4490:0 4491:0 4492:0 4493:0 4494:0 4495:0 4496:0 4497:0 4498:0 4499:0 4500:0 4501:0 4502:0 4503:0 4504:0 4505:0 4506:0 4507:0 4508:0 4509:0 4510:0 4511:0 4512:0 4513:0 4514:0 4515:0 4516:0 4517:0 4518:0 4519:0 4520:0 4521:0 4522:0 4523:0 4524:0 4525:0 4526:0 4527:0 4528:0 4529:0 4530:0 4531:0 4532:0 4533:0 4534:0 4535:0 4536:0 4537:0.00668967 4538:0 4539:0 4540:0 4541:0 4542:0 4543:0 4544:0 4545:0.0102248 4546:0 4547:0 4548:0 4549:0 4550:0 4551:0 4552:0.00769112 4553:0.00103735 4554:0 4555:0 4556:0 4557:0 4558:0 4559:0 4560:0.0105355 4561:0.00555278 4562:0 4563:0 4564:0 4565:0 4566:0 4567:0 4568:0.00909405 4569:0.00896685 4570:0 4571:0 4572:0 4573:0.0205062 4574:0.02354 4575:0.00753571 4576:0.00892435 4577:0 4578:0 4579:0.0238727 4580:0.0296261 4581:0.0284334 4582:0.0353763 4583:0.019186 4584:0.00775823 4585:0 4586:0 4587:0.0508448 4588:0.0152276 4589:0 4590:0 4591:0 4592:0.000529017 4593:0.0023612 4594:0 4595:0 4596:0 4597:0 4598:0 4599:0 4600:0.0194439 4601:0 4602:0 4603:0 4604:0 4605:0 4606:0 4607:0.00582512 4608:0.0629277 4609:0.186279 4610:0.243338 4611:0.239244 4612:0.232545 4613:0.230614 4614:0.262929 4615:0.249085 4616:0.229565 4617:0.251641 4618:0.255247 4619:0.239544 4620:0.222235 4621:0.247846 4622:0.232982 4623:0.255058 4624:0.253017 4625:0.275687 4626:0.261341 4627:0.286243 4628:0.346067 4629:0.37017 4630:0.366032 4631:0.291752 4632:0.253464 4633:0.279544 4634:0.285818 4635:0.342312 4636:0.443234 4637:0.449679 4638:0.437784 4639:0.38909 4640:0.275171 4641:0.283762 4642:0.327931 4643:0.424184 4644:0.409088 4645:0.420768 4646:0.456719 4647:0.416298 4648:0.266673 4649:0.280065 4650:0.329287 4651:0.409087 4652:0.37637 4653:0.334655 4654:0.34606 4655:0.338233 4656:0.27155 4657:0.325793 4658:0.303613 4659:0.327688 4660:0.330111 4661:0.289609 4662:0.282467 4663:0.291271 4664:0.292783 4665:0.325475 4666:0.321594 4667:0.322684 4668:0.295338 4669:0.279816 4670:0.270711 4671:0.279082 4672:0.284169 4673:0.041542 4674:0.0753225 4675:0.0879856 4676:0.0895153 4677:0.0872136 4678:0.0931017 4679:0.091983 4680:0.155221 4681:0.085764 4682:0.10423 4683:0.104112 4684:0.0872346 4685:0.11904 4686:0.105107 4687:0.112314 4688:0.143364 4689:0.127221 4690:0.116689 4691:0.140283 4692:0.156776 4693:0.184416 4694:0.180765 4695:0.12313 4696:0.169932 4697:0.121818 4698:0.122168 4699:0.162322 4700:0.255746 4701:0.250807 4702:0.256113 4703:0.21653 4704:0.188625 4705:0.115309 4706:0.173418 4707:0.195932 4708:0.216004 4709:0.231672 4710:0.235568 4711:0.215658 4712:0.184937 4713:0.106854 4714:0.140016 4715:0.199914 4716:0.194848 4717:0.157544 4718:0.147557 4719:0.181114 4720:0.219859 4721:0.164446 4722:0.166295 4723:0.157233 4724:0.175692 4725:0.14945 4726:0.133115 4727:0.128247 4728:0.242641 4729:0.18189 4730:0.201869 4731:0.174386 4732:0.145081 4733:0.13231 4734:0.128073 4735:0.100628 4736:0.197327 4737:0 4738:0 4739:0 4740:0 4741:0 4742:0 4743:0 4744:0.0318599 4745:0 4746:0 4747:0 4748:0 4749:0 4750:0 4751:0 4752:0.0102821 4753:0 4754:0 4755:0 4756:0 4757:0 4758:0 4759:0 4760:0.0272923 4761:0 4762:0 4763:0 4764:0 4765:0 4766:0 4767:0.0115502 4768:0.0177232 4769:0 4770:0 4771:0 4772:0 4773:0 4774:0 4775:0 4776:0.0132824 4777:0 4778:0 4779:0 4780:0 4781:0 4782:0 4783:0 4784:0.0277744 4785:0 4786:0 4787:0 4788:0 4789:0 4790:0 4791:0 4792:0.0308044 4793:0.0423182 4794:0.0618608 4795:0.0587142 4796:0.0404638 4797:0.0413315 4798:0.033021 4799:0.0168369 4800:0.0445016 4801:0 4802:0 4803:0 4804:0 4805:0 4806:0 4807:0 4808:0 4809:0 4810:0 4811:0 4812:0 4813:0 4814:0 4815:0 4816:0 4817:0 4818:0 4819:0 4820:0 4821:0 4822:0 4823:0 4824:0 4825:0 4826:0 4827:0 4828:0 4829:0 4830:0 4831:0 4832:0 4833:0 4834:0 4835:0 4836:0 4837:0 4838:0 4839:0 4840:0 4841:0 4842:0 4843:0 4844:0 4845:0 4846:0 4847:0 4848:0 4849:0 4850:0 4851:0 4852:0 4853:0 4854:0 4855:0 4856:0 4857:0 4858:0 4859:0 4860:0 4861:0 4862:0 4863:0 4864:0 4865:0.0442853 4866:0.0171599 4867:0.00844606 4868:0.00443067 4869:0.00304376 4870:0.00742682 4871:0.00401536 4872:0 4873:0.031435 4874:0.0143205 4875:0.0068015 4876:0.0222437 4877:0.0333681 4878:0.0109767 4879:0.0112592 4880:0.021938 4881:0.0182208 4882:0 4883:0.0297458 4884:0.0350755 4885:0.0308818 4886:0.0248007 4887:0.0194338 4888:0 4889:0.0140827 4890:0.033312 4891:0.0119457 4892:0 4893:0 4894:0 4895:0.0199584 4896:0 4897:0.0263874 4898:0.0198613 4899:0 4900:0 4901:0 4902:0 4903:0.00567645 4904:0.0169723 4905:0.0228228 4906:0.00613309 4907:0 4908:0.00229256 4909:0.00431033 4910:0.0259605 4911:0 4912:0 4913:0.0225126 4914:0.0216006 4915:0.0102301 4916:0 4917:0 4918:0.00995771 4919:0.00450945 4920:0 4921:0.0115867 4922:0.0257771 4923:0.0278704 4924:0.0158596 4925:0.00655559 4926:0.00783561 4927:0.0246654 4928:0 4929:0.142206 4930:0.148622 4931:0.136721 4932:0.137915 4933:0.131399 4934:0.142149 4935:0.144236 4936:0.107154 4937:0.151425 4938:0.14113 4939:0.128639 4940:0.113328 4941:0.151572 4942:0.145959 4943:0.143626 4944:0.104143 4945:0.146899 4946:0.142962 4947:0.158952 4948:0.232923 4949:0.219999 4950:0.199524 4951:0.157625 4952:0.131405 4953:0.139773 4954:0.175543 4955:0.238667 4956:0.295005 4957:0.290868 4958:0.267624 4959:0.174549 4960:0.150082 4961:0.148315 4962:0.276472 4963:0.263748 4964:0.296081 4965:0.279298 4966:0.279197 4967:0.178967 4968:0.140336 4969:0.178175 4970:0.227928 4971:0.245013 4972:0.212891 4973:0.204234 4974:0.20353 4975:0.159086 4976:0.152556 4977:0.184476 4978:0.19051 4979:0.215273 4980:0.191466 4981:0.163155 4982:0.157842 4983:0.177129 4984:0.16838 4985:0.150692 4986:0.170932 4987:0.174502 4988:0.158283 4989:0.155857 4990:0.142976 4991:0.131096 4992:0.0779667 4993:0.115076 4994:0.12205 4995:0.126143 4996:0.126142 4997:0.121415 4998:0.136812 4999:0.129143 5000:0.125774 5001:0.157127 5002:0.14457 5003:0.138455 5004:0.142634 5005:0.150766 5006:0.13444 5007:0.14257 5008:0.145956 5009:0.158369 5010:0.149785 5011:0.151025 5012:0.229709 5013:0.212508 5014:0.213109 5015:0.163769 5016:0.146144 5017:0.171111 5018:0.188003 5019:0.219603 5020:0.232917 5021:0.232525 5022:0.235536 5023:0.179452 5024:0.159587 5025:0.170048 5026:0.242003 5027:0.209663 5028:0.203277 5029:0.159996 5030:0.21799 5031:0.165716 5032:0.183557 5033:0.173071 5034:0.160022 5035:0.195358 5036:0.188726 5037:0.189519 5038:0.148547 5039:0.167265 5040:0.193672 5041:0.206902 5042:0.163806 5043:0.151066 5044:0.139663 5045:0.160683 5046:0.149859 5047:0.186761 5048:0.185022 5049:0.219265 5050:0.194007 5051:0.178283 5052:0.168489 5053:0.169997 5054:0.16241 5055:0.177943 5056:0.179912 5057:0.0121924 5058:0 5059:0 5060:0 5061:0 5062:0 5063:0 5064:0 5065:0 5066:0 5067:0 5068:0 5069:0 5070:0 5071:0 5072:0 5073:0 5074:0 5075:0 5076:0 5077:0 5078:0 5079:0 5080:0 5081:0 5082:0 5083:0 5084:0 5085:0 5086:0 5087:0 5088:0 5089:0 5090:0 5091:0 5092:0 5093:0 5094:0 5095:0 5096:0 5097:0 5098:0 5099:0 5100:0 5101:0 5102:0 5103:0 5104:0 5105:0 5106:0 5107:0 5108:0 5109:0 5110:0 5111:0 5112:0 5113:0 5114:0 5115:0 5116:0 5117:0 5118:0 5119:0 5120:0 5121:0.108274 5122:0.118248 5123:0.105157 5124:0.110454 5125:0.103812 5126:0.109259 5127:0.110959 5128:0.112476 5129:0.122292 5130:0.0637837 5131:0.0569352 5132:0.0684237 5133:0.0748985 5134:0.0825608 5135:0.0602738 5136:0.0593561 5137:0.141204 5138:0.0648449 5139:0.0738272 5140:0.0814597 5141:0.0843304 5142:0.0798837 5143:0.0676743 5144:0.0558656 5145:0.142175 5146:0.0666535 5147:0.100464 5148:0.0802774 5149:0.0657211 5150:0.0762817 5151:0.0568979 5152:0.0573399 5153:0.14041 5154:0.0837505 5155:0.105658 5156:0.0850611 5157:0.0888604 5158:0.0776393 5159:0.050883 5160:0.053383 5161:0.144297 5162:0.111039 5163:0.0701942 5164:0.0478324 5165:0.068777 5166:0.0820641 5167:0.0588395 5168:0.0444484 5169:0.156282 5170:0.0707698 5171:0.0871618 5172:0.0717524 5173:0.0595882 5174:0.0716746 5175:0.05371 5176:0.0664696 5177:0.158224 5178:0.0376944 5179:0.0424919 5180:0.0465152 5181:0.056321 5182:0.0551461 5183:0.0522975 5184:0.0613844 5185:0 5186:0 5187:0 5188:0 5189:0 5190:0 5191:0 5192:0 5193:0 5194:0 5195:0 5196:0 5197:0 5198:0 5199:0 5200:0 5201:0 5202:0 5203:0 5204:0 5205:0 5206:0 5207:0 5208:0 5209:0 5210:0 5211:0 5212:0 5213:0 5214:0 5215:0 5216:0 5217:0 5218:0.0305537 5219:0 5220:0 5221:0 5222:0 5223:0 5224:0 5225:0 5226:0 5227:0 5228:0 5229:0 5230:0 5231:0 5232:0 5233:0 5234:0 5235:0 5236:0 5237:0 5238:0 5239:0 5240:0 5241:0 5242:0 5243:0 5244:0 5245:0 5246:0 5247:0 5248:0 5249:0.0838375 5250:0.086942 5251:0.0919286 5252:0.0829346 5253:0.0836952 5254:0.0895415 5255:0.0921868 5256:0.0802604 5257:0.0859665 5258:0.0990568 5259:0.0973242 5260:0.0965964 5261:0.0953704 5262:0.106918 5263:0.10252 5264:0.088045 5265:0.125362 5266:0.10768 5267:0.107119 5268:0.107806 5269:0.126396 5270:0.117808 5271:0.11652 5272:0.0900113 5273:0.120468 5274:0.092263 5275:0.129198 5276:0.186196 5277:0.18651 5278:0.158612 5279:0.133387 5280:0.103021 5281:0.109869 5282:0.126768 5283:0.164208 5284:0.180529 5285:0.183495 5286:0.197045 5287:0.172387 5288:0.0995887 5289:0.108877 5290:0.169463 5291:0.167579 5292:0.132431 5293:0.118029 5294:0.161034 5295:0.14599 5296:0.100165 5297:0.133808 5298:0.120504 5299:0.130471 5300:0.151287 5301:0.113186 5302:0.109548 5303:0.106094 5304:0.129813 5305:0.136334 5306:0.100055 5307:0.100847 5308:0.0928159 5309:0.100335 5310:0.095421 5311:0.102568 5312:0.129261 5313:0.0950048 5314:0.0994154 5315:0.100609 5316:0.0950365 5317:0.1024 5318:0.105396 5319:0.107884 5320:0.0990669 5321:0.105709 5322:0.111229 5323:0.109246 5324:0.13189 5325:0.131522 5326:0.113361 5327:0.124385 5328:0.119189 5329:0.123969 5330:0.116037 5331:0.136908 5332:0.126829 5333:0.140087 5334:0.125376 5335:0.118305 5336:0.114795 5337:0.119865 5338:0.151712 5339:0.148308 5340:0.151058 5341:0.165754 5342:0.17004 5343:0.146913 5344:0.115218 5345:0.128988 5346:0.158035 5347:0.153853 5348:0.134657 5349:0.164404 5350:0.174756 5351:0.175685 5352:0.128964 5353:0.133547 5354:0.111766 5355:0.156909 5356:0.152465 5357:0.134595 5358:0.12042 5359:0.148235 5360:0.122681 5361:0.123303 5362:0.107407 5363:0.133294 5364:0.142365 5365:0.133396 5366:0.11705 5367:0.130358 5368:0.106658 5369:0.145643 5370:0.121824 5371:0.133426 5372:0.133947 5373:0.135004 5374:0.139644 5375:0.145767 5376:0.131086 5377:0.028955 5378:0.0536196 5379:0.0593073 5380:0.0596116 5381:0.0572043 5382:0.0576382 5383:0.0659129 5384:0.125697 5385:0.0558199 5386:0.0624344 5387:0.0655489 5388:0.0567873 5389:0.0865638 5390:0.0685223 5391:0.078425 5392:0.105915 5393:0.0768685 5394:0.0772404 5395:0.0747894 5396:0.064035 5397:0.0685771 5398:0.0793714 5399:0.0719532 5400:0.106328 5401:0.0793839 5402:0.0801831 5403:0.0518644 5404:0.0862457 5405:0.131077 5406:0.113699 5407:0.129012 5408:0.105368 5409:0.0747922 5410:0.0730427 5411:0.112485 5412:0.156432 5413:0.152278 5414:0.12888 5415:0.140381 5416:0.102709 5417:0.0708824 5418:0.114748 5419:0.134814 5420:0.138752 5421:0.10065 5422:0.110101 5423:0.101775 5424:0.108101 5425:0.0813887 5426:0.0864518 5427:0.100688 5428:0.10511 5429:0.0888524 5430:0.0806815 5431:0.0761852 5432:0.120025 5433:0.0676868 5434:0.0755346 5435:0.0715942 5436:0.0757631 5437:0.0711262 5438:0.0792621 5439:0.0733981 5440:0.109063 5441:0.122117 5442:0.0992936 5443:0.0885025 5444:0.0765807 5445:0.0858361 5446:0.0957117 5447:0.0748615 5448:0.0617175 5449:0.122782 5450:0.094618 5451:0.0900385 5452:0.118624 5453:0.151255 5454:0.103565 5455:0.0964013 5456:0.110106 5457:0.128089 5458:0.0957829 5459:0.149615 5460:0.196174 5461:0.137123 5462:0.14217 5463:0.09888 5464:0.107836 5465:0.107668 5466:0.168537 5467:0.179895 5468:0.171864 5469:0.13908 5470:0.126447 5471:0.0943296 5472:0.0928015 5473:0.106422 5474:0.210773 5475:0.156092 5476:0.125008 5477:0.0727081 5478:0.121079 5479:0.0522345 5480:0.111665 5481:0.116155 5482:0.128117 5483:0.129134 5484:0.107095 5485:0.0996064 5486:0.0712646 5487:0.144703 5488:0.149478 5489:0.139062 5490:0.0946144 5491:0.0834471 5492:0.073807 5493:0.0966704 5494:0.109403 5495:0.138167 5496:0.119407 5497:0.147172 5498:0.137538 5499:0.103855 5500:0.104654 5501:0.10346 5502:0.0846062 5503:0.0871999 5504:0.0910514 5505:0.0831966 5506:0.0983779 5507:0.0908606 5508:0.0894595 5509:0.0880017 5510:0.106884 5511:0.107925 5512:0.0818951 5513:0.102085 5514:0.0860755 5515:0.0743649 5516:0.0718274 5517:0.112466 5518:0.0926853 5519:0.107081 5520:0.065297 5521:0.113937 5522:0.0816608 5523:0.117203 5524:0.16148 5525:0.164996 5526:0.14342 5527:0.0988933 5528:0.0869606 5529:0.119753 5530:0.0768726 5531:0.172266 5532:0.246452 5533:0.242178 5534:0.189925 5535:0.166086 5536:0.0945764 5537:0.114744 5538:0.210909 5539:0.21776 5540:0.210874 5541:0.205016 5542:0.210101 5543:0.200678 5544:0.0776963 5545:0.13024 5546:0.162979 5547:0.20716 5548:0.177961 5549:0.134345 5550:0.13498 5551:0.117692 5552:0.0831604 5553:0.143971 5554:0.124695 5555:0.130587 5556:0.139168 5557:0.103396 5558:0.107226 5559:0.10165 5560:0.145642 5561:0.127659 5562:0.0916867 5563:0.0930991 5564:0.0931391 5565:0.0959007 5566:0.08713 5567:0.0724928 5568:0.11697 5569:0 5570:0 5571:0 5572:0 5573:0 5574:0 5575:0 5576:0 5577:0 5578:0 5579:0 5580:0 5581:0 5582:0 5583:0 5584:0 5585:0 5586:0 5587:0 5588:0 5589:0 5590:0 5591:0 5592:0 5593:0 5594:0 5595:0 5596:0 5597:0 5598:0 5599:0 5600:0 5601:0 5602:0 5603:0 5604:0 5605:0 5606:0 5607:0 5608:0 5609:0 5610:0 5611:0 5612:0 5613:0 5614:0 5615:0 5616:0 5617:0 5618:0 5619:0 5620:0 5621:0 5622:0 5623:0 5624:0 5625:0 5626:0 5627:0 5628:0 5629:0 5630:0 5631:0 5632:0 5633:0 5634:0 5635:0 5636:0 5637:0 5638:0 5639:0 5640:0 5641:0 5642:0 5643:0 5644:0 5645:0 5646:0 5647:0 5648:0 5649:0 5650:0 5651:0 5652:0 5653:0 5654:0 5655:0 5656:0 5657:0 5658:0 5659:0 5660:0 5661:0 5662:0 5663:0 5664:0 5665:0 5666:0 5667:0 5668:0 5669:0 5670:0 5671:0 5672:0 5673:0 5674:0 5675:0 5676:0 5677:0 5678:0 5679:0 5680:0 5681:0.0147538 5682:0 5683:0 5684:0 5685:0 5686:0 5687:0 5688:0 5689:0.00467721 5690:0 5691:0 5692:0 5693:0 5694:0 5695:0 5696:0 5697:0.0571354 5698:0.0834594 5699:0.0895948 5700:0.0848521 5701:0.0817116 5702:0.0944471 5703:0.0887728 5704:0.104323 5705:0.0859763 5706:0.0808875 5707:0.0873538 5708:0.0788542 5709:0.078668 5710:0.0757357 5711:0.0859849 5712:0.0737494 5713:0.10886 5714:0.0870097 5715:0.0988456 5716:0.0943711 5717:0.0956201 5718:0.132589 5719:0.0841626 5720:0.0927655 5721:0.117195 5722:0.0862346 5723:0.103596 5724:0.146239 5725:0.142151 5726:0.166938 5727:0.132704 5728:0.0948874 5729:0.0979177 5730:0.116153 5731:0.122516 5732:0.117891 5733:0.127012 5734:0.167623 5735:0.142431 5736:0.113131 5737:0.0939481 5738:0.0985266 5739:0.141048 5740:0.121668 5741:0.101422 5742:0.110927 5743:0.124939 5744:0.131514 5745:0.100619 5746:0.100161 5747:0.101994 5748:0.114186 5749:0.094225 5750:0.101306 5751:0.0999461 5752:0.134613 5753:0.107231 5754:0.111602 5755:0.0914286 5756:0.0772887 5757:0.0911826 5758:0.0869623 5759:0.0950554 5760:0.109677 5761:0 5762:0 5763:0 5764:0 5765:0 5766:0 5767:0 5768:0.0360804 5769:0 5770:0 5771:0 5772:0 5773:0 5774:0 5775:0 5776:0 5777:0 5778:0 5779:0 5780:0 5781:0 5782:0 5783:0 5784:0 5785:0 5786:0 5787:0 5788:0 5789:0 5790:0 5791:0 5792:0 5793:0 5794:0 5795:0 5796:0 5797:0 5798:0 5799:0 5800:0 5801:0 5802:0 5803:0 5804:0 5805:0 5806:0 5807:0 5808:0 5809:0 5810:0 5811:0 5812:0 5813:0 5814:0 5815:0 5816:0 5817:0 5818:0 5819:0 5820:0 5821:0 5822:0 5823:0 5824:0 5825:0.0272244 5826:0 5827:0 5828:0 5829:0 5830:0 5831:0 5832:0 5833:0.0341965 5834:0 5835:0 5836:0 5837:0 5838:0 5839:0 5840:0 5841:0.0236772 5842:0 5843:0 5844:0 5845:0 5846:0 5847:0 5848:0 5849:0.0202426 5850:0 5851:0 5852:0 5853:0 5854:0 5855:0 5856:0 5857:0.0226896 5858:0 5859:0 5860:0 5861:0 5862:0 5863:0 5864:0 5865:0.029758 5866:0 5867:0 5868:0 5869:0 5870:0 5871:0 5872:0 5873:0.0586226 5874:0 5875:0 5876:0 5877:0 5878:0 5879:0 5880:0 5881:0.037815 5882:0 5883:0 5884:0 5885:0 5886:0 5887:0 5888:0 5889:0 5890:0.00800459 5891:0.0125468 5892:0.0119462 5893:0.00725755 5894:0.00898012 5895:0.0103349 5896:0.0630702 5897:0.0118398 5898:0.0119637 5899:0.0330958 5900:0.0126924 5901:0.0219715 5902:0.0286927 5903:0.0157038 5904:0.02784 5905:0.0159117 5906:0.0259839 5907:0.00174537 5908:0 5909:0 5910:0.0151567 5911:0.0277902 5912:0.0530088 5913:0.0235116 5914:0.0177835 5915:0 5916:0 5917:0 5918:0 5919:0.0485441 5920:0.0385765 5921:0.0150621 5922:0 5923:0.0133199 5924:0.0346054 5925:0.00375529 5926:0 5927:0.0242788 5928:0.0385121 5929:0.013246 5930:0.0183858 5931:0 5932:0.0264014 5933:0.0281005 5934:0.030745 5935:0 5936:0.0415911 5937:0.0347622 5938:0.0368932 5939:0.0210986 5940:0.00741996 5941:0.00775697 5942:0.0150233 5943:0 5944:0.0510745 5945:0.0413799 5946:0.0421282 5947:0.0517205 5948:0.0390604 5949:0.0368096 5950:0.0411367 5951:0.0324735 5952:0.0378133 5953:0.0651023 5954:0.0392203 5955:0.046172 5956:0.0432154 5957:0.0445221 5958:0.0448857 5959:0.0303565 5960:0.0396317 5961:0.0752295 5962:0.0430574 5963:0.0365423 5964:0.0547112 5965:0.0601459 5966:0.050362 5967:0.0503677 5968:0.0446284 5969:0.07492 5970:0.0387983 5971:0.0805041 5972:0.0587723 5973:0.0385829 5974:0.0394227 5975:0.0471375 5976:0.0332996 5977:0.0668428 5978:0.0501187 5979:0.0973645 5980:0.0596481 5981:0.0371088 5982:0.0344771 5983:0.0180649 5984:0.0368491 5985:0.0723487 5986:0.0927128 5987:0.0450811 5988:0.0321593 5989:0.0712853 5990:0.0882519 5991:0.0562314 5992:0.0554063 5993:0.0847814 5994:0.0743709 5995:0.0846244 5996:0.0333989 5997:0.0595091 5998:0.0495334 5999:0.0892195 6000:0.0674499 6001:0.0989203 6002:0.0547417 6003:0.052632 6004:0.0557519 6005:0.0562923 6006:0.0485276 6007:0.0705582 6008:0.0740398 6009:0.0905929 6010:0.0711094 6011:0.0617251 6012:0.0639487 6013:0.0593526 6014:0.0669917 6015:0.072115 6016:0.0556093 6017:0 6018:0 6019:0 6020:0 6021:0 6022:0 6023:0 6024:0 6025:0 6026:0 6027:0 6028:0 6029:0 6030:0 6031:0 6032:0 6033:0 6034:0 6035:0 6036:0 6037:0 6038:0 6039:0 6040:0 6041:0 6042:0 6043:0 6044:0 6045:0 6046:0 6047:0 6048:0 6049:0 6050:0 6051:0 6052:0 6053:0 6054:0 6055:0 6056:0 6057:0 6058:0 6059:0 6060:0 6061:0 6062:0 6063:0 6064:0 6065:0.00471688 6066:0 6067:0 6068:0 6069:0 6070:0 6071:0 6072:0 6073:0 6074:0 6075:0 6076:0 6077:0 6078:0 6079:0 6080:0.0155143 6081:0.130646 6082:0.176435 6083:0.178959 6084:0.184126 6085:0.177567 6086:0.192115 6087:0.195885 6088:0.180402 6089:0.168151 6090:0.166592 6091:0.170711 6092:0.165994 6093:0.185973 6094:0.18128 6095:0.180123 6096:0.160643 6097:0.18182 6098:0.189105 6099:0.175422 6100:0.223789 6101:0.243115 6102:0.209254 6103:0.195557 6104:0.164366 6105:0.181756 6106:0.18905 6107:0.261248 6108:0.317985 6109:0.335088 6110:0.294478 6111:0.279963 6112:0.180969 6113:0.160941 6114:0.253263 6115:0.297813 6116:0.333 6117:0.333132 6118:0.294115 6119:0.28545 6120:0.163873 6121:0.176895 6122:0.258833 6123:0.291214 6124:0.265313 6125:0.242817 6126:0.253958 6127:0.210425 6128:0.179653 6129:0.207317 6130:0.212591 6131:0.242073 6132:0.234977 6133:0.207713 6134:0.185647 6135:0.199144 6136:0.218652 6137:0.18063 6138:0.212789 6139:0.210014 6140:0.200129 6141:0.193363 6142:0.192793 6143:0.188121 6144:0.197412 6145:0.0286016 6146:0.00994086 6147:0 6148:0.000908997 6149:0 6150:0.00117782 6151:0 6152:0.024403 6153:0.0240335 6154:0.000533355 6155:0.0168381 6156:0.00163049 6157:0.0135632 6158:0.00148366 6159:0.000844689 6160:0.0447143 6161:0.0171136 6162:0 6163:0.0123991 6164:0.00465677 6165:0.00992234 6166:0.00212074 6167:0.00248604 6168:0.0398395 6169:0.0159338 6170:0.00990406 6171:0.0184546 6172:0 6173:0 6174:0 6175:0.00368873 6176:0.0439429 6177:0.0156242 6178:0.0187553 6179:0.00405558 6180:0 6181:0 6182:0 6183:0 6184:0.037599 6185:0.0208162 6186:0 6187:0 6188:0 6189:0 6190:0 6191:0 6192:0.0461141 6193:0.0138245 6194:0 6195:0 6196:0 6197:0 6198:0 6199:0 6200:0.0302866 6201:0.0533169 6202:0.00835219 6203:0.00878999 6204:0.0172273 6205:0.0180716 6206:0.0244094 6207:0.0305347 6208:0.0422523 6209:0.0879976 6210:0.0738992 6211:0.0475315 6212:0.0549581 6213:0.0603067 6214:0.0572203 6215:0.0591115 6216:0.0421866 6217:0.089434 6218:0.0340958 6219:0.0260922 6220:0.0472548 6221:0.0341121 6222:0.036311 6223:0.0153359 6224:0.0142114 6225:0.111224 6226:0.0259857 6227:0.0348491 6228:0.032076 6229:0.0405111 6230:0.0292439 6231:0.0325736 6232:0.018741 6233:0.113182 6234:0.036832 6235:0.0518807 6236:0.0427011 6237:0 6238:0.0188081 6239:0.0228873 6240:0.0145941 6241:0.115455 6242:0.0545062 6243:0.0414563 6244:0.0266643 6245:0.0207332 6246:0.0307515 6247:0 6248:0.0275495 6249:0.109014 6250:0.0480934 6251:0.0193831 6252:0.00394299 6253:0.0187811 6254:0.0301731 6255:0.0215741 6256:0.0396298 6257:0.123941 6258:0.0291894 6259:0.0216953 6260:0.02296 6261:0.0235326 6262:0.0265206 6263:0.0455014 6264:0.0252755 6265:0.134002 6266:0.041948 6267:0.0369547 6268:0.0338305 6269:0.0313367 6270:0.0245202 6271:0.0303161 6272:0.0477297 6273:0.101809 6274:0.0922747 6275:0.0940002 6276:0.0897931 6277:0.0939438 6278:0.102333 6279:0.0965273 6280:0.0841769 6281:0.0739623 6282:0.0372756 6283:0.0341923 6284:0.0569911 6285:0.0305618 6286:0.0457033 6287:0.0401933 6288:0.0318417 6289:0.0772029 6290:0.0399659 6291:0.0782865 6292:0.0815952 6293:0.0650255 6294:0.0457213 6295:0.0369174 6296:0.0332802 6297:0.0777854 6298:0.0855649 6299:0.0941546 6300:0.0888938 6301:0.0825188 6302:0.0779367 6303:0.0596846 6304:0.0323784 6305:0.0739635 6306:0.110183 6307:0.0948793 6308:0.0604624 6309:0.0419868 6310:0.0449095 6311:0.022362 6312:0.035007 6313:0.0792199 6314:0.0287698 6315:0.0393221 6316:0.0477947 6317:0.0416406 6318:0.0179706 6319:0.0672157 6320:0.0441642 6321:0.0972645 6322:0.0299824 6323:0.0256592 6324:0.0256246 6325:0.0405913 6326:0.0479418 6327:0.0764954 6328:0.0368675 6329:0.112473 6330:0.0709986 6331:0.0395152 6332:0.0504068 6333:0.0519588 6334:0.0427033 6335:0.0438636 6336:0.0171457 6337:0 6338:0 6339:0 6340:0 6341:0 6342:0 6343:0 6344:0.00485831 6345:0 6346:0 6347:0 6348:0 6349:0 6350:0 6351:0 6352:0 6353:0 6354:0 6355:0 6356:0 6357:0 6358:0 6359:0 6360:0 6361:0 6362:0 6363:0 6364:0 6365:0 6366:0 6367:0 6368:0 6369:0 6370:0 6371:0 6372:0 6373:0 6374:0 6375:0 6376:0.00466683 6377:0 6378:0 6379:0 6380:0 6381:0 6382:0 6383:0 6384:0.019804 6385:0.0128561 6386:0 6387:0 6388:0 6389:0 6390:0 6391:0 6392:0 6393:0.027021 6394:0 6395:0 6396:0 6397:0 6398:0 6399:0 6400:0 6401:0.0326145 6402:0.0387392 6403:0.0417901 6404:0.0442169 6405:0.0471798 6406:0.0453534 6407:0.0573996 6408:0.0530901 6409:0.0193297 6410:0.0341638 6411:0.049595 6412:0.0391539 6413:0.0541832 6414:0.0539191 6415:0.0482302 6416:0.0372311 6417:0.00993817 6418:0.0436318 6419:0.0237703 6420:0.0232635 6421:0 6422:0.0294675 6423:0.0314913 6424:0.0326212 6425:0.0273756 6426:0.0416852 6427:0.009482 6428:0 6429:0 6430:0 6431:0.0170836 6432:0.0324172 6433:0.00504191 6434:0.010191 6435:0.0294864 6436:0.0483372 6437:0.0542788 6438:0.0759479 6439:0.0498228 6440:0.0363824 6441:0.0285794 6442:0.0660712 6443:0.0449096 6444:0.053659 6445:0.0842217 6446:0.0925106 6447:0.0451703 6448:0.0352497 6449:0.0139934 6450:0.0542198 6451:0.0605763 6452:0.0578048 6453:0.0471335 6454:0.0396809 6455:0.0326903 6456:0.0291289 6457:0.0614492 6458:0.0707333 6459:0.0645452 6460:0.058864 6461:0.0611098 6462:0.0654896 6463:0.0538803 6464:0.0271718 6465:0.00155538 6466:0.0420876 6467:0.0435056 6468:0.0471963 6469:0.0423639 6470:0.0474669 6471:0.0497447 6472:0.0568838 6473:0.0245437 6474:0.0229442 6475:0.0209229 6476:0.0149525 6477:0.0373287 6478:0.0270201 6479:0.0305826 6480:0.0356533 6481:0.0255362 6482:0.0244472 6483:0.0235274 6484:0.0683861 6485:0.0592539 6486:0.0558481 6487:0.041224 6488:0.0459208 6489:0.0324074 6490:0.033403 6491:0.041651 6492:0.0736554 6493:0.0768875 6494:0.0907197 6495:0.0432267 6496:0.0570265 6497:0.0419882 6498:0.0555799 6499:0.0489288 6500:0.0629372 6501:0.0356029 6502:0.0595252 6503:0.0278814 6504:0.0506578 6505:0.0242284 6506:0.0542206 6507:0.0304297 6508:0.056143 6509:0.0374176 6510:0.0443073 6511:0.00409253 6512:0.0824957 6513:0.0562049 6514:0.040402 6515:0.0312548 6516:0.0308098 6517:0.0271187 6518:0.0346926 6519:0.0223985 6520:0.0668102 6521:0.0444618 6522:0.0619598 6523:0.0558647 6524:0.0400576 6525:0.0247267 6526:0.0244668 6527:0.0202903 6528:0.0509732 6529:0.0317257 6530:0.0290186 6531:0.024531 6532:0.0266751 6533:0.0274946 6534:0.0259772 6535:0.0236324 6536:0.0265683 6537:0.0172858 6538:0.0231071 6539:0.0157894 6540:0.0182915 6541:0.0191396 6542:0.0231033 6543:0.0209692 6544:0.0230785 6545:0.0102934 6546:0.0169154 6547:0.0305418 6548:0.021085 6549:0.0299681 6550:0.0138897 6551:0.0217354 6552:0.0334444 6553:0.00453637 6554:0.0256278 6555:0.0240477 6556:0 6557:0 6558:0 6559:0.0104717 6560:0.0305091 6561:0.0219666 6562:0.0332943 6563:0 6564:0 6565:0 6566:0 6567:0.030041 6568:0.0329548 6569:0.0108094 6570:0 6571:0 6572:0 6573:0.00132843 6574:0 6575:0.0374162 6576:0.018953 6577:0.0100416 6578:0.00431385 6579:0 6580:0.0180609 6581:0.0208313 6582:0.0192977 6583:0.0159534 6584:0.00806604 6585:0.0479119 6586:0.0598356 6587:0.0546991 6588:0.0580992 6589:0.0559918 6590:0.0522949 6591:0.0392447 6592:0.0485199 6593:0.128078 6594:0.125588 6595:0.123522 6596:0.123276 6597:0.125304 6598:0.128385 6599:0.13046 6600:0.124491 6601:0.135844 6602:0.115352 6603:0.110806 6604:0.116719 6605:0.121147 6606:0.115496 6607:0.115793 6608:0.102447 6609:0.150383 6610:0.122021 6611:0.118232 6612:0.134648 6613:0.131969 6614:0.121857 6615:0.123035 6616:0.114619 6617:0.156991 6618:0.12675 6619:0.138906 6620:0.202635 6621:0.188693 6622:0.160782 6623:0.123735 6624:0.12796 6625:0.1541 6626:0.193817 6627:0.159199 6628:0.222107 6629:0.194819 6630:0.222818 6631:0.136446 6632:0.131736 6633:0.147124 6634:0.21305 6635:0.201499 6636:0.155107 6637:0.173226 6638:0.19347 6639:0.148042 6640:0.132151 6641:0.175523 6642:0.14671 6643:0.156478 6644:0.161364 6645:0.125819 6646:0.12416 6647:0.132381 6648:0.147298 6649:0.169035 6650:0.171052 6651:0.135982 6652:0.13594 6653:0.127431 6654:0.129845 6655:0.150568 6656:0.136838 6657:0.043959 6658:0.0579925 6659:0.0390447 6660:0.0447701 6661:0.0388028 6662:0.0472149 6663:0.0477984 6664:0.0380596 6665:0.0637932 6666:0.0468545 6667:0.0448024 6668:0.035963 6669:0.0501875 6670:0.0499333 6671:0.0469414 6672:0.0358768 6673:0.0613313 6674:0.0453858 6675:0.0420678 6676:0.0667132 6677:0.0715009 6678:0.0700976 6679:0.0654149 6680:0.027965 6681:0.0616404 6682:0.0318959 6683:0.0849226 6684:0.0924661 6685:0.0844627 6686:0.0630497 6687:0.0584405 6688:0.0289901 6689:0.0550644 6690:0.0700175 6691:0.0748101 6692:0.0776116 6693:0.067121 6694:0.0665257 6695:0.0583611 6696:0.0364138 6697:0.0749764 6698:0.0879521 6699:0.043618 6700:0.0420851 6701:0.0817211 6702:0.0676695 6703:0.0598389 6704:0.0684057 6705:0.0895348 6706:0.0684444 6707:0.0637261 6708:0.0531794 6709:0.0478628 6710:0.045328 6711:0.0606236 6712:0.0528478 6713:0.0805475 6714:0.070244 6715:0.05399 6716:0.0479027 6717:0.0462038 6718:0.04975 6719:0.0461799 6720:0.0566083 6721:0 6722:0 6723:0 6724:0 6725:0 6726:0 6727:0 6728:0 6729:0 6730:0 6731:0 6732:0 6733:0 6734:0.0129684 6735:0 6736:0 6737:0 6738:0 6739:0 6740:0.0306771 6741:0.0203596 6742:0.013156 6743:0.0212119 6744:0 6745:0 6746:0 6747:0.0236738 6748:0.0220455 6749:0.00612545 6750:0.00672156 6751:0 6752:0 6753:0 6754:0.0072588 6755:0.0245613 6756:0.0356881 6757:0.0311693 6758:0.0306634 6759:0 6760:0 6761:0 6762:0.0343858 6763:0.0272378 6764:0 6765:0 6766:0.0458273 6767:0 6768:0 6769:0 6770:0.000416112 6771:0.0160589 6772:0.00590697 6773:0 6774:0 6775:0 6776:0 6777:0.0112334 6778:0 6779:0 6780:0 6781:0 6782:0 6783:0.00706908 6784:0.0230434 6785:0.0584561 6786:0.100681 6787:0.113898 6788:0.117943 6789:0.120677 6790:0.138891 6791:0.140197 6792:0.142129 6793:0.0750795 6794:0.103462 6795:0.113993 6796:0.11924 6797:0.126935 6798:0.135657 6799:0.127538 6800:0.108104 6801:0.099618 6802:0.125242 6803:0.121197 6804:0.134363 6805:0.1439 6806:0.12588 6807:0.112858 6808:0.1177 6809:0.117971 6810:0.124015 6811:0.146391 6812:0.209445 6813:0.230621 6814:0.210667 6815:0.175589 6816:0.12487 6817:0.0984851 6818:0.194457 6819:0.213703 6820:0.24274 6821:0.253216 6822:0.280976 6823:0.203572 6824:0.12713 6825:0.095495 6826:0.206513 6827:0.213814 6828:0.219503 6829:0.167842 6830:0.222626 6831:0.1603 6832:0.145633 6833:0.0921974 6834:0.154915 6835:0.192837 6836:0.175467 6837:0.137135 6838:0.129682 6839:0.134186 6840:0.188372 6841:0.108704 6842:0.156895 6843:0.150758 6844:0.153585 6845:0.145223 6846:0.139031 6847:0.134855 6848:0.187866 6849:0.0168711 6850:0.0598008 6851:0.0639284 6852:0.0625776 6853:0.0704065 6854:0.0731061 6855:0.0789068 6856:0.0703903 6857:0.0448477 6858:0.042921 6859:0.0366443 6860:0.0560833 6861:0.0434889 6862:0.0538452 6863:0.0502198 6864:0.0430995 6865:0.0730034 6866:0.0602261 6867:0.073111 6868:0.0875013 6869:0.102238 6870:0.0846309 6871:0.0559277 6872:0.0507746 6873:0.0648372 6874:0.0881349 6875:0.108084 6876:0.186259 6877:0.202506 6878:0.2011 6879:0.120227 6880:0.0582573 6881:0.0687093 6882:0.128613 6883:0.182306 6884:0.173784 6885:0.181864 6886:0.162634 6887:0.132686 6888:0.0574528 6889:0.0571467 6890:0.122259 6891:0.15694 6892:0.136713 6893:0.0857622 6894:0.092559 6895:0.0940607 6896:0.0667218 6897:0.0723914 6898:0.0628638 6899:0.111256 6900:0.117268 6901:0.074596 6902:0.0687091 6903:0.0934153 6904:0.077693 6905:0.0824379 6906:0.0491184 6907:0.0554501 6908:0.0481963 6909:0.0494381 6910:0.0483183 6911:0.0599834 6912:0.0705025 6913:0.186012 6914:0.208996 6915:0.219389 6916:0.207893 6917:0.214572 6918:0.21618 6919:0.221699 6920:0.208839 6921:0.210006 6922:0.206169 6923:0.202332 6924:0.207852 6925:0.227247 6926:0.216976 6927:0.218638 6928:0.210219 6929:0.239309 6930:0.212147 6931:0.266178 6932:0.247639 6933:0.283284 6934:0.243172 6935:0.222638 6936:0.211412 6937:0.226522 6938:0.264426 6939:0.266473 6940:0.295507 6941:0.288235 6942:0.309317 6943:0.245939 6944:0.214902 6945:0.256213 6946:0.281235 6947:0.291589 6948:0.273418 6949:0.329338 6950:0.33386 6951:0.294805 6952:0.247918 6953:0.24922 6954:0.272874 6955:0.338563 6956:0.290451 6957:0.252147 6958:0.267324 6959:0.277301 6960:0.247149 6961:0.257324 6962:0.246344 6963:0.250294 6964:0.268228 6965:0.243405 6966:0.236678 6967:0.240932 6968:0.218713 6969:0.252776 6970:0.230246 6971:0.237591 6972:0.228895 6973:0.228598 6974:0.215387 6975:0.21185 6976:0.215796 6977:0 6978:0.0219282 6979:0.0296258 6980:0.0220102 6981:0.0195231 6982:0.0303666 6983:0.03039 6984:0.111787 6985:0 6986:0 6987:0 6988:0 6989:0.0205877 6990:0 6991:0.00441706 6992:0.0857103 6993:0 6994:0.00639672 6995:0 6996:0.0172973 6997:0.00242018 6998:0.0436291 6999:0.0306801 7000:0.0787061 7001:0 7002:0.0101325 7003:0 7004:0 7005:0.0504629 7006:0.0202147 7007:0.10799 7008:0.0830171 7009:0 7010:0 7011:0.0237075 7012:0.0291248 7013:0.00853654 7014:0 7015:0.0937314 7016:0.0845768 7017:0 7018:0 7019:0.0262846 7020:0.0489239 7021:0.0276094 7022:0 7023:0.0610275 7024:0.0897189 7025:0.0163725 7026:0.0043496 7027:0 7028:0.0191576 7029:0.0378667 7030:0 7031:0 7032:0.0834473 7033:0.00579672 7034:0.00796098 7035:0.00426815 7036:0 7037:0.000444007 7038:0.00207586 7039:0 7040:0.0727473 7041:0.0351144 7042:0.0308799 7043:0.0270317 7044:0.0246713 7045:0.0294892 7046:0.0326214 7047:0.0329609 7048:0.0162183 7049:0.0243597 7050:0 7051:0 7052:0.00901935 7053:0.0145328 7054:0.00766467 7055:0 7056:0 7057:0.0333779 7058:0 7059:0.0259017 7060:0.0346217 7061:0.0398146 7062:0.0120606 7063:0.0132457 7064:0 7065:0.0348511 7066:0.0458355 7067:0.0509748 7068:0.0620289 7069:0.0397711 7070:0.0283465 7071:0.00199816 7072:0 7073:0.0300118 7074:0.0825587 7075:0.0413313 7076:0.0467992 7077:0 7078:0.0122975 7079:0 7080:0 7081:0.0310912 7082:0.0484034 7083:0 7084:0 7085:0 7086:0 7087:0 7088:0 7089:0.0503681 7090:0.00773222 7091:0.0108556 7092:0 7093:0 7094:0 7095:0.0249558 7096:0 7097:0.0595695 7098:0.00904752 7099:0 7100:0 7101:0 7102:0 7103:0.00494591 7104:0 7105:0.0598834 7106:0.051756 7107:0.0458573 7108:0.0405467 7109:0.0416806 7110:0.0452363 7111:0.0449257 7112:0.0352873 7113:0.0740965 7114:0.0429347 7115:0.0425117 7116:0.0598987 7117:0.0535647 7118:0.0515206 7119:0.0453153 7120:0.0287497 7121:0.0978302 7122:0.0352674 7123:0.0480475 7124:0.056539 7125:0.0689046 7126:0.0297142 7127:0.0564883 7128:0.031989 7129:0.0936046 7130:0.0284065 7131:0.0743048 7132:0.0660833 7133:0.0265616 7134:0.0400114 7135:0.0328722 7136:0.0426109 7137:0.0885968 7138:0.0425468 7139:0.048874 7140:0.0415247 7141:0.0506921 7142:0.052313 7143:0.0590668 7144:0.0507684 7145:0.0955805 7146:0.0592513 7147:0.0715813 7148:0.0388193 7149:0.0406391 7150:0.0946556 7151:0.0786315 7152:0.0570947 7153:0.122528 7154:0.0651297 7155:0.0593377 7156:0.0593547 7157:0.0509221 7158:0.0488878 7159:0.0538296 7160:0.0610835 7161:0.133789 7162:0.0831143 7163:0.0615276 7164:0.0561599 7165:0.058773 7166:0.0486184 7167:0.0450652 7168:0.0848672 7169:0.12252 7170:0.136941 7171:0.129861 7172:0.137928 7173:0.138534 7174:0.14035 7175:0.135746 7176:0.128925 7177:0.14564 7178:0.146594 7179:0.139507 7180:0.150785 7181:0.149427 7182:0.145174 7183:0.13959 7184:0.122234 7185:0.140914 7186:0.139044 7187:0.157257 7188:0.199752 7189:0.19829 7190:0.159617 7191:0.172523 7192:0.128138 7193:0.132211 7194:0.144082 7195:0.206568 7196:0.227282 7197:0.211649 7198:0.197966 7199:0.160406 7200:0.129206 7201:0.135963 7202:0.205211 7203:0.202635 7204:0.189881 7205:0.160338 7206:0.172946 7207:0.109405 7208:0.13038 7209:0.135547 7210:0.174371 7211:0.174917 7212:0.161951 7213:0.148407 7214:0.159852 7215:0.111981 7216:0.149971 7217:0.134011 7218:0.161721 7219:0.1618 7220:0.132566 7221:0.155453 7222:0.139509 7223:0.156339 7224:0.163172 7225:0.136483 7226:0.14731 7227:0.141895 7228:0.133553 7229:0.13855 7230:0.133882 7231:0.157597 7232:0.130394 7233:0 7234:0 7235:0 7236:0 7237:0 7238:0 7239:0.00629766 7240:0.0036552 7241:0 7242:0 7243:0 7244:0 7245:0 7246:0 7247:0 7248:0 7249:0 7250:0 7251:0 7252:0 7253:0 7254:0 7255:0 7256:0.00582573 7257:0.000135452 7258:0 7259:0.00274701 7260:0.0203651 7261:0.00854593 7262:0.00127861 7263:0 7264:0 7265:0 7266:0 7267:0.0233273 7268:0.0434238 7269:0.0424422 7270:0.049789 7271:0.0172787 7272:0.00733705 7273:0 7274:0.0313721 7275:0.0289997 7276:0.0245507 7277:0.0128339 7278:0.0373182 7279:0 7280:0.00388357 7281:0 7282:0 7283:0.00749173 7284:0.014854 7285:0 7286:0 7287:0 7288:0.00869508 7289:0.0282046 7290:0.00807476 7291:0.0198915 7292:0.0130517 7293:0.0138043 7294:0.014908 7295:0.0183369 7296:0.0173344 7297:0 7298:0.0158768 7299:0.0211248 7300:0.0132657 7301:0.0159294 7302:0.0140829 7303:0.0246655 7304:0.0462159 7305:0 7306:0.00683147 7307:0.00931348 7308:0.011771 7309:0.024513 7310:0.0290107 7311:0.0189115 7312:0.0434423 7313:0.0257677 7314:0.021646 7315:0.0086646 7316:0.0362446 7317:0.035703 7318:0.039701 7319:0.0340601 7320:0.0417258 7321:0.0279169 7322:0.0157554 7323:0.00983417 7324:0.0257772 7325:0.0462246 7326:0.0677156 7327:0.0672097 7328:0.0570077 7329:0.0184914 7330:0.019739 7331:0.0546232 7332:0.0573547 7333:0.0705701 7334:0.0502027 7335:0.0661147 7336:0.0483774 7337:0.0152175 7338:0.0116985 7339:0.0589095 7340:0.0431929 7341:0.0292028 7342:0.0405458 7343:0.0460386 7344:0.0567775 7345:0.0338849 7346:0.0300771 7347:0.0286794 7348:0.0381368 7349:0.0242634 7350:0.0194194 7351:0.00774499 7352:0.0681694 7353:0.0215188 7354:0.00552488 7355:0.0218223 7356:0.0218423 7357:0.0186918 7358:0.0180287 7359:0.0403978 7360:0.0640332 7361:0.0295146 7362:0.0166507 7363:0.00881671 7364:0.0100101 7365:0.0100135 7366:0.00978149 7367:0.00248866 7368:0.0050382 7369:0 7370:0 7371:0 7372:0 7373:0 7374:0 7375:0 7376:0 7377:0 7378:0 7379:0.000808992 7380:0.00925497 7381:0.00688922 7382:0.00731989 7383:0 7384:0 7385:0 7386:0.0102419 7387:0 7388:0.00416701 7389:0 7390:0.0370553 7391:0 7392:0 7393:0 7394:0.0102616 7395:0 7396:0 7397:0 7398:0 7399:0.00256707 7400:0 7401:0 7402:0 7403:0 7404:0 7405:0 7406:0 7407:0 7408:0 7409:0 7410:0 7411:0 7412:0 7413:0 7414:0 7415:0 7416:0.00362308 7417:0 7418:0 7419:0 7420:0 7421:0 7422:0 7423:0 7424:0 7425:0 7426:0 7427:0 7428:0 7429:0 7430:0 7431:0 7432:0 7433:0 7434:0 7435:0 7436:0 7437:0 7438:0 7439:0 7440:0 7441:0 7442:0 7443:0 7444:0 7445:0 7446:0 7447:0 7448:0 7449:0 7450:0 7451:0 7452:0 7453:0 7454:0 7455:0 7456:0 7457:0 7458:0.0228706 7459:0 7460:0.00779371 7461:0 7462:0.00629557 7463:0 7464:0 7465:0 7466:0 7467:0 7468:0 7469:0 7470:0 7471:0 7472:0 7473:0 7474:0 7475:0 7476:0 7477:0 7478:0 7479:0 7480:0 7481:0 7482:0 7483:0 7484:0 7485:0 7486:0 7487:0 7488:0.0173257 7489:0.0867896 7490:0.0946083 7491:0.0977503 7492:0.0898633 7493:0.0931441 7494:0.0941886 7495:0.0954354 7496:0.148864 7497:0.0939817 7498:0.0951495 7499:0.0964482 7500:0.103557 7501:0.0949475 7502:0.103396 7503:0.105788 7504:0.155903 7505:0.109634 7506:0.0982165 7507:0.118689 7508:0.115938 7509:0.124093 7510:0.115449 7511:0.102608 7512:0.15668 7513:0.11572 7514:0.129091 7515:0.134775 7516:0.154536 7517:0.151999 7518:0.165266 7519:0.141098 7520:0.158237 7521:0.124305 7522:0.145104 7523:0.155874 7524:0.147905 7525:0.161377 7526:0.166266 7527:0.172788 7528:0.1716 7529:0.100773 7530:0.0998604 7531:0.146986 7532:0.141753 7533:0.108573 7534:0.120403 7535:0.14317 7536:0.184395 7537:0.106028 7538:0.0990752 7539:0.0992877 7540:0.132258 7541:0.104405 7542:0.108081 7543:0.123762 7544:0.2204 7545:0.12142 7546:0.0923788 7547:0.0879814 7548:0.0899147 7549:0.0919686 7550:0.0867851 7551:0.0867598 7552:0.205199 7553:0 7554:0 7555:0 7556:0 7557:0 7558:0 7559:0 7560:0 7561:0 7562:0 7563:0 7564:0 7565:0 7566:0 7567:0 7568:0 7569:0 7570:0 7571:0 7572:0 7573:0 7574:0 7575:0 7576:0 7577:0 7578:0 7579:0 7580:0 7581:0 7582:0 7583:0 7584:0 7585:0 7586:0 7587:0 7588:0 7589:0 7590:0 7591:0 7592:0 7593:0 7594:0 7595:0 7596:0 7597:0 7598:0 7599:0 7600:0 7601:0 7602:0 7603:0 7604:0 7605:0 7606:0 7607:0 7608:0 7609:0.0279013 7610:0 7611:0 7612:0 7613:0 7614:0 7615:0 7616:0.0131934 7617:0.0475114 7618:0.0839595 7619:0.0848804 7620:0.0833359 7621:0.0759434 7622:0.0797394 7623:0.0924394 7624:0.0949287 7625:0.0639126 7626:0.0688887 7627:0.071747 7628:0.0730941 7629:0.0796473 7630:0.0763161 7631:0.0895023 7632:0.0732602 7633:0.0687525 7634:0.0619463 7635:0.0706245 7636:0.0680609 7637:0.087225 7638:0.0654935 7639:0.0795647 7640:0.0677503 7641:0.0871059 7642:0.0754141 7643:0.0803294 7644:0.0748293 7645:0.0663231 7646:0.0866629 7647:0.0567061 7648:0.0732128 7649:0.0841184 7650:0.0562685 7651:0.0866419 7652:0.106328 7653:0.124449 7654:0.113665 7655:0.114795 7656:0.0717909 7657:0.069707 7658:0.0850693 7659:0.126919 7660:0.0990027 7661:0.105892 7662:0.128736 7663:0.144429 7664:0.0802796 7665:0.0700223 7666:0.0797019 7667:0.084189 7668:0.107793 7669:0.0891988 7670:0.0844238 7671:0.0692507 7672:0.091111 7673:0.0864147 7674:0.0819044 7675:0.0780913 7676:0.0779366 7677:0.0774582 7678:0.0739449 7679:0.0716688 7680:0.111746 7681:0.0827686 7682:0.114879 7683:0.128986 7684:0.118858 7685:0.129772 7686:0.135352 7687:0.138971 7688:0.127307 7689:0.125846 7690:0.13647 7691:0.124276 7692:0.120545 7693:0.146118 7694:0.141649 7695:0.137186 7696:0.115803 7697:0.152951 7698:0.136778 7699:0.140273 7700:0.171299 7701:0.197575 7702:0.173296 7703:0.169222 7704:0.132393 7705:0.151252 7706:0.187703 7707:0.197072 7708:0.257464 7709:0.257095 7710:0.246206 7711:0.1864 7712:0.135616 7713:0.145827 7714:0.220028 7715:0.202582 7716:0.236335 7717:0.229526 7718:0.252268 7719:0.190574 7720:0.155417 7721:0.149925 7722:0.187786 7723:0.221679 7724:0.205938 7725:0.179294 7726:0.173777 7727:0.172477 7728:0.153362 7729:0.147498 7730:0.158063 7731:0.171318 7732:0.169842 7733:0.158289 7734:0.142381 7735:0.141584 7736:0.142591 7737:0.155455 7738:0.15828 7739:0.152012 7740:0.149305 7741:0.144216 7742:0.136926 7743:0.139421 7744:0.157437 7745:0 7746:0 7747:0 7748:0 7749:0 7750:0 7751:0 7752:0 7753:0 7754:0 7755:0 7756:0 7757:0 7758:0 7759:0 7760:0 7761:0 7762:0 7763:0 7764:0 7765:0 7766:0 7767:0 7768:0 7769:0 7770:0 7771:0 7772:0 7773:0 7774:0 7775:0 7776:0 7777:0 7778:0 7779:0 7780:0 7781:0 7782:0 7783:0 7784:0 7785:0 7786:0 7787:0 7788:0 7789:0 7790:0 7791:0 7792:0 7793:0 7794:0 7795:0 7796:0 7797:0 7798:0 7799:0 7800:0 7801:0 7802:0 7803:0 7804:0 7805:0 7806:0 7807:0 7808:0 7809:0 7810:0 7811:0 7812:0 7813:0 7814:0 7815:0 7816:0 7817:0.00535956 7818:0 7819:0 7820:0 7821:0 7822:0 7823:0 7824:0 7825:0.0492092 7826:0 7827:0 7828:0 7829:0 7830:0 7831:0 7832:0 7833:0.0497512 7834:0 7835:0 7836:0.0142777 7837:0 7838:0.00392254 7839:0 7840:0 7841:0.0340475 7842:0.0392576 7843:0 7844:0.00658759 7845:0.00560737 7846:0.0233003 7847:0 7848:0 7849:0.0268991 7850:0.0220986 7851:0.00598785 7852:0 7853:0 7854:0.00326007 7855:0 7856:0 7857:0.01967 7858:0 7859:0 7860:0 7861:0 7862:0 7863:0 7864:0 7865:0.0406857 7866:0.0324522 7867:0.0197832 7868:0.0106344 7869:0.0144305 7870:0.00979878 7871:0.00918195 7872:0.0134865 7873:0.276278 7874:0.335321 7875:0.345193 7876:0.344709 7877:0.350612 7878:0.38765 7879:0.381101 7880:0.349461 7881:0.341514 7882:0.333682 7883:0.33661 7884:0.32286 7885:0.361973 7886:0.358095 7887:0.367276 7888:0.327502 7889:0.400525 7890:0.366069 7891:0.400242 7892:0.485698 7893:0.486162 7894:0.448074 7895:0.365192 7896:0.356705 7897:0.39504 7898:0.358073 7899:0.515882 7900:0.672085 7901:0.701374 7902:0.647027 7903:0.554006 7904:0.368494 7905:0.367504 7906:0.586782 7907:0.636029 7908:0.64043 7909:0.670594 7910:0.683759 7911:0.590207 7912:0.376496 7913:0.38772 7914:0.52815 7915:0.617943 7916:0.547195 7917:0.440991 7918:0.499204 7919:0.489634 7920:0.430595 7921:0.432791 7922:0.443434 7923:0.49101 7924:0.503779 7925:0.423307 7926:0.379263 7927:0.438574 7928:0.482257 7929:0.495296 7930:0.467489 7931:0.425753 7932:0.392429 7933:0.395922 7934:0.383114 7935:0.387005 7936:0.459296 7937:0 7938:0 7939:0 7940:0 7941:0 7942:0 7943:0 7944:0 7945:0 7946:0 7947:0 7948:0 7949:0 7950:0 7951:0 7952:0 7953:0 7954:0 7955:0 7956:0 7957:0 7958:0 7959:0 7960:0 7961:0 7962:0 7963:0 7964:0 7965:0 7966:0 7967:0 7968:0 7969:0 7970:0 7971:0 7972:0 7973:0 7974:0 7975:0 7976:0 7977:0 7978:0 7979:0 7980:0 7981:0 7982:0 7983:0 7984:0 7985:0 7986:0 7987:0 7988:0 7989:0 7990:0 7991:0 7992:0 7993:0 7994:0 7995:0 7996:0 7997:0 7998:0 7999:0 8000:0 8001:0.0678999 8002:0.0905366 8003:0.104454 8004:0.109309 8005:0.0950223 8006:0.100782 8007:0.118235 8008:0.0998373 8009:0.0647852 8010:0.0910565 8011:0.091736 8012:0.0999759 8013:0.093987 8014:0.106217 8015:0.109975 8016:0.0780156 8017:0.0951371 8018:0.106278 8019:0.101317 8020:0.122972 8021:0.157307 8022:0.152802 8023:0.108342 8024:0.084805 8025:0.0964677 8026:0.12704 8027:0.181335 8028:0.21244 8029:0.216015 8030:0.215022 8031:0.149821 8032:0.08381 8033:0.0988482 8034:0.194485 8035:0.19651 8036:0.232786 8037:0.20919 8038:0.205847 8039:0.177228 8040:0.101188 8041:0.101841 8042:0.194124 8043:0.17462 8044:0.163827 8045:0.155542 8046:0.172433 8047:0.14988 8048:0.104235 8049:0.107614 8050:0.126923 8051:0.173461 8052:0.160272 8053:0.128995 8054:0.124139 8055:0.1363 8056:0.120423 8057:0.125372 8058:0.1286 8059:0.103984 8060:0.100694 8061:0.109372 8062:0.104646 8063:0.110717 8064:0.0982136 8065:0.0866706 8066:0.0701093 8067:0.0748234 8068:0.0665995 8069:0.0771713 8070:0.0823689 8071:0.0695945 8072:0.0657524 8073:0.113722 8074:0.0631843 8075:0.0628888 8076:0.0784952 8077:0.0685752 8078:0.0957003 8079:0.0842175 8080:0.049102 8081:0.139038 8082:0.0800146 8083:0.0704228 8084:0.0622997 8085:0.0462524 8086:0.0573682 8087:0.069273 8088:0.0593789 8089:0.140412 8090:0.0789859 8091:0.0878038 8092:0.114233 8093:0.119948 8094:0.100263 8095:0.0718481 8096:0.0559511 8097:0.130538 8098:0.113931 8099:0.106462 8100:0.133553 8101:0.165258 8102:0.157794 8103:0.108167 8104:0.051507 8105:0.124308 8106:0.144136 8107:0.144884 8108:0.10553 8109:0.10378 8110:0.146546 8111:0.121792 8112:0.045921 8113:0.125397 8114:0.0877183 8115:0.105696 8116:0.107536 8117:0.094514 8118:0.0736534 8119:0.0819247 8120:0.0734993 8121:0.133972 8122:0.0935068 8123:0.0871508 8124:0.0840033 8125:0.0864395 8126:0.0770528 8127:0.106705 8128:0.0873843 8129:0 8130:0.00133841 8131:0 8132:0 8133:0 8134:0 8135:0.0115341 8136:0.0676179 8137:0.0130292 8138:0 8139:0.00477684 8140:0 8141:0.0105168 8142:0 8143:0.000698876 8144:0.0816931 8145:0.0119663 8146:0 8147:0 8148:0.0262174 8149:0.0532243 8150:0.0326352 8151:0.0395769 8152:0.0956833 8153:0.0182246 8154:0.0032663 8155:0.0191013 8156:0 8157:0.0125088 8158:0.0194919 8159:0.0682282 8160:0.0922975 8161:0 8162:0 8163:0.0423009 8164:0 8165:0 8166:0.0123885 8167:0.0333288 8168:0.0994323 8169:0 8170:0.00946214 8171:0 8172:0.0432249 8173:0.00969943 8174:0.026073 8175:0 8176:0.152269 8177:0.0277568 8178:0.0108492 8179:0.0114618 8180:0 8181:0 8182:0 8183:0 8184:0.152188 8185:0.0013606 8186:0.0289172 8187:0.0209488 8188:0.00367311 8189:0 8190:0 8191:0 8192:0.0818084\n"]}],"source":["!head -1 train_svm.txt"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ml-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}