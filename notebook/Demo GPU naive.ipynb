{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKQ9h1l2X+aPVD3SYPLYO+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdFicdwzveXF","executionInfo":{"status":"ok","timestamp":1766671887342,"user_tz":-420,"elapsed":16,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"0f427adf-407b-41bd-c5fa-d3e2988438f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.h\n"]}],"source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"]},{"cell_type":"code","source":["%%writefile load_data.cu\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lh0D-AiGzuMW","executionInfo":{"status":"ok","timestamp":1766671887403,"user_tz":-420,"elapsed":59,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"c1f43011-740e-41dc-d518-d94888b9bb2b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","\n","__global__ void conv2d_forward_naive(\n","    const float* __restrict__ input,    // [N, C_in, H, W]\n","    const float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    const float* __restrict__ bias,     // [C_out]\n","    float* __restrict__ output,         // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    const float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    const float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    const float* __restrict__ x,       // forward output/input to ReLU\n","    const float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    const float* __restrict__ input,\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_backward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input_naive(\n","    const float* __restrict__ dY,\n","    const float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias_naive(\n","    const float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    const float* __restrict__ grad,\n","    int size,\n","    float lr);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INj20KYszx7-","executionInfo":{"status":"ok","timestamp":1766671887410,"user_tz":-420,"elapsed":5,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"b3a45a59-c5c9-44a0-d31d-c4b45d023cab"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers.cu\n","#include \"gpu_layers.h\"\n","\n","// --------------- Conv2D forward ------------------\n","__global__ void conv2d_forward_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ weight,\n","    const float* __restrict__ bias,\n","    float* __restrict__ output,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n      = nc / C_out;\n","    int c_out  = nc % C_out;\n","    if (n >= N) return;\n","\n","    float sum = bias ? bias[c_out] : 0.0f;\n","\n","    for (int c_in = 0; c_in < C_in; ++c_in) {\n","        for (int kh = 0; kh < K; ++kh) {\n","            for (int kw = 0; kw < K; ++kw) {\n","                int h_in = h_out * stride + kh - pad;\n","                int w_in = w_out * stride + kw - pad;\n","                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n","                    continue;\n","\n","                int in_idx = ((n * C_in + c_in) * H + h_in) * W + w_in;\n","                int w_idx = (((c_out * C_in + c_in) * K) + kh) * K + kw;\n","                sum += weight[w_idx] * input[in_idx];\n","            }\n","        }\n","    }\n","    int out_idx = ((n * C_out + c_out) * H_out + h_out) * W_out + w_out;\n","    output[out_idx] = sum;\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = v > 0.0f ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 ------------------\n","__global__ void maxpool2x2_forward(\n","    const float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 ------------------\n","__global__ void upsample2x2_forward(\n","    const float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in  = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // reduce trong block\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    const float* __restrict__ x,\n","    const float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        grad_x[i] = (v > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    const float* __restrict__ input,\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    // Tìm max\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    // Chỉ ghi vào vị trí max (grad_in đã được zero trước đó)\n","    // Mỗi pooling window độc lập, không có conflict\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc   = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input_naive(\n","    const float* __restrict__ dY,\n","    const float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int w = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w >= W || h >= H) return;\n","\n","    int n = nc / C_in;\n","    int c_in = nc % C_in;\n","    if (n >= N) return;\n","\n","    float sum = 0.0f;\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        for (int kh = 0; kh < K; ++kh) {\n","            for (int kw = 0; kw < K; ++kw) {\n","                int h_out = h + pad - kh;\n","                int w_out = w + pad - kw;\n","\n","                if (h_out % stride != 0 || w_out % stride != 0) continue;\n","\n","                h_out /= stride;\n","                w_out /= stride;\n","\n","                if (h_out < 0 || h_out >= H_out ||\n","                    w_out < 0 || w_out >= W_out)\n","                    continue;\n","\n","                int dy_idx = idx4(n, c_out, h_out, w_out,\n","                                  C_out, H_out, W_out);\n","\n","                int kh_flip = K - 1 - kh;\n","                int kw_flip = K - 1 - kw;\n","                int w_idx = (((c_out * C_in + c_in) * K) + kh_flip) * K + kw_flip;\n","                sum += dY[dy_idx] * weight[w_idx];\n","            }\n","        }\n","    }\n","\n","    int dx_idx = idx4(n, c_in, h, w, C_in, H, W);\n","    dX[dx_idx] = sum;\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","// Mỗi thread tính toàn bộ gradient cho 1 weight element\n","// Không có conflict vì mỗi thread ghi vào vị trí riêng biệt\n","__global__ void conv2d_backward_weight_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    int total = C_out * C_in * K * K;\n","    if (idx >= total) return;\n","\n","    int kw = idx % K;\n","    int tmp = idx / K;\n","    int kh = tmp % K;\n","    tmp /= K;\n","    int c_in = tmp % C_in;\n","    int c_out = tmp / C_in;\n","\n","    float sum = 0.0f;\n","    for (int n = 0; n < N; ++n) {\n","        for (int h_out = 0; h_out < H_out; ++h_out) {\n","            for (int w_out = 0; w_out < W_out; ++w_out) {\n","                int h_in = h_out * stride + kh - pad;\n","                int w_in = w_out * stride + kw - pad;\n","                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n","                    continue;\n","\n","                int in_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                int dy_idx = idx4(n, c_out, h_out, w_out,\n","                                  C_out, H_out, W_out);\n","                sum += dY[dy_idx] * input[in_idx];\n","            }\n","        }\n","    }\n","    dW[idx] += sum;\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","// Mỗi thread tính gradient cho 1 bias element\n","__global__ void conv2d_backward_bias_naive(\n","    const float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int c_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (c_out >= C_out) return;\n","\n","    float sum = 0.0f;\n","    for (int n = 0; n < N; ++n) {\n","        for (int h = 0; h < H_out; ++h) {\n","            for (int w = 0; w < W_out; ++w) {\n","                int idx = idx4(n, c_out, h, w, C_out, H_out, W_out);\n","                sum += dY[idx];\n","            }\n","        }\n","    }\n","    dB[c_out] += sum;\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    const float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5NkPY9VzyaG","executionInfo":{"status":"ok","timestamp":1766671887435,"user_tz":-420,"elapsed":26,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"cf3942d4-a8f3-4843-95e2-134c915f6adf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers.h\"\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0;\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UX68Gqiuzz4u","executionInfo":{"status":"ok","timestamp":1766671887447,"user_tz":-420,"elapsed":5,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"154c73db-4343-400d-9d63-8bfdfdbc2316"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder.cu\n","// your GPUAutoencoder implementation\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","#include \"gpu_layers.h\"\n","#include \"gpu_autoencoder.h\"\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","    const int K = 3;\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","    // find max weight bytes\n","    size_t max_w_bytes = w1_bytes;\n","    if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","    if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","    if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","    if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","    // find max bias bytes\n","    size_t max_b_bytes = b1_bytes;\n","    if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","    if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","    if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","    if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","    float *h_w = (float*)malloc(max_w_bytes);\n","    float *h_b = (float*)malloc(max_b_bytes);\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ----------\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int K = 3;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ------------- copy input to device -------------\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ========= ENCODER =========\n","    // conv1: 3 -> 256, same 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_b1, ae->d_h1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16, then pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_b2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT is ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","    // conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_b3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv4: 128 -> 256, 16x16, then upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_b4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv5: 256 -> 3, 32x32 (no activation, usually MSE on raw output)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_b5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","    }\n","\n","    // ------------- (optional) compute MSE loss -------------\n","    float loss_value = 0.0f;\n","        if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        // kernel giờ trả về SUM(diff^2) vào d_loss\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;  // MSE = sum / size\n","    }\n","\n","\n","    // ------------- copy output back to host -------------\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H; // 32\n","    const int W0 = ae->W; // 32\n","    const int K = 3;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Zero all gradient buffers\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","        CHECK_CUDA(cudaDeviceSynchronize());\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_u2, ae->d_gout, ae->d_gw5,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gout, ae->d_gb5,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int size = N * 256 * 16 * 16;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_u1, ae->d_gh4, ae->d_gw4,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh4, ae->d_gb4,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int size = N * 128 * 8 * 8;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_p2, ae->d_gh3, ae->d_gw3,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh3, ae->d_gb3,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward: P2 <- H2 =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16;\n","\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int size = N * 128 * 16 * 16;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_p1, ae->d_gh2, ae->d_gw2,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh2, ae->d_gb2,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward: P1 <- H1 =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32;\n","\n","        // Zero gradient buffer\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int size = N * 256 * 32 * 32;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_x0, ae->d_gh1, ae->d_gw1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh1, ae->d_gb1,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int K = 3;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Copy input [N_batch, 3, 32, 32] to GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_b1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, K, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_b2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // FIX: Copy latent [N_batch, 128, 8, 8] correctly\n","    size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vqO1JIe0xQ8","executionInfo":{"status":"ok","timestamp":1766671887543,"user_tz":-420,"elapsed":94,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"5289afcd-47a0-4284-c3e2-cb15644fc18d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder.cu\n"]}]},{"cell_type":"code","source":["%%writefile main_gpu.cu\n","#include <cstdio>\n","#include <ctime>\n","#include <cuda_runtime.h>\n","#include <cstdlib>   // exit()\n","\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder.h\"\n","\n","// GpuTimer dùng cudaEvent để đo time\n","struct GpuTimer {\n","    cudaEvent_t start, stop;\n","    GpuTimer() {\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","    }\n","    ~GpuTimer() {\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","    void tic() {\n","        cudaEventRecord(start, 0);\n","    }\n","    float toc() {\n","        cudaEventRecord(stop, 0);\n","        cudaEventSynchronize(stop);\n","        float ms = 0.0f;\n","        cudaEventElapsedTime(&ms, start, stop);\n","        return ms;\n","    }\n","};\n","\n","void print_gpu_memory() {\n","    size_t free_byte, total_byte;\n","    cudaError_t status = cudaMemGetInfo(&free_byte, &total_byte);\n","\n","    if (status == cudaSuccess) {\n","        double total_mb = (double)total_byte / (1024.0 * 1024.0);\n","        double free_mb = (double)free_byte / (1024.0 * 1024.0);\n","        double used_mb = total_mb - free_mb;\n","\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB / Total: %.2f MB\\n\", used_mb, total_mb);\n","    }\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","    printf(\"[MAIN] Start program\\n\");\n","    fflush(stdout);\n","\n","    // ---- Check GPU device ----\n","    int deviceCount = 0;\n","    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n","    if (err != cudaSuccess) {\n","        fprintf(stderr, \"[MAIN] cudaGetDeviceCount error: %s\\n\",\n","                cudaGetErrorString(err));\n","        return 1;\n","    }\n","    printf(\"[MAIN] Num CUDA devices = %d\\n\", deviceCount);\n","    fflush(stdout);\n","\n","    // ---- Load CIFAR-10 on CPU ----\n","    Cifar10 data;\n","    load_cifar10(&data);   // in: CIFAR-10 loaded successfully ...\n","    printf(\"[MAIN] After load_cifar10\\n\");\n","    fflush(stdout);\n","\n","    normalize_cifar10(&data);\n","    printf(\"[MAIN] After normalize_cifar10\\n\");\n","    fflush(stdout);\n","\n","    // ---- Mở file log GPU (giống format CPU) ----\n","    FILE* log_gpu = fopen(\"training_gpu.txt\", \"w\");\n","    if (!log_gpu) {\n","        fprintf(stderr, \"[MAIN] Cannot open training_gpu.txt for writing\\n\");\n","        return 1;\n","    }\n","\n","    GpuTimer epoch_timer;\n","\n","    int batch_size = 64;\n","    int epochs     = 2;\n","    float lr       = 1e-3f;\n","\n","    int num_batches = TRAIN_NUM / batch_size;\n","\n","    printf(\"[MAIN] Start training loop (epochs=%d, num_batches=%d)\\n\",\n","           epochs, num_batches);\n","    fflush(stdout);\n","\n","    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float)); // buffer tạm\n","\n","    // ---- Init GPU autoencoder ----\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","\n","    // Biến để tích lũy total time & final loss\n","    double total_gpu_time_ms = 0.0;\n","    double final_loss = 0.0;\n","\n","    for (int epoch = 0; epoch < epochs; ++epoch) {\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","\n","        // In giống CPU\n","        printf(\"Epoch %d/%d\\n\", epoch + 1, epochs);\n","        fflush(stdout);\n","        fprintf(log_gpu, \"Epoch %d/%d\\n\", epoch + 1, epochs);\n","\n","        epoch_timer.tic();\n","\n","        for (int b = 0; b < num_batches; ++b) {\n","            get_next_batch(&data, batch_size, b, h_batch);\n","\n","            float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n","            gpu_autoencoder_backward(&ae, lr);\n","\n","            epoch_loss += loss;\n","\n","            if ((b + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\",\n","                       epoch + 1, b + 1, num_batches, loss);\n","                fflush(stdout);\n","            }\n","        }\n","\n","        float ms = epoch_timer.toc();\n","        total_gpu_time_ms += ms;\n","\n","        double avg_loss = epoch_loss / num_batches;\n","        final_loss = avg_loss;  // loss của epoch cuối sẽ là final loss\n","\n","        double epoch_time_sec = ms / 1000.0;\n","\n","        // In ra màn hình\n","        printf(\"Epoch %d finished. Avg Loss: %f, time: %.2f seconds\\n\",\n","               epoch + 1, avg_loss, epoch_time_sec);\n","        fflush(stdout);\n","\n","        // Ghi giống hệt CPU vào file training_gpu.txt\n","        fprintf(log_gpu,\n","                \"Epoch %d finished. Avg Loss: %f, time: %.2f seconds\\n\",\n","                epoch + 1, avg_loss, epoch_time_sec);\n","        fflush(log_gpu);\n","    }\n","\n","    printf(\"[MAIN] Training finished\\n\");\n","    fflush(stdout);\n","\n","    // ---- SUMMARY trên màn hình ----\n","    printf(\"\\n*** Training Summary (GPU) ***\\n\");\n","    printf(\"Total training time: %.2f seconds.\\n\", total_gpu_time_ms / 1000.0);\n","    printf(\"Final reconstruction loss: %f\\n\", final_loss);\n","    print_gpu_memory();\n","    fflush(stdout);\n","\n","    // ---- SUMMARY ghi xuống training_gpu.txt ----\n","    fprintf(log_gpu, \"\\n*** Training Summary ***\\n\");\n","    fprintf(log_gpu, \"Total training time: %.2f seconds.\\n\",\n","            total_gpu_time_ms / 1000.0);\n","    fprintf(log_gpu, \"Final reconstruction loss: %f\\n\", final_loss);\n","    fclose(log_gpu);\n","\n","    // save weights\n","    gpu_autoencoder_save_weights(&ae, \"ae_weights_gpu_naive.bin\");\n","\n","    // ---- cleanup ----\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_output);\n","    free_cifar10(&data);\n","\n","    printf(\"[MAIN] Program finished\\n\");\n","    fflush(stdout);\n","\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaAIG5OC0y9a","executionInfo":{"status":"ok","timestamp":1766671887597,"user_tz":-420,"elapsed":52,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"9d88be3b-3677-4c1b-b9dc-3a4e95a91c40"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main_gpu.cu\n"]}]},{"cell_type":"code","source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","!tar -xzvf cifar-10-binary.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Y3UDRre01aH","executionInfo":{"status":"ok","timestamp":1766671893838,"user_tz":-420,"elapsed":6240,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"f23f33f5-91a6-461f-93fb-2b412b3e2dd1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-25 14:11:27--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  46.9MB/s    in 3.9s    \n","\n","2025-12-25 14:11:31 (42.0 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n","\n","cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 main_gpu.cu gpu_autoencoder.cu gpu_layers.cu load_data.cu -o autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7t1FrWA04m7","executionInfo":{"status":"ok","timestamp":1766671899560,"user_tz":-420,"elapsed":5712,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"49c4d1ad-c31d-430f-ef93-18348f7253eb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(21)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(22)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(460)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(461)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKyEYqyc06Wh","executionInfo":{"status":"ok","timestamp":1766672499542,"user_tz":-420,"elapsed":599981,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"d3f2d09f-9104-4130-9f6b-cfbb89fa7ef1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=2, num_batches=781)\n","Epoch 1/2\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.069835\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.062484\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.058678\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.058653\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.048739\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.054894\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.045116\n","Epoch 1 finished. Avg Loss: 0.067721, time: 295.44 seconds\n","Epoch 2/2\n","[TRAIN] Epoch 2, batch 100/781, loss = 0.050996\n","[TRAIN] Epoch 2, batch 200/781, loss = 0.045700\n","[TRAIN] Epoch 2, batch 300/781, loss = 0.050239\n","[TRAIN] Epoch 2, batch 400/781, loss = 0.051497\n","[TRAIN] Epoch 2, batch 500/781, loss = 0.044980\n","[TRAIN] Epoch 2, batch 600/781, loss = 0.043353\n","[TRAIN] Epoch 2, batch 700/781, loss = 0.040243\n","Epoch 2 finished. Avg Loss: 0.047354, time: 303.42 seconds\n","[MAIN] Training finished\n","\n","*** Training Summary (GPU) ***\n","Total training time: 598.86 seconds.\n","Final reconstruction loss: 0.047354\n","[SYSTEM] Memory Usage: 472.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights_gpu_naive.bin\n","[MAIN] Program finished\n"]}]},{"cell_type":"code","source":["%%writefile extract_svm_features.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cstring>\n","#include <cuda_runtime.h>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder.h\"\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    if (argc < 2) {\n","        fprintf(stderr,\n","                \"Usage: %s <ae_weights.bin>\\n\",\n","                argv[0]);\n","        return 1;\n","    }\n","    const char* weight_file = argv[1];\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","    gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = TRAIN_NUM; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = TEST_NUM; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRGCDxmV09DD","executionInfo":{"status":"ok","timestamp":1766672499549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"919cbffa-0f38-4565-a0ad-178684c839f5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing extract_svm_features.cu\n"]}]},{"cell_type":"code","source":["!nvcc  -arch=sm_75 -O2 -o extract_svm_features \\\n","    extract_svm_features.cu gpu_autoencoder.cu gpu_layers.cu load_data.cu \\\n","    -lcudart"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjYRwQN10--P","executionInfo":{"status":"ok","timestamp":1766672504126,"user_tz":-420,"elapsed":4575,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"292cdc82-ecb6-42d0-b924-d0879ecde6b4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(21)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(22)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(460)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(461)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./extract_svm_features \"ae_weights_gpu_naive.bin\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJV7aFTf0_gu","executionInfo":{"status":"ok","timestamp":1766672720516,"user_tz":-420,"elapsed":216389,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"79271369-5ba2-4271-c839-c437f518ad51"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","Loaded weights from ae_weights_gpu_naive.bin\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/782 done\n","[SVM][TRAIN] Batch 2/782 done\n","[SVM][TRAIN] Batch 3/782 done\n","[SVM][TRAIN] Batch 4/782 done\n","[SVM][TRAIN] Batch 5/782 done\n","[SVM][TRAIN] Batch 6/782 done\n","[SVM][TRAIN] Batch 7/782 done\n","[SVM][TRAIN] Batch 8/782 done\n","[SVM][TRAIN] Batch 9/782 done\n","[SVM][TRAIN] Batch 10/782 done\n","[SVM][TRAIN] Batch 11/782 done\n","[SVM][TRAIN] Batch 12/782 done\n","[SVM][TRAIN] Batch 13/782 done\n","[SVM][TRAIN] Batch 14/782 done\n","[SVM][TRAIN] Batch 15/782 done\n","[SVM][TRAIN] Batch 16/782 done\n","[SVM][TRAIN] Batch 17/782 done\n","[SVM][TRAIN] Batch 18/782 done\n","[SVM][TRAIN] Batch 19/782 done\n","[SVM][TRAIN] Batch 20/782 done\n","[SVM][TRAIN] Batch 21/782 done\n","[SVM][TRAIN] Batch 22/782 done\n","[SVM][TRAIN] Batch 23/782 done\n","[SVM][TRAIN] Batch 24/782 done\n","[SVM][TRAIN] Batch 25/782 done\n","[SVM][TRAIN] Batch 26/782 done\n","[SVM][TRAIN] Batch 27/782 done\n","[SVM][TRAIN] Batch 28/782 done\n","[SVM][TRAIN] Batch 29/782 done\n","[SVM][TRAIN] Batch 30/782 done\n","[SVM][TRAIN] Batch 31/782 done\n","[SVM][TRAIN] Batch 32/782 done\n","[SVM][TRAIN] Batch 33/782 done\n","[SVM][TRAIN] Batch 34/782 done\n","[SVM][TRAIN] Batch 35/782 done\n","[SVM][TRAIN] Batch 36/782 done\n","[SVM][TRAIN] Batch 37/782 done\n","[SVM][TRAIN] Batch 38/782 done\n","[SVM][TRAIN] Batch 39/782 done\n","[SVM][TRAIN] Batch 40/782 done\n","[SVM][TRAIN] Batch 41/782 done\n","[SVM][TRAIN] Batch 42/782 done\n","[SVM][TRAIN] Batch 43/782 done\n","[SVM][TRAIN] Batch 44/782 done\n","[SVM][TRAIN] Batch 45/782 done\n","[SVM][TRAIN] Batch 46/782 done\n","[SVM][TRAIN] Batch 47/782 done\n","[SVM][TRAIN] Batch 48/782 done\n","[SVM][TRAIN] Batch 49/782 done\n","[SVM][TRAIN] Batch 50/782 done\n","[SVM][TRAIN] Batch 51/782 done\n","[SVM][TRAIN] Batch 52/782 done\n","[SVM][TRAIN] Batch 53/782 done\n","[SVM][TRAIN] Batch 54/782 done\n","[SVM][TRAIN] Batch 55/782 done\n","[SVM][TRAIN] Batch 56/782 done\n","[SVM][TRAIN] Batch 57/782 done\n","[SVM][TRAIN] Batch 58/782 done\n","[SVM][TRAIN] Batch 59/782 done\n","[SVM][TRAIN] Batch 60/782 done\n","[SVM][TRAIN] Batch 61/782 done\n","[SVM][TRAIN] Batch 62/782 done\n","[SVM][TRAIN] Batch 63/782 done\n","[SVM][TRAIN] Batch 64/782 done\n","[SVM][TRAIN] Batch 65/782 done\n","[SVM][TRAIN] Batch 66/782 done\n","[SVM][TRAIN] Batch 67/782 done\n","[SVM][TRAIN] Batch 68/782 done\n","[SVM][TRAIN] Batch 69/782 done\n","[SVM][TRAIN] Batch 70/782 done\n","[SVM][TRAIN] Batch 71/782 done\n","[SVM][TRAIN] Batch 72/782 done\n","[SVM][TRAIN] Batch 73/782 done\n","[SVM][TRAIN] Batch 74/782 done\n","[SVM][TRAIN] Batch 75/782 done\n","[SVM][TRAIN] Batch 76/782 done\n","[SVM][TRAIN] Batch 77/782 done\n","[SVM][TRAIN] Batch 78/782 done\n","[SVM][TRAIN] Batch 79/782 done\n","[SVM][TRAIN] Batch 80/782 done\n","[SVM][TRAIN] Batch 81/782 done\n","[SVM][TRAIN] Batch 82/782 done\n","[SVM][TRAIN] Batch 83/782 done\n","[SVM][TRAIN] Batch 84/782 done\n","[SVM][TRAIN] Batch 85/782 done\n","[SVM][TRAIN] Batch 86/782 done\n","[SVM][TRAIN] Batch 87/782 done\n","[SVM][TRAIN] Batch 88/782 done\n","[SVM][TRAIN] Batch 89/782 done\n","[SVM][TRAIN] Batch 90/782 done\n","[SVM][TRAIN] Batch 91/782 done\n","[SVM][TRAIN] Batch 92/782 done\n","[SVM][TRAIN] Batch 93/782 done\n","[SVM][TRAIN] Batch 94/782 done\n","[SVM][TRAIN] Batch 95/782 done\n","[SVM][TRAIN] Batch 96/782 done\n","[SVM][TRAIN] Batch 97/782 done\n","[SVM][TRAIN] Batch 98/782 done\n","[SVM][TRAIN] Batch 99/782 done\n","[SVM][TRAIN] Batch 100/782 done\n","[SVM][TRAIN] Batch 101/782 done\n","[SVM][TRAIN] Batch 102/782 done\n","[SVM][TRAIN] Batch 103/782 done\n","[SVM][TRAIN] Batch 104/782 done\n","[SVM][TRAIN] Batch 105/782 done\n","[SVM][TRAIN] Batch 106/782 done\n","[SVM][TRAIN] Batch 107/782 done\n","[SVM][TRAIN] Batch 108/782 done\n","[SVM][TRAIN] Batch 109/782 done\n","[SVM][TRAIN] Batch 110/782 done\n","[SVM][TRAIN] Batch 111/782 done\n","[SVM][TRAIN] Batch 112/782 done\n","[SVM][TRAIN] Batch 113/782 done\n","[SVM][TRAIN] Batch 114/782 done\n","[SVM][TRAIN] Batch 115/782 done\n","[SVM][TRAIN] Batch 116/782 done\n","[SVM][TRAIN] Batch 117/782 done\n","[SVM][TRAIN] Batch 118/782 done\n","[SVM][TRAIN] Batch 119/782 done\n","[SVM][TRAIN] Batch 120/782 done\n","[SVM][TRAIN] Batch 121/782 done\n","[SVM][TRAIN] Batch 122/782 done\n","[SVM][TRAIN] Batch 123/782 done\n","[SVM][TRAIN] Batch 124/782 done\n","[SVM][TRAIN] Batch 125/782 done\n","[SVM][TRAIN] Batch 126/782 done\n","[SVM][TRAIN] Batch 127/782 done\n","[SVM][TRAIN] Batch 128/782 done\n","[SVM][TRAIN] Batch 129/782 done\n","[SVM][TRAIN] Batch 130/782 done\n","[SVM][TRAIN] Batch 131/782 done\n","[SVM][TRAIN] Batch 132/782 done\n","[SVM][TRAIN] Batch 133/782 done\n","[SVM][TRAIN] Batch 134/782 done\n","[SVM][TRAIN] Batch 135/782 done\n","[SVM][TRAIN] Batch 136/782 done\n","[SVM][TRAIN] Batch 137/782 done\n","[SVM][TRAIN] Batch 138/782 done\n","[SVM][TRAIN] Batch 139/782 done\n","[SVM][TRAIN] Batch 140/782 done\n","[SVM][TRAIN] Batch 141/782 done\n","[SVM][TRAIN] Batch 142/782 done\n","[SVM][TRAIN] Batch 143/782 done\n","[SVM][TRAIN] Batch 144/782 done\n","[SVM][TRAIN] Batch 145/782 done\n","[SVM][TRAIN] Batch 146/782 done\n","[SVM][TRAIN] Batch 147/782 done\n","[SVM][TRAIN] Batch 148/782 done\n","[SVM][TRAIN] Batch 149/782 done\n","[SVM][TRAIN] Batch 150/782 done\n","[SVM][TRAIN] Batch 151/782 done\n","[SVM][TRAIN] Batch 152/782 done\n","[SVM][TRAIN] Batch 153/782 done\n","[SVM][TRAIN] Batch 154/782 done\n","[SVM][TRAIN] Batch 155/782 done\n","[SVM][TRAIN] Batch 156/782 done\n","[SVM][TRAIN] Batch 157/782 done\n","[SVM][TRAIN] Batch 158/782 done\n","[SVM][TRAIN] Batch 159/782 done\n","[SVM][TRAIN] Batch 160/782 done\n","[SVM][TRAIN] Batch 161/782 done\n","[SVM][TRAIN] Batch 162/782 done\n","[SVM][TRAIN] Batch 163/782 done\n","[SVM][TRAIN] Batch 164/782 done\n","[SVM][TRAIN] Batch 165/782 done\n","[SVM][TRAIN] Batch 166/782 done\n","[SVM][TRAIN] Batch 167/782 done\n","[SVM][TRAIN] Batch 168/782 done\n","[SVM][TRAIN] Batch 169/782 done\n","[SVM][TRAIN] Batch 170/782 done\n","[SVM][TRAIN] Batch 171/782 done\n","[SVM][TRAIN] Batch 172/782 done\n","[SVM][TRAIN] Batch 173/782 done\n","[SVM][TRAIN] Batch 174/782 done\n","[SVM][TRAIN] Batch 175/782 done\n","[SVM][TRAIN] Batch 176/782 done\n","[SVM][TRAIN] Batch 177/782 done\n","[SVM][TRAIN] Batch 178/782 done\n","[SVM][TRAIN] Batch 179/782 done\n","[SVM][TRAIN] Batch 180/782 done\n","[SVM][TRAIN] Batch 181/782 done\n","[SVM][TRAIN] Batch 182/782 done\n","[SVM][TRAIN] Batch 183/782 done\n","[SVM][TRAIN] Batch 184/782 done\n","[SVM][TRAIN] Batch 185/782 done\n","[SVM][TRAIN] Batch 186/782 done\n","[SVM][TRAIN] Batch 187/782 done\n","[SVM][TRAIN] Batch 188/782 done\n","[SVM][TRAIN] Batch 189/782 done\n","[SVM][TRAIN] Batch 190/782 done\n","[SVM][TRAIN] Batch 191/782 done\n","[SVM][TRAIN] Batch 192/782 done\n","[SVM][TRAIN] Batch 193/782 done\n","[SVM][TRAIN] Batch 194/782 done\n","[SVM][TRAIN] Batch 195/782 done\n","[SVM][TRAIN] Batch 196/782 done\n","[SVM][TRAIN] Batch 197/782 done\n","[SVM][TRAIN] Batch 198/782 done\n","[SVM][TRAIN] Batch 199/782 done\n","[SVM][TRAIN] Batch 200/782 done\n","[SVM][TRAIN] Batch 201/782 done\n","[SVM][TRAIN] Batch 202/782 done\n","[SVM][TRAIN] Batch 203/782 done\n","[SVM][TRAIN] Batch 204/782 done\n","[SVM][TRAIN] Batch 205/782 done\n","[SVM][TRAIN] Batch 206/782 done\n","[SVM][TRAIN] Batch 207/782 done\n","[SVM][TRAIN] Batch 208/782 done\n","[SVM][TRAIN] Batch 209/782 done\n","[SVM][TRAIN] Batch 210/782 done\n","[SVM][TRAIN] Batch 211/782 done\n","[SVM][TRAIN] Batch 212/782 done\n","[SVM][TRAIN] Batch 213/782 done\n","[SVM][TRAIN] Batch 214/782 done\n","[SVM][TRAIN] Batch 215/782 done\n","[SVM][TRAIN] Batch 216/782 done\n","[SVM][TRAIN] Batch 217/782 done\n","[SVM][TRAIN] Batch 218/782 done\n","[SVM][TRAIN] Batch 219/782 done\n","[SVM][TRAIN] Batch 220/782 done\n","[SVM][TRAIN] Batch 221/782 done\n","[SVM][TRAIN] Batch 222/782 done\n","[SVM][TRAIN] Batch 223/782 done\n","[SVM][TRAIN] Batch 224/782 done\n","[SVM][TRAIN] Batch 225/782 done\n","[SVM][TRAIN] Batch 226/782 done\n","[SVM][TRAIN] Batch 227/782 done\n","[SVM][TRAIN] Batch 228/782 done\n","[SVM][TRAIN] Batch 229/782 done\n","[SVM][TRAIN] Batch 230/782 done\n","[SVM][TRAIN] Batch 231/782 done\n","[SVM][TRAIN] Batch 232/782 done\n","[SVM][TRAIN] Batch 233/782 done\n","[SVM][TRAIN] Batch 234/782 done\n","[SVM][TRAIN] Batch 235/782 done\n","[SVM][TRAIN] Batch 236/782 done\n","[SVM][TRAIN] Batch 237/782 done\n","[SVM][TRAIN] Batch 238/782 done\n","[SVM][TRAIN] Batch 239/782 done\n","[SVM][TRAIN] Batch 240/782 done\n","[SVM][TRAIN] Batch 241/782 done\n","[SVM][TRAIN] Batch 242/782 done\n","[SVM][TRAIN] Batch 243/782 done\n","[SVM][TRAIN] Batch 244/782 done\n","[SVM][TRAIN] Batch 245/782 done\n","[SVM][TRAIN] Batch 246/782 done\n","[SVM][TRAIN] Batch 247/782 done\n","[SVM][TRAIN] Batch 248/782 done\n","[SVM][TRAIN] Batch 249/782 done\n","[SVM][TRAIN] Batch 250/782 done\n","[SVM][TRAIN] Batch 251/782 done\n","[SVM][TRAIN] Batch 252/782 done\n","[SVM][TRAIN] Batch 253/782 done\n","[SVM][TRAIN] Batch 254/782 done\n","[SVM][TRAIN] Batch 255/782 done\n","[SVM][TRAIN] Batch 256/782 done\n","[SVM][TRAIN] Batch 257/782 done\n","[SVM][TRAIN] Batch 258/782 done\n","[SVM][TRAIN] Batch 259/782 done\n","[SVM][TRAIN] Batch 260/782 done\n","[SVM][TRAIN] Batch 261/782 done\n","[SVM][TRAIN] Batch 262/782 done\n","[SVM][TRAIN] Batch 263/782 done\n","[SVM][TRAIN] Batch 264/782 done\n","[SVM][TRAIN] Batch 265/782 done\n","[SVM][TRAIN] Batch 266/782 done\n","[SVM][TRAIN] Batch 267/782 done\n","[SVM][TRAIN] Batch 268/782 done\n","[SVM][TRAIN] Batch 269/782 done\n","[SVM][TRAIN] Batch 270/782 done\n","[SVM][TRAIN] Batch 271/782 done\n","[SVM][TRAIN] Batch 272/782 done\n","[SVM][TRAIN] Batch 273/782 done\n","[SVM][TRAIN] Batch 274/782 done\n","[SVM][TRAIN] Batch 275/782 done\n","[SVM][TRAIN] Batch 276/782 done\n","[SVM][TRAIN] Batch 277/782 done\n","[SVM][TRAIN] Batch 278/782 done\n","[SVM][TRAIN] Batch 279/782 done\n","[SVM][TRAIN] Batch 280/782 done\n","[SVM][TRAIN] Batch 281/782 done\n","[SVM][TRAIN] Batch 282/782 done\n","[SVM][TRAIN] Batch 283/782 done\n","[SVM][TRAIN] Batch 284/782 done\n","[SVM][TRAIN] Batch 285/782 done\n","[SVM][TRAIN] Batch 286/782 done\n","[SVM][TRAIN] Batch 287/782 done\n","[SVM][TRAIN] Batch 288/782 done\n","[SVM][TRAIN] Batch 289/782 done\n","[SVM][TRAIN] Batch 290/782 done\n","[SVM][TRAIN] Batch 291/782 done\n","[SVM][TRAIN] Batch 292/782 done\n","[SVM][TRAIN] Batch 293/782 done\n","[SVM][TRAIN] Batch 294/782 done\n","[SVM][TRAIN] Batch 295/782 done\n","[SVM][TRAIN] Batch 296/782 done\n","[SVM][TRAIN] Batch 297/782 done\n","[SVM][TRAIN] Batch 298/782 done\n","[SVM][TRAIN] Batch 299/782 done\n","[SVM][TRAIN] Batch 300/782 done\n","[SVM][TRAIN] Batch 301/782 done\n","[SVM][TRAIN] Batch 302/782 done\n","[SVM][TRAIN] Batch 303/782 done\n","[SVM][TRAIN] Batch 304/782 done\n","[SVM][TRAIN] Batch 305/782 done\n","[SVM][TRAIN] Batch 306/782 done\n","[SVM][TRAIN] Batch 307/782 done\n","[SVM][TRAIN] Batch 308/782 done\n","[SVM][TRAIN] Batch 309/782 done\n","[SVM][TRAIN] Batch 310/782 done\n","[SVM][TRAIN] Batch 311/782 done\n","[SVM][TRAIN] Batch 312/782 done\n","[SVM][TRAIN] Batch 313/782 done\n","[SVM][TRAIN] Batch 314/782 done\n","[SVM][TRAIN] Batch 315/782 done\n","[SVM][TRAIN] Batch 316/782 done\n","[SVM][TRAIN] Batch 317/782 done\n","[SVM][TRAIN] Batch 318/782 done\n","[SVM][TRAIN] Batch 319/782 done\n","[SVM][TRAIN] Batch 320/782 done\n","[SVM][TRAIN] Batch 321/782 done\n","[SVM][TRAIN] Batch 322/782 done\n","[SVM][TRAIN] Batch 323/782 done\n","[SVM][TRAIN] Batch 324/782 done\n","[SVM][TRAIN] Batch 325/782 done\n","[SVM][TRAIN] Batch 326/782 done\n","[SVM][TRAIN] Batch 327/782 done\n","[SVM][TRAIN] Batch 328/782 done\n","[SVM][TRAIN] Batch 329/782 done\n","[SVM][TRAIN] Batch 330/782 done\n","[SVM][TRAIN] Batch 331/782 done\n","[SVM][TRAIN] Batch 332/782 done\n","[SVM][TRAIN] Batch 333/782 done\n","[SVM][TRAIN] Batch 334/782 done\n","[SVM][TRAIN] Batch 335/782 done\n","[SVM][TRAIN] Batch 336/782 done\n","[SVM][TRAIN] Batch 337/782 done\n","[SVM][TRAIN] Batch 338/782 done\n","[SVM][TRAIN] Batch 339/782 done\n","[SVM][TRAIN] Batch 340/782 done\n","[SVM][TRAIN] Batch 341/782 done\n","[SVM][TRAIN] Batch 342/782 done\n","[SVM][TRAIN] Batch 343/782 done\n","[SVM][TRAIN] Batch 344/782 done\n","[SVM][TRAIN] Batch 345/782 done\n","[SVM][TRAIN] Batch 346/782 done\n","[SVM][TRAIN] Batch 347/782 done\n","[SVM][TRAIN] Batch 348/782 done\n","[SVM][TRAIN] Batch 349/782 done\n","[SVM][TRAIN] Batch 350/782 done\n","[SVM][TRAIN] Batch 351/782 done\n","[SVM][TRAIN] Batch 352/782 done\n","[SVM][TRAIN] Batch 353/782 done\n","[SVM][TRAIN] Batch 354/782 done\n","[SVM][TRAIN] Batch 355/782 done\n","[SVM][TRAIN] Batch 356/782 done\n","[SVM][TRAIN] Batch 357/782 done\n","[SVM][TRAIN] Batch 358/782 done\n","[SVM][TRAIN] Batch 359/782 done\n","[SVM][TRAIN] Batch 360/782 done\n","[SVM][TRAIN] Batch 361/782 done\n","[SVM][TRAIN] Batch 362/782 done\n","[SVM][TRAIN] Batch 363/782 done\n","[SVM][TRAIN] Batch 364/782 done\n","[SVM][TRAIN] Batch 365/782 done\n","[SVM][TRAIN] Batch 366/782 done\n","[SVM][TRAIN] Batch 367/782 done\n","[SVM][TRAIN] Batch 368/782 done\n","[SVM][TRAIN] Batch 369/782 done\n","[SVM][TRAIN] Batch 370/782 done\n","[SVM][TRAIN] Batch 371/782 done\n","[SVM][TRAIN] Batch 372/782 done\n","[SVM][TRAIN] Batch 373/782 done\n","[SVM][TRAIN] Batch 374/782 done\n","[SVM][TRAIN] Batch 375/782 done\n","[SVM][TRAIN] Batch 376/782 done\n","[SVM][TRAIN] Batch 377/782 done\n","[SVM][TRAIN] Batch 378/782 done\n","[SVM][TRAIN] Batch 379/782 done\n","[SVM][TRAIN] Batch 380/782 done\n","[SVM][TRAIN] Batch 381/782 done\n","[SVM][TRAIN] Batch 382/782 done\n","[SVM][TRAIN] Batch 383/782 done\n","[SVM][TRAIN] Batch 384/782 done\n","[SVM][TRAIN] Batch 385/782 done\n","[SVM][TRAIN] Batch 386/782 done\n","[SVM][TRAIN] Batch 387/782 done\n","[SVM][TRAIN] Batch 388/782 done\n","[SVM][TRAIN] Batch 389/782 done\n","[SVM][TRAIN] Batch 390/782 done\n","[SVM][TRAIN] Batch 391/782 done\n","[SVM][TRAIN] Batch 392/782 done\n","[SVM][TRAIN] Batch 393/782 done\n","[SVM][TRAIN] Batch 394/782 done\n","[SVM][TRAIN] Batch 395/782 done\n","[SVM][TRAIN] Batch 396/782 done\n","[SVM][TRAIN] Batch 397/782 done\n","[SVM][TRAIN] Batch 398/782 done\n","[SVM][TRAIN] Batch 399/782 done\n","[SVM][TRAIN] Batch 400/782 done\n","[SVM][TRAIN] Batch 401/782 done\n","[SVM][TRAIN] Batch 402/782 done\n","[SVM][TRAIN] Batch 403/782 done\n","[SVM][TRAIN] Batch 404/782 done\n","[SVM][TRAIN] Batch 405/782 done\n","[SVM][TRAIN] Batch 406/782 done\n","[SVM][TRAIN] Batch 407/782 done\n","[SVM][TRAIN] Batch 408/782 done\n","[SVM][TRAIN] Batch 409/782 done\n","[SVM][TRAIN] Batch 410/782 done\n","[SVM][TRAIN] Batch 411/782 done\n","[SVM][TRAIN] Batch 412/782 done\n","[SVM][TRAIN] Batch 413/782 done\n","[SVM][TRAIN] Batch 414/782 done\n","[SVM][TRAIN] Batch 415/782 done\n","[SVM][TRAIN] Batch 416/782 done\n","[SVM][TRAIN] Batch 417/782 done\n","[SVM][TRAIN] Batch 418/782 done\n","[SVM][TRAIN] Batch 419/782 done\n","[SVM][TRAIN] Batch 420/782 done\n","[SVM][TRAIN] Batch 421/782 done\n","[SVM][TRAIN] Batch 422/782 done\n","[SVM][TRAIN] Batch 423/782 done\n","[SVM][TRAIN] Batch 424/782 done\n","[SVM][TRAIN] Batch 425/782 done\n","[SVM][TRAIN] Batch 426/782 done\n","[SVM][TRAIN] Batch 427/782 done\n","[SVM][TRAIN] Batch 428/782 done\n","[SVM][TRAIN] Batch 429/782 done\n","[SVM][TRAIN] Batch 430/782 done\n","[SVM][TRAIN] Batch 431/782 done\n","[SVM][TRAIN] Batch 432/782 done\n","[SVM][TRAIN] Batch 433/782 done\n","[SVM][TRAIN] Batch 434/782 done\n","[SVM][TRAIN] Batch 435/782 done\n","[SVM][TRAIN] Batch 436/782 done\n","[SVM][TRAIN] Batch 437/782 done\n","[SVM][TRAIN] Batch 438/782 done\n","[SVM][TRAIN] Batch 439/782 done\n","[SVM][TRAIN] Batch 440/782 done\n","[SVM][TRAIN] Batch 441/782 done\n","[SVM][TRAIN] Batch 442/782 done\n","[SVM][TRAIN] Batch 443/782 done\n","[SVM][TRAIN] Batch 444/782 done\n","[SVM][TRAIN] Batch 445/782 done\n","[SVM][TRAIN] Batch 446/782 done\n","[SVM][TRAIN] Batch 447/782 done\n","[SVM][TRAIN] Batch 448/782 done\n","[SVM][TRAIN] Batch 449/782 done\n","[SVM][TRAIN] Batch 450/782 done\n","[SVM][TRAIN] Batch 451/782 done\n","[SVM][TRAIN] Batch 452/782 done\n","[SVM][TRAIN] Batch 453/782 done\n","[SVM][TRAIN] Batch 454/782 done\n","[SVM][TRAIN] Batch 455/782 done\n","[SVM][TRAIN] Batch 456/782 done\n","[SVM][TRAIN] Batch 457/782 done\n","[SVM][TRAIN] Batch 458/782 done\n","[SVM][TRAIN] Batch 459/782 done\n","[SVM][TRAIN] Batch 460/782 done\n","[SVM][TRAIN] Batch 461/782 done\n","[SVM][TRAIN] Batch 462/782 done\n","[SVM][TRAIN] Batch 463/782 done\n","[SVM][TRAIN] Batch 464/782 done\n","[SVM][TRAIN] Batch 465/782 done\n","[SVM][TRAIN] Batch 466/782 done\n","[SVM][TRAIN] Batch 467/782 done\n","[SVM][TRAIN] Batch 468/782 done\n","[SVM][TRAIN] Batch 469/782 done\n","[SVM][TRAIN] Batch 470/782 done\n","[SVM][TRAIN] Batch 471/782 done\n","[SVM][TRAIN] Batch 472/782 done\n","[SVM][TRAIN] Batch 473/782 done\n","[SVM][TRAIN] Batch 474/782 done\n","[SVM][TRAIN] Batch 475/782 done\n","[SVM][TRAIN] Batch 476/782 done\n","[SVM][TRAIN] Batch 477/782 done\n","[SVM][TRAIN] Batch 478/782 done\n","[SVM][TRAIN] Batch 479/782 done\n","[SVM][TRAIN] Batch 480/782 done\n","[SVM][TRAIN] Batch 481/782 done\n","[SVM][TRAIN] Batch 482/782 done\n","[SVM][TRAIN] Batch 483/782 done\n","[SVM][TRAIN] Batch 484/782 done\n","[SVM][TRAIN] Batch 485/782 done\n","[SVM][TRAIN] Batch 486/782 done\n","[SVM][TRAIN] Batch 487/782 done\n","[SVM][TRAIN] Batch 488/782 done\n","[SVM][TRAIN] Batch 489/782 done\n","[SVM][TRAIN] Batch 490/782 done\n","[SVM][TRAIN] Batch 491/782 done\n","[SVM][TRAIN] Batch 492/782 done\n","[SVM][TRAIN] Batch 493/782 done\n","[SVM][TRAIN] Batch 494/782 done\n","[SVM][TRAIN] Batch 495/782 done\n","[SVM][TRAIN] Batch 496/782 done\n","[SVM][TRAIN] Batch 497/782 done\n","[SVM][TRAIN] Batch 498/782 done\n","[SVM][TRAIN] Batch 499/782 done\n","[SVM][TRAIN] Batch 500/782 done\n","[SVM][TRAIN] Batch 501/782 done\n","[SVM][TRAIN] Batch 502/782 done\n","[SVM][TRAIN] Batch 503/782 done\n","[SVM][TRAIN] Batch 504/782 done\n","[SVM][TRAIN] Batch 505/782 done\n","[SVM][TRAIN] Batch 506/782 done\n","[SVM][TRAIN] Batch 507/782 done\n","[SVM][TRAIN] Batch 508/782 done\n","[SVM][TRAIN] Batch 509/782 done\n","[SVM][TRAIN] Batch 510/782 done\n","[SVM][TRAIN] Batch 511/782 done\n","[SVM][TRAIN] Batch 512/782 done\n","[SVM][TRAIN] Batch 513/782 done\n","[SVM][TRAIN] Batch 514/782 done\n","[SVM][TRAIN] Batch 515/782 done\n","[SVM][TRAIN] Batch 516/782 done\n","[SVM][TRAIN] Batch 517/782 done\n","[SVM][TRAIN] Batch 518/782 done\n","[SVM][TRAIN] Batch 519/782 done\n","[SVM][TRAIN] Batch 520/782 done\n","[SVM][TRAIN] Batch 521/782 done\n","[SVM][TRAIN] Batch 522/782 done\n","[SVM][TRAIN] Batch 523/782 done\n","[SVM][TRAIN] Batch 524/782 done\n","[SVM][TRAIN] Batch 525/782 done\n","[SVM][TRAIN] Batch 526/782 done\n","[SVM][TRAIN] Batch 527/782 done\n","[SVM][TRAIN] Batch 528/782 done\n","[SVM][TRAIN] Batch 529/782 done\n","[SVM][TRAIN] Batch 530/782 done\n","[SVM][TRAIN] Batch 531/782 done\n","[SVM][TRAIN] Batch 532/782 done\n","[SVM][TRAIN] Batch 533/782 done\n","[SVM][TRAIN] Batch 534/782 done\n","[SVM][TRAIN] Batch 535/782 done\n","[SVM][TRAIN] Batch 536/782 done\n","[SVM][TRAIN] Batch 537/782 done\n","[SVM][TRAIN] Batch 538/782 done\n","[SVM][TRAIN] Batch 539/782 done\n","[SVM][TRAIN] Batch 540/782 done\n","[SVM][TRAIN] Batch 541/782 done\n","[SVM][TRAIN] Batch 542/782 done\n","[SVM][TRAIN] Batch 543/782 done\n","[SVM][TRAIN] Batch 544/782 done\n","[SVM][TRAIN] Batch 545/782 done\n","[SVM][TRAIN] Batch 546/782 done\n","[SVM][TRAIN] Batch 547/782 done\n","[SVM][TRAIN] Batch 548/782 done\n","[SVM][TRAIN] Batch 549/782 done\n","[SVM][TRAIN] Batch 550/782 done\n","[SVM][TRAIN] Batch 551/782 done\n","[SVM][TRAIN] Batch 552/782 done\n","[SVM][TRAIN] Batch 553/782 done\n","[SVM][TRAIN] Batch 554/782 done\n","[SVM][TRAIN] Batch 555/782 done\n","[SVM][TRAIN] Batch 556/782 done\n","[SVM][TRAIN] Batch 557/782 done\n","[SVM][TRAIN] Batch 558/782 done\n","[SVM][TRAIN] Batch 559/782 done\n","[SVM][TRAIN] Batch 560/782 done\n","[SVM][TRAIN] Batch 561/782 done\n","[SVM][TRAIN] Batch 562/782 done\n","[SVM][TRAIN] Batch 563/782 done\n","[SVM][TRAIN] Batch 564/782 done\n","[SVM][TRAIN] Batch 565/782 done\n","[SVM][TRAIN] Batch 566/782 done\n","[SVM][TRAIN] Batch 567/782 done\n","[SVM][TRAIN] Batch 568/782 done\n","[SVM][TRAIN] Batch 569/782 done\n","[SVM][TRAIN] Batch 570/782 done\n","[SVM][TRAIN] Batch 571/782 done\n","[SVM][TRAIN] Batch 572/782 done\n","[SVM][TRAIN] Batch 573/782 done\n","[SVM][TRAIN] Batch 574/782 done\n","[SVM][TRAIN] Batch 575/782 done\n","[SVM][TRAIN] Batch 576/782 done\n","[SVM][TRAIN] Batch 577/782 done\n","[SVM][TRAIN] Batch 578/782 done\n","[SVM][TRAIN] Batch 579/782 done\n","[SVM][TRAIN] Batch 580/782 done\n","[SVM][TRAIN] Batch 581/782 done\n","[SVM][TRAIN] Batch 582/782 done\n","[SVM][TRAIN] Batch 583/782 done\n","[SVM][TRAIN] Batch 584/782 done\n","[SVM][TRAIN] Batch 585/782 done\n","[SVM][TRAIN] Batch 586/782 done\n","[SVM][TRAIN] Batch 587/782 done\n","[SVM][TRAIN] Batch 588/782 done\n","[SVM][TRAIN] Batch 589/782 done\n","[SVM][TRAIN] Batch 590/782 done\n","[SVM][TRAIN] Batch 591/782 done\n","[SVM][TRAIN] Batch 592/782 done\n","[SVM][TRAIN] Batch 593/782 done\n","[SVM][TRAIN] Batch 594/782 done\n","[SVM][TRAIN] Batch 595/782 done\n","[SVM][TRAIN] Batch 596/782 done\n","[SVM][TRAIN] Batch 597/782 done\n","[SVM][TRAIN] Batch 598/782 done\n","[SVM][TRAIN] Batch 599/782 done\n","[SVM][TRAIN] Batch 600/782 done\n","[SVM][TRAIN] Batch 601/782 done\n","[SVM][TRAIN] Batch 602/782 done\n","[SVM][TRAIN] Batch 603/782 done\n","[SVM][TRAIN] Batch 604/782 done\n","[SVM][TRAIN] Batch 605/782 done\n","[SVM][TRAIN] Batch 606/782 done\n","[SVM][TRAIN] Batch 607/782 done\n","[SVM][TRAIN] Batch 608/782 done\n","[SVM][TRAIN] Batch 609/782 done\n","[SVM][TRAIN] Batch 610/782 done\n","[SVM][TRAIN] Batch 611/782 done\n","[SVM][TRAIN] Batch 612/782 done\n","[SVM][TRAIN] Batch 613/782 done\n","[SVM][TRAIN] Batch 614/782 done\n","[SVM][TRAIN] Batch 615/782 done\n","[SVM][TRAIN] Batch 616/782 done\n","[SVM][TRAIN] Batch 617/782 done\n","[SVM][TRAIN] Batch 618/782 done\n","[SVM][TRAIN] Batch 619/782 done\n","[SVM][TRAIN] Batch 620/782 done\n","[SVM][TRAIN] Batch 621/782 done\n","[SVM][TRAIN] Batch 622/782 done\n","[SVM][TRAIN] Batch 623/782 done\n","[SVM][TRAIN] Batch 624/782 done\n","[SVM][TRAIN] Batch 625/782 done\n","[SVM][TRAIN] Batch 626/782 done\n","[SVM][TRAIN] Batch 627/782 done\n","[SVM][TRAIN] Batch 628/782 done\n","[SVM][TRAIN] Batch 629/782 done\n","[SVM][TRAIN] Batch 630/782 done\n","[SVM][TRAIN] Batch 631/782 done\n","[SVM][TRAIN] Batch 632/782 done\n","[SVM][TRAIN] Batch 633/782 done\n","[SVM][TRAIN] Batch 634/782 done\n","[SVM][TRAIN] Batch 635/782 done\n","[SVM][TRAIN] Batch 636/782 done\n","[SVM][TRAIN] Batch 637/782 done\n","[SVM][TRAIN] Batch 638/782 done\n","[SVM][TRAIN] Batch 639/782 done\n","[SVM][TRAIN] Batch 640/782 done\n","[SVM][TRAIN] Batch 641/782 done\n","[SVM][TRAIN] Batch 642/782 done\n","[SVM][TRAIN] Batch 643/782 done\n","[SVM][TRAIN] Batch 644/782 done\n","[SVM][TRAIN] Batch 645/782 done\n","[SVM][TRAIN] Batch 646/782 done\n","[SVM][TRAIN] Batch 647/782 done\n","[SVM][TRAIN] Batch 648/782 done\n","[SVM][TRAIN] Batch 649/782 done\n","[SVM][TRAIN] Batch 650/782 done\n","[SVM][TRAIN] Batch 651/782 done\n","[SVM][TRAIN] Batch 652/782 done\n","[SVM][TRAIN] Batch 653/782 done\n","[SVM][TRAIN] Batch 654/782 done\n","[SVM][TRAIN] Batch 655/782 done\n","[SVM][TRAIN] Batch 656/782 done\n","[SVM][TRAIN] Batch 657/782 done\n","[SVM][TRAIN] Batch 658/782 done\n","[SVM][TRAIN] Batch 659/782 done\n","[SVM][TRAIN] Batch 660/782 done\n","[SVM][TRAIN] Batch 661/782 done\n","[SVM][TRAIN] Batch 662/782 done\n","[SVM][TRAIN] Batch 663/782 done\n","[SVM][TRAIN] Batch 664/782 done\n","[SVM][TRAIN] Batch 665/782 done\n","[SVM][TRAIN] Batch 666/782 done\n","[SVM][TRAIN] Batch 667/782 done\n","[SVM][TRAIN] Batch 668/782 done\n","[SVM][TRAIN] Batch 669/782 done\n","[SVM][TRAIN] Batch 670/782 done\n","[SVM][TRAIN] Batch 671/782 done\n","[SVM][TRAIN] Batch 672/782 done\n","[SVM][TRAIN] Batch 673/782 done\n","[SVM][TRAIN] Batch 674/782 done\n","[SVM][TRAIN] Batch 675/782 done\n","[SVM][TRAIN] Batch 676/782 done\n","[SVM][TRAIN] Batch 677/782 done\n","[SVM][TRAIN] Batch 678/782 done\n","[SVM][TRAIN] Batch 679/782 done\n","[SVM][TRAIN] Batch 680/782 done\n","[SVM][TRAIN] Batch 681/782 done\n","[SVM][TRAIN] Batch 682/782 done\n","[SVM][TRAIN] Batch 683/782 done\n","[SVM][TRAIN] Batch 684/782 done\n","[SVM][TRAIN] Batch 685/782 done\n","[SVM][TRAIN] Batch 686/782 done\n","[SVM][TRAIN] Batch 687/782 done\n","[SVM][TRAIN] Batch 688/782 done\n","[SVM][TRAIN] Batch 689/782 done\n","[SVM][TRAIN] Batch 690/782 done\n","[SVM][TRAIN] Batch 691/782 done\n","[SVM][TRAIN] Batch 692/782 done\n","[SVM][TRAIN] Batch 693/782 done\n","[SVM][TRAIN] Batch 694/782 done\n","[SVM][TRAIN] Batch 695/782 done\n","[SVM][TRAIN] Batch 696/782 done\n","[SVM][TRAIN] Batch 697/782 done\n","[SVM][TRAIN] Batch 698/782 done\n","[SVM][TRAIN] Batch 699/782 done\n","[SVM][TRAIN] Batch 700/782 done\n","[SVM][TRAIN] Batch 701/782 done\n","[SVM][TRAIN] Batch 702/782 done\n","[SVM][TRAIN] Batch 703/782 done\n","[SVM][TRAIN] Batch 704/782 done\n","[SVM][TRAIN] Batch 705/782 done\n","[SVM][TRAIN] Batch 706/782 done\n","[SVM][TRAIN] Batch 707/782 done\n","[SVM][TRAIN] Batch 708/782 done\n","[SVM][TRAIN] Batch 709/782 done\n","[SVM][TRAIN] Batch 710/782 done\n","[SVM][TRAIN] Batch 711/782 done\n","[SVM][TRAIN] Batch 712/782 done\n","[SVM][TRAIN] Batch 713/782 done\n","[SVM][TRAIN] Batch 714/782 done\n","[SVM][TRAIN] Batch 715/782 done\n","[SVM][TRAIN] Batch 716/782 done\n","[SVM][TRAIN] Batch 717/782 done\n","[SVM][TRAIN] Batch 718/782 done\n","[SVM][TRAIN] Batch 719/782 done\n","[SVM][TRAIN] Batch 720/782 done\n","[SVM][TRAIN] Batch 721/782 done\n","[SVM][TRAIN] Batch 722/782 done\n","[SVM][TRAIN] Batch 723/782 done\n","[SVM][TRAIN] Batch 724/782 done\n","[SVM][TRAIN] Batch 725/782 done\n","[SVM][TRAIN] Batch 726/782 done\n","[SVM][TRAIN] Batch 727/782 done\n","[SVM][TRAIN] Batch 728/782 done\n","[SVM][TRAIN] Batch 729/782 done\n","[SVM][TRAIN] Batch 730/782 done\n","[SVM][TRAIN] Batch 731/782 done\n","[SVM][TRAIN] Batch 732/782 done\n","[SVM][TRAIN] Batch 733/782 done\n","[SVM][TRAIN] Batch 734/782 done\n","[SVM][TRAIN] Batch 735/782 done\n","[SVM][TRAIN] Batch 736/782 done\n","[SVM][TRAIN] Batch 737/782 done\n","[SVM][TRAIN] Batch 738/782 done\n","[SVM][TRAIN] Batch 739/782 done\n","[SVM][TRAIN] Batch 740/782 done\n","[SVM][TRAIN] Batch 741/782 done\n","[SVM][TRAIN] Batch 742/782 done\n","[SVM][TRAIN] Batch 743/782 done\n","[SVM][TRAIN] Batch 744/782 done\n","[SVM][TRAIN] Batch 745/782 done\n","[SVM][TRAIN] Batch 746/782 done\n","[SVM][TRAIN] Batch 747/782 done\n","[SVM][TRAIN] Batch 748/782 done\n","[SVM][TRAIN] Batch 749/782 done\n","[SVM][TRAIN] Batch 750/782 done\n","[SVM][TRAIN] Batch 751/782 done\n","[SVM][TRAIN] Batch 752/782 done\n","[SVM][TRAIN] Batch 753/782 done\n","[SVM][TRAIN] Batch 754/782 done\n","[SVM][TRAIN] Batch 755/782 done\n","[SVM][TRAIN] Batch 756/782 done\n","[SVM][TRAIN] Batch 757/782 done\n","[SVM][TRAIN] Batch 758/782 done\n","[SVM][TRAIN] Batch 759/782 done\n","[SVM][TRAIN] Batch 760/782 done\n","[SVM][TRAIN] Batch 761/782 done\n","[SVM][TRAIN] Batch 762/782 done\n","[SVM][TRAIN] Batch 763/782 done\n","[SVM][TRAIN] Batch 764/782 done\n","[SVM][TRAIN] Batch 765/782 done\n","[SVM][TRAIN] Batch 766/782 done\n","[SVM][TRAIN] Batch 767/782 done\n","[SVM][TRAIN] Batch 768/782 done\n","[SVM][TRAIN] Batch 769/782 done\n","[SVM][TRAIN] Batch 770/782 done\n","[SVM][TRAIN] Batch 771/782 done\n","[SVM][TRAIN] Batch 772/782 done\n","[SVM][TRAIN] Batch 773/782 done\n","[SVM][TRAIN] Batch 774/782 done\n","[SVM][TRAIN] Batch 775/782 done\n","[SVM][TRAIN] Batch 776/782 done\n","[SVM][TRAIN] Batch 777/782 done\n","[SVM][TRAIN] Batch 778/782 done\n","[SVM][TRAIN] Batch 779/782 done\n","[SVM][TRAIN] Batch 780/782 done\n","[SVM][TRAIN] Batch 781/782 done\n","[SVM][TRAIN] Batch 782/782 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/157 done\n","[SVM][TEST] Batch 2/157 done\n","[SVM][TEST] Batch 3/157 done\n","[SVM][TEST] Batch 4/157 done\n","[SVM][TEST] Batch 5/157 done\n","[SVM][TEST] Batch 6/157 done\n","[SVM][TEST] Batch 7/157 done\n","[SVM][TEST] Batch 8/157 done\n","[SVM][TEST] Batch 9/157 done\n","[SVM][TEST] Batch 10/157 done\n","[SVM][TEST] Batch 11/157 done\n","[SVM][TEST] Batch 12/157 done\n","[SVM][TEST] Batch 13/157 done\n","[SVM][TEST] Batch 14/157 done\n","[SVM][TEST] Batch 15/157 done\n","[SVM][TEST] Batch 16/157 done\n","[SVM][TEST] Batch 17/157 done\n","[SVM][TEST] Batch 18/157 done\n","[SVM][TEST] Batch 19/157 done\n","[SVM][TEST] Batch 20/157 done\n","[SVM][TEST] Batch 21/157 done\n","[SVM][TEST] Batch 22/157 done\n","[SVM][TEST] Batch 23/157 done\n","[SVM][TEST] Batch 24/157 done\n","[SVM][TEST] Batch 25/157 done\n","[SVM][TEST] Batch 26/157 done\n","[SVM][TEST] Batch 27/157 done\n","[SVM][TEST] Batch 28/157 done\n","[SVM][TEST] Batch 29/157 done\n","[SVM][TEST] Batch 30/157 done\n","[SVM][TEST] Batch 31/157 done\n","[SVM][TEST] Batch 32/157 done\n","[SVM][TEST] Batch 33/157 done\n","[SVM][TEST] Batch 34/157 done\n","[SVM][TEST] Batch 35/157 done\n","[SVM][TEST] Batch 36/157 done\n","[SVM][TEST] Batch 37/157 done\n","[SVM][TEST] Batch 38/157 done\n","[SVM][TEST] Batch 39/157 done\n","[SVM][TEST] Batch 40/157 done\n","[SVM][TEST] Batch 41/157 done\n","[SVM][TEST] Batch 42/157 done\n","[SVM][TEST] Batch 43/157 done\n","[SVM][TEST] Batch 44/157 done\n","[SVM][TEST] Batch 45/157 done\n","[SVM][TEST] Batch 46/157 done\n","[SVM][TEST] Batch 47/157 done\n","[SVM][TEST] Batch 48/157 done\n","[SVM][TEST] Batch 49/157 done\n","[SVM][TEST] Batch 50/157 done\n","[SVM][TEST] Batch 51/157 done\n","[SVM][TEST] Batch 52/157 done\n","[SVM][TEST] Batch 53/157 done\n","[SVM][TEST] Batch 54/157 done\n","[SVM][TEST] Batch 55/157 done\n","[SVM][TEST] Batch 56/157 done\n","[SVM][TEST] Batch 57/157 done\n","[SVM][TEST] Batch 58/157 done\n","[SVM][TEST] Batch 59/157 done\n","[SVM][TEST] Batch 60/157 done\n","[SVM][TEST] Batch 61/157 done\n","[SVM][TEST] Batch 62/157 done\n","[SVM][TEST] Batch 63/157 done\n","[SVM][TEST] Batch 64/157 done\n","[SVM][TEST] Batch 65/157 done\n","[SVM][TEST] Batch 66/157 done\n","[SVM][TEST] Batch 67/157 done\n","[SVM][TEST] Batch 68/157 done\n","[SVM][TEST] Batch 69/157 done\n","[SVM][TEST] Batch 70/157 done\n","[SVM][TEST] Batch 71/157 done\n","[SVM][TEST] Batch 72/157 done\n","[SVM][TEST] Batch 73/157 done\n","[SVM][TEST] Batch 74/157 done\n","[SVM][TEST] Batch 75/157 done\n","[SVM][TEST] Batch 76/157 done\n","[SVM][TEST] Batch 77/157 done\n","[SVM][TEST] Batch 78/157 done\n","[SVM][TEST] Batch 79/157 done\n","[SVM][TEST] Batch 80/157 done\n","[SVM][TEST] Batch 81/157 done\n","[SVM][TEST] Batch 82/157 done\n","[SVM][TEST] Batch 83/157 done\n","[SVM][TEST] Batch 84/157 done\n","[SVM][TEST] Batch 85/157 done\n","[SVM][TEST] Batch 86/157 done\n","[SVM][TEST] Batch 87/157 done\n","[SVM][TEST] Batch 88/157 done\n","[SVM][TEST] Batch 89/157 done\n","[SVM][TEST] Batch 90/157 done\n","[SVM][TEST] Batch 91/157 done\n","[SVM][TEST] Batch 92/157 done\n","[SVM][TEST] Batch 93/157 done\n","[SVM][TEST] Batch 94/157 done\n","[SVM][TEST] Batch 95/157 done\n","[SVM][TEST] Batch 96/157 done\n","[SVM][TEST] Batch 97/157 done\n","[SVM][TEST] Batch 98/157 done\n","[SVM][TEST] Batch 99/157 done\n","[SVM][TEST] Batch 100/157 done\n","[SVM][TEST] Batch 101/157 done\n","[SVM][TEST] Batch 102/157 done\n","[SVM][TEST] Batch 103/157 done\n","[SVM][TEST] Batch 104/157 done\n","[SVM][TEST] Batch 105/157 done\n","[SVM][TEST] Batch 106/157 done\n","[SVM][TEST] Batch 107/157 done\n","[SVM][TEST] Batch 108/157 done\n","[SVM][TEST] Batch 109/157 done\n","[SVM][TEST] Batch 110/157 done\n","[SVM][TEST] Batch 111/157 done\n","[SVM][TEST] Batch 112/157 done\n","[SVM][TEST] Batch 113/157 done\n","[SVM][TEST] Batch 114/157 done\n","[SVM][TEST] Batch 115/157 done\n","[SVM][TEST] Batch 116/157 done\n","[SVM][TEST] Batch 117/157 done\n","[SVM][TEST] Batch 118/157 done\n","[SVM][TEST] Batch 119/157 done\n","[SVM][TEST] Batch 120/157 done\n","[SVM][TEST] Batch 121/157 done\n","[SVM][TEST] Batch 122/157 done\n","[SVM][TEST] Batch 123/157 done\n","[SVM][TEST] Batch 124/157 done\n","[SVM][TEST] Batch 125/157 done\n","[SVM][TEST] Batch 126/157 done\n","[SVM][TEST] Batch 127/157 done\n","[SVM][TEST] Batch 128/157 done\n","[SVM][TEST] Batch 129/157 done\n","[SVM][TEST] Batch 130/157 done\n","[SVM][TEST] Batch 131/157 done\n","[SVM][TEST] Batch 132/157 done\n","[SVM][TEST] Batch 133/157 done\n","[SVM][TEST] Batch 134/157 done\n","[SVM][TEST] Batch 135/157 done\n","[SVM][TEST] Batch 136/157 done\n","[SVM][TEST] Batch 137/157 done\n","[SVM][TEST] Batch 138/157 done\n","[SVM][TEST] Batch 139/157 done\n","[SVM][TEST] Batch 140/157 done\n","[SVM][TEST] Batch 141/157 done\n","[SVM][TEST] Batch 142/157 done\n","[SVM][TEST] Batch 143/157 done\n","[SVM][TEST] Batch 144/157 done\n","[SVM][TEST] Batch 145/157 done\n","[SVM][TEST] Batch 146/157 done\n","[SVM][TEST] Batch 147/157 done\n","[SVM][TEST] Batch 148/157 done\n","[SVM][TEST] Batch 149/157 done\n","[SVM][TEST] Batch 150/157 done\n","[SVM][TEST] Batch 151/157 done\n","[SVM][TEST] Batch 152/157 done\n","[SVM][TEST] Batch 153/157 done\n","[SVM][TEST] Batch 154/157 done\n","[SVM][TEST] Batch 155/157 done\n","[SVM][TEST] Batch 156/157 done\n","[SVM][TEST] Batch 157/157 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}]},{"cell_type":"code","source":["# Tạo train_svm_small với 1000 dòng đầu\n","!head -n 1000 train_svm.txt > train_svm_small.txt\n","\n","# Tạo test_svm_small với 200 dòng đầu\n","!head -n 200 test_svm.txt > test_svm_small.txt"],"metadata":{"id":"TGPGi-i01CWo","executionInfo":{"status":"ok","timestamp":1766672720948,"user_tz":-420,"elapsed":433,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/cjlin1/libsvm.git\n","%cd libsvm\n","!make"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5J-3T_Hu1ITJ","executionInfo":{"status":"ok","timestamp":1766672725793,"user_tz":-420,"elapsed":4844,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"732b3b4b-198f-45a5-8d3b-c88dcab3c83c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'libsvm'...\n","remote: Enumerating objects: 4201, done.\u001b[K\n","remote: Counting objects: 100% (230/230), done.\u001b[K\n","remote: Compressing objects: 100% (112/112), done.\u001b[K\n","remote: Total 4201 (delta 140), reused 118 (delta 118), pack-reused 3971 (from 3)\u001b[K\n","Receiving objects: 100% (4201/4201), 9.92 MiB | 14.57 MiB/s, done.\n","Resolving deltas: 100% (2317/2317), done.\n","/content/libsvm\n","g++ -Wall -Wconversion -O3 -fPIC -c svm.cpp\n","g++ -Wall -Wconversion -O3 -fPIC svm-train.c svm.o -o svm-train -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-predict.c svm.o -o svm-predict -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-scale.c -o svm-scale\n"]}]},{"cell_type":"code","source":["%cd /content/libsvm\n","\n","!./svm-train -s 0 -t 0 -c 1.0 \\\n","  /content/train_svm_small.txt \\\n","  /content/model_ae_svm\n","\n","!./svm-predict \\\n","  /content/test_svm_small.txt \\\n","  /content/model_ae_svm \\\n","  /content/pred.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45KM7JKu1KyL","executionInfo":{"status":"ok","timestamp":1766672751733,"user_tz":-420,"elapsed":25938,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"186bef29-5082-4171-b641-dec032e7d97b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/libsvm\n","....*...*\n","optimization finished, #iter = 1520\n","nu = 0.237290\n","obj = -29.230427, rho = -4.034406\n","nSV = 108, nBSV = 19\n","......*..*\n","optimization finished, #iter = 1832\n","nu = 0.602565\n","obj = -83.948453, rho = 3.805344\n","nSV = 168, nBSV = 76\n",".....*..*\n","optimization finished, #iter = 1699\n","nu = 0.252019\n","obj = -30.450040, rho = -4.090413\n","nSV = 120, nBSV = 13\n","......*..*\n","optimization finished, #iter = 1800\n","nu = 0.466539\n","obj = -61.311492, rho = 2.825039\n","nSV = 141, nBSV = 54\n","......*...*\n","optimization finished, #iter = 1932\n","nu = 0.369825\n","obj = -46.043250, rho = -2.120927\n","nSV = 137, nBSV = 35\n","...*..*\n","optimization finished, #iter = 1168\n","nu = 0.152729\n","obj = -19.359932, rho = -1.255318\n","nSV = 76, nBSV = 10\n",".....*..*\n","optimization finished, #iter = 1557\n","nu = 0.514323\n","obj = -67.342534, rho = 2.698691\n","nSV = 147, nBSV = 65\n",".....*...*\n","optimization finished, #iter = 1663\n","nu = 0.446206\n","obj = -53.775817, rho = -0.014430\n","nSV = 136, nBSV = 41\n","...*..*\n","optimization finished, #iter = 1111\n","nu = 0.198382\n","obj = -23.717155, rho = 0.185837\n","nSV = 75, nBSV = 15\n","....*...*\n","optimization finished, #iter = 1415\n","nu = 0.247096\n","obj = -29.917985, rho = 5.780209\n","nSV = 97, nBSV = 18\n","........*...*\n","optimization finished, #iter = 2509\n","nu = 0.461134\n","obj = -59.821758, rho = 0.975385\n","nSV = 163, nBSV = 40\n","....*..*\n","optimization finished, #iter = 1385\n","nu = 0.217627\n","obj = -25.311204, rho = 4.045614\n","nSV = 93, nBSV = 20\n","......*...*\n","optimization finished, #iter = 1877\n","nu = 0.303025\n","obj = -36.416444, rho = 1.653233\n","nSV = 126, nBSV = 22\n",".....*...*\n","optimization finished, #iter = 1693\n","nu = 0.310950\n","obj = -37.712723, rho = 3.426051\n","nSV = 119, nBSV = 26\n","......*...*\n","optimization finished, #iter = 1921\n","nu = 0.308008\n","obj = -35.042197, rho = 5.731919\n","nSV = 113, nBSV = 22\n",".....*..*\n","optimization finished, #iter = 1473\n","nu = 0.226090\n","obj = -24.351421, rho = 3.996378\n","nSV = 99, nBSV = 11\n","....*...*\n","optimization finished, #iter = 1495\n","nu = 0.303950\n","obj = -37.710218, rho = 1.796872\n","nSV = 112, nBSV = 24\n","....*..*\n","optimization finished, #iter = 1374\n","nu = 0.245573\n","obj = -28.709144, rho = -6.679780\n","nSV = 102, nBSV = 19\n",".....*...*\n","optimization finished, #iter = 1609\n","nu = 0.633725\n","obj = -90.946260, rho = -0.593387\n","nSV = 162, nBSV = 90\n",".....*...*\n","optimization finished, #iter = 1617\n","nu = 0.460647\n","obj = -57.306018, rho = -6.383523\n","nSV = 140, nBSV = 48\n","...*.*\n","optimization finished, #iter = 912\n","nu = 0.223809\n","obj = -26.993293, rho = -3.254932\n","nSV = 77, nBSV = 21\n",".......*....*\n","optimization finished, #iter = 2123\n","nu = 0.469428\n","obj = -58.778593, rho = -1.317047\n","nSV = 134, nBSV = 47\n","......*..*\n","optimization finished, #iter = 1594\n","nu = 0.471738\n","obj = -56.889980, rho = -3.088721\n","nSV = 125, nBSV = 47\n",".....*..*\n","optimization finished, #iter = 1463\n","nu = 0.274257\n","obj = -35.825836, rho = -2.407538\n","nSV = 97, nBSV = 25\n",".....*..*\n","optimization finished, #iter = 1535\n","nu = 0.223261\n","obj = -25.602446, rho = 5.601159\n","nSV = 99, nBSV = 12\n","......*..*\n","optimization finished, #iter = 1871\n","nu = 0.296848\n","obj = -34.982485, rho = 2.766663\n","nSV = 120, nBSV = 16\n","......*..*\n","optimization finished, #iter = 1847\n","nu = 0.283395\n","obj = -34.645281, rho = 4.198916\n","nSV = 112, nBSV = 20\n",".......*...*\n","optimization finished, #iter = 2066\n","nu = 0.312751\n","obj = -35.704198, rho = 5.651858\n","nSV = 120, nBSV = 19\n","......*...*\n","optimization finished, #iter = 1820\n","nu = 0.254714\n","obj = -26.807436, rho = 3.980027\n","nSV = 113, nBSV = 7\n",".....*..*\n","optimization finished, #iter = 1637\n","nu = 0.281211\n","obj = -35.112062, rho = 3.069193\n","nSV = 115, nBSV = 26\n","........*...*\n","optimization finished, #iter = 2318\n","nu = 0.376800\n","obj = -45.042066, rho = -4.554359\n","nSV = 127, nBSV = 32\n","....*.*\n","optimization finished, #iter = 1136\n","nu = 0.245160\n","obj = -30.209757, rho = -3.342150\n","nSV = 87, nBSV = 23\n",".......*...*\n","optimization finished, #iter = 1998\n","nu = 0.451364\n","obj = -55.828060, rho = -0.937884\n","nSV = 125, nBSV = 46\n",".......*....*\n","optimization finished, #iter = 2034\n","nu = 0.479784\n","obj = -57.775981, rho = -2.404718\n","nSV = 129, nBSV = 48\n",".....*...*\n","optimization finished, #iter = 1627\n","nu = 0.349221\n","obj = -45.020303, rho = -1.809661\n","nSV = 111, nBSV = 34\n","....*..*\n","optimization finished, #iter = 1366\n","nu = 0.204628\n","obj = -22.505760, rho = 0.690170\n","nSV = 84, nBSV = 11\n",".......*...*\n","optimization finished, #iter = 2095\n","nu = 0.418893\n","obj = -48.725452, rho = 3.760604\n","nSV = 133, nBSV = 36\n",".......*...*\n","optimization finished, #iter = 1903\n","nu = 0.413969\n","obj = -46.003146, rho = 2.441898\n","nSV = 133, nBSV = 29\n",".....*..*\n","optimization finished, #iter = 1573\n","nu = 0.309744\n","obj = -38.348038, rho = 1.187024\n","nSV = 116, nBSV = 28\n","....*..*\n","optimization finished, #iter = 1270\n","nu = 0.245257\n","obj = -27.761824, rho = 1.240600\n","nSV = 86, nBSV = 18\n","....*..*\n","optimization finished, #iter = 1161\n","nu = 0.186650\n","obj = -19.408314, rho = 2.333250\n","nSV = 81, nBSV = 13\n","......*....*\n","optimization finished, #iter = 2054\n","nu = 0.515875\n","obj = -73.764220, rho = -0.189953\n","nSV = 152, nBSV = 63\n","......*...*\n","optimization finished, #iter = 1740\n","nu = 0.562278\n","obj = -66.255759, rho = -1.972910\n","nSV = 144, nBSV = 60\n",".......*...*\n","optimization finished, #iter = 1958\n","nu = 0.303043\n","obj = -35.419111, rho = -0.241672\n","nSV = 105, nBSV = 23\n",".....*..*\n","optimization finished, #iter = 1363\n","nu = 0.252406\n","obj = -28.220672, rho = -0.685442\n","nSV = 89, nBSV = 21\n","Total nSV = 959\n","Accuracy = 39.5% (79/200) (classification)\n"]}]}]}