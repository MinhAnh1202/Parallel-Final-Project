{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# GPU Optimization Version 1"],"metadata":{"id":"a24EAxJA_QUB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pyGBuzdCjy0K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765944666699,"user_tz":-420,"elapsed":9838,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"8177fa9a-7bea-4004-e21e-ab619b10c2f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n","!apt update\n","!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n","!apt --fix-broken install"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtS48Y61O1mE","executionInfo":{"status":"ok","timestamp":1765944337851,"user_tz":-420,"elapsed":108011,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"608f9508-2de9-4e2f-9103-594f72ded3a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-17 04:03:49--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n","Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 2.16.106.154, 2.16.106.147\n","Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|2.16.106.154|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 317705436 (303M) [application/x-deb]\n","Saving to: ‘nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb’\n","\n","nsight-systems-2023 100%[===================>] 302.99M   288MB/s    in 1.1s    \n","\n","2025-12-17 04:03:51 (288 MB/s) - ‘nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb’ saved [317705436/317705436]\n","\n","Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n","Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,205 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:11 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n","Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,851 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.3 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,965 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [36.9 kB]\n","Get:23 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,546 kB]\n","Fetched 38.2 MB in 4s (9,860 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Note, selecting 'nsight-systems-2023.2.3' instead of './nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb'\n","The following additional packages will be installed:\n","  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n","  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n","  libxkbcommon-x11-0 libxtst6\n","The following NEW packages will be installed:\n","  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n","  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n","  libxkbcommon-x11-0 libxtst6 nsight-systems-2023.2.3\n","0 upgraded, 13 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 318 MB of archives.\n","After this operation, 1,302 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n","Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.2.3 2023.2.3.1001-32894139v0 [318 MB]\n","Fetched 318 MB in 1min 20s (3,981 kB/s)\n","Selecting previously unselected package libtinfo5:amd64.\n","(Reading database ... 121689 files and directories currently installed.)\n","Preparing to unpack .../00-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n","Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n","Selecting previously unselected package libxcb-xinerama0:amd64.\n","Preparing to unpack .../01-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n","Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n","Selecting previously unselected package libxcb-xinput0:amd64.\n","Preparing to unpack .../02-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n","Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n","Selecting previously unselected package libxcb-xkb1:amd64.\n","Preparing to unpack .../03-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n","Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n","Selecting previously unselected package libxcomposite1:amd64.\n","Preparing to unpack .../04-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n","Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n","Selecting previously unselected package libxkbcommon-x11-0:amd64.\n","Preparing to unpack .../05-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n","Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libxtst6:amd64.\n","Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package libxcb-icccm4:amd64.\n","Preparing to unpack .../07-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n","Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n","Selecting previously unselected package libxcb-util1:amd64.\n","Preparing to unpack .../08-libxcb-util1_0.4.0-1build2_amd64.deb ...\n","Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n","Selecting previously unselected package libxcb-image0:amd64.\n","Preparing to unpack .../09-libxcb-image0_0.4.0-2_amd64.deb ...\n","Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n","Selecting previously unselected package libxcb-keysyms1:amd64.\n","Preparing to unpack .../10-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n","Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n","Selecting previously unselected package libxcb-render-util0:amd64.\n","Preparing to unpack .../11-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n","Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n","Selecting previously unselected package nsight-systems-2023.2.3.\n","Preparing to unpack .../12-nsight-systems-2023.2.3_2023.2.3.1001-32894139v0_amd64.deb ...\n","Unpacking nsight-systems-2023.2.3 (2023.2.3.1001-32894139v0) ...\n","Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n","Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n","Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n","Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n","Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n","Setting up libxcb-image0:amd64 (0.4.0-2) ...\n","Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n","Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n","Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n","Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n","Setting up nsight-systems-2023.2.3 (2023.2.3.1001-32894139v0) ...\n","update-alternatives: using /opt/nvidia/nsight-systems/2023.2.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n","update-alternatives: using /opt/nvidia/nsight-systems/2023.2.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n"]}]},{"cell_type":"code","source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","!tar -xzvf cifar-10-binary.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umGmEDJwuu4F","executionInfo":{"status":"ok","timestamp":1765983490135,"user_tz":-420,"elapsed":16773,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"0192b058-8046-4010-d692-3743521fc486"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-17 14:57:53--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  12.5MB/s    in 14s     \n","\n","2025-12-17 14:58:08 (11.9 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n","\n","cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"markdown","source":["## 1) Huấn luyện Autoencoder:"],"metadata":{"id":"_0qiSIz__jl8"}},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mllk3VwATtDf","executionInfo":{"status":"ok","timestamp":1765983463654,"user_tz":-420,"elapsed":43,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"e5491d3d-29f0-42b2-ca30-620577a43339"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.cu\n","#include \"gpu_layers_opt1.h\"\n","\n","// --------------- Conv2D forward (optimization 1) ------------------\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // Load input tile into shared memory\n","        for (int i = ty; i < BLOCK_H; i += blockDim.y) {\n","            for (int j = tx; j < BLOCK_W; j += blockDim.x) {\n","                int h_in = row_start + i;\n","                int w_in = col_start + j;\n","\n","                if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                    size_t input_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                    smem[i][j] = input[input_idx];\n","                } else {\n","                    smem[i][j] = 0.0f;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","\n","        // Compute convolution\n","        for (int i = 0; i < K; ++i) {\n","            for (int j = 0; j < K; ++j) {\n","                size_t weight_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","                value += smem[ty + i][tx + j] * weight[weight_idx];\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    // Shared Memory chứa dY\n","    __shared__ float s_dY[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    // Tọa độ dX (output của kernel này)\n","    int h_out = blockIdx.y * TILE_H + ty;\n","    int w_out = blockIdx.x * TILE_W + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n = nc / C_in;\n","    //if (h_out >= H || w_out >= W || n >= N) return;\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // Tọa độ dY cần tải\n","        int h_global = h_out;\n","        int w_global = w_out;\n","\n","        // Tải main tile\n","        if (h_global >= 0 && h_global < H_out && w_global >= 0 && w_global < W_out) {\n","            s_dY[ty + pad][tx + pad] = dY[idx4(n, c_out, h_global, w_global, C_out, H_out, W_out)];\n","        } else {\n","            s_dY[ty + pad][tx + pad] = 0.0f;\n","        }\n","\n","        // Tải biên trên/dưới\n","        if (ty < pad) {\n","            // Biên trên\n","            int h_top = blockIdx.y * TILE_H + ty - pad;\n","            if (h_top >= 0 && h_top < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[ty][tx + pad] = dY[idx4(n, c_out, h_top, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty][tx + pad] = 0.0f;\n","            }\n","\n","            // Biên dưới\n","            int h_bottom = blockIdx.y * TILE_H + TILE_H + pad - 1 - ty;\n","            if (h_bottom >= 0 && h_bottom < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = dY[idx4(n, c_out, h_bottom, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","            }\n","        }\n","\n","        // Tải biên trái/phải\n","        if (tx < pad) {\n","            // Biên trái\n","            int w_left = blockIdx.x * TILE_W + tx - pad;\n","            if (w_left >= 0 && w_left < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][tx] = dY[idx4(n, c_out, h_global, w_left, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][tx] = 0.0f;\n","            }\n","\n","            // Biên phải\n","            int w_right = blockIdx.x * TILE_W + TILE_W + pad - 1 - tx;\n","            if (w_right >= 0 && w_right < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = dY[idx4(n, c_out, h_global, w_right, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","            }\n","        }\n","\n","        // Tải 4 góc (Thread (0,0) tải)\n","        if (tx == 0 && ty == 0) {\n","            // Góc trên trái [0][0]\n","            int h_c = blockIdx.y * TILE_H - pad;\n","            int w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[0][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc trên phải [0][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới trái [17][0]\n","            h_c = blockIdx.y * TILE_H + TILE_H + pad - 1;\n","            w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới phải [17][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","        }\n","\n","        __syncthreads();\n","\n","        // Tính convolution\n","            for (int kh = 0; kh < K; ++kh) {\n","                for (int kw = 0; kw < K; ++kw) {\n","                    int smem_y = ty + 2 * pad - kh;\n","                    int smem_x = tx + 2* pad - kw;\n","\n","                    size_t w_idx = idx4(c_out, c_in, K - 1 - kh, K - 1 - kw, C_in, K, K);\n","                    value += s_dY[smem_y][smem_x] * weight[w_idx];\n","                }\n","            }\n","        __syncthreads();\n","    }\n","\n","    if (h_out < H && w_out < W && n < N) {\n","        dX[idx4(n, c_in, h_out, w_out, C_in, H, W)] = value;\n","    }\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    float dw[K][K];\n","    for (int i = 0; i < K; ++i)\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // Load s_dY\n","                int h_out = block_h * TILE_H + ty;\n","                int w_out = block_w * TILE_W + tx;\n","\n","                if (h_out < H_out && w_out < W_out) {\n","                    s_dY[ty][tx] = dY[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)];\n","                } else {\n","                    s_dY[ty][tx] = 0.0f;\n","                }\n","\n","                // Load s_in\n","                int h_in_base = block_h * TILE_H + ty;\n","                int w_in_base = block_w * TILE_W + tx;\n","\n","                // Main tile\n","                if (h_in_base >= 0 && h_in_base < H && w_in_base >= 0 && w_in_base < W) {\n","                    s_in[ty + pad][tx + pad] = input[idx4(n, c_in, h_in_base, w_in_base, C_in, H, W)];\n","                } else {\n","                    s_in[ty + pad][tx + pad] = 0.0f;\n","                }\n","\n","                // Top/bottom borders\n","                if (ty < pad) {\n","                    int h_top = block_h * TILE_H - pad + ty;\n","                    if (h_top >= 0 && h_top < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[ty][tx + pad] = input[idx4(n, c_in, h_top, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[ty][tx + pad] = 0.0f;\n","                    }\n","\n","                    int h_bottom = block_h * TILE_H + TILE_H + pad - 1 - ty;\n","                    if (h_bottom >= 0 && h_bottom < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = input[idx4(n, c_in, h_bottom, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","                    }\n","                }\n","\n","                // Left/right borders\n","                if (tx < pad) {\n","                    int w_left = block_w * TILE_W - pad + tx;\n","                    if (w_left >= 0 && w_left < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][tx] = input[idx4(n, c_in, h_in_base, w_left, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][tx] = 0.0f;\n","                    }\n","\n","                    int w_right = block_w * TILE_W + TILE_W + pad - 1 - tx;\n","                    if (w_right >= 0 && w_right < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = input[idx4(n, c_in, h_in_base, w_right, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","                    }\n","                }\n","\n","                // Thread (0,0) loads 4 corners\n","                if (tx == 0 && ty == 0) {\n","                    int h_c = block_h * TILE_H - pad;\n","                    int w_c = block_w * TILE_W - pad;\n","                    s_in[0][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    h_c = block_h * TILE_H + TILE_H + pad - 1;\n","                    w_c = block_w * TILE_W - pad;\n","                    s_in[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","                }\n","\n","                __syncthreads();\n","\n","                // Compute dW: X * dY\n","                float val_dy = s_dY[ty][tx];\n","                for (int kh = 0; kh < K; ++kh) {\n","                    for (int kw = 0; kw < K; ++kw) {\n","                        dw[kh][kw] += s_in[ty + kh][tx + kw] * val_dy;\n","                    }\n","                }\n","\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    for (int i = 0; i < K; ++i) {\n","        for (int j = 0; j < K; ++j) {\n","            // Tính index global của dW[i][j] cho cặp filter (c_out, c_in)\n","            size_t dw_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","            // Cộng giá trị dw[i][j] của thread hiện tại vào bộ nhớ global\n","            atomicAdd(&dW[dw_idx], dw[i][j]);\n","        }\n","    }\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","    extern __shared__ float sdata[];\n","    int tid = threadIdx.x;\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","    float sum = 0.0f;\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","    sdata[tid] = sum;\n","    __syncthreads();\n","\n","    // Reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        dB[c] = sdata[0];\n","    }\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T1wg7t4T7La","executionInfo":{"status":"ok","timestamp":1765983463997,"user_tz":-420,"elapsed":34,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"170f2035-77ac-40c1-e122-18a55faf0b0d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt1.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers_opt1.h\"\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0; ///\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights, biases ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhtU3GMYT-vO","executionInfo":{"status":"ok","timestamp":1765983464368,"user_tz":-420,"elapsed":3,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"35d89e90-a115-4efc-f75f-0772de580a52"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt1.cu\n","#include \"gpu_autoencoder_opt1.h\"\n","#include <cmath>\n","\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","// find max weight bytes\n","size_t max_w_bytes = w1_bytes;\n","if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","// find max bias bytes\n","size_t max_b_bytes = b1_bytes;\n","if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","float *h_w = (float*)malloc(max_w_bytes);\n","float *h_b = (float*)malloc(max_b_bytes);\n","\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ---------- [batch_size, channels, height, width]\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","\n","// Forward pass for ONE batch (no backward yet)\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ------------- copy input to device -------------\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ========= ENCODER =========\n","    // conv1: 3 -> 256, same 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b1, C_out * sizeof(float)));\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16, then pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b2, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT is ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","    // conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b3, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b3, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv4: 128 -> 256, 16x16, then upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b4, 256 * sizeof(float)));\n","        update_dc_bias(ae->d_b4, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv5: 256 -> 3, 32x32 (no activation, usually MSE on raw output)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b5, 3 * sizeof(float)));\n","        update_dc_bias(ae->d_b5, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","    }\n","\n","    // ------------- (optional) compute MSE loss -------------\n","    float loss_value = 0.0f;\n","        if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        // kernel giờ trả về SUM(diff^2) vào d_loss\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;  // MSE = sum / size\n","    }\n","\n","\n","    // ------------- copy output back to host -------------\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H; // 32\n","    const int W0 = ae->W; // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    //Zero all gradient buffers\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        // dU2\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW5\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_u2, ae->d_gout, ae->d_gw5, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB5\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gout, ae->d_gb5, N, C_out, H, W);\n","\n","        // SGD update W5,B5\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward: U2 <- H4 =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;   // H4 size\n","        int Hu = 32, Wu = 32; // U2 size\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int C = 256, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward: 128->256, 16x16 (input U1, output H4) =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        // dU1\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW4\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_u1, ae->d_gh4, ae->d_gw4, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB4\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh4, ae->d_gb4, N, C_out, H, W);\n","\n","        // SGD W4,B4\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward: U1 <- H3 =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;   // H3 size\n","        int Hu = 16, Wu = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int C = 128, H = 8, W = 8;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward: 128->128, 8x8 (input P2, output H3) =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        // dP2\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW3\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        CHECK_CUDA(cudaMemset(ae->d_gw3, 0, C_out * C_in * K * K * sizeof(float)));\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_p2, ae->d_gh3, ae->d_gw3, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB3\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh3, ae->d_gb3, N, C_out, H, W);\n","\n","        // SGD W3,B3\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward: P2 <- H2 =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16; // H2 size\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int C = 128, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward: 256->128, 16x16 (input P1, output H2) =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        // dP1\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW2\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        CHECK_CUDA(cudaMemset(ae->d_gw2, 0, C_out * C_in * K * K * sizeof(float)));\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_p1, ae->d_gh2, ae->d_gw2, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB2\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh2, ae->d_gb2, N, C_out, H, W);\n","\n","        // SGD W2,B2\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward: P1 <- H1 =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32; // H1 size\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int C = 256, H = 32, W = 32;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward: 3->256, 32x32 (input X0, output H1) =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        // dX0 (không dùng tiếp nhưng tính cho đủ)\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW1\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_x0, ae->d_gh1, ae->d_gw1, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB1\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh1, ae->d_gb1, N, C_out, H, W);\n","\n","        // SGD W1,B1\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // copy input [N_batch, 3, 32, 32] lên GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, rồi ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b1, 256 * sizeof(float)));\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, rồi ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b2, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","\n","         cudaError_t err = cudaGetLastError();\n","   if (err != cudaSuccess) {\n","       fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n","   }\n","   cudaDeviceSynchronize();\n","    }\n","    cudaDeviceSynchronize();\n","\n","size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNI8x13dUDV8","executionInfo":{"status":"ok","timestamp":1765983464726,"user_tz":-420,"elapsed":41,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"c56d5f45-b1e0-40d8-e797-13420b856f74"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvY6SJ9kUFdH","executionInfo":{"status":"ok","timestamp":1765983464979,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"2ada76ff-2e7a-4109-93ad-15f61e8635a5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.h\n"]}]},{"cell_type":"code","source":["%%writefile load_data.cu\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","    // for (int i = 0; i < 2; i++) {\n","    //     printf(\"Label: %d\\n\", data->test_labels[i]);\n","    //     for (int j = 0; j < IMG_SIZE; j++) {\n","    //         printf(\"%f \", data->test_images[i*IMG_SIZE + j]);\n","    //     }\n","    // }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMx-8h6pUIHS","executionInfo":{"status":"ok","timestamp":1765983465264,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"4b6a27f8-078a-400b-8a80-c4bfa172803f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.cu\n"]}]},{"cell_type":"code","source":["%%writefile main_gpu_opt1.cu\n","// the UPDATED main() code with argc/argv\n","// main_gpu.cu (DEBUG VERSION)\n","#include <cstdio>\n","#include <ctime>\n","#include <cuda_runtime.h>\n","#include <cstdlib>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt1.h\"\n","\n","// GpuTimer dùng cudaEvent để đo time\n","struct GpuTimer {\n","    cudaEvent_t start, stop;\n","    GpuTimer() {\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","    }\n","    ~GpuTimer() {\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","    void tic() {\n","        cudaEventRecord(start, 0);\n","    }\n","    float toc() {\n","        cudaEventRecord(stop, 0);\n","        cudaEventSynchronize(stop);\n","        float ms = 0.0f;\n","        cudaEventElapsedTime(&ms, start, stop);\n","        return ms;\n","    }\n","};\n","\n","void print_gpu_memory() {\n","    size_t free_byte, total_byte;\n","    cudaError_t status = cudaMemGetInfo(&free_byte, &total_byte);\n","\n","    if (status == cudaSuccess) {\n","        double total_mb = (double)total_byte / (1024.0 * 1024.0);\n","        double free_mb = (double)free_byte / (1024.0 * 1024.0);\n","        double used_mb = total_mb - free_mb;\n","\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB / Total: %.2f MB\\n\", used_mb, total_mb);\n","    }\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","\n","    printf(\"[MAIN] Start program\\n\");\n","    fflush(stdout);\n","\n","    // ---- Check GPU device ----\n","    int deviceCount = 0;\n","    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n","    if (err != cudaSuccess) {\n","        fprintf(stderr, \"[MAIN] cudaGetDeviceCount error: %s\\n\",\n","                cudaGetErrorString(err));\n","        return 1;\n","    }\n","    printf(\"[MAIN] Num CUDA devices = %d\\n\", deviceCount);\n","    fflush(stdout);\n","\n","    // ---- Load CIFAR-10 on CPU ----\n","    Cifar10 data;\n","    load_cifar10(&data);   // câu lệnh này in: CIFAR-10 loaded successfully ...\n","    printf(\"[MAIN] After load_cifar10\\n\");\n","    fflush(stdout);\n","\n","    normalize_cifar10(&data);\n","    printf(\"[MAIN] After normalize_cifar10\\n\");\n","    fflush(stdout);\n","\n","    GpuTimer epoch_timer;\n","\n","    int batch_size = 64;\n","    int epochs     = 20;\n","    float lr       = 1e-3f;\n","    float total_time = 0.0f;\n","\n","    int num_batches = TRAIN_NUM / batch_size;\n","\n","    printf(\"[MAIN] Start training loop (epochs=%d, num_batches=%d)\\n\",\n","       epochs, num_batches);\n","    fflush(stdout);\n","\n","\n","    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float)); // dùng làm buffer tạm\n","\n","    // ---- Init GPU autoencoder ----\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","\n","\n","    for (int epoch = 0; epoch < epochs; ++epoch) {\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","\n","        epoch_timer.tic();\n","\n","        for (int b = 0; b < num_batches; ++b) {\n","            get_next_batch(&data, batch_size, b, h_batch);\n","\n","            float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n","            gpu_autoencoder_backward(&ae, lr);\n","\n","            epoch_loss += loss;\n","\n","            if ((b + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\",\n","                       epoch, b + 1, num_batches, loss);\n","                fflush(stdout);\n","            }\n","        }\n","\n","        float ms = epoch_timer.toc();\n","        total_time += ms;\n","        printf(\"==> Epoch %d done. Avg loss = %f, time = %.3f ms (%.3f s)\\n\",\n","               epoch, epoch_loss / num_batches, ms, ms / 1000.0f);\n","        fflush(stdout);\n","    }\n","\n","    printf(\"[MAIN] Training finished\\n\");\n","    printf(\"[MAIN] Total training time = %.3f s\\n\", total_time / 1000.0f);\n","    print_gpu_memory();\n","    fflush(stdout);\n","\n","    // save weights\n","    gpu_autoencoder_save_weights(&ae, \"ae_weights.bin\");\n","\n","    // ---- cleanup ----\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_output);\n","    free_cifar10(&data);\n","\n","    printf(\"[MAIN] Program finished\\n\");\n","    fflush(stdout);\n","\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TW_V3KJpUKgN","executionInfo":{"status":"ok","timestamp":1765983465609,"user_tz":-420,"elapsed":7,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"927bd0de-83d8-4afe-a8dc-cb7c84aabcb5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main_gpu_opt1.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 main_gpu_opt1.cu gpu_autoencoder_opt1.cu gpu_layers_opt1.cu load_data.cu -o autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xf_iarc9u6YS","executionInfo":{"status":"ok","timestamp":1765944682088,"user_tz":-420,"elapsed":6655,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"61157a62-532b-4c75-af76-c2a75f01e186"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(469)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(470)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJtcww_cu90u","executionInfo":{"status":"ok","timestamp":1765948051629,"user_tz":-420,"elapsed":3369534,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"1e796b79-6344-4b6e-de6b-3d431dbc2e40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=20, num_batches=781)\n","[TRAIN] Epoch 0, batch 100/781, loss = 0.086233\n","[TRAIN] Epoch 0, batch 200/781, loss = 0.056952\n","[TRAIN] Epoch 0, batch 300/781, loss = 0.066579\n","[TRAIN] Epoch 0, batch 400/781, loss = 0.052316\n","[TRAIN] Epoch 0, batch 500/781, loss = 0.057060\n","[TRAIN] Epoch 0, batch 600/781, loss = 0.052664\n","[TRAIN] Epoch 0, batch 700/781, loss = 0.052484\n","==> Epoch 0 done. Avg loss = 0.069887, time = 166149.469 ms (166.149 s)\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.052241\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.044003\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.046434\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.045793\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.042899\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.045240\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.040449\n","==> Epoch 1 done. Avg loss = 0.047252, time = 168542.906 ms (168.543 s)\n","[TRAIN] Epoch 2, batch 100/781, loss = 0.034766\n","[TRAIN] Epoch 2, batch 200/781, loss = 0.038874\n","[TRAIN] Epoch 2, batch 300/781, loss = 0.040698\n","[TRAIN] Epoch 2, batch 400/781, loss = 0.035377\n","[TRAIN] Epoch 2, batch 500/781, loss = 0.038092\n","[TRAIN] Epoch 2, batch 600/781, loss = 0.035100\n","[TRAIN] Epoch 2, batch 700/781, loss = 0.035693\n","==> Epoch 2 done. Avg loss = 0.039051, time = 168487.203 ms (168.487 s)\n","[TRAIN] Epoch 3, batch 100/781, loss = 0.033834\n","[TRAIN] Epoch 3, batch 200/781, loss = 0.035122\n","[TRAIN] Epoch 3, batch 300/781, loss = 0.028685\n","[TRAIN] Epoch 3, batch 400/781, loss = 0.033567\n","[TRAIN] Epoch 3, batch 500/781, loss = 0.029576\n","[TRAIN] Epoch 3, batch 600/781, loss = 0.028938\n","[TRAIN] Epoch 3, batch 700/781, loss = 0.029414\n","==> Epoch 3 done. Avg loss = 0.032295, time = 168578.234 ms (168.578 s)\n","[TRAIN] Epoch 4, batch 100/781, loss = 0.028475\n","[TRAIN] Epoch 4, batch 200/781, loss = 0.028677\n","[TRAIN] Epoch 4, batch 300/781, loss = 0.028210\n","[TRAIN] Epoch 4, batch 400/781, loss = 0.027574\n","[TRAIN] Epoch 4, batch 500/781, loss = 0.024948\n","[TRAIN] Epoch 4, batch 600/781, loss = 0.026791\n","[TRAIN] Epoch 4, batch 700/781, loss = 0.026406\n","==> Epoch 4 done. Avg loss = 0.027590, time = 168633.328 ms (168.633 s)\n","[TRAIN] Epoch 5, batch 100/781, loss = 0.024154\n","[TRAIN] Epoch 5, batch 200/781, loss = 0.025901\n","[TRAIN] Epoch 5, batch 300/781, loss = 0.026658\n","[TRAIN] Epoch 5, batch 400/781, loss = 0.022021\n","[TRAIN] Epoch 5, batch 500/781, loss = 0.024472\n","[TRAIN] Epoch 5, batch 600/781, loss = 0.025887\n","[TRAIN] Epoch 5, batch 700/781, loss = 0.023630\n","==> Epoch 5 done. Avg loss = 0.024846, time = 168469.109 ms (168.469 s)\n","[TRAIN] Epoch 6, batch 100/781, loss = 0.022622\n","[TRAIN] Epoch 6, batch 200/781, loss = 0.023220\n","[TRAIN] Epoch 6, batch 300/781, loss = 0.025678\n","[TRAIN] Epoch 6, batch 400/781, loss = 0.023405\n","[TRAIN] Epoch 6, batch 500/781, loss = 0.022720\n","[TRAIN] Epoch 6, batch 600/781, loss = 0.023031\n","[TRAIN] Epoch 6, batch 700/781, loss = 0.024399\n","==> Epoch 6 done. Avg loss = 0.023257, time = 168674.000 ms (168.674 s)\n","[TRAIN] Epoch 7, batch 100/781, loss = 0.022754\n","[TRAIN] Epoch 7, batch 200/781, loss = 0.021682\n","[TRAIN] Epoch 7, batch 300/781, loss = 0.023529\n","[TRAIN] Epoch 7, batch 400/781, loss = 0.022843\n","[TRAIN] Epoch 7, batch 500/781, loss = 0.022786\n","[TRAIN] Epoch 7, batch 600/781, loss = 0.021218\n","[TRAIN] Epoch 7, batch 700/781, loss = 0.020554\n","==> Epoch 7 done. Avg loss = 0.022130, time = 168450.844 ms (168.451 s)\n","[TRAIN] Epoch 8, batch 100/781, loss = 0.021925\n","[TRAIN] Epoch 8, batch 200/781, loss = 0.021250\n","[TRAIN] Epoch 8, batch 300/781, loss = 0.020299\n","[TRAIN] Epoch 8, batch 400/781, loss = 0.020655\n","[TRAIN] Epoch 8, batch 500/781, loss = 0.020346\n","[TRAIN] Epoch 8, batch 600/781, loss = 0.021033\n","[TRAIN] Epoch 8, batch 700/781, loss = 0.021796\n","==> Epoch 8 done. Avg loss = 0.021186, time = 168505.234 ms (168.505 s)\n","[TRAIN] Epoch 9, batch 100/781, loss = 0.019783\n","[TRAIN] Epoch 9, batch 200/781, loss = 0.020344\n","[TRAIN] Epoch 9, batch 300/781, loss = 0.018415\n","[TRAIN] Epoch 9, batch 400/781, loss = 0.021532\n","[TRAIN] Epoch 9, batch 500/781, loss = 0.022765\n","[TRAIN] Epoch 9, batch 600/781, loss = 0.019877\n","[TRAIN] Epoch 9, batch 700/781, loss = 0.020056\n","==> Epoch 9 done. Avg loss = 0.020361, time = 168679.609 ms (168.680 s)\n","[TRAIN] Epoch 10, batch 100/781, loss = 0.020152\n","[TRAIN] Epoch 10, batch 200/781, loss = 0.019901\n","[TRAIN] Epoch 10, batch 300/781, loss = 0.019579\n","[TRAIN] Epoch 10, batch 400/781, loss = 0.018995\n","[TRAIN] Epoch 10, batch 500/781, loss = 0.018584\n","[TRAIN] Epoch 10, batch 600/781, loss = 0.020652\n","[TRAIN] Epoch 10, batch 700/781, loss = 0.018987\n","==> Epoch 10 done. Avg loss = 0.019637, time = 168400.047 ms (168.400 s)\n","[TRAIN] Epoch 11, batch 100/781, loss = 0.017562\n","[TRAIN] Epoch 11, batch 200/781, loss = 0.019025\n","[TRAIN] Epoch 11, batch 300/781, loss = 0.020062\n","[TRAIN] Epoch 11, batch 400/781, loss = 0.017592\n","[TRAIN] Epoch 11, batch 500/781, loss = 0.019776\n","[TRAIN] Epoch 11, batch 600/781, loss = 0.019537\n","[TRAIN] Epoch 11, batch 700/781, loss = 0.018732\n","==> Epoch 11 done. Avg loss = 0.019001, time = 168548.094 ms (168.548 s)\n","[TRAIN] Epoch 12, batch 100/781, loss = 0.019660\n","[TRAIN] Epoch 12, batch 200/781, loss = 0.019312\n","[TRAIN] Epoch 12, batch 300/781, loss = 0.018008\n","[TRAIN] Epoch 12, batch 400/781, loss = 0.018528\n","[TRAIN] Epoch 12, batch 500/781, loss = 0.018512\n","[TRAIN] Epoch 12, batch 600/781, loss = 0.016946\n","[TRAIN] Epoch 12, batch 700/781, loss = 0.018313\n","==> Epoch 12 done. Avg loss = 0.018457, time = 168469.641 ms (168.470 s)\n","[TRAIN] Epoch 13, batch 100/781, loss = 0.018903\n","[TRAIN] Epoch 13, batch 200/781, loss = 0.019062\n","[TRAIN] Epoch 13, batch 300/781, loss = 0.017163\n","[TRAIN] Epoch 13, batch 400/781, loss = 0.018317\n","[TRAIN] Epoch 13, batch 500/781, loss = 0.015952\n","[TRAIN] Epoch 13, batch 600/781, loss = 0.018903\n","[TRAIN] Epoch 13, batch 700/781, loss = 0.017626\n","==> Epoch 13 done. Avg loss = 0.017998, time = 168617.938 ms (168.618 s)\n","[TRAIN] Epoch 14, batch 100/781, loss = 0.017428\n","[TRAIN] Epoch 14, batch 200/781, loss = 0.016627\n","[TRAIN] Epoch 14, batch 300/781, loss = 0.016823\n","[TRAIN] Epoch 14, batch 400/781, loss = 0.015014\n","[TRAIN] Epoch 14, batch 500/781, loss = 0.019017\n","[TRAIN] Epoch 14, batch 600/781, loss = 0.017348\n","[TRAIN] Epoch 14, batch 700/781, loss = 0.016648\n","==> Epoch 14 done. Avg loss = 0.017610, time = 168555.078 ms (168.555 s)\n","[TRAIN] Epoch 15, batch 100/781, loss = 0.018272\n","[TRAIN] Epoch 15, batch 200/781, loss = 0.019570\n","[TRAIN] Epoch 15, batch 300/781, loss = 0.016749\n","[TRAIN] Epoch 15, batch 400/781, loss = 0.019832\n","[TRAIN] Epoch 15, batch 500/781, loss = 0.018449\n","[TRAIN] Epoch 15, batch 600/781, loss = 0.017582\n","[TRAIN] Epoch 15, batch 700/781, loss = 0.016779\n","==> Epoch 15 done. Avg loss = 0.017288, time = 168497.984 ms (168.498 s)\n","[TRAIN] Epoch 16, batch 100/781, loss = 0.017302\n","[TRAIN] Epoch 16, batch 200/781, loss = 0.016079\n","[TRAIN] Epoch 16, batch 300/781, loss = 0.018189\n","[TRAIN] Epoch 16, batch 400/781, loss = 0.016094\n","[TRAIN] Epoch 16, batch 500/781, loss = 0.016510\n","[TRAIN] Epoch 16, batch 600/781, loss = 0.016664\n","[TRAIN] Epoch 16, batch 700/781, loss = 0.016063\n","==> Epoch 16 done. Avg loss = 0.017027, time = 168592.641 ms (168.593 s)\n","[TRAIN] Epoch 17, batch 100/781, loss = 0.016130\n","[TRAIN] Epoch 17, batch 200/781, loss = 0.015105\n","[TRAIN] Epoch 17, batch 300/781, loss = 0.017029\n","[TRAIN] Epoch 17, batch 400/781, loss = 0.015707\n","[TRAIN] Epoch 17, batch 500/781, loss = 0.019025\n","[TRAIN] Epoch 17, batch 600/781, loss = 0.016317\n","[TRAIN] Epoch 17, batch 700/781, loss = 0.016737\n","==> Epoch 17 done. Avg loss = 0.016816, time = 168462.297 ms (168.462 s)\n","[TRAIN] Epoch 18, batch 100/781, loss = 0.017042\n","[TRAIN] Epoch 18, batch 200/781, loss = 0.017246\n","[TRAIN] Epoch 18, batch 300/781, loss = 0.017451\n","[TRAIN] Epoch 18, batch 400/781, loss = 0.018816\n","[TRAIN] Epoch 18, batch 500/781, loss = 0.017866\n","[TRAIN] Epoch 18, batch 600/781, loss = 0.014618\n","[TRAIN] Epoch 18, batch 700/781, loss = 0.014808\n","==> Epoch 18 done. Avg loss = 0.016636, time = 168714.062 ms (168.714 s)\n","[TRAIN] Epoch 19, batch 100/781, loss = 0.017838\n","[TRAIN] Epoch 19, batch 200/781, loss = 0.016116\n","[TRAIN] Epoch 19, batch 300/781, loss = 0.015516\n","[TRAIN] Epoch 19, batch 400/781, loss = 0.016976\n","[TRAIN] Epoch 19, batch 500/781, loss = 0.015132\n","[TRAIN] Epoch 19, batch 600/781, loss = 0.015997\n","[TRAIN] Epoch 19, batch 700/781, loss = 0.015832\n","==> Epoch 19 done. Avg loss = 0.016480, time = 168523.016 ms (168.523 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 3368.551 s\n","[SYSTEM] Memory Usage: 474.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n"]}]},{"cell_type":"code","source":["!cp ae_weights.bin \"/content/drive/MyDrive/Parallel - Final Project/ae_weights.bin\""],"metadata":{"id":"uGz9keKg8X1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvprof ./autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTysm71UqDvv","executionInfo":{"status":"ok","timestamp":1765951428468,"user_tz":-420,"elapsed":3375634,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"8e05be72-b4dd-44e4-a15d-fd379711c38f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","==19020== NVPROF is profiling process 19020, command: ./autoencoder_gpu\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=20, num_batches=781)\n","[TRAIN] Epoch 0, batch 100/781, loss = 0.095315\n","[TRAIN] Epoch 0, batch 200/781, loss = 0.063234\n","[TRAIN] Epoch 0, batch 300/781, loss = 0.054953\n","[TRAIN] Epoch 0, batch 400/781, loss = 0.047522\n","[TRAIN] Epoch 0, batch 500/781, loss = 0.059681\n","[TRAIN] Epoch 0, batch 600/781, loss = 0.054428\n","[TRAIN] Epoch 0, batch 700/781, loss = 0.049625\n","==> Epoch 0 done. Avg loss = 0.069004, time = 168639.641 ms (168.640 s)\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.048943\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.048583\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.049358\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.050574\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.048369\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.046613\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.045602\n","==> Epoch 1 done. Avg loss = 0.046856, time = 168473.156 ms (168.473 s)\n","[TRAIN] Epoch 2, batch 100/781, loss = 0.044978\n","[TRAIN] Epoch 2, batch 200/781, loss = 0.039682\n","[TRAIN] Epoch 2, batch 300/781, loss = 0.045353\n","[TRAIN] Epoch 2, batch 400/781, loss = 0.043281\n","[TRAIN] Epoch 2, batch 500/781, loss = 0.038519\n","[TRAIN] Epoch 2, batch 600/781, loss = 0.037979\n","[TRAIN] Epoch 2, batch 700/781, loss = 0.034660\n","==> Epoch 2 done. Avg loss = 0.039530, time = 168583.594 ms (168.584 s)\n","[TRAIN] Epoch 3, batch 100/781, loss = 0.034362\n","[TRAIN] Epoch 3, batch 200/781, loss = 0.034489\n","[TRAIN] Epoch 3, batch 300/781, loss = 0.032240\n","[TRAIN] Epoch 3, batch 400/781, loss = 0.033105\n","[TRAIN] Epoch 3, batch 500/781, loss = 0.032212\n","[TRAIN] Epoch 3, batch 600/781, loss = 0.032365\n","[TRAIN] Epoch 3, batch 700/781, loss = 0.031256\n","==> Epoch 3 done. Avg loss = 0.033217, time = 168407.047 ms (168.407 s)\n","[TRAIN] Epoch 4, batch 100/781, loss = 0.029582\n","[TRAIN] Epoch 4, batch 200/781, loss = 0.032743\n","[TRAIN] Epoch 4, batch 300/781, loss = 0.028216\n","[TRAIN] Epoch 4, batch 400/781, loss = 0.026893\n","[TRAIN] Epoch 4, batch 500/781, loss = 0.028561\n","[TRAIN] Epoch 4, batch 600/781, loss = 0.026443\n","[TRAIN] Epoch 4, batch 700/781, loss = 0.025884\n","==> Epoch 4 done. Avg loss = 0.028527, time = 168609.938 ms (168.610 s)\n","[TRAIN] Epoch 5, batch 100/781, loss = 0.027029\n","[TRAIN] Epoch 5, batch 200/781, loss = 0.027220\n","[TRAIN] Epoch 5, batch 300/781, loss = 0.025242\n","[TRAIN] Epoch 5, batch 400/781, loss = 0.023170\n","[TRAIN] Epoch 5, batch 500/781, loss = 0.028446\n","[TRAIN] Epoch 5, batch 600/781, loss = 0.024736\n","[TRAIN] Epoch 5, batch 700/781, loss = 0.024702\n","==> Epoch 5 done. Avg loss = 0.025657, time = 168564.219 ms (168.564 s)\n","[TRAIN] Epoch 6, batch 100/781, loss = 0.023536\n","[TRAIN] Epoch 6, batch 200/781, loss = 0.026389\n","[TRAIN] Epoch 6, batch 300/781, loss = 0.024082\n","[TRAIN] Epoch 6, batch 400/781, loss = 0.024490\n","[TRAIN] Epoch 6, batch 500/781, loss = 0.023952\n","[TRAIN] Epoch 6, batch 600/781, loss = 0.024397\n","[TRAIN] Epoch 6, batch 700/781, loss = 0.024256\n","==> Epoch 6 done. Avg loss = 0.023935, time = 168434.750 ms (168.435 s)\n","[TRAIN] Epoch 7, batch 100/781, loss = 0.022993\n","[TRAIN] Epoch 7, batch 200/781, loss = 0.021353\n","[TRAIN] Epoch 7, batch 300/781, loss = 0.021037\n","[TRAIN] Epoch 7, batch 400/781, loss = 0.021399\n","[TRAIN] Epoch 7, batch 500/781, loss = 0.022872\n","[TRAIN] Epoch 7, batch 600/781, loss = 0.021924\n","[TRAIN] Epoch 7, batch 700/781, loss = 0.022363\n","==> Epoch 7 done. Avg loss = 0.022713, time = 168603.797 ms (168.604 s)\n","[TRAIN] Epoch 8, batch 100/781, loss = 0.021968\n","[TRAIN] Epoch 8, batch 200/781, loss = 0.022571\n","[TRAIN] Epoch 8, batch 300/781, loss = 0.024011\n","[TRAIN] Epoch 8, batch 400/781, loss = 0.019859\n","[TRAIN] Epoch 8, batch 500/781, loss = 0.021070\n","[TRAIN] Epoch 8, batch 600/781, loss = 0.023968\n","[TRAIN] Epoch 8, batch 700/781, loss = 0.020722\n","==> Epoch 8 done. Avg loss = 0.021680, time = 168525.875 ms (168.526 s)\n","[TRAIN] Epoch 9, batch 100/781, loss = 0.021153\n","[TRAIN] Epoch 9, batch 200/781, loss = 0.022125\n","[TRAIN] Epoch 9, batch 300/781, loss = 0.020299\n","[TRAIN] Epoch 9, batch 400/781, loss = 0.021520\n","[TRAIN] Epoch 9, batch 500/781, loss = 0.019841\n","[TRAIN] Epoch 9, batch 600/781, loss = 0.019465\n","[TRAIN] Epoch 9, batch 700/781, loss = 0.021256\n","==> Epoch 9 done. Avg loss = 0.020762, time = 168597.688 ms (168.598 s)\n","[TRAIN] Epoch 10, batch 100/781, loss = 0.018341\n","[TRAIN] Epoch 10, batch 200/781, loss = 0.021120\n","[TRAIN] Epoch 10, batch 300/781, loss = 0.021076\n","[TRAIN] Epoch 10, batch 400/781, loss = 0.020403\n","[TRAIN] Epoch 10, batch 500/781, loss = 0.020476\n","[TRAIN] Epoch 10, batch 600/781, loss = 0.019616\n","[TRAIN] Epoch 10, batch 700/781, loss = 0.020499\n","==> Epoch 10 done. Avg loss = 0.019968, time = 168496.953 ms (168.497 s)\n","[TRAIN] Epoch 11, batch 100/781, loss = 0.019787\n","[TRAIN] Epoch 11, batch 200/781, loss = 0.019540\n","[TRAIN] Epoch 11, batch 300/781, loss = 0.019593\n","[TRAIN] Epoch 11, batch 400/781, loss = 0.017335\n","[TRAIN] Epoch 11, batch 500/781, loss = 0.018728\n","[TRAIN] Epoch 11, batch 600/781, loss = 0.017224\n","[TRAIN] Epoch 11, batch 700/781, loss = 0.017965\n","==> Epoch 11 done. Avg loss = 0.019259, time = 168616.344 ms (168.616 s)\n","[TRAIN] Epoch 12, batch 100/781, loss = 0.018155\n","[TRAIN] Epoch 12, batch 200/781, loss = 0.018560\n","[TRAIN] Epoch 12, batch 300/781, loss = 0.016353\n","[TRAIN] Epoch 12, batch 400/781, loss = 0.020681\n","[TRAIN] Epoch 12, batch 500/781, loss = 0.017741\n","[TRAIN] Epoch 12, batch 600/781, loss = 0.018696\n","[TRAIN] Epoch 12, batch 700/781, loss = 0.016692\n","==> Epoch 12 done. Avg loss = 0.018628, time = 168576.328 ms (168.576 s)\n","[TRAIN] Epoch 13, batch 100/781, loss = 0.018545\n","[TRAIN] Epoch 13, batch 200/781, loss = 0.018245\n","[TRAIN] Epoch 13, batch 300/781, loss = 0.018418\n","[TRAIN] Epoch 13, batch 400/781, loss = 0.017926\n","[TRAIN] Epoch 13, batch 500/781, loss = 0.016812\n","[TRAIN] Epoch 13, batch 600/781, loss = 0.017404\n","[TRAIN] Epoch 13, batch 700/781, loss = 0.018194\n","==> Epoch 13 done. Avg loss = 0.018077, time = 168549.469 ms (168.549 s)\n","[TRAIN] Epoch 14, batch 100/781, loss = 0.018181\n","[TRAIN] Epoch 14, batch 200/781, loss = 0.017955\n","[TRAIN] Epoch 14, batch 300/781, loss = 0.017413\n","[TRAIN] Epoch 14, batch 400/781, loss = 0.016880\n","[TRAIN] Epoch 14, batch 500/781, loss = 0.016599\n","[TRAIN] Epoch 14, batch 600/781, loss = 0.018112\n","[TRAIN] Epoch 14, batch 700/781, loss = 0.018696\n","==> Epoch 14 done. Avg loss = 0.017599, time = 168597.391 ms (168.597 s)\n","[TRAIN] Epoch 15, batch 100/781, loss = 0.016540\n","[TRAIN] Epoch 15, batch 200/781, loss = 0.016404\n","[TRAIN] Epoch 15, batch 300/781, loss = 0.017710\n","[TRAIN] Epoch 15, batch 400/781, loss = 0.019338\n","[TRAIN] Epoch 15, batch 500/781, loss = 0.018114\n","[TRAIN] Epoch 15, batch 600/781, loss = 0.016663\n","[TRAIN] Epoch 15, batch 700/781, loss = 0.018012\n","==> Epoch 15 done. Avg loss = 0.017193, time = 168553.859 ms (168.554 s)\n","[TRAIN] Epoch 16, batch 100/781, loss = 0.016501\n","[TRAIN] Epoch 16, batch 200/781, loss = 0.016791\n","[TRAIN] Epoch 16, batch 300/781, loss = 0.018988\n","[TRAIN] Epoch 16, batch 400/781, loss = 0.018681\n","[TRAIN] Epoch 16, batch 500/781, loss = 0.016362\n","[TRAIN] Epoch 16, batch 600/781, loss = 0.017346\n","[TRAIN] Epoch 16, batch 700/781, loss = 0.017184\n","==> Epoch 16 done. Avg loss = 0.016847, time = 168620.938 ms (168.621 s)\n","[TRAIN] Epoch 17, batch 100/781, loss = 0.015129\n","[TRAIN] Epoch 17, batch 200/781, loss = 0.016445\n","[TRAIN] Epoch 17, batch 300/781, loss = 0.016662\n","[TRAIN] Epoch 17, batch 400/781, loss = 0.017910\n","[TRAIN] Epoch 17, batch 500/781, loss = 0.016611\n","[TRAIN] Epoch 17, batch 600/781, loss = 0.014275\n","[TRAIN] Epoch 17, batch 700/781, loss = 0.014676\n","==> Epoch 17 done. Avg loss = 0.016550, time = 168552.000 ms (168.552 s)\n","[TRAIN] Epoch 18, batch 100/781, loss = 0.015519\n","[TRAIN] Epoch 18, batch 200/781, loss = 0.017579\n","[TRAIN] Epoch 18, batch 300/781, loss = 0.017020\n","[TRAIN] Epoch 18, batch 400/781, loss = 0.016123\n","[TRAIN] Epoch 18, batch 500/781, loss = 0.016939\n","[TRAIN] Epoch 18, batch 600/781, loss = 0.016806\n","[TRAIN] Epoch 18, batch 700/781, loss = 0.016479\n","==> Epoch 18 done. Avg loss = 0.016298, time = 168669.188 ms (168.669 s)\n","[TRAIN] Epoch 19, batch 100/781, loss = 0.018054\n","[TRAIN] Epoch 19, batch 200/781, loss = 0.016451\n","[TRAIN] Epoch 19, batch 300/781, loss = 0.015409\n","[TRAIN] Epoch 19, batch 400/781, loss = 0.014656\n","[TRAIN] Epoch 19, batch 500/781, loss = 0.016290\n","[TRAIN] Epoch 19, batch 600/781, loss = 0.015343\n","[TRAIN] Epoch 19, batch 700/781, loss = 0.015386\n","==> Epoch 19 done. Avg loss = 0.016074, time = 168676.531 ms (168.677 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 3371.349 s\n","[SYSTEM] Memory Usage: 486.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n","==19020== Profiling application: ./autoencoder_gpu\n","==19020== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   37.78%  1.3e+03s     78100  16.266ms  2.5128ms  50.026ms  conv2d_backward_weight(float*, float*, float*, int, int, int, int, int, int, int)\n","                   30.74%  1.0e+03s     78100  13.234ms  2.3857ms  56.371ms  conv2d_backward_input(float*, float*, float*, int, int, int, int, int, int, int)\n","                   29.19%  981.311s     78100  12.565ms  2.3498ms  53.541ms  conv2d_forward(float*, float*, float*, int, int, int, int, int, int, int)\n","                    0.46%  15.5361s     62480  248.66us  20.063us  706.64us  relu_backward(float*, float*, float*, int)\n","                    0.39%  13.2273s     31240  423.41us  96.637us  758.41us  maxpool2x2_backward(float*, float*, float*, int, int, int, int)\n","                    0.35%  11.7057s     62480  187.35us  9.7270us  656.37us  relu_forward(float*, int)\n","                    0.27%  8.95845s     78100  114.70us  12.735us  334.68us  conv2d_backward_bias(float*, float*, int, int, int, int)\n","                    0.23%  7.68845s     31240  246.11us  52.286us  825.06us  upsample2x2_forward(float*, float*, int, int, int, int)\n","                    0.17%  5.75978s     31240  184.37us  49.919us  318.87us  maxpool2x2_forward(float*, float*, int, int, int, int)\n","                    0.17%  5.56379s     31240  178.10us  46.814us  309.43us  upsample2x2_backward(float*, float*, int, int, int, int)\n","                    0.16%  5.25771s    234300  22.440us     384ns  290.10us  [CUDA memset]\n","                    0.03%  1.04212s     15630  66.674us     704ns  116.03us  [CUDA memcpy HtoD]\n","                    0.03%  980.12ms     31250  31.363us     959ns  92.829us  [CUDA memcpy DtoH]\n","                    0.02%  627.46ms    156200  4.0170us  1.4080us  11.008us  sgd_update(float*, float*, int, float)\n","                    0.01%  189.77ms     15620  12.149us  11.872us  24.703us  mse_loss_forward(float*, float*, float*, int)\n","                    0.00%  153.54ms     15620  9.8290us  9.1200us  12.319us  mse_loss_backward(float*, float*, float*, int)\n","                    0.00%  152.24ms     78100  1.9490us  1.3430us  3.4880us  [CUDA memcpy DtoD]\n","      API calls:   99.70%  3.4e+03s     46880  71.639ms  10.769us  206.79ms  cudaMemcpy\n","                    0.12%  4.18575s    749760  5.5820us  3.0980us  4.1121ms  cudaLaunchKernel\n","                    0.09%  3.00977s        20  150.49ms  149.87ms  151.18ms  cudaEventSynchronize\n","                    0.05%  1.59494s    234300  6.8070us  2.0620us  2.3837ms  cudaMemset\n","                    0.03%  1.10107s     78100  14.098us  5.0160us  1.9592ms  cudaMemcpyToSymbol\n","                    0.00%  73.812ms         2  36.906ms  2.3280us  73.810ms  cudaEventCreate\n","                    0.00%  10.964ms        41  267.41us  2.7270us  1.1039ms  cudaFree\n","                    0.00%  1.6805ms        41  40.987us  1.8350us  131.22us  cudaMalloc\n","                    0.00%  467.51us        40  11.687us  6.7130us  34.134us  cudaEventRecord\n","                    0.00%  148.24us       114  1.3000us     103ns  64.995us  cuDeviceGetAttribute\n","                    0.00%  136.01us        20  6.8000us  5.5950us  8.7350us  cudaEventElapsedTime\n","                    0.00%  74.490us         1  74.490us  74.490us  74.490us  cudaMemGetInfo\n","                    0.00%  11.610us         1  11.610us  11.610us  11.610us  cuDeviceGetName\n","                    0.00%  7.9760us         2  3.9880us     847ns  7.1290us  cudaEventDestroy\n","                    0.00%  5.4930us         1  5.4930us  5.4930us  5.4930us  cuDeviceGetPCIBusId\n","                    0.00%  1.3550us         3     451ns     132ns     934ns  cuDeviceGetCount\n","                    0.00%     823ns         1     823ns     823ns     823ns  cuDeviceTotalMem\n","                    0.00%     706ns         2     353ns     149ns     557ns  cuDeviceGet\n","                    0.00%     459ns         1     459ns     459ns     459ns  cudaGetDeviceCount\n","                    0.00%     410ns         1     410ns     410ns     410ns  cuModuleGetLoadingMode\n","                    0.00%     231ns         1     231ns     231ns     231ns  cuDeviceGetUuid\n"]}]},{"cell_type":"markdown","source":["## 2) Phân tích Occupancy và Băng thông:"],"metadata":{"id":"tKug1IhT_rk8"}},{"cell_type":"code","source":["%%writefile main_gpu_ncu.cu\n","// the UPDATED main() code with argc/argv\n","// main_gpu.cu (DEBUG VERSION)\n","#include <cstdio>\n","#include <ctime>\n","#include <cuda_runtime.h>\n","#include <cstdlib>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt1.h\"\n","\n","// GpuTimer dùng cudaEvent để đo time\n","struct GpuTimer {\n","    cudaEvent_t start, stop;\n","    GpuTimer() {\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","    }\n","    ~GpuTimer() {\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","    void tic() {\n","        cudaEventRecord(start, 0);\n","    }\n","    float toc() {\n","        cudaEventRecord(stop, 0);\n","        cudaEventSynchronize(stop);\n","        float ms = 0.0f;\n","        cudaEventElapsedTime(&ms, start, stop);\n","        return ms;\n","    }\n","};\n","\n","void print_gpu_memory() {\n","    size_t free_byte, total_byte;\n","    cudaError_t status = cudaMemGetInfo(&free_byte, &total_byte);\n","\n","    if (status == cudaSuccess) {\n","        double total_mb = (double)total_byte / (1024.0 * 1024.0);\n","        double free_mb = (double)free_byte / (1024.0 * 1024.0);\n","        double used_mb = total_mb - free_mb;\n","\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB / Total: %.2f MB\\n\", used_mb, total_mb);\n","    }\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","\n","    printf(\"[MAIN] Start program\\n\");\n","    fflush(stdout);\n","\n","    // ---- Check GPU device ----\n","    int deviceCount = 0;\n","    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n","    if (err != cudaSuccess) {\n","        fprintf(stderr, \"[MAIN] cudaGetDeviceCount error: %s\\n\",\n","                cudaGetErrorString(err));\n","        return 1;\n","    }\n","    printf(\"[MAIN] Num CUDA devices = %d\\n\", deviceCount);\n","    fflush(stdout);\n","\n","    // ---- Load CIFAR-10 on CPU ----\n","    Cifar10 data;\n","    load_cifar10(&data);   // câu lệnh này in: CIFAR-10 loaded successfully ...\n","    printf(\"[MAIN] After load_cifar10\\n\");\n","    fflush(stdout);\n","\n","    normalize_cifar10(&data);\n","    printf(\"[MAIN] After normalize_cifar10\\n\");\n","    fflush(stdout);\n","\n","    GpuTimer epoch_timer;\n","\n","    int batch_size = 64;\n","    int epochs     = 1;\n","    float lr       = 1e-3f;\n","    float total_time = 0.0f;\n","    int num_images = 64;\n","\n","    int num_batches = num_images / batch_size;\n","\n","    printf(\"[MAIN] Start training loop (epochs=%d, num_batches=%d)\\n\",\n","       epochs, num_batches);\n","    fflush(stdout);\n","\n","\n","    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float)); // dùng làm buffer tạm\n","\n","    // ---- Init GPU autoencoder ----\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","\n","\n","    for (int epoch = 0; epoch < epochs; ++epoch) {\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","\n","        epoch_timer.tic();\n","\n","        for (int b = 0; b < num_batches; ++b) {\n","            get_next_batch(&data, batch_size, b, h_batch);\n","\n","            float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n","            gpu_autoencoder_backward(&ae, lr);\n","\n","            epoch_loss += loss;\n","\n","            if ((b + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\",\n","                       epoch, b + 1, num_batches, loss);\n","                fflush(stdout);\n","            }\n","        }\n","\n","        float ms = epoch_timer.toc();\n","        total_time += ms;\n","        printf(\"==> Epoch %d done. Avg loss = %f, time = %.3f ms (%.3f s)\\n\",\n","               epoch, epoch_loss / num_batches, ms, ms / 1000.0f);\n","        fflush(stdout);\n","    }\n","\n","    printf(\"[MAIN] Training finished\\n\");\n","    printf(\"[MAIN] Total training time = %.3f s\\n\", total_time / 1000.0f);\n","    print_gpu_memory();\n","    fflush(stdout);\n","\n","    // save weights\n","    gpu_autoencoder_save_weights(&ae, \"ae_weights.bin\");\n","\n","    // ---- cleanup ----\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_output);\n","    free_cifar10(&data);\n","\n","    printf(\"[MAIN] Program finished\\n\");\n","    fflush(stdout);\n","\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwTk0271tlBX","executionInfo":{"status":"ok","timestamp":1765983490146,"user_tz":-420,"elapsed":9,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"178a81c9-fd26-4553-adb7-aefddd9ee75d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main_gpu_ncu.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 main_gpu_ncu.cu gpu_autoencoder_opt1.cu gpu_layers_opt1.cu load_data.cu -o ncu_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B80LFoYwuBdV","executionInfo":{"status":"ok","timestamp":1765983496085,"user_tz":-420,"elapsed":5939,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"ff0a123b-5f66-4f37-f28c-ff26360f4a63"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(469)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(470)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["# Đo Occupancy và số lượng thanh ghi sử dụng\n","!ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active ./ncu_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jabf58NzuKg1","outputId":"0ad16f35-9971-4531-a186-3b7cea57ea44","executionInfo":{"status":"ok","timestamp":1765983502372,"user_tz":-420,"elapsed":4140,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","==PROF== Connected to process 810 (/content/ncu_gpu)\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=1, num_batches=1)\n","==PROF== Profiling \"conv2d_forward\" - 0: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_forward(float *, int)\" - 1: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"maxpool2x2_forward\" - 2: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_forward\" - 3: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_forward(float *, int)\" - 4: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"maxpool2x2_forward\" - 5: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_forward\" - 6: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_forward(float *, int)\" - 7: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"upsample2x2_forward\" - 8: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_forward\" - 9: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_forward(float *, int)\" - 10: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"upsample2x2_forward\" - 11: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_forward\" - 12: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"mse_loss_forward\" - 13: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"mse_loss_backward\" - 14: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_input\" - 15: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_weight\" - 16: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_bias\" - 17: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 18: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 19: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"upsample2x2_backward\" - 20: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_backward\" - 21: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_input\" - 22: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_weight\" - 23: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_bias\" - 24: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 25: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 26: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"upsample2x2_backward\" - 27: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_backward\" - 28: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_input\" - 29: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_weight\" - 30: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_bias\" - 31: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 32: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 33: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"maxpool2x2_backward\" - 34: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_backward\" - 35: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_input\" - 36: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_weight\" - 37: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_bias\" - 38: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 39: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 40: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"maxpool2x2_backward\" - 41: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"relu_backward\" - 42: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_input\" - 43: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_weight\" - 44: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"conv2d_backward_bias\" - 45: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 46: 0%....50%....100% - 1 pass\n","==PROF== Profiling \"sgd_update\" - 47: 0%....50%....100% - 1 pass\n","==> Epoch 0 done. Avg loss = 0.273522, time = 1508.752 ms (1.509 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 1.509 s\n","[SYSTEM] Memory Usage: 496.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n","==PROF== Disconnected from process 810\n","[810] ncu_gpu@127.0.0.1\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        97.31\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        77.88\n","    ------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_forward(float *, float *, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        88.35\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.42\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        76.85\n","    ------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_forward(float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        60.78\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.41\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        73.11\n","    ------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_forward(float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        81.08\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.67\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (16384, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        77.49\n","    ------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_forward(float *, float *, int, int, int, int) (2, 2, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        81.26\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        94.41\n","    ------------------------------------------------- ----------- ------------\n","\n","  mse_loss_forward(float *, float *, float *, int) (768, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        86.79\n","    ------------------------------------------------- ----------- ------------\n","\n","  mse_loss_backward(float *, float *, float *, int) (768, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        77.37\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        97.47\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        94.09\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (3, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        24.99\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (27, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        24.49\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        16.56\n","    ------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_backward(float *, float *, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        88.67\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (16384, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        79.27\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.46\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 32768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.13\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        80.71\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1152, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        75.58\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        24.56\n","    ------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_backward(float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        59.72\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        76.01\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.44\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        98.94\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (128, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        76.30\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (576, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        77.50\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        19.99\n","    ------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_backward(float *, float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        67.93\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        78.56\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.69\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 32768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        99.13\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (128, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        77.37\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1152, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        76.66\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        19.92\n","    ------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_backward(float *, float *, float *, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        94.04\n","    ------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        81.48\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        94.32\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        94.17\n","    ------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        80.89\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (27, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        24.53\n","    ------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    ------------------------------------------------- ----------- ------------\n","    Metric Name                                       Metric Unit Metric Value\n","    ------------------------------------------------- ----------- ------------\n","    sm__warps_active.avg.pct_of_peak_sustained_active           %        24.65\n","    ------------------------------------------------- ----------- ------------\n","\n"]}]},{"cell_type":"code","source":["# Đo lưu lượng DRAM và % băng thông sử dụng\n","!ncu --metrics dram__bytes_read.sum,dram__bytes_write.sum,dram__throughput.avg.pct_of_peak_sustained_elapsed ./ncu_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca57ZvU3uSGz","executionInfo":{"status":"ok","timestamp":1765983507899,"user_tz":-420,"elapsed":5525,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"9bb1006b-e88a-4bc2-b1ad-e2720795c6b2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","==PROF== Connected to process 900 (/content/ncu_gpu)\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=1, num_batches=1)\n","==PROF== Profiling \"conv2d_forward\" - 0: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_forward(float *, int)\" - 1: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"maxpool2x2_forward\" - 2: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_forward\" - 3: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_forward(float *, int)\" - 4: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"maxpool2x2_forward\" - 5: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_forward\" - 6: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_forward(float *, int)\" - 7: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"upsample2x2_forward\" - 8: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_forward\" - 9: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_forward(float *, int)\" - 10: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"upsample2x2_forward\" - 11: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_forward\" - 12: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"mse_loss_forward\" - 13: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"mse_loss_backward\" - 14: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_input\" - 15: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_weight\" - 16: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_bias\" - 17: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 18: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 19: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"upsample2x2_backward\" - 20: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_backward\" - 21: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_input\" - 22: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_weight\" - 23: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_bias\" - 24: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 25: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 26: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"upsample2x2_backward\" - 27: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_backward\" - 28: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_input\" - 29: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_weight\" - 30: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_bias\" - 31: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 32: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 33: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"maxpool2x2_backward\" - 34: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_backward\" - 35: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_input\" - 36: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_weight\" - 37: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_bias\" - 38: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 39: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 40: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"maxpool2x2_backward\" - 41: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"relu_backward\" - 42: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_input\" - 43: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_weight\" - 44: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"conv2d_backward_bias\" - 45: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 46: 0%....50%....100% - 3 passes\n","==PROF== Profiling \"sgd_update\" - 47: 0%....50%....100% - 3 passes\n","==> Epoch 0 done. Avg loss = 0.234148, time = 3458.623 ms (3.459 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 3.459 s\n","[SYSTEM] Memory Usage: 634.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n","==PROF== Disconnected from process 900\n","[900] ncu_gpu@127.0.0.1\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         1.21\n","    dram__bytes_write.sum                                    Mbyte        75.41\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         4.41\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        73.37\n","    dram__bytes_write.sum                                    Mbyte        73.47\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        81.20\n","    -------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_forward(float *, float *, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        72.19\n","    dram__bytes_write.sum                                    Mbyte        18.98\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        92.29\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        26.91\n","    dram__bytes_write.sum                                    Mbyte        10.12\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.22\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         9.21\n","    dram__bytes_write.sum                                    Mbyte         8.24\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        77.01\n","    -------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_forward(float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        10.23\n","    dram__bytes_write.sum                                    Mbyte         2.72\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        56.96\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         4.77\n","    dram__bytes_write.sum                                    Mbyte         1.87\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.08\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         2.34\n","    dram__bytes_write.sum                                    Mbyte         1.24\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        63.15\n","    -------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_forward(float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         2.61\n","    dram__bytes_write.sum                                    Mbyte         8.47\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        39.01\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        14.91\n","    dram__bytes_write.sum                                    Mbyte        19.25\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.20\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_forward(float *, int) (16384, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        18.39\n","    dram__bytes_write.sum                                    Mbyte        17.55\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        79.42\n","    -------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_forward(float *, float *, int, int, int, int) (2, 2, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        20.23\n","    dram__bytes_write.sum                                    Mbyte        74.68\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        42.62\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_forward(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        95.62\n","    dram__bytes_write.sum                                    Mbyte         2.09\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         5.34\n","    -------------------------------------------------- ----------- ------------\n","\n","  mse_loss_forward(float *, float *, float *, int) (768, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         2.00\n","    dram__bytes_write.sum                                    Kbyte         7.23\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        25.74\n","    -------------------------------------------------- ----------- ------------\n","\n","  mse_loss_backward(float *, float *, float *, int) (768, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         1.77\n","    dram__bytes_write.sum                                    Kbyte       207.26\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        57.95\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         1.29\n","    dram__bytes_write.sum                                    Mbyte        75.30\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         4.17\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte       313.19\n","    dram__bytes_write.sum                                    Mbyte         1.33\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        17.61\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (3, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         1.10\n","    dram__bytes_write.sum                                    Kbyte         4.70\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         1.83\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (27, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte        65.54\n","    dram__bytes_write.sum                                     byte          160\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         5.28\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte         3.52\n","    dram__bytes_write.sum                                     byte            0\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.31\n","    -------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_backward(float *, float *, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        72.14\n","    dram__bytes_write.sum                                    Mbyte        18.99\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        92.43\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (16384, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        32.81\n","    dram__bytes_write.sum                                    Mbyte        17.77\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        76.57\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        24.86\n","    dram__bytes_write.sum                                    Mbyte        10.07\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.19\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 32768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Gbyte         2.92\n","    dram__bytes_write.sum                                    Mbyte         2.75\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        14.89\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        22.82\n","    dram__bytes_write.sum                                    Mbyte         1.31\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        65.47\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1152, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         2.61\n","    dram__bytes_write.sum                                    Kbyte       332.77\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        70.89\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte         5.63\n","    dram__bytes_write.sum                                     byte          160\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.49\n","    -------------------------------------------------- ----------- ------------\n","\n","  upsample2x2_backward(float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        10.21\n","    dram__bytes_write.sum                                    Mbyte         2.75\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        55.88\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (2048, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         4.23\n","    dram__bytes_write.sum                                    Mbyte         1.25\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        66.82\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         4.93\n","    dram__bytes_write.sum                                    Mbyte         1.91\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.08\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         8.92\n","    dram__bytes_write.sum                                    Mbyte         1.63\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.11\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (128, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         2.90\n","    dram__bytes_write.sum                                    Kbyte         7.81\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        44.56\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (576, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         1.32\n","    dram__bytes_write.sum                                    Kbyte       191.87\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        54.84\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte         4.42\n","    dram__bytes_write.sum                                     byte          160\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.40\n","    -------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_backward(float *, float *, float *, int, int, int, int) (1, 1, 8192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        19.70\n","    dram__bytes_write.sum                                    Mbyte         6.42\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        86.55\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        15.77\n","    dram__bytes_write.sum                                    Mbyte         8.27\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        74.10\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        13.71\n","    dram__bytes_write.sum                                    Mbyte        19.30\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.18\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 32768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Gbyte         2.91\n","    dram__bytes_write.sum                                    Mbyte         2.76\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        14.84\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (128, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        11.27\n","    dram__bytes_write.sum                                    Mbyte         1.32\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        63.82\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1152, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte         2.61\n","    dram__bytes_write.sum                                    Kbyte       352.83\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        70.29\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte         4.48\n","    dram__bytes_write.sum                                     byte          736\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.44\n","    -------------------------------------------------- ----------- ------------\n","\n","  maxpool2x2_backward(float *, float *, float *, int, int, int, int) (1, 1, 16384)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte       148.37\n","    dram__bytes_write.sum                                    Mbyte        54.25\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        87.74\n","    -------------------------------------------------- ----------- ------------\n","\n","  relu_backward(float *, float *, float *, int) (65536, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte       123.46\n","    dram__bytes_write.sum                                    Mbyte        74.10\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        77.01\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_input(float *, float *, float *, int, int, int, int, int, int, int) (2, 2, 192)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte       104.47\n","    dram__bytes_write.sum                                    Mbyte         2.00\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         5.43\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_weight(float *, float *, float *, int, int, int, int, int, int, int) (1, 1, 768)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte       103.72\n","    dram__bytes_write.sum                                    Mbyte         1.28\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         5.94\n","    -------------------------------------------------- ----------- ------------\n","\n","  conv2d_backward_bias(float *, float *, int, int, int, int) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Mbyte        95.52\n","    dram__bytes_write.sum                                    Mbyte         1.31\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %        69.43\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (27, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte        65.92\n","    dram__bytes_write.sum                                    Kbyte         2.02\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         5.46\n","    -------------------------------------------------- ----------- ------------\n","\n","  sgd_update(float *, float *, int, float) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n","    Section: Command line profiler metrics\n","    -------------------------------------------------- ----------- ------------\n","    Metric Name                                        Metric Unit Metric Value\n","    -------------------------------------------------- ----------- ------------\n","    dram__bytes_read.sum                                     Kbyte         5.63\n","    dram__bytes_write.sum                                     byte            0\n","    dram__throughput.avg.pct_of_peak_sustained_elapsed           %         0.48\n","    -------------------------------------------------- ----------- ------------\n","\n"]}]},{"cell_type":"markdown","source":["## 3) Trích xuất đặc trưng:"],"metadata":{"id":"woN0TzYz_4Ug"}},{"cell_type":"code","source":["%%writefile extract_svm_features.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cstring>\n","#include <cuda_runtime.h>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt1.h\"\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    if (argc < 2) {\n","        fprintf(stderr,\n","                \"Usage: %s <ae_weights.bin>\\n\",\n","                argv[0]);\n","        return 1;\n","    }\n","    const char* weight_file = argv[1];\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","    gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = TRAIN_NUM; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = TEST_NUM; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}\n"],"metadata":{"id":"dm0fAEwWdqr8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765982663372,"user_tz":-420,"elapsed":45,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"f7a9b9bd-35cf-49f6-adcb-f7fdefb91f09"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing extract_svm_features.cu\n"]}]},{"cell_type":"code","source":["!nvcc  -arch=sm_75 -O2 -o extract_svm_features \\\n","    extract_svm_features.cu gpu_autoencoder_opt1.cu gpu_layers_opt1.cu load_data.cu \\\n","    -lcudart\n"],"metadata":{"id":"zFmbvO0Ad7r7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765982668742,"user_tz":-420,"elapsed":4627,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"d7c2d96f-38f8-410b-863f-cd2fc3499388"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(469)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(470)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./extract_svm_features ae_weights.bin"],"metadata":{"id":"0YN5zeMtd_6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765982893826,"user_tz":-420,"elapsed":225078,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"7466397c-daa8-4e4f-e8e6-b962c7c4ef8b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","Loaded weights from ae_weights.bin\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/782 done\n","[SVM][TRAIN] Batch 2/782 done\n","[SVM][TRAIN] Batch 3/782 done\n","[SVM][TRAIN] Batch 4/782 done\n","[SVM][TRAIN] Batch 5/782 done\n","[SVM][TRAIN] Batch 6/782 done\n","[SVM][TRAIN] Batch 7/782 done\n","[SVM][TRAIN] Batch 8/782 done\n","[SVM][TRAIN] Batch 9/782 done\n","[SVM][TRAIN] Batch 10/782 done\n","[SVM][TRAIN] Batch 11/782 done\n","[SVM][TRAIN] Batch 12/782 done\n","[SVM][TRAIN] Batch 13/782 done\n","[SVM][TRAIN] Batch 14/782 done\n","[SVM][TRAIN] Batch 15/782 done\n","[SVM][TRAIN] Batch 16/782 done\n","[SVM][TRAIN] Batch 17/782 done\n","[SVM][TRAIN] Batch 18/782 done\n","[SVM][TRAIN] Batch 19/782 done\n","[SVM][TRAIN] Batch 20/782 done\n","[SVM][TRAIN] Batch 21/782 done\n","[SVM][TRAIN] Batch 22/782 done\n","[SVM][TRAIN] Batch 23/782 done\n","[SVM][TRAIN] Batch 24/782 done\n","[SVM][TRAIN] Batch 25/782 done\n","[SVM][TRAIN] Batch 26/782 done\n","[SVM][TRAIN] Batch 27/782 done\n","[SVM][TRAIN] Batch 28/782 done\n","[SVM][TRAIN] Batch 29/782 done\n","[SVM][TRAIN] Batch 30/782 done\n","[SVM][TRAIN] Batch 31/782 done\n","[SVM][TRAIN] Batch 32/782 done\n","[SVM][TRAIN] Batch 33/782 done\n","[SVM][TRAIN] Batch 34/782 done\n","[SVM][TRAIN] Batch 35/782 done\n","[SVM][TRAIN] Batch 36/782 done\n","[SVM][TRAIN] Batch 37/782 done\n","[SVM][TRAIN] Batch 38/782 done\n","[SVM][TRAIN] Batch 39/782 done\n","[SVM][TRAIN] Batch 40/782 done\n","[SVM][TRAIN] Batch 41/782 done\n","[SVM][TRAIN] Batch 42/782 done\n","[SVM][TRAIN] Batch 43/782 done\n","[SVM][TRAIN] Batch 44/782 done\n","[SVM][TRAIN] Batch 45/782 done\n","[SVM][TRAIN] Batch 46/782 done\n","[SVM][TRAIN] Batch 47/782 done\n","[SVM][TRAIN] Batch 48/782 done\n","[SVM][TRAIN] Batch 49/782 done\n","[SVM][TRAIN] Batch 50/782 done\n","[SVM][TRAIN] Batch 51/782 done\n","[SVM][TRAIN] Batch 52/782 done\n","[SVM][TRAIN] Batch 53/782 done\n","[SVM][TRAIN] Batch 54/782 done\n","[SVM][TRAIN] Batch 55/782 done\n","[SVM][TRAIN] Batch 56/782 done\n","[SVM][TRAIN] Batch 57/782 done\n","[SVM][TRAIN] Batch 58/782 done\n","[SVM][TRAIN] Batch 59/782 done\n","[SVM][TRAIN] Batch 60/782 done\n","[SVM][TRAIN] Batch 61/782 done\n","[SVM][TRAIN] Batch 62/782 done\n","[SVM][TRAIN] Batch 63/782 done\n","[SVM][TRAIN] Batch 64/782 done\n","[SVM][TRAIN] Batch 65/782 done\n","[SVM][TRAIN] Batch 66/782 done\n","[SVM][TRAIN] Batch 67/782 done\n","[SVM][TRAIN] Batch 68/782 done\n","[SVM][TRAIN] Batch 69/782 done\n","[SVM][TRAIN] Batch 70/782 done\n","[SVM][TRAIN] Batch 71/782 done\n","[SVM][TRAIN] Batch 72/782 done\n","[SVM][TRAIN] Batch 73/782 done\n","[SVM][TRAIN] Batch 74/782 done\n","[SVM][TRAIN] Batch 75/782 done\n","[SVM][TRAIN] Batch 76/782 done\n","[SVM][TRAIN] Batch 77/782 done\n","[SVM][TRAIN] Batch 78/782 done\n","[SVM][TRAIN] Batch 79/782 done\n","[SVM][TRAIN] Batch 80/782 done\n","[SVM][TRAIN] Batch 81/782 done\n","[SVM][TRAIN] Batch 82/782 done\n","[SVM][TRAIN] Batch 83/782 done\n","[SVM][TRAIN] Batch 84/782 done\n","[SVM][TRAIN] Batch 85/782 done\n","[SVM][TRAIN] Batch 86/782 done\n","[SVM][TRAIN] Batch 87/782 done\n","[SVM][TRAIN] Batch 88/782 done\n","[SVM][TRAIN] Batch 89/782 done\n","[SVM][TRAIN] Batch 90/782 done\n","[SVM][TRAIN] Batch 91/782 done\n","[SVM][TRAIN] Batch 92/782 done\n","[SVM][TRAIN] Batch 93/782 done\n","[SVM][TRAIN] Batch 94/782 done\n","[SVM][TRAIN] Batch 95/782 done\n","[SVM][TRAIN] Batch 96/782 done\n","[SVM][TRAIN] Batch 97/782 done\n","[SVM][TRAIN] Batch 98/782 done\n","[SVM][TRAIN] Batch 99/782 done\n","[SVM][TRAIN] Batch 100/782 done\n","[SVM][TRAIN] Batch 101/782 done\n","[SVM][TRAIN] Batch 102/782 done\n","[SVM][TRAIN] Batch 103/782 done\n","[SVM][TRAIN] Batch 104/782 done\n","[SVM][TRAIN] Batch 105/782 done\n","[SVM][TRAIN] Batch 106/782 done\n","[SVM][TRAIN] Batch 107/782 done\n","[SVM][TRAIN] Batch 108/782 done\n","[SVM][TRAIN] Batch 109/782 done\n","[SVM][TRAIN] Batch 110/782 done\n","[SVM][TRAIN] Batch 111/782 done\n","[SVM][TRAIN] Batch 112/782 done\n","[SVM][TRAIN] Batch 113/782 done\n","[SVM][TRAIN] Batch 114/782 done\n","[SVM][TRAIN] Batch 115/782 done\n","[SVM][TRAIN] Batch 116/782 done\n","[SVM][TRAIN] Batch 117/782 done\n","[SVM][TRAIN] Batch 118/782 done\n","[SVM][TRAIN] Batch 119/782 done\n","[SVM][TRAIN] Batch 120/782 done\n","[SVM][TRAIN] Batch 121/782 done\n","[SVM][TRAIN] Batch 122/782 done\n","[SVM][TRAIN] Batch 123/782 done\n","[SVM][TRAIN] Batch 124/782 done\n","[SVM][TRAIN] Batch 125/782 done\n","[SVM][TRAIN] Batch 126/782 done\n","[SVM][TRAIN] Batch 127/782 done\n","[SVM][TRAIN] Batch 128/782 done\n","[SVM][TRAIN] Batch 129/782 done\n","[SVM][TRAIN] Batch 130/782 done\n","[SVM][TRAIN] Batch 131/782 done\n","[SVM][TRAIN] Batch 132/782 done\n","[SVM][TRAIN] Batch 133/782 done\n","[SVM][TRAIN] Batch 134/782 done\n","[SVM][TRAIN] Batch 135/782 done\n","[SVM][TRAIN] Batch 136/782 done\n","[SVM][TRAIN] Batch 137/782 done\n","[SVM][TRAIN] Batch 138/782 done\n","[SVM][TRAIN] Batch 139/782 done\n","[SVM][TRAIN] Batch 140/782 done\n","[SVM][TRAIN] Batch 141/782 done\n","[SVM][TRAIN] Batch 142/782 done\n","[SVM][TRAIN] Batch 143/782 done\n","[SVM][TRAIN] Batch 144/782 done\n","[SVM][TRAIN] Batch 145/782 done\n","[SVM][TRAIN] Batch 146/782 done\n","[SVM][TRAIN] Batch 147/782 done\n","[SVM][TRAIN] Batch 148/782 done\n","[SVM][TRAIN] Batch 149/782 done\n","[SVM][TRAIN] Batch 150/782 done\n","[SVM][TRAIN] Batch 151/782 done\n","[SVM][TRAIN] Batch 152/782 done\n","[SVM][TRAIN] Batch 153/782 done\n","[SVM][TRAIN] Batch 154/782 done\n","[SVM][TRAIN] Batch 155/782 done\n","[SVM][TRAIN] Batch 156/782 done\n","[SVM][TRAIN] Batch 157/782 done\n","[SVM][TRAIN] Batch 158/782 done\n","[SVM][TRAIN] Batch 159/782 done\n","[SVM][TRAIN] Batch 160/782 done\n","[SVM][TRAIN] Batch 161/782 done\n","[SVM][TRAIN] Batch 162/782 done\n","[SVM][TRAIN] Batch 163/782 done\n","[SVM][TRAIN] Batch 164/782 done\n","[SVM][TRAIN] Batch 165/782 done\n","[SVM][TRAIN] Batch 166/782 done\n","[SVM][TRAIN] Batch 167/782 done\n","[SVM][TRAIN] Batch 168/782 done\n","[SVM][TRAIN] Batch 169/782 done\n","[SVM][TRAIN] Batch 170/782 done\n","[SVM][TRAIN] Batch 171/782 done\n","[SVM][TRAIN] Batch 172/782 done\n","[SVM][TRAIN] Batch 173/782 done\n","[SVM][TRAIN] Batch 174/782 done\n","[SVM][TRAIN] Batch 175/782 done\n","[SVM][TRAIN] Batch 176/782 done\n","[SVM][TRAIN] Batch 177/782 done\n","[SVM][TRAIN] Batch 178/782 done\n","[SVM][TRAIN] Batch 179/782 done\n","[SVM][TRAIN] Batch 180/782 done\n","[SVM][TRAIN] Batch 181/782 done\n","[SVM][TRAIN] Batch 182/782 done\n","[SVM][TRAIN] Batch 183/782 done\n","[SVM][TRAIN] Batch 184/782 done\n","[SVM][TRAIN] Batch 185/782 done\n","[SVM][TRAIN] Batch 186/782 done\n","[SVM][TRAIN] Batch 187/782 done\n","[SVM][TRAIN] Batch 188/782 done\n","[SVM][TRAIN] Batch 189/782 done\n","[SVM][TRAIN] Batch 190/782 done\n","[SVM][TRAIN] Batch 191/782 done\n","[SVM][TRAIN] Batch 192/782 done\n","[SVM][TRAIN] Batch 193/782 done\n","[SVM][TRAIN] Batch 194/782 done\n","[SVM][TRAIN] Batch 195/782 done\n","[SVM][TRAIN] Batch 196/782 done\n","[SVM][TRAIN] Batch 197/782 done\n","[SVM][TRAIN] Batch 198/782 done\n","[SVM][TRAIN] Batch 199/782 done\n","[SVM][TRAIN] Batch 200/782 done\n","[SVM][TRAIN] Batch 201/782 done\n","[SVM][TRAIN] Batch 202/782 done\n","[SVM][TRAIN] Batch 203/782 done\n","[SVM][TRAIN] Batch 204/782 done\n","[SVM][TRAIN] Batch 205/782 done\n","[SVM][TRAIN] Batch 206/782 done\n","[SVM][TRAIN] Batch 207/782 done\n","[SVM][TRAIN] Batch 208/782 done\n","[SVM][TRAIN] Batch 209/782 done\n","[SVM][TRAIN] Batch 210/782 done\n","[SVM][TRAIN] Batch 211/782 done\n","[SVM][TRAIN] Batch 212/782 done\n","[SVM][TRAIN] Batch 213/782 done\n","[SVM][TRAIN] Batch 214/782 done\n","[SVM][TRAIN] Batch 215/782 done\n","[SVM][TRAIN] Batch 216/782 done\n","[SVM][TRAIN] Batch 217/782 done\n","[SVM][TRAIN] Batch 218/782 done\n","[SVM][TRAIN] Batch 219/782 done\n","[SVM][TRAIN] Batch 220/782 done\n","[SVM][TRAIN] Batch 221/782 done\n","[SVM][TRAIN] Batch 222/782 done\n","[SVM][TRAIN] Batch 223/782 done\n","[SVM][TRAIN] Batch 224/782 done\n","[SVM][TRAIN] Batch 225/782 done\n","[SVM][TRAIN] Batch 226/782 done\n","[SVM][TRAIN] Batch 227/782 done\n","[SVM][TRAIN] Batch 228/782 done\n","[SVM][TRAIN] Batch 229/782 done\n","[SVM][TRAIN] Batch 230/782 done\n","[SVM][TRAIN] Batch 231/782 done\n","[SVM][TRAIN] Batch 232/782 done\n","[SVM][TRAIN] Batch 233/782 done\n","[SVM][TRAIN] Batch 234/782 done\n","[SVM][TRAIN] Batch 235/782 done\n","[SVM][TRAIN] Batch 236/782 done\n","[SVM][TRAIN] Batch 237/782 done\n","[SVM][TRAIN] Batch 238/782 done\n","[SVM][TRAIN] Batch 239/782 done\n","[SVM][TRAIN] Batch 240/782 done\n","[SVM][TRAIN] Batch 241/782 done\n","[SVM][TRAIN] Batch 242/782 done\n","[SVM][TRAIN] Batch 243/782 done\n","[SVM][TRAIN] Batch 244/782 done\n","[SVM][TRAIN] Batch 245/782 done\n","[SVM][TRAIN] Batch 246/782 done\n","[SVM][TRAIN] Batch 247/782 done\n","[SVM][TRAIN] Batch 248/782 done\n","[SVM][TRAIN] Batch 249/782 done\n","[SVM][TRAIN] Batch 250/782 done\n","[SVM][TRAIN] Batch 251/782 done\n","[SVM][TRAIN] Batch 252/782 done\n","[SVM][TRAIN] Batch 253/782 done\n","[SVM][TRAIN] Batch 254/782 done\n","[SVM][TRAIN] Batch 255/782 done\n","[SVM][TRAIN] Batch 256/782 done\n","[SVM][TRAIN] Batch 257/782 done\n","[SVM][TRAIN] Batch 258/782 done\n","[SVM][TRAIN] Batch 259/782 done\n","[SVM][TRAIN] Batch 260/782 done\n","[SVM][TRAIN] Batch 261/782 done\n","[SVM][TRAIN] Batch 262/782 done\n","[SVM][TRAIN] Batch 263/782 done\n","[SVM][TRAIN] Batch 264/782 done\n","[SVM][TRAIN] Batch 265/782 done\n","[SVM][TRAIN] Batch 266/782 done\n","[SVM][TRAIN] Batch 267/782 done\n","[SVM][TRAIN] Batch 268/782 done\n","[SVM][TRAIN] Batch 269/782 done\n","[SVM][TRAIN] Batch 270/782 done\n","[SVM][TRAIN] Batch 271/782 done\n","[SVM][TRAIN] Batch 272/782 done\n","[SVM][TRAIN] Batch 273/782 done\n","[SVM][TRAIN] Batch 274/782 done\n","[SVM][TRAIN] Batch 275/782 done\n","[SVM][TRAIN] Batch 276/782 done\n","[SVM][TRAIN] Batch 277/782 done\n","[SVM][TRAIN] Batch 278/782 done\n","[SVM][TRAIN] Batch 279/782 done\n","[SVM][TRAIN] Batch 280/782 done\n","[SVM][TRAIN] Batch 281/782 done\n","[SVM][TRAIN] Batch 282/782 done\n","[SVM][TRAIN] Batch 283/782 done\n","[SVM][TRAIN] Batch 284/782 done\n","[SVM][TRAIN] Batch 285/782 done\n","[SVM][TRAIN] Batch 286/782 done\n","[SVM][TRAIN] Batch 287/782 done\n","[SVM][TRAIN] Batch 288/782 done\n","[SVM][TRAIN] Batch 289/782 done\n","[SVM][TRAIN] Batch 290/782 done\n","[SVM][TRAIN] Batch 291/782 done\n","[SVM][TRAIN] Batch 292/782 done\n","[SVM][TRAIN] Batch 293/782 done\n","[SVM][TRAIN] Batch 294/782 done\n","[SVM][TRAIN] Batch 295/782 done\n","[SVM][TRAIN] Batch 296/782 done\n","[SVM][TRAIN] Batch 297/782 done\n","[SVM][TRAIN] Batch 298/782 done\n","[SVM][TRAIN] Batch 299/782 done\n","[SVM][TRAIN] Batch 300/782 done\n","[SVM][TRAIN] Batch 301/782 done\n","[SVM][TRAIN] Batch 302/782 done\n","[SVM][TRAIN] Batch 303/782 done\n","[SVM][TRAIN] Batch 304/782 done\n","[SVM][TRAIN] Batch 305/782 done\n","[SVM][TRAIN] Batch 306/782 done\n","[SVM][TRAIN] Batch 307/782 done\n","[SVM][TRAIN] Batch 308/782 done\n","[SVM][TRAIN] Batch 309/782 done\n","[SVM][TRAIN] Batch 310/782 done\n","[SVM][TRAIN] Batch 311/782 done\n","[SVM][TRAIN] Batch 312/782 done\n","[SVM][TRAIN] Batch 313/782 done\n","[SVM][TRAIN] Batch 314/782 done\n","[SVM][TRAIN] Batch 315/782 done\n","[SVM][TRAIN] Batch 316/782 done\n","[SVM][TRAIN] Batch 317/782 done\n","[SVM][TRAIN] Batch 318/782 done\n","[SVM][TRAIN] Batch 319/782 done\n","[SVM][TRAIN] Batch 320/782 done\n","[SVM][TRAIN] Batch 321/782 done\n","[SVM][TRAIN] Batch 322/782 done\n","[SVM][TRAIN] Batch 323/782 done\n","[SVM][TRAIN] Batch 324/782 done\n","[SVM][TRAIN] Batch 325/782 done\n","[SVM][TRAIN] Batch 326/782 done\n","[SVM][TRAIN] Batch 327/782 done\n","[SVM][TRAIN] Batch 328/782 done\n","[SVM][TRAIN] Batch 329/782 done\n","[SVM][TRAIN] Batch 330/782 done\n","[SVM][TRAIN] Batch 331/782 done\n","[SVM][TRAIN] Batch 332/782 done\n","[SVM][TRAIN] Batch 333/782 done\n","[SVM][TRAIN] Batch 334/782 done\n","[SVM][TRAIN] Batch 335/782 done\n","[SVM][TRAIN] Batch 336/782 done\n","[SVM][TRAIN] Batch 337/782 done\n","[SVM][TRAIN] Batch 338/782 done\n","[SVM][TRAIN] Batch 339/782 done\n","[SVM][TRAIN] Batch 340/782 done\n","[SVM][TRAIN] Batch 341/782 done\n","[SVM][TRAIN] Batch 342/782 done\n","[SVM][TRAIN] Batch 343/782 done\n","[SVM][TRAIN] Batch 344/782 done\n","[SVM][TRAIN] Batch 345/782 done\n","[SVM][TRAIN] Batch 346/782 done\n","[SVM][TRAIN] Batch 347/782 done\n","[SVM][TRAIN] Batch 348/782 done\n","[SVM][TRAIN] Batch 349/782 done\n","[SVM][TRAIN] Batch 350/782 done\n","[SVM][TRAIN] Batch 351/782 done\n","[SVM][TRAIN] Batch 352/782 done\n","[SVM][TRAIN] Batch 353/782 done\n","[SVM][TRAIN] Batch 354/782 done\n","[SVM][TRAIN] Batch 355/782 done\n","[SVM][TRAIN] Batch 356/782 done\n","[SVM][TRAIN] Batch 357/782 done\n","[SVM][TRAIN] Batch 358/782 done\n","[SVM][TRAIN] Batch 359/782 done\n","[SVM][TRAIN] Batch 360/782 done\n","[SVM][TRAIN] Batch 361/782 done\n","[SVM][TRAIN] Batch 362/782 done\n","[SVM][TRAIN] Batch 363/782 done\n","[SVM][TRAIN] Batch 364/782 done\n","[SVM][TRAIN] Batch 365/782 done\n","[SVM][TRAIN] Batch 366/782 done\n","[SVM][TRAIN] Batch 367/782 done\n","[SVM][TRAIN] Batch 368/782 done\n","[SVM][TRAIN] Batch 369/782 done\n","[SVM][TRAIN] Batch 370/782 done\n","[SVM][TRAIN] Batch 371/782 done\n","[SVM][TRAIN] Batch 372/782 done\n","[SVM][TRAIN] Batch 373/782 done\n","[SVM][TRAIN] Batch 374/782 done\n","[SVM][TRAIN] Batch 375/782 done\n","[SVM][TRAIN] Batch 376/782 done\n","[SVM][TRAIN] Batch 377/782 done\n","[SVM][TRAIN] Batch 378/782 done\n","[SVM][TRAIN] Batch 379/782 done\n","[SVM][TRAIN] Batch 380/782 done\n","[SVM][TRAIN] Batch 381/782 done\n","[SVM][TRAIN] Batch 382/782 done\n","[SVM][TRAIN] Batch 383/782 done\n","[SVM][TRAIN] Batch 384/782 done\n","[SVM][TRAIN] Batch 385/782 done\n","[SVM][TRAIN] Batch 386/782 done\n","[SVM][TRAIN] Batch 387/782 done\n","[SVM][TRAIN] Batch 388/782 done\n","[SVM][TRAIN] Batch 389/782 done\n","[SVM][TRAIN] Batch 390/782 done\n","[SVM][TRAIN] Batch 391/782 done\n","[SVM][TRAIN] Batch 392/782 done\n","[SVM][TRAIN] Batch 393/782 done\n","[SVM][TRAIN] Batch 394/782 done\n","[SVM][TRAIN] Batch 395/782 done\n","[SVM][TRAIN] Batch 396/782 done\n","[SVM][TRAIN] Batch 397/782 done\n","[SVM][TRAIN] Batch 398/782 done\n","[SVM][TRAIN] Batch 399/782 done\n","[SVM][TRAIN] Batch 400/782 done\n","[SVM][TRAIN] Batch 401/782 done\n","[SVM][TRAIN] Batch 402/782 done\n","[SVM][TRAIN] Batch 403/782 done\n","[SVM][TRAIN] Batch 404/782 done\n","[SVM][TRAIN] Batch 405/782 done\n","[SVM][TRAIN] Batch 406/782 done\n","[SVM][TRAIN] Batch 407/782 done\n","[SVM][TRAIN] Batch 408/782 done\n","[SVM][TRAIN] Batch 409/782 done\n","[SVM][TRAIN] Batch 410/782 done\n","[SVM][TRAIN] Batch 411/782 done\n","[SVM][TRAIN] Batch 412/782 done\n","[SVM][TRAIN] Batch 413/782 done\n","[SVM][TRAIN] Batch 414/782 done\n","[SVM][TRAIN] Batch 415/782 done\n","[SVM][TRAIN] Batch 416/782 done\n","[SVM][TRAIN] Batch 417/782 done\n","[SVM][TRAIN] Batch 418/782 done\n","[SVM][TRAIN] Batch 419/782 done\n","[SVM][TRAIN] Batch 420/782 done\n","[SVM][TRAIN] Batch 421/782 done\n","[SVM][TRAIN] Batch 422/782 done\n","[SVM][TRAIN] Batch 423/782 done\n","[SVM][TRAIN] Batch 424/782 done\n","[SVM][TRAIN] Batch 425/782 done\n","[SVM][TRAIN] Batch 426/782 done\n","[SVM][TRAIN] Batch 427/782 done\n","[SVM][TRAIN] Batch 428/782 done\n","[SVM][TRAIN] Batch 429/782 done\n","[SVM][TRAIN] Batch 430/782 done\n","[SVM][TRAIN] Batch 431/782 done\n","[SVM][TRAIN] Batch 432/782 done\n","[SVM][TRAIN] Batch 433/782 done\n","[SVM][TRAIN] Batch 434/782 done\n","[SVM][TRAIN] Batch 435/782 done\n","[SVM][TRAIN] Batch 436/782 done\n","[SVM][TRAIN] Batch 437/782 done\n","[SVM][TRAIN] Batch 438/782 done\n","[SVM][TRAIN] Batch 439/782 done\n","[SVM][TRAIN] Batch 440/782 done\n","[SVM][TRAIN] Batch 441/782 done\n","[SVM][TRAIN] Batch 442/782 done\n","[SVM][TRAIN] Batch 443/782 done\n","[SVM][TRAIN] Batch 444/782 done\n","[SVM][TRAIN] Batch 445/782 done\n","[SVM][TRAIN] Batch 446/782 done\n","[SVM][TRAIN] Batch 447/782 done\n","[SVM][TRAIN] Batch 448/782 done\n","[SVM][TRAIN] Batch 449/782 done\n","[SVM][TRAIN] Batch 450/782 done\n","[SVM][TRAIN] Batch 451/782 done\n","[SVM][TRAIN] Batch 452/782 done\n","[SVM][TRAIN] Batch 453/782 done\n","[SVM][TRAIN] Batch 454/782 done\n","[SVM][TRAIN] Batch 455/782 done\n","[SVM][TRAIN] Batch 456/782 done\n","[SVM][TRAIN] Batch 457/782 done\n","[SVM][TRAIN] Batch 458/782 done\n","[SVM][TRAIN] Batch 459/782 done\n","[SVM][TRAIN] Batch 460/782 done\n","[SVM][TRAIN] Batch 461/782 done\n","[SVM][TRAIN] Batch 462/782 done\n","[SVM][TRAIN] Batch 463/782 done\n","[SVM][TRAIN] Batch 464/782 done\n","[SVM][TRAIN] Batch 465/782 done\n","[SVM][TRAIN] Batch 466/782 done\n","[SVM][TRAIN] Batch 467/782 done\n","[SVM][TRAIN] Batch 468/782 done\n","[SVM][TRAIN] Batch 469/782 done\n","[SVM][TRAIN] Batch 470/782 done\n","[SVM][TRAIN] Batch 471/782 done\n","[SVM][TRAIN] Batch 472/782 done\n","[SVM][TRAIN] Batch 473/782 done\n","[SVM][TRAIN] Batch 474/782 done\n","[SVM][TRAIN] Batch 475/782 done\n","[SVM][TRAIN] Batch 476/782 done\n","[SVM][TRAIN] Batch 477/782 done\n","[SVM][TRAIN] Batch 478/782 done\n","[SVM][TRAIN] Batch 479/782 done\n","[SVM][TRAIN] Batch 480/782 done\n","[SVM][TRAIN] Batch 481/782 done\n","[SVM][TRAIN] Batch 482/782 done\n","[SVM][TRAIN] Batch 483/782 done\n","[SVM][TRAIN] Batch 484/782 done\n","[SVM][TRAIN] Batch 485/782 done\n","[SVM][TRAIN] Batch 486/782 done\n","[SVM][TRAIN] Batch 487/782 done\n","[SVM][TRAIN] Batch 488/782 done\n","[SVM][TRAIN] Batch 489/782 done\n","[SVM][TRAIN] Batch 490/782 done\n","[SVM][TRAIN] Batch 491/782 done\n","[SVM][TRAIN] Batch 492/782 done\n","[SVM][TRAIN] Batch 493/782 done\n","[SVM][TRAIN] Batch 494/782 done\n","[SVM][TRAIN] Batch 495/782 done\n","[SVM][TRAIN] Batch 496/782 done\n","[SVM][TRAIN] Batch 497/782 done\n","[SVM][TRAIN] Batch 498/782 done\n","[SVM][TRAIN] Batch 499/782 done\n","[SVM][TRAIN] Batch 500/782 done\n","[SVM][TRAIN] Batch 501/782 done\n","[SVM][TRAIN] Batch 502/782 done\n","[SVM][TRAIN] Batch 503/782 done\n","[SVM][TRAIN] Batch 504/782 done\n","[SVM][TRAIN] Batch 505/782 done\n","[SVM][TRAIN] Batch 506/782 done\n","[SVM][TRAIN] Batch 507/782 done\n","[SVM][TRAIN] Batch 508/782 done\n","[SVM][TRAIN] Batch 509/782 done\n","[SVM][TRAIN] Batch 510/782 done\n","[SVM][TRAIN] Batch 511/782 done\n","[SVM][TRAIN] Batch 512/782 done\n","[SVM][TRAIN] Batch 513/782 done\n","[SVM][TRAIN] Batch 514/782 done\n","[SVM][TRAIN] Batch 515/782 done\n","[SVM][TRAIN] Batch 516/782 done\n","[SVM][TRAIN] Batch 517/782 done\n","[SVM][TRAIN] Batch 518/782 done\n","[SVM][TRAIN] Batch 519/782 done\n","[SVM][TRAIN] Batch 520/782 done\n","[SVM][TRAIN] Batch 521/782 done\n","[SVM][TRAIN] Batch 522/782 done\n","[SVM][TRAIN] Batch 523/782 done\n","[SVM][TRAIN] Batch 524/782 done\n","[SVM][TRAIN] Batch 525/782 done\n","[SVM][TRAIN] Batch 526/782 done\n","[SVM][TRAIN] Batch 527/782 done\n","[SVM][TRAIN] Batch 528/782 done\n","[SVM][TRAIN] Batch 529/782 done\n","[SVM][TRAIN] Batch 530/782 done\n","[SVM][TRAIN] Batch 531/782 done\n","[SVM][TRAIN] Batch 532/782 done\n","[SVM][TRAIN] Batch 533/782 done\n","[SVM][TRAIN] Batch 534/782 done\n","[SVM][TRAIN] Batch 535/782 done\n","[SVM][TRAIN] Batch 536/782 done\n","[SVM][TRAIN] Batch 537/782 done\n","[SVM][TRAIN] Batch 538/782 done\n","[SVM][TRAIN] Batch 539/782 done\n","[SVM][TRAIN] Batch 540/782 done\n","[SVM][TRAIN] Batch 541/782 done\n","[SVM][TRAIN] Batch 542/782 done\n","[SVM][TRAIN] Batch 543/782 done\n","[SVM][TRAIN] Batch 544/782 done\n","[SVM][TRAIN] Batch 545/782 done\n","[SVM][TRAIN] Batch 546/782 done\n","[SVM][TRAIN] Batch 547/782 done\n","[SVM][TRAIN] Batch 548/782 done\n","[SVM][TRAIN] Batch 549/782 done\n","[SVM][TRAIN] Batch 550/782 done\n","[SVM][TRAIN] Batch 551/782 done\n","[SVM][TRAIN] Batch 552/782 done\n","[SVM][TRAIN] Batch 553/782 done\n","[SVM][TRAIN] Batch 554/782 done\n","[SVM][TRAIN] Batch 555/782 done\n","[SVM][TRAIN] Batch 556/782 done\n","[SVM][TRAIN] Batch 557/782 done\n","[SVM][TRAIN] Batch 558/782 done\n","[SVM][TRAIN] Batch 559/782 done\n","[SVM][TRAIN] Batch 560/782 done\n","[SVM][TRAIN] Batch 561/782 done\n","[SVM][TRAIN] Batch 562/782 done\n","[SVM][TRAIN] Batch 563/782 done\n","[SVM][TRAIN] Batch 564/782 done\n","[SVM][TRAIN] Batch 565/782 done\n","[SVM][TRAIN] Batch 566/782 done\n","[SVM][TRAIN] Batch 567/782 done\n","[SVM][TRAIN] Batch 568/782 done\n","[SVM][TRAIN] Batch 569/782 done\n","[SVM][TRAIN] Batch 570/782 done\n","[SVM][TRAIN] Batch 571/782 done\n","[SVM][TRAIN] Batch 572/782 done\n","[SVM][TRAIN] Batch 573/782 done\n","[SVM][TRAIN] Batch 574/782 done\n","[SVM][TRAIN] Batch 575/782 done\n","[SVM][TRAIN] Batch 576/782 done\n","[SVM][TRAIN] Batch 577/782 done\n","[SVM][TRAIN] Batch 578/782 done\n","[SVM][TRAIN] Batch 579/782 done\n","[SVM][TRAIN] Batch 580/782 done\n","[SVM][TRAIN] Batch 581/782 done\n","[SVM][TRAIN] Batch 582/782 done\n","[SVM][TRAIN] Batch 583/782 done\n","[SVM][TRAIN] Batch 584/782 done\n","[SVM][TRAIN] Batch 585/782 done\n","[SVM][TRAIN] Batch 586/782 done\n","[SVM][TRAIN] Batch 587/782 done\n","[SVM][TRAIN] Batch 588/782 done\n","[SVM][TRAIN] Batch 589/782 done\n","[SVM][TRAIN] Batch 590/782 done\n","[SVM][TRAIN] Batch 591/782 done\n","[SVM][TRAIN] Batch 592/782 done\n","[SVM][TRAIN] Batch 593/782 done\n","[SVM][TRAIN] Batch 594/782 done\n","[SVM][TRAIN] Batch 595/782 done\n","[SVM][TRAIN] Batch 596/782 done\n","[SVM][TRAIN] Batch 597/782 done\n","[SVM][TRAIN] Batch 598/782 done\n","[SVM][TRAIN] Batch 599/782 done\n","[SVM][TRAIN] Batch 600/782 done\n","[SVM][TRAIN] Batch 601/782 done\n","[SVM][TRAIN] Batch 602/782 done\n","[SVM][TRAIN] Batch 603/782 done\n","[SVM][TRAIN] Batch 604/782 done\n","[SVM][TRAIN] Batch 605/782 done\n","[SVM][TRAIN] Batch 606/782 done\n","[SVM][TRAIN] Batch 607/782 done\n","[SVM][TRAIN] Batch 608/782 done\n","[SVM][TRAIN] Batch 609/782 done\n","[SVM][TRAIN] Batch 610/782 done\n","[SVM][TRAIN] Batch 611/782 done\n","[SVM][TRAIN] Batch 612/782 done\n","[SVM][TRAIN] Batch 613/782 done\n","[SVM][TRAIN] Batch 614/782 done\n","[SVM][TRAIN] Batch 615/782 done\n","[SVM][TRAIN] Batch 616/782 done\n","[SVM][TRAIN] Batch 617/782 done\n","[SVM][TRAIN] Batch 618/782 done\n","[SVM][TRAIN] Batch 619/782 done\n","[SVM][TRAIN] Batch 620/782 done\n","[SVM][TRAIN] Batch 621/782 done\n","[SVM][TRAIN] Batch 622/782 done\n","[SVM][TRAIN] Batch 623/782 done\n","[SVM][TRAIN] Batch 624/782 done\n","[SVM][TRAIN] Batch 625/782 done\n","[SVM][TRAIN] Batch 626/782 done\n","[SVM][TRAIN] Batch 627/782 done\n","[SVM][TRAIN] Batch 628/782 done\n","[SVM][TRAIN] Batch 629/782 done\n","[SVM][TRAIN] Batch 630/782 done\n","[SVM][TRAIN] Batch 631/782 done\n","[SVM][TRAIN] Batch 632/782 done\n","[SVM][TRAIN] Batch 633/782 done\n","[SVM][TRAIN] Batch 634/782 done\n","[SVM][TRAIN] Batch 635/782 done\n","[SVM][TRAIN] Batch 636/782 done\n","[SVM][TRAIN] Batch 637/782 done\n","[SVM][TRAIN] Batch 638/782 done\n","[SVM][TRAIN] Batch 639/782 done\n","[SVM][TRAIN] Batch 640/782 done\n","[SVM][TRAIN] Batch 641/782 done\n","[SVM][TRAIN] Batch 642/782 done\n","[SVM][TRAIN] Batch 643/782 done\n","[SVM][TRAIN] Batch 644/782 done\n","[SVM][TRAIN] Batch 645/782 done\n","[SVM][TRAIN] Batch 646/782 done\n","[SVM][TRAIN] Batch 647/782 done\n","[SVM][TRAIN] Batch 648/782 done\n","[SVM][TRAIN] Batch 649/782 done\n","[SVM][TRAIN] Batch 650/782 done\n","[SVM][TRAIN] Batch 651/782 done\n","[SVM][TRAIN] Batch 652/782 done\n","[SVM][TRAIN] Batch 653/782 done\n","[SVM][TRAIN] Batch 654/782 done\n","[SVM][TRAIN] Batch 655/782 done\n","[SVM][TRAIN] Batch 656/782 done\n","[SVM][TRAIN] Batch 657/782 done\n","[SVM][TRAIN] Batch 658/782 done\n","[SVM][TRAIN] Batch 659/782 done\n","[SVM][TRAIN] Batch 660/782 done\n","[SVM][TRAIN] Batch 661/782 done\n","[SVM][TRAIN] Batch 662/782 done\n","[SVM][TRAIN] Batch 663/782 done\n","[SVM][TRAIN] Batch 664/782 done\n","[SVM][TRAIN] Batch 665/782 done\n","[SVM][TRAIN] Batch 666/782 done\n","[SVM][TRAIN] Batch 667/782 done\n","[SVM][TRAIN] Batch 668/782 done\n","[SVM][TRAIN] Batch 669/782 done\n","[SVM][TRAIN] Batch 670/782 done\n","[SVM][TRAIN] Batch 671/782 done\n","[SVM][TRAIN] Batch 672/782 done\n","[SVM][TRAIN] Batch 673/782 done\n","[SVM][TRAIN] Batch 674/782 done\n","[SVM][TRAIN] Batch 675/782 done\n","[SVM][TRAIN] Batch 676/782 done\n","[SVM][TRAIN] Batch 677/782 done\n","[SVM][TRAIN] Batch 678/782 done\n","[SVM][TRAIN] Batch 679/782 done\n","[SVM][TRAIN] Batch 680/782 done\n","[SVM][TRAIN] Batch 681/782 done\n","[SVM][TRAIN] Batch 682/782 done\n","[SVM][TRAIN] Batch 683/782 done\n","[SVM][TRAIN] Batch 684/782 done\n","[SVM][TRAIN] Batch 685/782 done\n","[SVM][TRAIN] Batch 686/782 done\n","[SVM][TRAIN] Batch 687/782 done\n","[SVM][TRAIN] Batch 688/782 done\n","[SVM][TRAIN] Batch 689/782 done\n","[SVM][TRAIN] Batch 690/782 done\n","[SVM][TRAIN] Batch 691/782 done\n","[SVM][TRAIN] Batch 692/782 done\n","[SVM][TRAIN] Batch 693/782 done\n","[SVM][TRAIN] Batch 694/782 done\n","[SVM][TRAIN] Batch 695/782 done\n","[SVM][TRAIN] Batch 696/782 done\n","[SVM][TRAIN] Batch 697/782 done\n","[SVM][TRAIN] Batch 698/782 done\n","[SVM][TRAIN] Batch 699/782 done\n","[SVM][TRAIN] Batch 700/782 done\n","[SVM][TRAIN] Batch 701/782 done\n","[SVM][TRAIN] Batch 702/782 done\n","[SVM][TRAIN] Batch 703/782 done\n","[SVM][TRAIN] Batch 704/782 done\n","[SVM][TRAIN] Batch 705/782 done\n","[SVM][TRAIN] Batch 706/782 done\n","[SVM][TRAIN] Batch 707/782 done\n","[SVM][TRAIN] Batch 708/782 done\n","[SVM][TRAIN] Batch 709/782 done\n","[SVM][TRAIN] Batch 710/782 done\n","[SVM][TRAIN] Batch 711/782 done\n","[SVM][TRAIN] Batch 712/782 done\n","[SVM][TRAIN] Batch 713/782 done\n","[SVM][TRAIN] Batch 714/782 done\n","[SVM][TRAIN] Batch 715/782 done\n","[SVM][TRAIN] Batch 716/782 done\n","[SVM][TRAIN] Batch 717/782 done\n","[SVM][TRAIN] Batch 718/782 done\n","[SVM][TRAIN] Batch 719/782 done\n","[SVM][TRAIN] Batch 720/782 done\n","[SVM][TRAIN] Batch 721/782 done\n","[SVM][TRAIN] Batch 722/782 done\n","[SVM][TRAIN] Batch 723/782 done\n","[SVM][TRAIN] Batch 724/782 done\n","[SVM][TRAIN] Batch 725/782 done\n","[SVM][TRAIN] Batch 726/782 done\n","[SVM][TRAIN] Batch 727/782 done\n","[SVM][TRAIN] Batch 728/782 done\n","[SVM][TRAIN] Batch 729/782 done\n","[SVM][TRAIN] Batch 730/782 done\n","[SVM][TRAIN] Batch 731/782 done\n","[SVM][TRAIN] Batch 732/782 done\n","[SVM][TRAIN] Batch 733/782 done\n","[SVM][TRAIN] Batch 734/782 done\n","[SVM][TRAIN] Batch 735/782 done\n","[SVM][TRAIN] Batch 736/782 done\n","[SVM][TRAIN] Batch 737/782 done\n","[SVM][TRAIN] Batch 738/782 done\n","[SVM][TRAIN] Batch 739/782 done\n","[SVM][TRAIN] Batch 740/782 done\n","[SVM][TRAIN] Batch 741/782 done\n","[SVM][TRAIN] Batch 742/782 done\n","[SVM][TRAIN] Batch 743/782 done\n","[SVM][TRAIN] Batch 744/782 done\n","[SVM][TRAIN] Batch 745/782 done\n","[SVM][TRAIN] Batch 746/782 done\n","[SVM][TRAIN] Batch 747/782 done\n","[SVM][TRAIN] Batch 748/782 done\n","[SVM][TRAIN] Batch 749/782 done\n","[SVM][TRAIN] Batch 750/782 done\n","[SVM][TRAIN] Batch 751/782 done\n","[SVM][TRAIN] Batch 752/782 done\n","[SVM][TRAIN] Batch 753/782 done\n","[SVM][TRAIN] Batch 754/782 done\n","[SVM][TRAIN] Batch 755/782 done\n","[SVM][TRAIN] Batch 756/782 done\n","[SVM][TRAIN] Batch 757/782 done\n","[SVM][TRAIN] Batch 758/782 done\n","[SVM][TRAIN] Batch 759/782 done\n","[SVM][TRAIN] Batch 760/782 done\n","[SVM][TRAIN] Batch 761/782 done\n","[SVM][TRAIN] Batch 762/782 done\n","[SVM][TRAIN] Batch 763/782 done\n","[SVM][TRAIN] Batch 764/782 done\n","[SVM][TRAIN] Batch 765/782 done\n","[SVM][TRAIN] Batch 766/782 done\n","[SVM][TRAIN] Batch 767/782 done\n","[SVM][TRAIN] Batch 768/782 done\n","[SVM][TRAIN] Batch 769/782 done\n","[SVM][TRAIN] Batch 770/782 done\n","[SVM][TRAIN] Batch 771/782 done\n","[SVM][TRAIN] Batch 772/782 done\n","[SVM][TRAIN] Batch 773/782 done\n","[SVM][TRAIN] Batch 774/782 done\n","[SVM][TRAIN] Batch 775/782 done\n","[SVM][TRAIN] Batch 776/782 done\n","[SVM][TRAIN] Batch 777/782 done\n","[SVM][TRAIN] Batch 778/782 done\n","[SVM][TRAIN] Batch 779/782 done\n","[SVM][TRAIN] Batch 780/782 done\n","[SVM][TRAIN] Batch 781/782 done\n","[SVM][TRAIN] Batch 782/782 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/157 done\n","[SVM][TEST] Batch 2/157 done\n","[SVM][TEST] Batch 3/157 done\n","[SVM][TEST] Batch 4/157 done\n","[SVM][TEST] Batch 5/157 done\n","[SVM][TEST] Batch 6/157 done\n","[SVM][TEST] Batch 7/157 done\n","[SVM][TEST] Batch 8/157 done\n","[SVM][TEST] Batch 9/157 done\n","[SVM][TEST] Batch 10/157 done\n","[SVM][TEST] Batch 11/157 done\n","[SVM][TEST] Batch 12/157 done\n","[SVM][TEST] Batch 13/157 done\n","[SVM][TEST] Batch 14/157 done\n","[SVM][TEST] Batch 15/157 done\n","[SVM][TEST] Batch 16/157 done\n","[SVM][TEST] Batch 17/157 done\n","[SVM][TEST] Batch 18/157 done\n","[SVM][TEST] Batch 19/157 done\n","[SVM][TEST] Batch 20/157 done\n","[SVM][TEST] Batch 21/157 done\n","[SVM][TEST] Batch 22/157 done\n","[SVM][TEST] Batch 23/157 done\n","[SVM][TEST] Batch 24/157 done\n","[SVM][TEST] Batch 25/157 done\n","[SVM][TEST] Batch 26/157 done\n","[SVM][TEST] Batch 27/157 done\n","[SVM][TEST] Batch 28/157 done\n","[SVM][TEST] Batch 29/157 done\n","[SVM][TEST] Batch 30/157 done\n","[SVM][TEST] Batch 31/157 done\n","[SVM][TEST] Batch 32/157 done\n","[SVM][TEST] Batch 33/157 done\n","[SVM][TEST] Batch 34/157 done\n","[SVM][TEST] Batch 35/157 done\n","[SVM][TEST] Batch 36/157 done\n","[SVM][TEST] Batch 37/157 done\n","[SVM][TEST] Batch 38/157 done\n","[SVM][TEST] Batch 39/157 done\n","[SVM][TEST] Batch 40/157 done\n","[SVM][TEST] Batch 41/157 done\n","[SVM][TEST] Batch 42/157 done\n","[SVM][TEST] Batch 43/157 done\n","[SVM][TEST] Batch 44/157 done\n","[SVM][TEST] Batch 45/157 done\n","[SVM][TEST] Batch 46/157 done\n","[SVM][TEST] Batch 47/157 done\n","[SVM][TEST] Batch 48/157 done\n","[SVM][TEST] Batch 49/157 done\n","[SVM][TEST] Batch 50/157 done\n","[SVM][TEST] Batch 51/157 done\n","[SVM][TEST] Batch 52/157 done\n","[SVM][TEST] Batch 53/157 done\n","[SVM][TEST] Batch 54/157 done\n","[SVM][TEST] Batch 55/157 done\n","[SVM][TEST] Batch 56/157 done\n","[SVM][TEST] Batch 57/157 done\n","[SVM][TEST] Batch 58/157 done\n","[SVM][TEST] Batch 59/157 done\n","[SVM][TEST] Batch 60/157 done\n","[SVM][TEST] Batch 61/157 done\n","[SVM][TEST] Batch 62/157 done\n","[SVM][TEST] Batch 63/157 done\n","[SVM][TEST] Batch 64/157 done\n","[SVM][TEST] Batch 65/157 done\n","[SVM][TEST] Batch 66/157 done\n","[SVM][TEST] Batch 67/157 done\n","[SVM][TEST] Batch 68/157 done\n","[SVM][TEST] Batch 69/157 done\n","[SVM][TEST] Batch 70/157 done\n","[SVM][TEST] Batch 71/157 done\n","[SVM][TEST] Batch 72/157 done\n","[SVM][TEST] Batch 73/157 done\n","[SVM][TEST] Batch 74/157 done\n","[SVM][TEST] Batch 75/157 done\n","[SVM][TEST] Batch 76/157 done\n","[SVM][TEST] Batch 77/157 done\n","[SVM][TEST] Batch 78/157 done\n","[SVM][TEST] Batch 79/157 done\n","[SVM][TEST] Batch 80/157 done\n","[SVM][TEST] Batch 81/157 done\n","[SVM][TEST] Batch 82/157 done\n","[SVM][TEST] Batch 83/157 done\n","[SVM][TEST] Batch 84/157 done\n","[SVM][TEST] Batch 85/157 done\n","[SVM][TEST] Batch 86/157 done\n","[SVM][TEST] Batch 87/157 done\n","[SVM][TEST] Batch 88/157 done\n","[SVM][TEST] Batch 89/157 done\n","[SVM][TEST] Batch 90/157 done\n","[SVM][TEST] Batch 91/157 done\n","[SVM][TEST] Batch 92/157 done\n","[SVM][TEST] Batch 93/157 done\n","[SVM][TEST] Batch 94/157 done\n","[SVM][TEST] Batch 95/157 done\n","[SVM][TEST] Batch 96/157 done\n","[SVM][TEST] Batch 97/157 done\n","[SVM][TEST] Batch 98/157 done\n","[SVM][TEST] Batch 99/157 done\n","[SVM][TEST] Batch 100/157 done\n","[SVM][TEST] Batch 101/157 done\n","[SVM][TEST] Batch 102/157 done\n","[SVM][TEST] Batch 103/157 done\n","[SVM][TEST] Batch 104/157 done\n","[SVM][TEST] Batch 105/157 done\n","[SVM][TEST] Batch 106/157 done\n","[SVM][TEST] Batch 107/157 done\n","[SVM][TEST] Batch 108/157 done\n","[SVM][TEST] Batch 109/157 done\n","[SVM][TEST] Batch 110/157 done\n","[SVM][TEST] Batch 111/157 done\n","[SVM][TEST] Batch 112/157 done\n","[SVM][TEST] Batch 113/157 done\n","[SVM][TEST] Batch 114/157 done\n","[SVM][TEST] Batch 115/157 done\n","[SVM][TEST] Batch 116/157 done\n","[SVM][TEST] Batch 117/157 done\n","[SVM][TEST] Batch 118/157 done\n","[SVM][TEST] Batch 119/157 done\n","[SVM][TEST] Batch 120/157 done\n","[SVM][TEST] Batch 121/157 done\n","[SVM][TEST] Batch 122/157 done\n","[SVM][TEST] Batch 123/157 done\n","[SVM][TEST] Batch 124/157 done\n","[SVM][TEST] Batch 125/157 done\n","[SVM][TEST] Batch 126/157 done\n","[SVM][TEST] Batch 127/157 done\n","[SVM][TEST] Batch 128/157 done\n","[SVM][TEST] Batch 129/157 done\n","[SVM][TEST] Batch 130/157 done\n","[SVM][TEST] Batch 131/157 done\n","[SVM][TEST] Batch 132/157 done\n","[SVM][TEST] Batch 133/157 done\n","[SVM][TEST] Batch 134/157 done\n","[SVM][TEST] Batch 135/157 done\n","[SVM][TEST] Batch 136/157 done\n","[SVM][TEST] Batch 137/157 done\n","[SVM][TEST] Batch 138/157 done\n","[SVM][TEST] Batch 139/157 done\n","[SVM][TEST] Batch 140/157 done\n","[SVM][TEST] Batch 141/157 done\n","[SVM][TEST] Batch 142/157 done\n","[SVM][TEST] Batch 143/157 done\n","[SVM][TEST] Batch 144/157 done\n","[SVM][TEST] Batch 145/157 done\n","[SVM][TEST] Batch 146/157 done\n","[SVM][TEST] Batch 147/157 done\n","[SVM][TEST] Batch 148/157 done\n","[SVM][TEST] Batch 149/157 done\n","[SVM][TEST] Batch 150/157 done\n","[SVM][TEST] Batch 151/157 done\n","[SVM][TEST] Batch 152/157 done\n","[SVM][TEST] Batch 153/157 done\n","[SVM][TEST] Batch 154/157 done\n","[SVM][TEST] Batch 155/157 done\n","[SVM][TEST] Batch 156/157 done\n","[SVM][TEST] Batch 157/157 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}]},{"cell_type":"code","source":["!head -1 train_svm.txt"],"metadata":{"id":"BVgZ8yBYeFLE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765982894240,"user_tz":-420,"elapsed":413,"user":{"displayName":"Trần Hoàng Kim Ngân","userId":"03605743943745384487"}},"outputId":"ccedd803-0673-4bca-b112-12c7b556f09e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["6 1:0.0804102 2:0.113586 3:0.1118 4:0.1103 5:0.108482 6:0.112092 7:0.131064 8:0.127372 9:0.0963327 10:0.0902453 11:0.089428 12:0.0802626 13:0.115053 14:0.124319 15:0.104251 16:0.0839012 17:0.112526 18:0.1091 19:0.0773689 20:0.117622 21:0.128095 22:0.108301 23:0.104102 24:0.102764 25:0.105041 26:0.0920782 27:0.128262 28:0.208585 29:0.192834 30:0.182007 31:0.138484 32:0.110814 33:0.108318 34:0.211639 35:0.21795 36:0.225795 37:0.23139 38:0.213428 39:0.13844 40:0.117564 41:0.124014 42:0.24337 43:0.222373 44:0.154003 45:0.129603 46:0.180292 47:0.131287 48:0.109848 49:0.146509 50:0.132759 51:0.177338 52:0.180704 53:0.123681 54:0.106609 55:0.11601 56:0.159655 57:0.149696 58:0.151778 59:0.134801 60:0.128704 61:0.128791 62:0.114167 63:0.142805 64:0.148778 65:0 66:0 67:0 68:0.00182778 69:0 70:0.0052575 71:0 72:0.0244691 73:0.00926536 74:0.0155198 75:0.0123379 76:0.0233127 77:0.0547432 78:0.0537391 79:0.0249411 80:0.0591028 81:0.0103051 82:0.0156108 83:0.0564105 84:0.0655661 85:0.0400153 86:0.0628458 87:0.0337694 88:0.0366668 89:0.0123784 90:0.0399139 91:0.0143121 92:0 93:0.00134116 94:0.0316416 95:0.0510858 96:0.0471938 97:0.00750645 98:0.0345498 99:0.0150471 100:0.00607985 101:0.0127275 102:0.00564942 103:0.0461168 104:0.0410206 105:0.0165556 106:0.00497533 107:0.018207 108:0.0367192 109:0.0218291 110:0.0172237 111:0.0472263 112:0.0413383 113:0.0437225 114:0.0382638 115:0.0111156 116:0.00974593 117:0.025613 118:0.024707 119:0.0175384 120:0.0387346 121:0.0438085 122:0.0570217 123:0.0453354 124:0.0329876 125:0.0209654 126:0.0267404 127:0.0124657 128:0.0814654 129:0 130:0 131:0 132:0 133:0 134:0 135:0 136:0.0343148 137:0 138:0 139:0 140:0 141:0 142:0 143:0 144:0.0280162 145:0 146:0 147:0 148:0 149:0 150:0 151:0 152:0.0490404 153:0 154:0 155:0 156:0 157:0 158:0 159:0 160:0.046942 161:0 162:0 163:0 164:0 165:0 166:0 167:0 168:0.0490909 169:0 170:0 171:0 172:0 173:0 174:0 175:0 176:0.0542341 177:0 178:0 179:0 180:0 181:0 182:0 183:0 184:0.0353132 185:0.0662929 186:0.0781674 187:0.058416 188:0.0564599 189:0.052094 190:0.0518526 191:0.0438405 192:0.0839753 193:0 194:0 195:0 196:0 197:0 198:0 199:0 200:0 201:0 202:0 203:0 204:0 205:0 206:0 207:0 208:0 209:0 210:0 211:0 212:0 213:0 214:0 215:0 216:0 217:0 218:0 219:0 220:0 221:0 222:0 223:0 224:0 225:0 226:0 227:0 228:0 229:0 230:0 231:0 232:0 233:0 234:0 235:0 236:0 237:0 238:0 239:0 240:0 241:0 242:0 243:0 244:0 245:0 246:0 247:0 248:0 249:0 250:0 251:0 252:0 253:0 254:0 255:0 256:0 257:0.00295021 258:0.00890094 259:0.0110094 260:0.0169904 261:0.0187497 262:0.00261305 263:0.0160719 264:0.0597845 265:0 266:0.00556181 267:0.0272597 268:0.0121533 269:0.0088178 270:0.0244533 271:0.00315978 272:0.0583743 273:0 274:0.0175248 275:0 276:0 277:0 278:0.0195651 279:0.0194004 280:0.06183 281:0.00464424 282:0.00447937 283:0 284:0 285:0 286:0 287:0.0365133 288:0.0473975 289:0 290:0 291:0 292:0.00564917 293:0 294:0 295:0.0219557 296:0.0373775 297:0 298:0.00395906 299:0 300:0.0107603 301:0.0175307 302:0.0149229 303:0 304:0.0310306 305:0 306:0.002551 307:0.00232954 308:0 309:0 310:0.00287961 311:0 312:0.0555507 313:0.0388583 314:0.0395079 315:0.0563282 316:0.0462184 317:0.0478574 318:0.0499755 319:0.0386627 320:0.0900931 321:0.0106741 322:0.0538441 323:0.048509 324:0.0539825 325:0.0495259 326:0.0541863 327:0.0568746 328:0.0838441 329:0.0243906 330:0.0182654 331:0.0235276 332:0.00248963 333:0.0325705 334:0.0217201 335:0.0316617 336:0.072845 337:0.0314772 338:0.0218879 339:0.0200151 340:0.0322611 341:0.0354086 342:0.0498947 343:0.0212649 344:0.0640435 345:0.0285606 346:0.0155372 347:0.00374266 348:0.0536884 349:0.0553333 350:0.0512329 351:0.0409649 352:0.0621697 353:0.0242889 354:0.033733 355:0.0351187 356:0.0405008 357:0.0111906 358:0.062413 359:0.0429174 360:0.0583082 361:0.0230344 362:0.0302481 363:0.0500941 364:0.0550687 365:0.0227492 366:0.0271849 367:0.0066531 368:0.0864766 369:0.0296414 370:0.0200223 371:0.0211204 372:0.0255923 373:0.0252049 374:0.0288325 375:0.00835039 376:0.0879545 377:0.00675449 378:0.0452283 379:0.0429949 380:0.0300925 381:0.0296607 382:0.0332066 383:0.0235571 384:0.0620307 385:0.0293115 386:0.0301067 387:0.0368164 388:0.0338083 389:0.0340972 390:0.0238069 391:0.035919 392:0.0444281 393:0.00967842 394:0.0196345 395:0.0307991 396:0.0434573 397:0.0358769 398:0.0371469 399:0.0320941 400:0.042568 401:0 402:0.0281638 403:0.0287494 404:0.017014 405:0 406:0.0475381 407:0.0297673 408:0.0214583 409:0 410:0.0314133 411:0 412:0 413:0 414:0 415:0.039149 416:0.0213047 417:0 418:0 419:0 420:0 421:0 422:0 423:0.0124069 424:0.0235056 425:0 426:0.0121956 427:0 428:0.0161304 429:0.0282269 430:0.0200941 431:0 432:0 433:0 434:0.0172757 435:0.0237095 436:0.0183873 437:0.0160921 438:0.0206247 439:0.0169318 440:0 441:0.00320824 442:0.0106273 443:0.032605 444:0.0243948 445:0.0244323 446:0.0314229 447:0.0415016 448:0 449:0.0252745 450:0.0407932 451:0.0467596 452:0.0382942 453:0.0350738 454:0.0331797 455:0.0525037 456:0.047765 457:0.0518053 458:0.0467037 459:0.0488264 460:0.0378745 461:0.0547874 462:0.0406872 463:0.0591341 464:0.056831 465:0.0766609 466:0.0447451 467:0.0541187 468:0.0554022 469:0.0744752 470:0.0590854 471:0.0637985 472:0.0585201 473:0.0819688 474:0.057052 475:0.0846267 476:0.104012 477:0.103338 478:0.0965483 479:0.0885949 480:0.0645086 481:0.0812387 482:0.0704743 483:0.0992934 484:0.103558 485:0.115617 486:0.135828 487:0.141512 488:0.0848731 489:0.0663702 490:0.057708 491:0.123257 492:0.0901762 493:0.0828632 494:0.0825478 495:0.108468 496:0.0749707 497:0.100867 498:0.0732586 499:0.0684177 500:0.0911933 501:0.0639535 502:0.052055 503:0.0604653 504:0.0850562 505:0.113678 506:0.0685231 507:0.0596344 508:0.0518269 509:0.0556353 510:0.0482149 511:0.0595306 512:0.0920702 513:0.161593 514:0.175838 515:0.166208 516:0.167335 517:0.173874 518:0.176422 519:0.182264 520:0.146187 521:0.182166 522:0.17084 523:0.167798 524:0.163604 525:0.178357 526:0.16395 527:0.178951 528:0.140969 529:0.193241 530:0.174795 531:0.176654 532:0.228266 533:0.211015 534:0.197547 535:0.188687 536:0.15337 537:0.191243 538:0.175976 539:0.244439 540:0.272616 541:0.289584 542:0.278507 543:0.25178 544:0.174009 545:0.189303 546:0.250625 547:0.289554 548:0.294958 549:0.307404 550:0.29384 551:0.270961 552:0.168502 553:0.191559 554:0.258198 555:0.270634 556:0.25233 557:0.213419 558:0.257444 559:0.238676 560:0.167692 561:0.212505 562:0.207234 563:0.229648 564:0.232708 565:0.199332 566:0.187855 567:0.200453 568:0.217599 569:0.218984 570:0.222063 571:0.213134 572:0.197003 573:0.199542 574:0.191076 575:0.189559 576:0.193164 577:0.00193197 578:0 579:0 580:0 581:0 582:0 583:0 584:0.0013922 585:0.00252846 586:0 587:0 588:0 589:0 590:0 591:0 592:0 593:1.36769e-05 594:0 595:0 596:0 597:0 598:0 599:0 600:0 601:0 602:0 603:0 604:0 605:0 606:0 607:0 608:0 609:0 610:0.017084 611:0 612:0.00308581 613:0 614:0 615:0 616:0 617:0 618:0 619:0 620:0 621:0 622:0 623:0 624:0 625:0 626:0 627:0 628:0 629:0 630:0 631:0 632:0 633:0 634:0.000119795 635:0.0134488 636:0.00356259 637:0 638:0.00244531 639:0.00973143 640:0 641:0 642:0 643:0 644:0 645:0 646:0 647:0 648:0 649:0 650:0 651:0 652:0 653:0 654:0 655:0 656:0 657:0.00544069 658:0 659:0 660:0.0209698 661:0 662:0.00780747 663:0 664:0 665:0 666:0.00523749 667:0.0249141 668:0.0275808 669:0 670:0 671:0 672:0 673:0 674:0.0524961 675:0.0170229 676:0 677:0 678:0 679:0 680:0 681:0 682:0 683:0 684:0 685:0 686:0 687:0 688:0.00279476 689:0.0144551 690:0 691:0 692:0 693:0 694:0 695:0 696:0 697:0.00433198 698:0 699:0 700:0 701:0 702:0 703:0 704:0 705:0.0222412 706:0.0538663 707:0.0585673 708:0.0536085 709:0.051164 710:0.0427544 711:0.0386542 712:0.0487329 713:0.0250157 714:0.0544381 715:0.0627051 716:0.0703594 717:0.0837786 718:0.0576466 719:0.0566797 720:0.0489778 721:0.0342047 722:0.0512911 723:0.0748406 724:0.0549701 725:0.0611741 726:0.0455546 727:0.0452357 728:0.0227551 729:0.0342791 730:0.0633781 731:0.0312614 732:0.00660112 733:0 734:0.00433377 735:0.0528883 736:0.0324676 737:0.0502148 738:0.0797271 739:0.0520903 740:0.0313555 741:0.0553139 742:0.00016938 743:0.0679523 744:0.0422181 745:0.0457713 746:0.0293107 747:0.0308673 748:0.0469032 749:0.0437437 750:0.0345128 751:0.0835812 752:0.0377065 753:0.0568637 754:0.0659449 755:0.0432969 756:0.0582076 757:0.0525071 758:0.0503882 759:0.0561606 760:0.0818834 761:0.0937191 762:0.103775 763:0.0975734 764:0.0774478 765:0.0686117 766:0.0802783 767:0.106991 768:0.0864317 769:0 770:0 771:0 772:0 773:0 774:0 775:0 776:0 777:0 778:0 779:0 780:0 781:0 782:0 783:0 784:0 785:0 786:0 787:0 788:0 789:0 790:0 791:0 792:0 793:0 794:0 795:0 796:0 797:0 798:0 799:0 800:0 801:0 802:0 803:0 804:0 805:0 806:0 807:0 808:0 809:0 810:0 811:0 812:0 813:0 814:0 815:0 816:0 817:0 818:0 819:0 820:0 821:0 822:0 823:0 824:0 825:0 826:0 827:0 828:0 829:0 830:0 831:0 832:0 833:0.0330254 834:0.0541815 835:0.0611541 836:0.0674015 837:0.0655446 838:0.0694988 839:0.0586208 840:0.0519772 841:0.0402347 842:0.0429626 843:0.0330052 844:0.02863 845:0.0329325 846:0.0413009 847:0.0496448 848:0.0287996 849:0.0649463 850:0.0380323 851:0.0728179 852:0.0670707 853:0.0491293 854:0.0378798 855:0.0380572 856:0.0418459 857:0.0535201 858:0.0286777 859:0.0786903 860:0.086584 861:0.0775496 862:0.086846 863:0.065357 864:0.036165 865:0.0735177 866:0.0968947 867:0.094086 868:0.0855987 869:0.0494853 870:0.0540609 871:0.0541652 872:0.0446275 873:0.0664397 874:0.0503045 875:0.0523601 876:0.0606578 877:0.0617285 878:0.0241509 879:0.0578691 880:0.0568259 881:0.0988296 882:0.0394282 883:0.0418578 884:0.0584524 885:0.045841 886:0.0474469 887:0.0587783 888:0.0515213 889:0.111199 890:0.0746745 891:0.0649415 892:0.0572074 893:0.0538164 894:0.0473667 895:0.031945 896:0.0489152 897:0.124084 898:0.162698 899:0.162779 900:0.157319 901:0.161752 902:0.174671 903:0.169363 904:0.170708 905:0.151018 906:0.15464 907:0.158716 908:0.16928 909:0.166235 910:0.178922 911:0.177949 912:0.162498 913:0.17212 914:0.165358 915:0.17931 916:0.205327 917:0.202592 918:0.204985 919:0.180893 920:0.172597 921:0.164659 922:0.200755 923:0.207074 924:0.274155 925:0.268452 926:0.288104 927:0.199352 928:0.178884 929:0.165938 930:0.261863 931:0.245581 932:0.270414 933:0.275195 934:0.305444 935:0.210898 936:0.176954 937:0.163741 938:0.254878 939:0.278001 940:0.245692 941:0.201559 942:0.246587 943:0.195759 944:0.206712 945:0.161815 946:0.191495 947:0.214094 948:0.212414 949:0.184298 950:0.177045 951:0.192493 952:0.212375 953:0.173673 954:0.190698 955:0.193671 956:0.179788 957:0.182269 958:0.173388 959:0.219629 960:0.209891 961:0.0442441 962:0.056587 963:0.0707091 964:0.0669151 965:0.0591715 966:0.0599963 967:0.0687279 968:0.110863 969:0.0625606 970:0.0614377 971:0.0773924 972:0.0714116 973:0.0927016 974:0.0574421 975:0.0832106 976:0.100761 977:0.0859309 978:0.0759726 979:0.0735741 980:0.0656634 981:0.0713458 982:0.100415 983:0.0658881 984:0.0938997 985:0.0873837 986:0.0802654 987:0.0889727 988:0.0803359 989:0.119698 990:0.10161 991:0.149597 992:0.0976481 993:0.0700795 994:0.0699068 995:0.122508 996:0.117762 997:0.114903 998:0.113098 999:0.168122 1000:0.0930405 1001:0.0637453 1002:0.0551952 1003:0.123938 1004:0.123733 1005:0.0868055 1006:0.0800924 1007:0.130373 1008:0.096434 1009:0.0768733 1010:0.0553531 1011:0.0752197 1012:0.1036 1013:0.0832814 1014:0.0692334 1015:0.0724106 1016:0.138994 1017:0.093893 1018:0.0858265 1019:0.087679 1020:0.074956 1021:0.0753658 1022:0.0761575 1023:0.0778824 1024:0.178623 1025:0.0632844 1026:0.102869 1027:0.11236 1028:0.108193 1029:0.107165 1030:0.108349 1031:0.121431 1032:0.130079 1033:0.0889767 1034:0.0866504 1035:0.0930615 1036:0.096218 1037:0.0975238 1038:0.114235 1039:0.103452 1040:0.0818199 1041:0.101823 1042:0.104013 1043:0.106932 1044:0.133759 1045:0.123217 1046:0.131893 1047:0.106053 1048:0.0901019 1049:0.0987437 1050:0.097251 1051:0.153249 1052:0.199089 1053:0.184178 1054:0.167806 1055:0.140158 1056:0.0979175 1057:0.0964835 1058:0.173185 1059:0.187773 1060:0.209675 1061:0.176327 1062:0.181312 1063:0.137838 1064:0.0908855 1065:0.0832575 1066:0.164828 1067:0.142395 1068:0.139393 1069:0.107353 1070:0.161015 1071:0.0995121 1072:0.0985017 1073:0.0834291 1074:0.110797 1075:0.137874 1076:0.134182 1077:0.108083 1078:0.0927735 1079:0.122143 1080:0.106 1081:0.0939118 1082:0.0895637 1083:0.0896664 1084:0.088504 1085:0.0968702 1086:0.0955689 1087:0.111997 1088:0.110664 1089:0.0221891 1090:0.0400021 1091:0.048067 1092:0.0565845 1093:0.0529428 1094:0.0637829 1095:0.057575 1096:0.0594644 1097:0.0406764 1098:0.0520253 1099:0.0469998 1100:0.0552502 1101:0.0514061 1102:0.055805 1103:0.0659481 1104:0.0694116 1105:0.0627252 1106:0.064598 1107:0.0592431 1108:0.0687465 1109:0.104063 1110:0.0993694 1111:0.0823712 1112:0.0714625 1113:0.0514737 1114:0.0570001 1115:0.0797707 1116:0.10459 1117:0.160061 1118:0.141843 1119:0.136231 1120:0.0743581 1121:0.0376969 1122:0.0823949 1123:0.12604 1124:0.113764 1125:0.101709 1126:0.0952704 1127:0.122669 1128:0.0768953 1129:0.039592 1130:0.0547989 1131:0.0782235 1132:0.118577 1133:0.0854368 1134:0.0349207 1135:0.0663649 1136:0.0820244 1137:0.0447335 1138:0.049194 1139:0.0631885 1140:0.0867989 1141:0.0821982 1142:0.066188 1143:0.0660229 1144:0.10606 1145:0.100503 1146:0.105889 1147:0.0991508 1148:0.0893041 1149:0.0868675 1150:0.0806725 1151:0.0575656 1152:0.107515 1153:0.00188542 1154:0.0229805 1155:0.0298897 1156:0.0234721 1157:0.0224167 1158:0.0250228 1159:0.0278522 1160:0.0305363 1161:0.0047996 1162:0.0212157 1163:0.0298859 1164:0.0350069 1165:0.0361994 1166:0.0451514 1167:0.03321 1168:0.0345065 1169:0.0318075 1170:0.0378994 1171:0.0257847 1172:0.0193375 1173:0.0118885 1174:0.0512874 1175:0.0221777 1176:0.0291634 1177:0.0545845 1178:0.0395709 1179:0.00413588 1180:0.0197931 1181:0.0243884 1182:0.0366592 1183:0.0434188 1184:0.0371659 1185:0.0524256 1186:0.00574751 1187:0.0453555 1188:0.0652822 1189:0.0957154 1190:0.0792532 1191:0.0792247 1192:0.0423009 1193:0.0275119 1194:0.0420136 1195:0.0522743 1196:0.0655996 1197:0.05349 1198:0.0762429 1199:0.0887428 1200:0.0306968 1201:0.0621886 1202:0.0389622 1203:0.0498821 1204:0.0631227 1205:0.0440709 1206:0.0349228 1207:0.035866 1208:0.0560183 1209:0.104004 1210:0.0842993 1211:0.0665274 1212:0.0520193 1213:0.048341 1214:0.0506455 1215:0.0465802 1216:0.0771907 1217:0 1218:0 1219:0 1220:0 1221:0.000227654 1222:0.00272833 1223:0.00310531 1224:0 1225:0 1226:0 1227:0 1228:0 1229:0 1230:0 1231:0 1232:0 1233:0 1234:0 1235:0 1236:0 1237:0 1238:0 1239:0 1240:0 1241:0 1242:0 1243:0 1244:0 1245:0 1246:0 1247:0 1248:0 1249:0 1250:0 1251:0 1252:0 1253:0 1254:0 1255:0 1256:0 1257:0 1258:0 1259:0 1260:0 1261:0 1262:0 1263:0 1264:0.000325656 1265:0 1266:0 1267:0 1268:0 1269:0 1270:0 1271:0 1272:0.00925662 1273:0 1274:0 1275:0 1276:0 1277:0 1278:0 1279:0 1280:0 1281:0.0263169 1282:0.0233731 1283:0.0135755 1284:0.00567617 1285:0.0078824 1286:0 1287:0.0208557 1288:0.0222317 1289:0.026611 1290:0.0112065 1291:0.0176259 1292:0.0131916 1293:0.0379261 1294:0.0319939 1295:0.0101939 1296:0.0178387 1297:0.0360141 1298:0.009184 1299:0 1300:0.0171827 1301:0.00989847 1302:0.0398551 1303:0.0152812 1304:0.01034 1305:0.038113 1306:0.00652422 1307:0.00195248 1308:0 1309:0 1310:0 1311:0 1312:0.0125032 1313:0.0372878 1314:0 1315:0.0115978 1316:0.0355105 1317:0.0526965 1318:0.0077671 1319:0.0202799 1320:0 1321:0.0299322 1322:0.038524 1323:0.0189031 1324:0.00762292 1325:0.0205017 1326:0.0476511 1327:0.00804948 1328:0.00798689 1329:0.0176147 1330:0.0136446 1331:0.0316989 1332:0.0334168 1333:0.0074051 1334:0.00941851 1335:0.00280179 1336:0.00688201 1337:0.0770411 1338:0.0102416 1339:0.00804863 1340:0.00948915 1341:0.0176628 1342:0.0119695 1343:0.0259622 1344:0.0217164 1345:0.0378224 1346:0.0551074 1347:0.0551712 1348:0.0553224 1349:0.0538357 1350:0.0606615 1351:0.0661415 1352:0.0811743 1353:0.0599768 1354:0.0461675 1355:0.0453676 1356:0.0494476 1357:0.0474286 1358:0.065807 1359:0.0470208 1360:0.0351182 1361:0.0788707 1362:0.0592845 1363:0.049874 1364:0.107148 1365:0.0952895 1366:0.0931826 1367:0.063752 1368:0.0546167 1369:0.0804717 1370:0.0535055 1371:0.144424 1372:0.120974 1373:0.150059 1374:0.141351 1375:0.0895425 1376:0.0693432 1377:0.0720749 1378:0.135411 1379:0.150031 1380:0.159493 1381:0.163428 1382:0.14502 1383:0.103383 1384:0.0707798 1385:0.0652719 1386:0.127633 1387:0.1145 1388:0.0806803 1389:0.0834235 1390:0.106675 1391:0.0577573 1392:0.0816971 1393:0.107634 1394:0.0754848 1395:0.100391 1396:0.0847953 1397:0.0585465 1398:0.0594447 1399:0.0966451 1400:0.0990631 1401:0.131257 1402:0.114542 1403:0.101662 1404:0.0789453 1405:0.0917893 1406:0.0803334 1407:0.10627 1408:0.0563967 1409:0 1410:0 1411:0 1412:0 1413:0 1414:0 1415:0 1416:0 1417:0 1418:0 1419:0 1420:0 1421:0 1422:0 1423:0 1424:0 1425:0 1426:0 1427:0 1428:0 1429:0 1430:0 1431:0 1432:0 1433:0 1434:0 1435:0 1436:0 1437:0 1438:0 1439:0 1440:0 1441:0 1442:0 1443:0 1444:0.00105744 1445:0 1446:0 1447:0 1448:0 1449:0 1450:0 1451:0 1452:0 1453:0 1454:0 1455:0 1456:0.00203061 1457:0.00224121 1458:0 1459:0 1460:0 1461:0 1462:0 1463:0 1464:0.0072123 1465:0.0752468 1466:0.0578104 1467:0.0439973 1468:0.0306994 1469:0.0304309 1470:0.0225796 1471:0.0141376 1472:0.0280617 1473:0.0324165 1474:0.0464746 1475:0.048046 1476:0.0461185 1477:0.0479357 1478:0.0415274 1479:0.0553748 1480:0.0679361 1481:0.0336597 1482:0.0409623 1483:0.0313455 1484:0.06514 1485:0.0374661 1486:0.0553038 1487:0.0548718 1488:0.034888 1489:0.0552649 1490:0.0548849 1491:0.0545742 1492:0.0376039 1493:0.0525643 1494:0.0622232 1495:0.0510937 1496:0.0350524 1497:0.0458775 1498:0.059489 1499:0.0564735 1500:0.0925007 1501:0.0757639 1502:0.106948 1503:0.0662621 1504:0.0377884 1505:0.0412461 1506:0.0721616 1507:0.0853567 1508:0.0720154 1509:0.0991217 1510:0.0859857 1511:0.0961582 1512:0.0567538 1513:0.0460237 1514:0.0771641 1515:0.0650288 1516:0.0441531 1517:0.0502168 1518:0.063966 1519:0.0879654 1520:0.0351654 1521:0.0369555 1522:0.0380912 1523:0.0546205 1524:0.0746637 1525:0.0580741 1526:0.0614021 1527:0.0652497 1528:0.0771876 1529:0.106563 1530:0.104932 1531:0.0798504 1532:0.0746724 1533:0.0674262 1534:0.0727865 1535:0.058204 1536:0.10099 1537:0.0462955 1538:0.0197402 1539:0.0248896 1540:0.0266704 1541:0.0205111 1542:0.0351272 1543:0.0342792 1544:0.0395061 1545:0.0605006 1546:0.00111563 1547:0.00800567 1548:0.0287412 1549:0 1550:0 1551:0.0031521 1552:0.0217658 1553:0.0630502 1554:0.00106437 1555:0.0227435 1556:0.0202249 1557:0.0147419 1558:0 1559:0 1560:0.030181 1561:0.0454593 1562:0.0440421 1563:0.0490885 1564:0.0518056 1565:0.0281968 1566:0.0379404 1567:0 1568:0.0130225 1569:0.0372703 1570:0.0664419 1571:0.018003 1572:0 1573:0 1574:0.0184723 1575:0 1576:0.0130706 1577:0.036242 1578:0 1579:0.00465355 1580:0 1581:0 1582:0 1583:0 1584:0.051298 1585:0.0291127 1586:0 1587:0 1588:0 1589:0 1590:0 1591:0.00863294 1592:0.0266753 1593:0.0564937 1594:0 1595:0 1596:0.00222216 1597:0.00301588 1598:0.00471422 1599:0.0160164 1600:0 1601:0.108589 1602:0.10927 1603:0.111099 1604:0.112359 1605:0.111627 1606:0.114183 1607:0.115646 1608:0.119547 1609:0.105824 1610:0.0901031 1611:0.0816831 1612:0.0872187 1613:0.125295 1614:0.102923 1615:0.0883424 1616:0.101444 1617:0.116118 1618:0.0909268 1619:0.107293 1620:0.111133 1621:0.130487 1622:0.113056 1623:0.110617 1624:0.0922463 1625:0.100957 1626:0.0799835 1627:0.106913 1628:0.11492 1629:0.125281 1630:0.137406 1631:0.146006 1632:0.0968394 1633:0.105025 1634:0.0877354 1635:0.129934 1636:0.0961232 1637:0.0874861 1638:0.0942971 1639:0.121832 1640:0.104089 1641:0.10666 1642:0.0652625 1643:0.0734368 1644:0.117717 1645:0.110549 1646:0.0716717 1647:0.0815684 1648:0.103222 1649:0.144315 1650:0.09063 1651:0.073677 1652:0.0806096 1653:0.0875608 1654:0.095807 1655:0.105072 1656:0.0945573 1657:0.141903 1658:0.133446 1659:0.113197 1660:0.100478 1661:0.0965423 1662:0.0974725 1663:0.0860613 1664:0.113836 1665:0 1666:0 1667:0 1668:0 1669:0 1670:0 1671:0 1672:0.0560403 1673:0 1674:0 1675:0 1676:0 1677:0 1678:0 1679:0 1680:0.013551 1681:0 1682:0 1683:0 1684:0 1685:0 1686:0 1687:0 1688:0.0198625 1689:0 1690:0 1691:0 1692:0 1693:0 1694:0 1695:0 1696:0.0200817 1697:0 1698:0 1699:0 1700:0 1701:0 1702:0 1703:0.0125116 1704:0.0309564 1705:0 1706:0 1707:0 1708:0 1709:0 1710:0 1711:0.0274459 1712:0.0358951 1713:0 1714:0 1715:0 1716:0 1717:0 1718:0 1719:0 1720:0.0467017 1721:0 1722:0 1723:0 1724:0 1725:0 1726:0 1727:0 1728:0.0702403 1729:0.010286 1730:0.0289517 1731:0.0121342 1732:0.002899 1733:0.0118021 1734:0.0188041 1735:0.0207667 1736:0.00836247 1737:0.0173072 1738:0.0215288 1739:0.0193954 1740:0.0271272 1741:0.0249327 1742:0.0363869 1743:0.0218139 1744:0.0284211 1745:0.00942704 1746:0.0149079 1747:0.0242514 1748:0.0558376 1749:0.0431207 1750:0.0435341 1751:0.0418055 1752:0.0136425 1753:0.00158486 1754:0.0197315 1755:0.0783809 1756:0.0519979 1757:0.0576325 1758:0.0505805 1759:0.0517591 1760:0.0111125 1761:0 1762:0.0421176 1763:0.0424622 1764:0.0608314 1765:0.0372817 1766:0.014936 1767:0.037363 1768:0.0179992 1769:0.0169479 1770:0.0444368 1771:0.0124059 1772:0.0448147 1773:0.0424133 1774:0.0397541 1775:0.0149099 1776:0.0204376 1777:0.0148062 1778:0.0408768 1779:0.048866 1780:0.026458 1781:0.0245458 1782:0.0247309 1783:0.0307564 1784:0.0338886 1785:0.00100365 1786:0.0186089 1787:0.024739 1788:0.0182977 1789:0.0260038 1790:0.0181359 1791:0.0397289 1792:0.0466972 1793:0.0583309 1794:0.0654744 1795:0.0591418 1796:0.0651612 1797:0.0667133 1798:0.0727099 1799:0.079122 1800:0.0596617 1801:0.0718039 1802:0.0700904 1803:0.0775279 1804:0.0728544 1805:0.104814 1806:0.100216 1807:0.0715574 1808:0.0646628 1809:0.0729892 1810:0.0798077 1811:0.0730948 1812:0.101372 1813:0.0944542 1814:0.0987711 1815:0.10657 1816:0.071995 1817:0.0825386 1818:0.0796073 1819:0.112826 1820:0.123669 1821:0.118888 1822:0.112731 1823:0.120836 1824:0.0749307 1825:0.0739198 1826:0.122175 1827:0.120828 1828:0.141774 1829:0.118842 1830:0.117282 1831:0.102976 1832:0.0708685 1833:0.0837533 1834:0.117246 1835:0.102853 1836:0.0913871 1837:0.106712 1838:0.114832 1839:0.0849409 1840:0.0742314 1841:0.0811394 1842:0.0863404 1843:0.0960584 1844:0.0719882 1845:0.0803771 1846:0.0721882 1847:0.0839212 1848:0.0880533 1849:0.0728912 1850:0.0945771 1851:0.0898801 1852:0.0809458 1853:0.0838656 1854:0.0838468 1855:0.0928088 1856:0.0557129 1857:0.0628098 1858:0.0655475 1859:0.063896 1860:0.0622982 1861:0.0654312 1862:0.0684675 1863:0.0689591 1864:0.0677177 1865:0.0487088 1866:0.0499024 1867:0.0558971 1868:0.0433784 1869:0.0555965 1870:0.04797 1871:0.0604396 1872:0.0557106 1873:0.070369 1874:0.0516315 1875:0.0434831 1876:0.0550423 1877:0.0633168 1878:0.065712 1879:0.0707278 1880:0.0636286 1881:0.0731071 1882:0.0406804 1883:0.0772539 1884:0.096553 1885:0.0974371 1886:0.0997436 1887:0.103937 1888:0.0647503 1889:0.0523197 1890:0.0646873 1891:0.0937711 1892:0.0857714 1893:0.077033 1894:0.0921721 1895:0.109828 1896:0.0691397 1897:0.0671471 1898:0.0543643 1899:0.0774455 1900:0.0824712 1901:0.060687 1902:0.0490698 1903:0.0885268 1904:0.0733676 1905:0.0813626 1906:0.0677339 1907:0.0642073 1908:0.0810025 1909:0.0662645 1910:0.0601824 1911:0.0598105 1912:0.100899 1913:0.0750856 1914:0.101821 1915:0.0928051 1916:0.0760455 1917:0.0724955 1918:0.0722809 1919:0.0615459 1920:0.0795486 1921:0.0611074 1922:0.109994 1923:0.103488 1924:0.101773 1925:0.108022 1926:0.114913 1927:0.12392 1928:0.126709 1929:0.0861059 1930:0.0856896 1931:0.101571 1932:0.0968009 1933:0.112987 1934:0.0887689 1935:0.107664 1936:0.0958465 1937:0.107055 1938:0.109543 1939:0.0935616 1940:0.0909567 1941:0.100954 1942:0.131354 1943:0.113318 1944:0.10849 1945:0.100353 1946:0.113542 1947:0.155591 1948:0.167916 1949:0.187069 1950:0.158352 1951:0.181943 1952:0.109818 1953:0.0926916 1954:0.144243 1955:0.174611 1956:0.191882 1957:0.193069 1958:0.174144 1959:0.183121 1960:0.101419 1961:0.0890199 1962:0.165047 1963:0.161803 1964:0.146083 1965:0.127723 1966:0.168452 1967:0.13386 1968:0.132863 1969:0.103875 1970:0.113273 1971:0.141528 1972:0.146144 1973:0.108779 1974:0.107949 1975:0.107257 1976:0.15975 1977:0.167991 1978:0.127923 1979:0.12618 1980:0.121527 1981:0.125976 1982:0.123377 1983:0.1247 1984:0.148624 1985:0 1986:0 1987:0 1988:0 1989:0 1990:0 1991:0 1992:0.0125889 1993:0.0102651 1994:0 1995:0.0056697 1996:0.000768853 1997:0.000844564 1998:0 1999:0.0137135 2000:0.030586 2001:0.00907578 2002:0 2003:0.0201434 2004:0.0292652 2005:0.0410884 2006:0.046789 2007:0.0194179 2008:0.0313868 2009:0.0155101 2010:0 2011:0.0239484 2012:0 2013:0.0201991 2014:0.0319023 2015:0.030838 2016:0.0365664 2017:0.0177168 2018:0 2019:0 2020:0 2021:0 2022:0 2023:0.000356862 2024:0.0165218 2025:0.00366304 2026:0 2027:0 2028:0 2029:0 2030:0 2031:0 2032:0.0326548 2033:0.0394053 2034:0.00459666 2035:0 2036:0 2037:0 2038:0 2039:0 2040:0.0148875 2041:0.0092458 2042:0.00785219 2043:0 2044:0 2045:0 2046:0 2047:0 2048:0.0213609 2049:0 2050:0 2051:0 2052:0 2053:0 2054:0 2055:0 2056:0.00475941 2057:0 2058:0 2059:0 2060:0 2061:0 2062:0 2063:0 2064:0.00206359 2065:0 2066:0 2067:0 2068:0 2069:0 2070:0 2071:0 2072:0 2073:0 2074:0 2075:0 2076:0 2077:0 2078:0 2079:0.00239315 2080:0.0071358 2081:0 2082:0 2083:0 2084:0 2085:0 2086:0 2087:0.0145278 2088:0 2089:0 2090:0 2091:0 2092:0 2093:0 2094:0 2095:0.00816698 2096:0 2097:0 2098:0 2099:0 2100:0 2101:0 2102:0 2103:0 2104:0.016014 2105:0 2106:0.00233097 2107:0.000409424 2108:0 2109:0 2110:0 2111:0.00156304 2112:0.012087 2113:0.0718703 2114:0.0802042 2115:0.0679085 2116:0.0714276 2117:0.0761276 2118:0.0785734 2119:0.0744421 2120:0.0719938 2121:0.0698608 2122:0.061095 2123:0.0647539 2124:0.0733132 2125:0.0778619 2126:0.0684303 2127:0.0697065 2128:0.065895 2129:0.0737281 2130:0.0617365 2131:0.0951846 2132:0.105503 2133:0.096509 2134:0.0918833 2135:0.0751844 2136:0.0751083 2137:0.0701854 2138:0.0971483 2139:0.111174 2140:0.0923325 2141:0.0732265 2142:0.0778656 2143:0.0464471 2144:0.0600365 2145:0.085599 2146:0.101391 2147:0.0818404 2148:0.0629545 2149:0.0685977 2150:0.0783655 2151:0.0654135 2152:0.064151 2153:0.0926735 2154:0.106843 2155:0.074962 2156:0.0542888 2157:0.0503026 2158:0.0548191 2159:0.0997052 2160:0.0806527 2161:0.0926358 2162:0.0726114 2163:0.0745725 2164:0.0811895 2165:0.0710535 2166:0.0717727 2167:0.0866926 2168:0.0395067 2169:0.0824693 2170:0.0737687 2171:0.0848865 2172:0.0739425 2173:0.0651024 2174:0.0566517 2175:0.051663 2176:0.061232 2177:0.0731948 2178:0.0926729 2179:0.105705 2180:0.101535 2181:0.108115 2182:0.109064 2183:0.114812 2184:0.0994066 2185:0.0860055 2186:0.0818947 2187:0.0872955 2188:0.0798986 2189:0.112671 2190:0.0948608 2191:0.0895147 2192:0.0874396 2193:0.100285 2194:0.096715 2195:0.122286 2196:0.141495 2197:0.152108 2198:0.148682 2199:0.131639 2200:0.0945344 2201:0.0894399 2202:0.120573 2203:0.146751 2204:0.190293 2205:0.200615 2206:0.197774 2207:0.168732 2208:0.0831461 2209:0.100692 2210:0.150387 2211:0.179327 2212:0.16731 2213:0.13726 2214:0.165787 2215:0.138484 2216:0.0875994 2217:0.103208 2218:0.109473 2219:0.150347 2220:0.154237 2221:0.122612 2222:0.0979994 2223:0.110015 2224:0.0952284 2225:0.118519 2226:0.109903 2227:0.109019 2228:0.11995 2229:0.110116 2230:0.112962 2231:0.107399 2232:0.105602 2233:0.127574 2234:0.135015 2235:0.124084 2236:0.107918 2237:0.108258 2238:0.093816 2239:0.0805581 2240:0.129982 2241:0 2242:0 2243:0 2244:0 2245:0 2246:0 2247:0 2248:0 2249:0 2250:0 2251:0 2252:0 2253:0 2254:0 2255:0 2256:0 2257:0 2258:0 2259:0 2260:0 2261:0 2262:0 2263:0 2264:0 2265:0 2266:0 2267:0 2268:0 2269:0 2270:0 2271:0 2272:0 2273:0 2274:0 2275:0 2276:0 2277:0 2278:0 2279:0 2280:0 2281:0 2282:0 2283:0 2284:0 2285:0 2286:0 2287:0 2288:0 2289:0 2290:0 2291:0 2292:0 2293:0 2294:0 2295:0 2296:0 2297:0 2298:0 2299:0 2300:0 2301:0 2302:0 2303:0 2304:0 2305:0.0420568 2306:0.0273054 2307:0 2308:0 2309:0 2310:0 2311:0 2312:0 2313:0.00962647 2314:0 2315:0 2316:0 2317:0 2318:0 2319:0 2320:0 2321:0.0104582 2322:0 2323:0 2324:0 2325:0 2326:0 2327:0 2328:0 2329:0.00821966 2330:0 2331:0 2332:0 2333:0 2334:0 2335:0 2336:0 2337:0.00855545 2338:0 2339:0 2340:0 2341:0 2342:0 2343:0 2344:0 2345:0.00611098 2346:0 2347:0 2348:0 2349:0 2350:0 2351:0 2352:0 2353:0.0192956 2354:0 2355:0 2356:0 2357:0 2358:0 2359:0 2360:0 2361:0.0534953 2362:0 2363:0 2364:0 2365:0 2366:0 2367:0 2368:0 2369:0 2370:0 2371:0 2372:0 2373:0 2374:0 2375:0 2376:0 2377:0 2378:0 2379:0 2380:0 2381:0 2382:0 2383:0 2384:0 2385:0 2386:0 2387:0 2388:0 2389:0 2390:0 2391:0 2392:0 2393:0 2394:0 2395:0 2396:0 2397:0 2398:0 2399:0 2400:0 2401:0 2402:0 2403:0 2404:0 2405:0 2406:0 2407:0 2408:0 2409:0 2410:0 2411:0 2412:0 2413:0 2414:0 2415:0 2416:0 2417:0 2418:0 2419:0 2420:0 2421:0 2422:0 2423:0 2424:0 2425:0 2426:0 2427:0 2428:0 2429:0 2430:0 2431:0 2432:0 2433:0.079588 2434:0.0658385 2435:0.068599 2436:0.0680574 2437:0.0654337 2438:0.0654573 2439:0.0653224 2440:0.0618472 2441:0.0830664 2442:0.0573721 2443:0.0622797 2444:0.0714166 2445:0.0806873 2446:0.0638214 2447:0.0600322 2448:0.0670645 2449:0.0860411 2450:0.0500719 2451:0.0703972 2452:0.0739231 2453:0.0289072 2454:0.06155 2455:0.0679668 2456:0.0579695 2457:0.0937525 2458:0.0741207 2459:0.0852259 2460:0.0569051 2461:0.0117032 2462:0.00799653 2463:0.0354376 2464:0.0556498 2465:0.0971393 2466:0.073307 2467:0.0430416 2468:0.0703844 2469:0.0808002 2470:0.070719 2471:0.0733056 2472:0.0704425 2473:0.0922923 2474:0.0910956 2475:0.0551685 2476:0.0743518 2477:0.0608824 2478:0.0860868 2479:0.0861267 2480:0.0956605 2481:0.105929 2482:0.059771 2483:0.0778013 2484:0.0623269 2485:0.0616147 2486:0.0717194 2487:0.0679641 2488:0.110137 2489:0.124665 2490:0.0632925 2491:0.0597516 2492:0.0597188 2493:0.0632752 2494:0.0723011 2495:0.102134 2496:0.0978432 2497:0.0304019 2498:0.00786468 2499:0.0150721 2500:0.0202123 2501:0.0147898 2502:0.0168457 2503:0.0226728 2504:0.0201559 2505:0.0326257 2506:0.00198715 2507:0 2508:0.00841814 2509:0.00361155 2510:0.0222882 2511:0.0132885 2512:0 2513:0.0391176 2514:0 2515:0 2516:0.0104328 2517:0 2518:0.0017366 2519:0.00416389 2520:0.002675 2521:0.0394301 2522:0.00874923 2523:0.00978874 2524:0.0282987 2525:0.0203309 2526:0.0291735 2527:0 2528:0.00271488 2529:0.0353607 2530:0.0374808 2531:0.0301916 2532:0.0451909 2533:0.0259943 2534:0.0261555 2535:0.00214766 2536:0 2537:0.0305798 2538:0.0503726 2539:0.00127053 2540:0.00291532 2541:0.0159477 2542:0.0246454 2543:0 2544:0.00141103 2545:0.0432784 2546:0.0186147 2547:0.0206483 2548:0 2549:0 2550:0.000781041 2551:0.0110394 2552:0.0247021 2553:0.105991 2554:0.104036 2555:0.0859184 2556:0.0792022 2557:0.078122 2558:0.0693659 2559:0.0756389 2560:0.0741472 2561:0 2562:0 2563:0 2564:0 2565:0 2566:0 2567:0 2568:0 2569:0 2570:0 2571:0 2572:0 2573:0 2574:0 2575:0 2576:0 2577:0 2578:0 2579:0 2580:0 2581:0 2582:0 2583:0 2584:0 2585:0 2586:0 2587:0 2588:0 2589:0 2590:0 2591:0 2592:0 2593:0 2594:0 2595:0 2596:0 2597:0.0165037 2598:0.0154716 2599:0.0171826 2600:0 2601:0 2602:0 2603:0 2604:0 2605:0 2606:0.00605902 2607:0.0114826 2608:0.00348536 2609:0 2610:0 2611:0 2612:0 2613:0 2614:0 2615:0 2616:0.0144955 2617:0.00419481 2618:0 2619:0 2620:0 2621:0 2622:0 2623:0 2624:0.0123802 2625:0.0112084 2626:0 2627:0 2628:0 2629:0 2630:0 2631:0 2632:0 2633:0 2634:0 2635:0 2636:0 2637:0 2638:0 2639:0 2640:0 2641:0 2642:0 2643:0 2644:0 2645:0 2646:0 2647:0 2648:0 2649:0 2650:0 2651:0 2652:0 2653:0 2654:0 2655:0 2656:0 2657:0 2658:0 2659:0 2660:0 2661:0 2662:0 2663:0 2664:0 2665:0 2666:0 2667:0 2668:0 2669:0 2670:0 2671:0 2672:0 2673:0 2674:0 2675:0 2676:0 2677:0 2678:0 2679:0 2680:0 2681:0 2682:0 2683:0 2684:0 2685:0 2686:0 2687:0 2688:0 2689:0.0116579 2690:0.0428884 2691:0.0509132 2692:0.044992 2693:0.0424941 2694:0.0532352 2695:0.0545271 2696:0.0528127 2697:0.0186922 2698:0.0216252 2699:0.0276466 2700:0.052728 2701:0.0421102 2702:0.0233829 2703:0.0438611 2704:0.0349136 2705:0.0307352 2706:0.0280042 2707:0.0562689 2708:0.0133855 2709:0.0336814 2710:0.0314504 2711:0.021452 2712:0.0261952 2713:0.0325465 2714:0.04641 2715:0.0367018 2716:0.00839396 2717:0.0490994 2718:0.049218 2719:0.0759009 2720:0.0164844 2721:0.0299339 2722:0.0229968 2723:0.084543 2724:0.108462 2725:0.101521 2726:0.0745691 2727:0.0878857 2728:0.0202247 2729:0.0371041 2730:0.0580809 2731:0.0879285 2732:0.0686924 2733:0.0509768 2734:0.0736302 2735:0.0828081 2736:0.0109091 2737:0.0325557 2738:0.0351167 2739:0.0590785 2740:0.0627624 2741:0.0518061 2742:0.0299569 2743:0.0320071 2744:0.0478925 2745:0.0594853 2746:0.0641407 2747:0.0560581 2748:0.0535918 2749:0.0599328 2750:0.0664084 2751:0.0738004 2752:0.0757352 2753:0.063366 2754:0.057683 2755:0.0573433 2756:0.0582689 2757:0.0584404 2758:0.0566527 2759:0.0564495 2760:0.0583241 2761:0.0522982 2762:0.0660034 2763:0.0634191 2764:0.084384 2765:0.0615659 2766:0.0860209 2767:0.0784528 2768:0.0680071 2769:0.0770042 2770:0.0718832 2771:0.0728264 2772:0.0499876 2773:0.0614588 2774:0.062069 2775:0.0709234 2776:0.0704535 2777:0.0683695 2778:0.0876263 2779:0.0691297 2780:0.0808408 2781:0.0890011 2782:0.105608 2783:0.0835389 2784:0.0730797 2785:0.0923152 2786:0.0787875 2787:0.073829 2788:0.0909775 2789:0.101482 2790:0.0779674 2791:0.0823812 2792:0.0827048 2793:0.0735488 2794:0.0750397 2795:0.0760858 2796:0.0678741 2797:0.0831164 2798:0.0813172 2799:0.0792416 2800:0.0838572 2801:0.085076 2802:0.0794526 2803:0.0849079 2804:0.0877946 2805:0.0838782 2806:0.0683324 2807:0.0818263 2808:0.0631022 2809:0.0638112 2810:0.0649062 2811:0.0594954 2812:0.0638832 2813:0.0646777 2814:0.0719937 2815:0.074916 2816:0.0647161 2817:0.00293176 2818:0 2819:0 2820:0 2821:0 2822:0 2823:0 2824:0 2825:0 2826:0 2827:0 2828:0 2829:0 2830:0 2831:0 2832:0 2833:0 2834:0 2835:0 2836:0 2837:0 2838:0 2839:0 2840:0 2841:0 2842:0 2843:0 2844:0 2845:0 2846:0 2847:0 2848:0 2849:0 2850:0 2851:0 2852:0 2853:0 2854:0 2855:0 2856:0 2857:0 2858:0 2859:0 2860:0 2861:0 2862:0 2863:0 2864:0 2865:0 2866:0 2867:0 2868:0 2869:0 2870:0 2871:0 2872:0 2873:0 2874:0 2875:0 2876:0 2877:0 2878:0 2879:0 2880:0 2881:0 2882:0 2883:0 2884:0 2885:0 2886:0 2887:0 2888:0 2889:0 2890:0 2891:0 2892:0 2893:0 2894:0 2895:0 2896:0 2897:0 2898:0 2899:0.00400232 2900:0 2901:0 2902:0 2903:0 2904:0 2905:0 2906:0 2907:0 2908:0 2909:0 2910:0 2911:0 2912:0 2913:0 2914:0 2915:0 2916:0 2917:0 2918:0 2919:0 2920:0 2921:0 2922:0 2923:0 2924:0 2925:0 2926:0 2927:0 2928:0 2929:0 2930:0 2931:0 2932:0 2933:0 2934:0 2935:0 2936:0 2937:0 2938:0 2939:0 2940:0 2941:0 2942:0 2943:0 2944:0 2945:0.00329913 2946:0.0196745 2947:0.0251157 2948:0.0172619 2949:0.02315 2950:0.0167119 2951:0.0190316 2952:0.0405004 2953:0 2954:0 2955:0.0135546 2956:0.0150927 2957:0.000271378 2958:0.012551 2959:0.00398236 2960:0.0316778 2961:0 2962:0.0117289 2963:0.00681197 2964:0.0155114 2965:0 2966:0 2967:0.00389127 2968:0.0335755 2969:0 2970:0.0143567 2971:0 2972:0 2973:0 2974:0 2975:0.0367363 2976:0.0422964 2977:0 2978:0 2979:0.0153984 2980:0.0372968 2981:0.0444375 2982:0.00640309 2983:0.071128 2984:0.0525663 2985:0 2986:0.00527206 2987:0 2988:0.0147606 2989:0.00720036 2990:0.0188754 2991:0.045511 2992:0.042213 2993:0.01512 2994:0.0133418 2995:0.0159764 2996:0.0361042 2997:0.0278229 2998:0.0133726 2999:0 3000:0.0446697 3001:0.0162133 3002:0.0624076 3003:0.0546764 3004:0.0335734 3005:0.0331938 3006:0.0321976 3007:0.0232023 3008:0.057814 3009:0 3010:0 3011:0 3012:0 3013:0 3014:0 3015:0 3016:0 3017:0 3018:0 3019:0 3020:0 3021:0 3022:0 3023:0 3024:0 3025:0 3026:0 3027:0 3028:0 3029:0 3030:0 3031:0 3032:0 3033:0 3034:0.00652555 3035:0.00567931 3036:0.00490142 3037:0.0117502 3038:0.0202452 3039:0 3040:0 3041:0 3042:0.0219019 3043:0.027125 3044:0 3045:0 3046:0 3047:0.0118052 3048:0 3049:0 3050:0 3051:0 3052:0 3053:0 3054:0 3055:0 3056:0 3057:0 3058:0 3059:0 3060:0 3061:0 3062:0 3063:0 3064:0.00907638 3065:0 3066:0 3067:0 3068:0 3069:0 3070:0 3071:0 3072:0.0207831 3073:0 3074:0 3075:0 3076:0 3077:0 3078:0 3079:0 3080:0 3081:0 3082:0 3083:0 3084:0 3085:0.00540347 3086:0 3087:0 3088:0.0132958 3089:0 3090:0 3091:0 3092:0.0193985 3093:0.00958243 3094:0 3095:0 3096:0.0122306 3097:0 3098:0.0126938 3099:0.021174 3100:0.0230002 3101:0.00192204 3102:0.0305896 3103:0 3104:0.0108141 3105:0.00451245 3106:0.0175409 3107:0 3108:0 3109:0 3110:0 3111:0 3112:0.0182721 3113:0 3114:0 3115:0 3116:0 3117:0 3118:0 3119:0 3120:0.0240141 3121:0 3122:0 3123:0 3124:0 3125:0 3126:0 3127:0 3128:0.0266221 3129:0.00588667 3130:0.00599013 3131:0 3132:0 3133:0 3134:0.00242612 3135:0.0148445 3136:0.0253391 3137:0.0230291 3138:0.0511228 3139:0.0535016 3140:0.0530857 3141:0.0514895 3142:0.0547533 3143:0.0640456 3144:0.0622789 3145:0.0459857 3146:0.0370442 3147:0.0459663 3148:0.0155999 3149:0.0363624 3150:0.0458582 3151:0.0544495 3152:0.0212063 3153:0.0613856 3154:0.0552141 3155:0.0361861 3156:0.0545643 3157:0.0632927 3158:0.0513951 3159:0.0579048 3160:0.0462026 3161:0.0657064 3162:0.0365345 3163:0.0741409 3164:0.11372 3165:0.142023 3166:0.119785 3167:0.0972223 3168:0.0547687 3169:0.0590156 3170:0.0608625 3171:0.125891 3172:0.120043 3173:0.103425 3174:0.126499 3175:0.0865937 3176:0.0408371 3177:0.0597307 3178:0.0778503 3179:0.0891259 3180:0.101336 3181:0.0600298 3182:0.0692798 3183:0.0478784 3184:0.0482215 3185:0.0591288 3186:0.0563315 3187:0.0615175 3188:0.0748536 3189:0.0570454 3190:0.0464192 3191:0.0502136 3192:0.0849266 3193:0.110903 3194:0.0696456 3195:0.06432 3196:0.06737 3197:0.0648027 3198:0.0556013 3199:0.068087 3200:0.067611 3201:0 3202:0 3203:0 3204:0 3205:0 3206:0 3207:0 3208:0 3209:0 3210:0 3211:0 3212:0 3213:0 3214:0 3215:0 3216:0 3217:0 3218:0 3219:0 3220:0 3221:0 3222:0 3223:0 3224:0 3225:0 3226:0 3227:0 3228:0 3229:0 3230:0 3231:0 3232:0 3233:0 3234:0 3235:0 3236:0 3237:0 3238:0 3239:0 3240:0 3241:0 3242:0 3243:0 3244:0 3245:0 3246:0 3247:0 3248:0 3249:0 3250:0 3251:0 3252:0 3253:0 3254:0 3255:0 3256:0 3257:0 3258:0 3259:0 3260:0 3261:0 3262:0 3263:0 3264:0 3265:0.000392364 3266:0 3267:0 3268:0 3269:0 3270:0 3271:0 3272:0 3273:0 3274:0 3275:0 3276:0 3277:0 3278:0 3279:0 3280:0 3281:0 3282:0 3283:0 3284:0 3285:0 3286:0 3287:0 3288:0 3289:0 3290:0 3291:0 3292:0 3293:0 3294:0 3295:0 3296:0 3297:0 3298:0 3299:0 3300:0 3301:0 3302:0 3303:0 3304:0 3305:0 3306:0 3307:0 3308:0 3309:0 3310:0 3311:0 3312:0 3313:0 3314:0 3315:0 3316:0 3317:0 3318:0 3319:0 3320:0 3321:0 3322:0 3323:0 3324:0 3325:0 3326:0 3327:0 3328:0 3329:0.0167393 3330:0.0324508 3331:0.0364327 3332:0.0380729 3333:0.0416699 3334:0.0494779 3335:0.0419008 3336:0.0595193 3337:0.0215849 3338:0.0462192 3339:0.0377858 3340:0.0464049 3341:0.0438928 3342:0.044841 3343:0.0563025 3344:0.0422146 3345:0.0381249 3346:0.0464612 3347:0.0709938 3348:0.0796506 3349:0.0953147 3350:0.0650678 3351:0.0515101 3352:0.0544201 3353:0.0303081 3354:0.0626992 3355:0.0760742 3356:0.102578 3357:0.0986003 3358:0.101228 3359:0.0808819 3360:0.0540235 3361:0.0424652 3362:0.086709 3363:0.0663265 3364:0.0521298 3365:0.0443511 3366:0.0603512 3367:0.0776631 3368:0.0680995 3369:0.0447246 3370:0.0441916 3371:0.0763056 3372:0.0630468 3373:0.0541367 3374:0.0262637 3375:0.0762444 3376:0.06143 3377:0.0690194 3378:0.0617159 3379:0.052963 3380:0.0685558 3381:0.0578527 3382:0.0549555 3383:0.0603629 3384:0.0737201 3385:0.101827 3386:0.090109 3387:0.0556909 3388:0.0541979 3389:0.050744 3390:0.0508566 3391:0.0443672 3392:0.0620495 3393:0.0503825 3394:0.0174456 3395:0.0249865 3396:0.0142307 3397:0.0115013 3398:0.0121519 3399:0.0148526 3400:0.0421688 3401:0.019256 3402:0.0154749 3403:0.0196538 3404:0.0195782 3405:0.0332189 3406:0.0234393 3407:0.0268806 3408:0.0362504 3409:0.0168524 3410:0.0255156 3411:0.0436637 3412:0.0432048 3413:0.0562472 3414:0.0388814 3415:0.0199752 3416:0.0288222 3417:0.0199199 3418:0.0494524 3419:0.0275151 3420:0.00753976 3421:0.00900419 3422:0.00917422 3423:0.0415467 3424:0.0366681 3425:0.0195699 3426:0.0534117 3427:0.014734 3428:0.0230166 3429:0.0443254 3430:0.0144529 3431:0.0552642 3432:0.0352854 3433:0.0243944 3434:0.00815407 3435:0.0342622 3436:0.04542 3437:0.0245669 3438:0.0206081 3439:0.0353732 3440:0.0227801 3441:0.0340691 3442:0.0240703 3443:0.0195898 3444:0.0376478 3445:0.0155825 3446:0.0154366 3447:0.0149154 3448:0.0247194 3449:0.0310695 3450:0.00130918 3451:0.0159215 3452:0.0185369 3453:0.00734273 3454:0.0106181 3455:0.0249807 3456:0.0438111 3457:0 3458:0 3459:0 3460:0 3461:0 3462:0 3463:0 3464:0 3465:0 3466:0 3467:0 3468:0 3469:0 3470:0 3471:0 3472:0 3473:0 3474:0 3475:0 3476:0 3477:0 3478:0 3479:0 3480:0 3481:0 3482:0 3483:0 3484:0 3485:0 3486:0 3487:0 3488:0 3489:0 3490:0 3491:0 3492:0 3493:0 3494:0 3495:0 3496:0 3497:0 3498:0 3499:0 3500:0 3501:0 3502:0 3503:0 3504:0 3505:0 3506:0 3507:0 3508:0 3509:0 3510:0 3511:0 3512:0 3513:0 3514:0 3515:0 3516:0 3517:0 3518:0 3519:0 3520:0 3521:0 3522:0 3523:0 3524:0 3525:0 3526:0 3527:0 3528:0 3529:0 3530:0 3531:0 3532:0 3533:0 3534:0 3535:0 3536:0 3537:0 3538:0 3539:0 3540:0 3541:0 3542:0 3543:0 3544:0 3545:0 3546:0 3547:0 3548:0 3549:0 3550:0 3551:0 3552:0 3553:0 3554:0 3555:0 3556:0 3557:0 3558:0 3559:0 3560:0 3561:0 3562:0 3563:0 3564:0 3565:0 3566:0 3567:0 3568:0 3569:0 3570:0 3571:0 3572:0 3573:0 3574:0 3575:0 3576:0 3577:0.00853517 3578:0.00721091 3579:0 3580:0 3581:0 3582:0 3583:0 3584:0 3585:0.0187783 3586:0.0110306 3587:0.00354943 3588:0.0117179 3589:0.0143825 3590:0.0109091 3591:0.00250743 3592:0.0291406 3593:0.00760942 3594:0 3595:0 3596:0 3597:0.0199305 3598:0 3599:0.0018531 3600:0 3601:0.0120576 3602:0 3603:0.0057381 3604:0 3605:0 3606:0 3607:0 3608:0 3609:0.0281907 3610:0.00940919 3611:0 3612:0 3613:0 3614:0 3615:0 3616:0 3617:0.0304567 3618:0 3619:0 3620:0 3621:0.00959539 3622:0.00760641 3623:0.0363299 3624:0 3625:0.00835579 3626:0 3627:0.0165632 3628:0.00275177 3629:0 3630:0.00446936 3631:0.0233979 3632:0 3633:0.0162725 3634:0 3635:0 3636:0.0060699 3637:0 3638:0 3639:0 3640:0.031328 3641:0.0160941 3642:0.0168871 3643:0.0144283 3644:0.00848009 3645:0.00834802 3646:0.00382887 3647:0.00528672 3648:0.0671961 3649:0.0443885 3650:0.0176117 3651:0.00764284 3652:0.00332336 3653:0.00471144 3654:0.00239446 3655:0.00183295 3656:0.0056359 3657:0.0297383 3658:0 3659:0 3660:0.00280463 3661:0 3662:0 3663:0 3664:0 3665:0.0241347 3666:0 3667:0 3668:0 3669:0 3670:0 3671:0 3672:0 3673:0.0235171 3674:0 3675:0 3676:0 3677:0 3678:0 3679:0 3680:0 3681:0.0327408 3682:0 3683:0 3684:0 3685:0 3686:0 3687:0 3688:0 3689:0.0303276 3690:0 3691:0 3692:0 3693:0 3694:0 3695:0 3696:0 3697:0.0209255 3698:0 3699:0 3700:0 3701:0 3702:0 3703:0 3704:0 3705:0.0968359 3706:0.0324164 3707:0.0304278 3708:0.0325979 3709:0.0284683 3710:0.0253961 3711:0.0563129 3712:0.0573737 3713:0 3714:0 3715:0 3716:0 3717:0 3718:0 3719:0 3720:0 3721:0 3722:0 3723:0 3724:0 3725:0 3726:0 3727:0 3728:0 3729:0 3730:0 3731:0 3732:0 3733:0 3734:0 3735:0 3736:0 3737:0 3738:0 3739:0.00657863 3740:0 3741:0 3742:0 3743:0.00604969 3744:0 3745:0 3746:0 3747:0.00230172 3748:0 3749:0 3750:0 3751:0 3752:0 3753:0 3754:0 3755:0 3756:0 3757:0 3758:0 3759:0 3760:0.0023344 3761:0 3762:0 3763:0 3764:0 3765:0 3766:0 3767:0 3768:0.015985 3769:0.00510845 3770:0 3771:0 3772:0 3773:0 3774:0 3775:0 3776:0.00530233 3777:0.00562815 3778:0.020505 3779:0.029857 3780:0.0283486 3781:0.0223776 3782:0.0307343 3783:0.0357847 3784:0.0404568 3785:0 3786:0 3787:0 3788:0 3789:0.0192098 3790:0 3791:0 3792:0 3793:0 3794:0 3795:0 3796:0.0384158 3797:0.0125694 3798:0.00672343 3799:0 3800:0 3801:0 3802:0.00427123 3803:0.0461709 3804:0.0672417 3805:0.0564102 3806:0.01989 3807:0 3808:0 3809:0 3810:0.08871 3811:0.0696427 3812:0.0344429 3813:0.0016063 3814:0.0253246 3815:0 3816:0 3817:0 3818:0.0221865 3819:0.00585688 3820:0 3821:0 3822:0 3823:0 3824:0 3825:0 3826:0 3827:0 3828:0 3829:0 3830:0 3831:0 3832:0.0256269 3833:0 3834:0 3835:0 3836:0 3837:0 3838:0 3839:0 3840:0 3841:0.013401 3842:0 3843:0 3844:0 3845:0.00137312 3846:0.00413663 3847:0 3848:0.00161415 3849:0 3850:0 3851:0 3852:0.0135232 3853:0.0105447 3854:0.0146549 3855:0 3856:0.0140202 3857:0 3858:0 3859:0.00614768 3860:0.025737 3861:0 3862:0.023454 3863:0 3864:0 3865:0 3866:0.0061338 3867:0 3868:0 3869:0 3870:0 3871:0 3872:0 3873:0 3874:0 3875:0 3876:0 3877:0 3878:0 3879:0.00627566 3880:0.0138688 3881:0 3882:0 3883:0 3884:0 3885:0 3886:0.00147884 3887:0 3888:0 3889:0 3890:0 3891:0 3892:0 3893:0 3894:0 3895:0 3896:0 3897:0 3898:0 3899:0 3900:0 3901:0 3902:0 3903:0.0101125 3904:0.00105232 3905:0.168578 3906:0.178073 3907:0.174013 3908:0.167243 3909:0.171356 3910:0.186377 3911:0.192117 3912:0.181511 3913:0.191402 3914:0.155335 3915:0.155331 3916:0.16306 3917:0.193354 3918:0.185116 3919:0.158433 3920:0.14127 3921:0.212239 3922:0.176449 3923:0.186172 3924:0.203505 3925:0.216032 3926:0.195578 3927:0.177571 3928:0.152057 3929:0.190417 3930:0.190891 3931:0.243042 3932:0.298952 3933:0.284587 3934:0.259736 3935:0.200065 3936:0.146272 3937:0.183283 3938:0.251108 3939:0.278082 3940:0.275608 3941:0.270375 3942:0.283089 3943:0.227637 3944:0.147543 3945:0.210704 3946:0.246931 3947:0.246717 3948:0.231048 3949:0.217103 3950:0.229094 3951:0.202497 3952:0.203596 3953:0.218294 3954:0.19475 3955:0.219712 3956:0.223111 3957:0.182242 3958:0.176344 3959:0.199037 3960:0.194914 3961:0.240651 3962:0.215276 3963:0.202204 3964:0.194717 3965:0.18491 3966:0.179226 3967:0.209176 3968:0.210922 3969:0.0606451 3970:0.0406041 3971:0.0449037 3972:0.0398755 3973:0.0467914 3974:0.0584258 3975:0.0435142 3976:0.0272565 3977:0.0692212 3978:0.0631327 3979:0.0496758 3980:0.0978543 3981:0.0774338 3982:0.0724102 3983:0.0695406 3984:0.0642284 3985:0.0816382 3986:0.0650695 3987:0.0903341 3988:0.0806419 3989:0.085628 3990:0.0681935 3991:0.0620752 3992:0.063378 3993:0.0800156 3994:0.131377 3995:0.131031 3996:0.132964 3997:0.0959303 3998:0.13419 3999:0.0544074 4000:0.0687641 4001:0.0838428 4002:0.135644 4003:0.0835316 4004:0.0686659 4005:0.104414 4006:0.135915 4007:0.0722677 4008:0.0883665 4009:0.0821628 4010:0.0655575 4011:0.110242 4012:0.0838056 4013:0.0665205 4014:0.065797 4015:0.104435 4016:0.0985837 4017:0.078026 4018:0.0799506 4019:0.0766999 4020:0.0802304 4021:0.0840508 4022:0.0730059 4023:0.0828329 4024:0.0586723 4025:0.0813701 4026:0.0584612 4027:0.0572089 4028:0.0543254 4029:0.0532334 4030:0.0514699 4031:0.0623431 4032:0.0529149 4033:0 4034:0.00299235 4035:0.0107043 4036:0.00958463 4037:0.00656017 4038:0.00466089 4039:0.0161784 4040:0.0586041 4041:0 4042:0 4043:0 4044:0 4045:0.00706518 4046:0.0171662 4047:0 4048:0.0508103 4049:0 4050:0.00719092 4051:0.00195518 4052:0 4053:0.0112369 4054:0.0120243 4055:0.000776867 4056:0.0445762 4057:0 4058:0 4059:0 4060:0 4061:0.00781739 4062:0.0232935 4063:0.0400417 4064:0.0460967 4065:0.000764474 4066:0.00317112 4067:0 4068:0.0247795 4069:0.0268382 4070:0.0385201 4071:0.049946 4072:0.0496529 4073:0.0111356 4074:0.00355818 4075:0.0141313 4076:0.0124408 4077:0.0149128 4078:0.0022485 4079:0.0224574 4080:0.0537438 4081:0.0300154 4082:0.00432742 4083:0 4084:0.0185069 4085:0.00968152 4086:0 4087:0 4088:0.0468639 4089:0.0297113 4090:0.0402052 4091:0.0408188 4092:0.0336636 4093:0.0284383 4094:0.0282221 4095:0.0235911 4096:0.0775888 4097:0.0381667 4098:0.00933756 4099:0.0174462 4100:0.0203838 4101:0.0193545 4102:0.0169316 4103:0.0202188 4104:0.0155921 4105:0.0385247 4106:0 4107:0.0102212 4108:0.0120618 4109:0.014462 4110:0.0200129 4111:0.0132759 4112:0.00114758 4113:0.0410323 4114:0 4115:0 4116:0.00356262 4117:0 4118:0.00573556 4119:0 4120:0 4121:0.0369996 4122:0.00751899 4123:0 4124:0.00838377 4125:0 4126:0 4127:0 4128:0 4129:0.0391129 4130:0.0329534 4131:0.000575498 4132:0.0360006 4133:0.0281168 4134:0.0365738 4135:0.0187831 4136:0 4137:0.0378582 4138:0.0341223 4139:0.0303562 4140:0.00523612 4141:0.0230502 4142:0.0433906 4143:0.0197117 4144:0.000526152 4145:0.0419457 4146:0.0114163 4147:0.0215919 4148:0.0222177 4149:0.0193497 4150:0.01491 4151:0.0101653 4152:0.0364334 4153:0.0990387 4154:0.0939989 4155:0.0747889 4156:0.0702658 4157:0.0711696 4158:0.071978 4159:0.0948966 4160:0.0662773 4161:0.000739512 4162:0 4163:0 4164:0 4165:0 4166:0 4167:0 4168:0 4169:0 4170:0 4171:0 4172:0 4173:0 4174:0 4175:0 4176:0 4177:0 4178:0 4179:0 4180:0 4181:0 4182:0 4183:0 4184:0 4185:0 4186:0 4187:0 4188:0 4189:0 4190:0 4191:0 4192:0 4193:0 4194:0 4195:0 4196:0 4197:0 4198:0 4199:0 4200:0 4201:0 4202:0 4203:0 4204:0 4205:0 4206:0 4207:0 4208:0 4209:0 4210:0 4211:0 4212:0 4213:0 4214:0 4215:0 4216:0 4217:0 4218:0 4219:0 4220:0 4221:0 4222:0 4223:0 4224:0 4225:0.0497113 4226:0.0526731 4227:0.0595293 4228:0.0596076 4229:0.0549871 4230:0.0576007 4231:0.0574371 4232:0.111061 4233:0.0749511 4234:0.0708985 4235:0.0660222 4236:0.074727 4237:0.113245 4238:0.0920474 4239:0.0831989 4240:0.131262 4241:0.101224 4242:0.0716546 4243:0.108999 4244:0.13013 4245:0.138945 4246:0.157826 4247:0.110279 4248:0.122902 4249:0.101715 4250:0.125764 4251:0.109411 4252:0.128222 4253:0.1445 4254:0.164599 4255:0.149489 4256:0.130925 4257:0.0970945 4258:0.159661 4259:0.136704 4260:0.144843 4261:0.114699 4262:0.152884 4263:0.146128 4264:0.148263 4265:0.0956841 4266:0.0795488 4267:0.128985 4268:0.144411 4269:0.100921 4270:0.0819684 4271:0.126365 4272:0.14021 4273:0.134245 4274:0.0941036 4275:0.0777829 4276:0.0882505 4277:0.10296 4278:0.0876543 4279:0.09411 4280:0.143904 4281:0.116133 4282:0.0881379 4283:0.0736258 4284:0.0678175 4285:0.0613058 4286:0.0651473 4287:0.08105 4288:0.145319 4289:0.0383675 4290:0.0288185 4291:0.0202615 4292:0.0179759 4293:0.0168887 4294:0.0200331 4295:0.0200582 4296:0.02792 4297:0.046852 4298:0 4299:0 4300:0.0110069 4301:0.00208057 4302:0.00943023 4303:0 4304:0 4305:0.0712858 4306:0.00400979 4307:0 4308:0.0273038 4309:0.0393264 4310:0.0146193 4311:0.00463467 4312:0 4313:0.0721619 4314:0.00616448 4315:0.0431619 4316:0.0385577 4317:0.0344368 4318:0.0404054 4319:0.00294321 4320:0 4321:0.0707408 4322:0.0262594 4323:0.0628472 4324:0.0576051 4325:0.0406628 4326:0.0298476 4327:0.00364524 4328:0.0077828 4329:0.0638775 4330:0.0426529 4331:0.0385029 4332:0.017554 4333:0 4334:0.0116855 4335:0 4336:0.0190629 4337:0.0763195 4338:0.0032024 4339:0.00376787 4340:0.00242788 4341:0 4342:0.00195163 4343:0.00849066 4344:0.0231249 4345:0.106138 4346:0.00156077 4347:0.00942728 4348:0.0123783 4349:0.0110137 4350:0.00478669 4351:0.00133854 4352:0.0297278 4353:0.00473262 4354:0.0120547 4355:0.0152738 4356:0.00476003 4357:0.0164956 4358:0.0159442 4359:0.00508529 4360:0 4361:0.0250747 4362:0.0266614 4363:0.0138241 4364:0.0305115 4365:0.0290647 4366:0.0177587 4367:0.0157163 4368:0.0266814 4369:0.0315181 4370:0.0239805 4371:0.0545976 4372:0.0717227 4373:0.0766292 4374:0.0498899 4375:0.0392586 4376:0.025183 4377:0.0181379 4378:0.0881726 4379:0.0590236 4380:0.0791907 4381:0.0844944 4382:0.0997368 4383:0.0623027 4384:0.0291399 4385:0.0458618 4386:0.0829121 4387:0.0568813 4388:0.0406404 4389:0.0235298 4390:0.0535892 4391:0.040732 4392:0.0379519 4393:0.0387917 4394:0.0230442 4395:0.0422347 4396:0.0457777 4397:0.0187831 4398:0.0343077 4399:0.044027 4400:0.0634487 4401:0.0681611 4402:0.0355287 4403:0.0117988 4404:0.0173403 4405:0.0260797 4406:0.0352151 4407:0.0438127 4408:0.0131441 4409:0.0862553 4410:0.0829914 4411:0.059999 4412:0.0468698 4413:0.0398014 4414:0.0316065 4415:0.049159 4416:0 4417:0.0357203 4418:0.0798888 4419:0.0882286 4420:0.076057 4421:0.0804616 4422:0.0857134 4423:0.0909879 4424:0.0937835 4425:0 4426:0 4427:0 4428:0 4429:0 4430:0 4431:0 4432:0.0209615 4433:0 4434:0 4435:0 4436:0.00382498 4437:0 4438:0.014459 4439:0 4440:0.0249531 4441:0 4442:0 4443:0 4444:0 4445:0 4446:0.0148349 4447:0.0153065 4448:0.0333513 4449:0 4450:0 4451:0 4452:0 4453:0 4454:0 4455:0 4456:0.0325211 4457:0 4458:0 4459:0 4460:0 4461:0 4462:0 4463:0 4464:0.0204869 4465:0 4466:0 4467:0 4468:0 4469:0 4470:0 4471:0 4472:0.0459997 4473:0 4474:0 4475:0 4476:0 4477:0 4478:0 4479:0 4480:0.0178737 4481:0 4482:0 4483:0 4484:0 4485:0 4486:0 4487:0 4488:0 4489:0 4490:0 4491:0 4492:0 4493:0 4494:0 4495:0 4496:0 4497:0 4498:0 4499:0 4500:0 4501:0 4502:0 4503:0 4504:0 4505:0 4506:0 4507:0 4508:0 4509:0 4510:0 4511:0 4512:0 4513:0 4514:0 4515:0 4516:0.00681742 4517:0 4518:0 4519:0 4520:0 4521:0 4522:0 4523:0 4524:0 4525:0 4526:0.00299868 4527:0 4528:0 4529:0 4530:0 4531:0 4532:0 4533:0 4534:0 4535:0 4536:0 4537:0 4538:0 4539:0 4540:0 4541:0 4542:0 4543:0 4544:0 4545:0 4546:0.0364281 4547:0.0495943 4548:0.0437675 4549:0.039318 4550:0.0557218 4551:0.0576317 4552:0.0691412 4553:0.0030009 4554:0.0289473 4555:0.0168062 4556:0.0423642 4557:0.0286677 4558:0.0322123 4559:0.0447714 4560:0.0341756 4561:0.0537276 4562:0.0334606 4563:0.0667581 4564:0.0267176 4565:0.0630642 4566:0.0409894 4567:0.032367 4568:0.0468206 4569:0.0589415 4570:0.0564551 4571:0.0509199 4572:0.0996282 4573:0.146114 4574:0.149807 4575:0.111848 4576:0.0487434 4577:0.0436632 4578:0.068411 4579:0.150556 4580:0.162859 4581:0.188596 4582:0.160606 4583:0.158931 4584:0.0536665 4585:0.0438926 4586:0.114571 4587:0.154448 4588:0.149395 4589:0.0948193 4590:0.118935 4591:0.137488 4592:0.0432859 4593:0.0625987 4594:0.055086 4595:0.0906955 4596:0.113471 4597:0.0678548 4598:0.0535021 4599:0.0537233 4600:0.103747 4601:0.0570251 4602:0.0422748 4603:0.0408038 4604:0.0468401 4605:0.0354302 4606:0.0427035 4607:0.0293725 4608:0.0817215 4609:0 4610:0.00147036 4611:0.0114602 4612:0.00779748 4613:0.00602567 4614:0.00594489 4615:0.00795588 4616:0.010593 4617:0 4618:0 4619:0 4620:0.00257618 4621:0 4622:0 4623:0.000442073 4624:0.00989506 4625:0 4626:0 4627:0.0105966 4628:0 4629:0 4630:0 4631:0 4632:0 4633:0 4634:0.00645131 4635:0 4636:0 4637:0 4638:0.000211809 4639:0.0301102 4640:0.00878435 4641:0 4642:0 4643:0 4644:0 4645:0.00370359 4646:0 4647:0.0445084 4648:0.00321413 4649:0 4650:0 4651:0 4652:0.00438645 4653:0 4654:0 4655:0.0406059 4656:0 4657:0.00738185 4658:0.00593786 4659:0 4660:0.00912591 4661:0.00185431 4662:0.00027675 4663:0 4664:0.0175678 4665:0 4666:0.0171043 4667:0.021693 4668:0.00347489 4669:0.00334537 4670:0.00300035 4671:0.00165363 4672:0.0213058 4673:0.0576979 4674:0.0578749 4675:0.0622107 4676:0.0613727 4677:0.0569153 4678:0.0598106 4679:0.0591554 4680:0.106235 4681:0.0859203 4682:0.073435 4683:0.077052 4684:0.081734 4685:0.0983944 4686:0.0788182 4687:0.0856032 4688:0.100285 4689:0.080453 4690:0.0784702 4691:0.0956015 4692:0.113821 4693:0.0989998 4694:0.127507 4695:0.0902534 4696:0.0980092 4697:0.0763463 4698:0.0949125 4699:0.112766 4700:0.124383 4701:0.127797 4702:0.131426 4703:0.139383 4704:0.0990337 4705:0.0834253 4706:0.0791459 4707:0.138748 4708:0.0998185 4709:0.0987079 4710:0.111173 4711:0.124855 4712:0.103708 4713:0.0562499 4714:0.0574886 4715:0.123079 4716:0.0998017 4717:0.0672788 4718:0.082101 4719:0.0943027 4720:0.116654 4721:0.0802528 4722:0.0775771 4723:0.0666979 4724:0.0986398 4725:0.080653 4726:0.0702772 4727:0.0802341 4728:0.126693 4729:0.100896 4730:0.0941058 4731:0.0973249 4732:0.0836455 4733:0.0804296 4734:0.0812439 4735:0.0862001 4736:0.147357 4737:0.0687242 4738:0.0532009 4739:0.0512123 4740:0.055867 4741:0.0524508 4742:0.0446988 4743:0.0487418 4744:0.0448611 4745:0.0575299 4746:0.035516 4747:0.0300093 4748:0.0376844 4749:0.0487705 4750:0.0471083 4751:0.0323878 4752:0.0485065 4753:0.0473201 4754:0.0302165 4755:0.0577495 4756:0.065976 4757:0.0412216 4758:0.0446734 4759:0.0502079 4760:0.0236491 4761:0.0472384 4762:0.0656781 4763:0.0351667 4764:0.038558 4765:0.0251966 4766:0.0380888 4767:0.0619062 4768:0.0219531 4769:0.0474444 4770:0.0612833 4771:0.0566023 4772:0.0215811 4773:0.0528112 4774:0.0423842 4775:0.0551232 4776:0.0271483 4777:0.0513151 4778:0.0336381 4779:0.0489063 4780:0.0595057 4781:0.0473112 4782:0.0365882 4783:0.0572724 4784:0.0179269 4785:0.0465698 4786:0.0321994 4787:0.0306652 4788:0.0399989 4789:0.0376499 4790:0.0500919 4791:0.0346639 4792:0.00398692 4793:0.0465416 4794:0.0421664 4795:0.0591877 4796:0.0532914 4797:0.0511079 4798:0.0485601 4799:0.0553072 4800:0.0828706 4801:0.0221175 4802:0 4803:0.000834155 4804:0 4805:0.00100729 4806:0 4807:0 4808:0 4809:0 4810:0 4811:0 4812:0 4813:0 4814:0 4815:0 4816:0 4817:0 4818:0 4819:0 4820:0 4821:0 4822:0 4823:0 4824:0 4825:0 4826:0 4827:0 4828:0 4829:0 4830:0 4831:0 4832:0 4833:0 4834:0 4835:0 4836:0 4837:0 4838:0 4839:0 4840:0 4841:0 4842:0 4843:0 4844:0 4845:0 4846:0 4847:0 4848:0 4849:0 4850:0 4851:0 4852:0 4853:0 4854:0 4855:0 4856:0 4857:0 4858:0 4859:0 4860:0 4861:0 4862:0 4863:0 4864:0 4865:0 4866:0 4867:0 4868:0 4869:0 4870:0 4871:0 4872:0 4873:0 4874:0 4875:0 4876:0 4877:0 4878:0 4879:0 4880:0 4881:0 4882:0 4883:0 4884:0 4885:0 4886:0 4887:0 4888:0 4889:0 4890:0 4891:0 4892:0 4893:0 4894:0 4895:0 4896:0 4897:0 4898:0 4899:0 4900:0 4901:0 4902:0 4903:0 4904:0 4905:0 4906:0 4907:0 4908:0 4909:0 4910:0 4911:0 4912:0 4913:0 4914:0 4915:0 4916:0 4917:0 4918:0 4919:0 4920:0 4921:0 4922:0 4923:0 4924:0 4925:0 4926:0 4927:0 4928:0 4929:0 4930:0 4931:0 4932:0 4933:0 4934:0 4935:0 4936:0 4937:0 4938:0 4939:0 4940:0 4941:0 4942:0 4943:0 4944:0 4945:0 4946:0 4947:0 4948:0 4949:0 4950:0 4951:0 4952:0 4953:0 4954:0 4955:0 4956:0 4957:0 4958:0 4959:0 4960:0 4961:0 4962:0 4963:0 4964:0 4965:0 4966:0 4967:0 4968:0 4969:0 4970:0 4971:0 4972:0 4973:0 4974:0 4975:0 4976:0 4977:0 4978:0 4979:0 4980:0 4981:0 4982:0 4983:0 4984:0 4985:0 4986:0 4987:0 4988:0 4989:0 4990:0 4991:0 4992:0 4993:0.077656 4994:0.103878 4995:0.093207 4996:0.100316 4997:0.0977108 4998:0.104734 4999:0.108245 5000:0.111583 5001:0.0979128 5002:0.0725191 5003:0.0712224 5004:0.063502 5005:0.0841355 5006:0.0781631 5007:0.0784257 5008:0.0472734 5009:0.112613 5010:0.0819349 5011:0.0813474 5012:0.135147 5013:0.10465 5014:0.107313 5015:0.0870559 5016:0.0706429 5017:0.107364 5018:0.105109 5019:0.142167 5020:0.159169 5021:0.15913 5022:0.144653 5023:0.114229 5024:0.0807415 5025:0.0997195 5026:0.196784 5027:0.158474 5028:0.149165 5029:0.12256 5030:0.165412 5031:0.0863684 5032:0.0723388 5033:0.0980339 5034:0.124478 5035:0.108881 5036:0.0962279 5037:0.0841662 5038:0.0905713 5039:0.0845472 5040:0.112668 5041:0.1195 5042:0.0946056 5043:0.0963371 5044:0.0886094 5045:0.0830084 5046:0.0829683 5047:0.111476 5048:0.0910842 5049:0.126568 5050:0.100354 5051:0.0700211 5052:0.0689598 5053:0.0841429 5054:0.08054 5055:0.0980679 5056:0.0477466 5057:0.0610242 5058:0.0734582 5059:0.0838188 5060:0.0770894 5061:0.0837287 5062:0.0836576 5063:0.086527 5064:0.0648652 5065:0.0628556 5066:0.0726388 5067:0.0679249 5068:0.0838477 5069:0.079008 5070:0.0723458 5071:0.064039 5072:0.064925 5073:0.0714039 5074:0.0736749 5075:0.0880012 5076:0.117279 5077:0.111988 5078:0.103227 5079:0.0964294 5080:0.0659998 5081:0.0616635 5082:0.0888531 5083:0.115061 5084:0.142065 5085:0.137896 5086:0.117118 5087:0.103489 5088:0.0677767 5089:0.0610689 5090:0.132187 5091:0.123898 5092:0.123867 5093:0.105382 5094:0.11123 5095:0.0981004 5096:0.087266 5097:0.0630594 5098:0.108538 5099:0.115433 5100:0.0909102 5101:0.0937612 5102:0.0965645 5103:0.0893502 5104:0.0905277 5105:0.0778078 5106:0.0926034 5107:0.0886592 5108:0.0687595 5109:0.0778037 5110:0.080127 5111:0.0957668 5112:0.0816178 5113:0.0896506 5114:0.10316 5115:0.0869405 5116:0.0798444 5117:0.0757942 5118:0.0763799 5119:0.0941619 5120:0.0522306 5121:0.0103591 5122:0 5123:0 5124:0 5125:0 5126:0 5127:0 5128:0 5129:0 5130:0 5131:0 5132:0 5133:0 5134:0 5135:0 5136:0 5137:0 5138:0 5139:0 5140:0 5141:0 5142:0 5143:0 5144:0 5145:0 5146:0 5147:0 5148:0 5149:0 5150:0 5151:0 5152:0 5153:0 5154:0 5155:0 5156:0 5157:0 5158:0 5159:0 5160:0 5161:0 5162:0 5163:0 5164:0 5165:0 5166:0 5167:0 5168:0 5169:0 5170:0 5171:0 5172:0 5173:0 5174:0 5175:0 5176:0 5177:0 5178:0 5179:0 5180:0 5181:0 5182:0 5183:0 5184:0 5185:0.141185 5186:0.161816 5187:0.162863 5188:0.158545 5189:0.158891 5190:0.169387 5191:0.17232 5192:0.180523 5193:0.133629 5194:0.128583 5195:0.130704 5196:0.155854 5197:0.135943 5198:0.13206 5199:0.131907 5200:0.146858 5201:0.148491 5202:0.129174 5203:0.161914 5204:0.182438 5205:0.17184 5206:0.181073 5207:0.134847 5208:0.161128 5209:0.136226 5210:0.173452 5211:0.20903 5212:0.260339 5213:0.226129 5214:0.220642 5215:0.158731 5216:0.153324 5217:0.176189 5218:0.247447 5219:0.2315 5220:0.192751 5221:0.196394 5222:0.249257 5223:0.157203 5224:0.158335 5225:0.160889 5226:0.212517 5227:0.194354 5228:0.175727 5229:0.143112 5230:0.178038 5231:0.152401 5232:0.20516 5233:0.152033 5234:0.148622 5235:0.183855 5236:0.174684 5237:0.141461 5238:0.140768 5239:0.158351 5240:0.196972 5241:0.154576 5242:0.150615 5243:0.141069 5244:0.130192 5245:0.137454 5246:0.128806 5247:0.179555 5248:0.156444 5249:0 5250:0 5251:0 5252:0 5253:0 5254:0 5255:0 5256:0 5257:0 5258:0 5259:0 5260:0 5261:0 5262:0 5263:0 5264:0 5265:0 5266:0 5267:0 5268:0 5269:0 5270:0 5271:0 5272:0 5273:0 5274:0 5275:0 5276:0 5277:0 5278:0 5279:0 5280:0 5281:0 5282:0 5283:0 5284:0 5285:0 5286:0 5287:0 5288:0 5289:0 5290:0 5291:0 5292:0 5293:0 5294:0 5295:0 5296:0 5297:0 5298:0 5299:0 5300:0 5301:0 5302:0 5303:0 5304:0 5305:0 5306:0 5307:0 5308:0 5309:0 5310:0 5311:0 5312:0 5313:0 5314:0 5315:0 5316:0 5317:0 5318:0 5319:0 5320:0 5321:0 5322:0 5323:0 5324:0 5325:0 5326:0 5327:0 5328:0 5329:0 5330:0 5331:0 5332:0 5333:0 5334:0 5335:0 5336:0 5337:0 5338:0 5339:0 5340:0 5341:0 5342:0 5343:0 5344:0 5345:0 5346:0 5347:0 5348:0 5349:0 5350:0 5351:0 5352:0 5353:0 5354:0 5355:0 5356:0 5357:0 5358:0 5359:0 5360:0 5361:0 5362:0 5363:0 5364:0 5365:0 5366:0 5367:0 5368:0 5369:0 5370:0 5371:0 5372:0 5373:0 5374:0 5375:0 5376:0 5377:0.0525774 5378:0.044115 5379:0.0480839 5380:0.0430742 5381:0.0476683 5382:0.0413026 5383:0.0460776 5384:0.0527304 5385:0.0457982 5386:0.0272693 5387:0.0369391 5388:0.0363598 5389:0.0399017 5390:0.0458368 5391:0.0259969 5392:0.01771 5393:0.0467105 5394:0.0259032 5395:0.0256066 5396:0.0418983 5397:0.0347617 5398:0.0376434 5399:0.0402476 5400:0.0231948 5401:0.0418015 5402:0.0364033 5403:0.0341488 5404:0.0453961 5405:0.0118477 5406:0.0183979 5407:0.0303968 5408:0.0145047 5409:0.0440352 5410:0.0416974 5411:0.0377735 5412:0.00423178 5413:0.00283262 5414:0 5415:0.0279146 5416:0.0116844 5417:0.0508632 5418:0.0382338 5419:0.0184724 5420:0.00485492 5421:0.0160327 5422:0.0175708 5423:0.00730028 5424:0.0193561 5425:0.0576176 5426:0.0288254 5427:0.0175952 5428:0.0215687 5429:0.0259393 5430:0.0328478 5431:0.0350545 5432:0.0322193 5433:0.047601 5434:0.0183175 5435:0.0223623 5436:0.0236682 5437:0.0214873 5438:0.0265503 5439:0.0421183 5440:0.0420128 5441:0.03807 5442:0.0750018 5443:0.0736249 5444:0.0751313 5445:0.0752401 5446:0.0832236 5447:0.0901219 5448:0.0925319 5449:0.00614389 5450:0 5451:0 5452:0 5453:0.00874228 5454:0 5455:0 5456:0.0192384 5457:0.00826562 5458:0 5459:0.0280684 5460:0.016292 5461:0.0315562 5462:0.0308651 5463:0.00363459 5464:0.0232152 5465:0 5466:0 5467:0.031288 5468:0.0329437 5469:0.0353411 5470:0.0286199 5471:0.0382936 5472:0.025682 5473:0 5474:0 5475:0.0600721 5476:0.00709364 5477:0 5478:0 5479:0.0118281 5480:0.0199613 5481:0 5482:0 5483:0 5484:0.00369941 5485:0 5486:0 5487:0 5488:0.0412202 5489:0.00820951 5490:0 5491:0 5492:0 5493:0 5494:0 5495:0 5496:0.0545154 5497:0 5498:0.00351557 5499:0 5500:0 5501:0 5502:0 5503:0 5504:0.0283633 5505:0 5506:0 5507:0 5508:0 5509:0 5510:0 5511:0 5512:0 5513:0 5514:0 5515:0 5516:0 5517:0 5518:0 5519:0 5520:0 5521:0 5522:0 5523:0 5524:0 5525:0 5526:0 5527:0 5528:0 5529:0 5530:0 5531:0.0012935 5532:0 5533:0 5534:0 5535:0 5536:0 5537:0 5538:0 5539:0 5540:0 5541:0 5542:0 5543:0 5544:0 5545:0 5546:0 5547:0 5548:0 5549:0 5550:0 5551:0 5552:0 5553:0 5554:0 5555:0 5556:0 5557:0 5558:0 5559:0 5560:0 5561:0 5562:0 5563:0 5564:0 5565:0 5566:0 5567:0 5568:0 5569:0.0471646 5570:0.0792429 5571:0.0866231 5572:0.0904238 5573:0.0897693 5574:0.093914 5575:0.0979099 5576:0.121851 5577:0.0937552 5578:0.0848337 5579:0.085838 5580:0.0905757 5581:0.103389 5582:0.0969643 5583:0.100351 5584:0.0923612 5585:0.118069 5586:0.111082 5587:0.0907079 5588:0.0781778 5589:0.113684 5590:0.126563 5591:0.114157 5592:0.108244 5593:0.125022 5594:0.102268 5595:0.107185 5596:0.217274 5597:0.233998 5598:0.205651 5599:0.154497 5600:0.107458 5601:0.112488 5602:0.172622 5603:0.197489 5604:0.229799 5605:0.24343 5606:0.219885 5607:0.172036 5608:0.113189 5609:0.107837 5610:0.188985 5611:0.197898 5612:0.152379 5613:0.129023 5614:0.18569 5615:0.129146 5616:0.130175 5617:0.146211 5618:0.124651 5619:0.154219 5620:0.168643 5621:0.129106 5622:0.112321 5623:0.1343 5624:0.145425 5625:0.162127 5626:0.157906 5627:0.155087 5628:0.13128 5629:0.131139 5630:0.124237 5631:0.125162 5632:0.12368 5633:0 5634:0 5635:0 5636:0 5637:0 5638:0 5639:0 5640:0 5641:0 5642:0 5643:0 5644:0 5645:0 5646:0 5647:0 5648:0 5649:0 5650:0 5651:0 5652:0 5653:0 5654:0 5655:0 5656:0 5657:0 5658:0 5659:0 5660:0 5661:0 5662:0 5663:0 5664:0 5665:0 5666:0 5667:0 5668:0 5669:0 5670:0 5671:0 5672:0 5673:0 5674:0 5675:0 5676:0 5677:0 5678:0 5679:0 5680:0 5681:0 5682:0 5683:0 5684:0 5685:0 5686:0 5687:0 5688:0 5689:0.0475359 5690:0 5691:0 5692:0.00578134 5693:0.00757512 5694:0.00934524 5695:0.0221558 5696:0.0204551 5697:0 5698:0 5699:0 5700:0 5701:0 5702:0 5703:0 5704:0 5705:0 5706:0 5707:0 5708:0 5709:0 5710:0 5711:0 5712:0 5713:0.0125555 5714:0 5715:0 5716:0 5717:0 5718:0 5719:0 5720:0 5721:0.0138571 5722:0 5723:0 5724:0 5725:0 5726:0 5727:0 5728:0 5729:0.0101009 5730:0 5731:0 5732:0 5733:0 5734:0 5735:0 5736:0 5737:0.00742055 5738:0 5739:0 5740:0 5741:0 5742:0 5743:0 5744:0 5745:0.0220902 5746:0 5747:0 5748:0 5749:0 5750:0 5751:0 5752:0 5753:0.033248 5754:0 5755:0 5756:0 5757:0 5758:0 5759:0 5760:0 5761:0.114316 5762:0.142009 5763:0.153034 5764:0.146108 5765:0.147371 5766:0.156211 5767:0.159089 5768:0.15729 5769:0.131697 5770:0.152367 5771:0.15717 5772:0.152814 5773:0.180014 5774:0.159611 5775:0.153842 5776:0.143838 5777:0.158733 5778:0.167291 5779:0.156941 5780:0.167023 5781:0.162312 5782:0.194009 5783:0.159533 5784:0.143917 5785:0.15612 5786:0.168151 5787:0.2035 5788:0.189632 5789:0.255094 5790:0.219325 5791:0.221576 5792:0.152182 5793:0.136409 5794:0.170743 5795:0.265791 5796:0.260711 5797:0.244929 5798:0.223556 5799:0.22257 5800:0.154082 5801:0.148439 5802:0.189939 5803:0.222393 5804:0.227227 5805:0.182491 5806:0.206758 5807:0.199829 5808:0.149529 5809:0.158092 5810:0.177355 5811:0.185257 5812:0.191759 5813:0.187393 5814:0.164569 5815:0.181819 5816:0.166028 5817:0.174616 5818:0.199614 5819:0.191618 5820:0.174212 5821:0.163388 5822:0.180745 5823:0.191691 5824:0.172727 5825:0.0609327 5826:0.0429793 5827:0.0376145 5828:0.0366978 5829:0.0383368 5830:0.0351974 5831:0.0317074 5832:0.0275233 5833:0.039177 5834:0.0336493 5835:0.0331224 5836:0.0588748 5837:0.0378625 5838:0.0408601 5839:0.038954 5840:0.0444615 5841:0.0420011 5842:0.0305133 5843:0.0505966 5844:0.061263 5845:0.0530948 5846:0.0268912 5847:0.0453801 5848:0.0426801 5849:0.0463363 5850:0.0785608 5851:0.0720399 5852:0.0212019 5853:0.0283137 5854:0.0267855 5855:0.0200633 5856:0.0402371 5857:0.0652139 5858:0.0743707 5859:0.0540422 5860:0.0256314 5861:0.0731491 5862:0.0328856 5863:0.0490096 5864:0.069065 5865:0.06814 5866:0.0309358 5867:0.0556804 5868:0.0580556 5869:0.0541694 5870:0.0521479 5871:0.084554 5872:0.0718923 5873:0.055279 5874:0.0265922 5875:0.0495535 5876:0.0523804 5877:0.0409112 5878:0.0412699 5879:0.0532336 5880:0.0193769 5881:0.0957416 5882:0.0696668 5883:0.0486035 5884:0.0503714 5885:0.0514152 5886:0.0544258 5887:0.061153 5888:0.0967696 5889:0.00973928 5890:0 5891:0 5892:0 5893:0 5894:0 5895:0 5896:0 5897:0 5898:0 5899:0 5900:0 5901:0 5902:0 5903:0 5904:0 5905:0 5906:0 5907:0 5908:0 5909:0 5910:0 5911:0 5912:0 5913:0 5914:0 5915:0 5916:0 5917:0 5918:0 5919:0 5920:0 5921:0 5922:0 5923:0 5924:0 5925:0 5926:0 5927:0 5928:0 5929:0 5930:0 5931:0 5932:0 5933:0 5934:0 5935:0 5936:0 5937:0 5938:0 5939:0 5940:0 5941:0 5942:0 5943:0 5944:0 5945:0 5946:0 5947:0 5948:0 5949:0 5950:0 5951:0 5952:0 5953:0.00453062 5954:0 5955:0.00902874 5956:0.0101672 5957:0.00515506 5958:0.000239706 5959:0.00900894 5960:0.00775823 5961:0.00375132 5962:0 5963:0 5964:0 5965:0 5966:0 5967:0 5968:0 5969:0 5970:0 5971:0.00615949 5972:0 5973:0 5974:0 5975:0 5976:0 5977:0 5978:0 5979:0 5980:0 5981:0 5982:0 5983:0 5984:0 5985:0 5986:0.00214672 5987:0 5988:0 5989:0 5990:0 5991:0 5992:0 5993:0 5994:0 5995:0 5996:0 5997:0 5998:0 5999:0 6000:0 6001:0 6002:0 6003:0 6004:0 6005:0 6006:0 6007:0 6008:0 6009:0 6010:0.0178087 6011:0.0251156 6012:0.00690172 6013:0.00391939 6014:0 6015:0 6016:0 6017:0.0454202 6018:0.0381381 6019:0.0433364 6020:0.0446366 6021:0.0420075 6022:0.0427626 6023:0.0449656 6024:0.0515778 6025:0.0243912 6026:0.0450454 6027:0.0387952 6028:0.0624256 6029:0.0680127 6030:0.0455925 6031:0.0515561 6032:0.0547077 6033:0.0510665 6034:0.0384481 6035:0.0664857 6036:0.0286561 6037:0.072698 6038:0.0332391 6039:0.0477864 6040:0.047212 6041:0.0560598 6042:0.0711293 6043:0.0352065 6044:0.0331673 6045:0.0355388 6046:0.0455206 6047:0.0368827 6048:0.0444439 6049:0.051037 6050:0.0798174 6051:0.060854 6052:0.0817123 6053:0.110498 6054:0.0703669 6055:0.100148 6056:0.0654947 6057:0.0376039 6058:0.0446019 6059:0.10654 6060:0.0687871 6061:0.0496403 6062:0.0839685 6063:0.129993 6064:0.0303284 6065:0.0529505 6066:0.0497748 6067:0.050155 6068:0.0959636 6069:0.062126 6070:0.0569108 6071:0.0429383 6072:0.0554419 6073:0.0593254 6074:0.0475828 6075:0.0449431 6076:0.0407597 6077:0.0339228 6078:0.0410288 6079:0.0467591 6080:0.0689041 6081:0.0435029 6082:0.0618665 6083:0.0786791 6084:0.0885026 6085:0.0859963 6086:0.0935876 6087:0.0739299 6088:0.088687 6089:0.0553376 6090:0.0628345 6091:0.0674772 6092:0.0934842 6093:0.088006 6094:0.0737996 6095:0.090284 6096:0.0749175 6097:0.0759577 6098:0.0752695 6099:0.103316 6100:0.0957562 6101:0.0915614 6102:0.092717 6103:0.0714513 6104:0.0771635 6105:0.0739011 6106:0.0830073 6107:0.104805 6108:0.157991 6109:0.152631 6110:0.170979 6111:0.0808542 6112:0.077986 6113:0.0803569 6114:0.175038 6115:0.132084 6116:0.14848 6117:0.149204 6118:0.163582 6119:0.0757486 6120:0.105711 6121:0.071863 6122:0.132726 6123:0.135543 6124:0.123903 6125:0.0817743 6126:0.113017 6127:0.101209 6128:0.119719 6129:0.101339 6130:0.0931992 6131:0.11745 6132:0.122049 6133:0.0818937 6134:0.0848128 6135:0.129102 6136:0.139041 6137:0.112025 6138:0.0970517 6139:0.0796872 6140:0.0850832 6141:0.0873351 6142:0.083344 6143:0.108435 6144:0.0973506 6145:0 6146:0 6147:0 6148:0 6149:0 6150:0 6151:0 6152:0 6153:0 6154:0 6155:0 6156:0.00541726 6157:0 6158:0 6159:0 6160:0 6161:0 6162:0 6163:0.0257101 6164:0.0228729 6165:0.0396867 6166:0.0356427 6167:0 6168:0 6169:0 6170:0.0593584 6171:0.0182086 6172:0 6173:0.0168367 6174:0.04471 6175:0.00312384 6176:0 6177:0 6178:0.0455497 6179:0.00127273 6180:0 6181:0 6182:0.0284204 6183:0.0138023 6184:0.000374354 6185:0 6186:0 6187:0.0383634 6188:0.0205596 6189:0.00173491 6190:0 6191:0.0235565 6192:0 6193:0.00802644 6194:0 6195:0 6196:0 6197:0 6198:0 6199:0.014739 6200:0 6201:0.00613596 6202:0 6203:0 6204:0 6205:0 6206:0 6207:0 6208:0 6209:0.024733 6210:0.0325713 6211:0.0262615 6212:0.0270362 6213:0.0302444 6214:0.0334498 6215:0.0360268 6216:0.042877 6217:0.0421599 6218:0.0319575 6219:0.0265632 6220:0.0309013 6221:0.0307453 6222:0.0275506 6223:0.0347603 6224:0.0305676 6225:0.0586454 6226:0.0324332 6227:0.0454878 6228:0.0729562 6229:0.0569832 6230:0.0706427 6231:0.0301342 6232:0.0366089 6233:0.0538025 6234:0.0638436 6235:0.0897864 6236:0.10799 6237:0.0929923 6238:0.0993278 6239:0.0572757 6240:0.0440834 6241:0.0486859 6242:0.111385 6243:0.0874083 6244:0.0739912 6245:0.0606147 6246:0.088516 6247:0.0754889 6248:0.0395351 6249:0.0490999 6250:0.0350665 6251:0.0627686 6252:0.0790515 6253:0.0252301 6254:0.02464 6255:0.0602685 6256:0.0541594 6257:0.0761856 6258:0.0370828 6259:0.0403877 6260:0.0498485 6261:0.0393149 6262:0.0372808 6263:0.061842 6264:0.0616703 6265:0.0934124 6266:0.073002 6267:0.0552771 6268:0.0389427 6269:0.0433038 6270:0.040141 6271:0.039218 6272:0.0353079 6273:0 6274:0.00116413 6275:0.00743015 6276:0.00164972 6277:0.0105281 6278:0.0177132 6279:0.0114109 6280:0.00702179 6281:0 6282:0 6283:0 6284:0 6285:0 6286:0 6287:0 6288:0 6289:0 6290:0 6291:0.00746747 6292:0.0313683 6293:0.0532827 6294:0.0283788 6295:0.00111882 6296:0 6297:0 6298:0.000338772 6299:0.0412734 6300:0.017576 6301:0.0686313 6302:0.0575027 6303:0.0597047 6304:0.00512782 6305:0 6306:0 6307:0.046151 6308:0.0450281 6309:0.0354793 6310:0.043924 6311:0.056999 6312:0 6313:0 6314:0 6315:0.0184357 6316:0.0396521 6317:0.0162629 6318:0.00957044 6319:0.00560737 6320:0.00608295 6321:0.0129187 6322:0.00667815 6323:0 6324:0.0043659 6325:0 6326:0 6327:0.00266453 6328:0.018801 6329:0 6330:0 6331:0 6332:0 6333:0 6334:0 6335:0 6336:0.0255832 6337:0.0326429 6338:0.0240938 6339:0.0130274 6340:0.014415 6341:0.020301 6342:0.0124405 6343:0.0153335 6344:0.00396742 6345:0.0388907 6346:0.0227212 6347:0.0213589 6348:0.0308378 6349:0.0241835 6350:0.035762 6351:0.0189713 6352:0.0288719 6353:0.0611007 6354:0.0245385 6355:0.0178736 6356:0.0224755 6357:0.0195321 6358:0.0268282 6359:0.0182127 6360:0.0241394 6361:0.0679341 6362:0.0206056 6363:0.0381696 6364:0.0190682 6365:0.00347094 6366:0.00760774 6367:0.0306362 6368:0.0282577 6369:0.0682759 6370:0.072224 6371:0.0295982 6372:0.0605081 6373:0.0245948 6374:0.0481655 6375:0.0236911 6376:0.0271648 6377:0.0597666 6378:0.0588011 6379:0.0132719 6380:0.00136527 6381:0.0342001 6382:0.0522813 6383:0.0441552 6384:0.0424764 6385:0.0602976 6386:0.0341043 6387:0.0332712 6388:0.0157829 6389:0.0192964 6390:0.0337802 6391:0.04403 6392:0.0388457 6393:0.114119 6394:0.040478 6395:0.0356587 6396:0.0293664 6397:0.038754 6398:0.0421212 6399:0.0526218 6400:0.0196503 6401:0 6402:0 6403:0 6404:0 6405:0 6406:0 6407:0 6408:0 6409:0 6410:0 6411:0 6412:0 6413:0 6414:0 6415:0 6416:0 6417:0 6418:0 6419:0 6420:0 6421:0 6422:0 6423:0 6424:0 6425:0 6426:0 6427:0 6428:0 6429:0 6430:0 6431:0 6432:0 6433:0 6434:0 6435:0 6436:0 6437:0 6438:0 6439:0 6440:0 6441:0 6442:0 6443:0 6444:0 6445:0 6446:0 6447:0 6448:0 6449:0 6450:0 6451:0 6452:0 6453:0 6454:0 6455:0 6456:0 6457:0.022717 6458:0 6459:0 6460:0 6461:0 6462:0 6463:0 6464:0.0147894 6465:0.0167967 6466:0 6467:0 6468:0 6469:0 6470:0 6471:0 6472:0 6473:0.0165971 6474:0 6475:0 6476:0 6477:0 6478:0 6479:0 6480:0 6481:0 6482:0 6483:0 6484:0 6485:0 6486:0 6487:0 6488:0 6489:0 6490:0 6491:0 6492:0 6493:0 6494:0 6495:0 6496:0 6497:0 6498:0 6499:0 6500:0 6501:0 6502:0 6503:0 6504:0 6505:0 6506:0 6507:0 6508:0 6509:0 6510:0 6511:0 6512:0 6513:0 6514:0 6515:0 6516:0 6517:0 6518:0 6519:0 6520:0 6521:0 6522:0 6523:0 6524:0 6525:0 6526:0 6527:0 6528:0 6529:0.0961945 6530:0.118146 6531:0.123781 6532:0.123572 6533:0.119236 6534:0.129149 6535:0.128858 6536:0.136123 6537:0.0970652 6538:0.0850669 6539:0.0769664 6540:0.0803709 6541:0.0903867 6542:0.0973984 6543:0.0960586 6544:0.0938588 6545:0.116448 6546:0.0934691 6547:0.102112 6548:0.140523 6549:0.128224 6550:0.126335 6551:0.100985 6552:0.0978428 6553:0.10662 6554:0.116002 6555:0.157398 6556:0.16487 6557:0.184167 6558:0.160844 6559:0.165036 6560:0.0912562 6561:0.102945 6562:0.155194 6563:0.17405 6564:0.155278 6565:0.145593 6566:0.172003 6567:0.159365 6568:0.0866001 6569:0.112071 6570:0.109589 6571:0.170322 6572:0.152043 6573:0.113758 6574:0.103612 6575:0.103467 6576:0.0987418 6577:0.10527 6578:0.0918534 6579:0.109849 6580:0.110037 6581:0.0979375 6582:0.0954871 6583:0.114624 6584:0.101398 6585:0.150373 6586:0.115122 6587:0.11308 6588:0.119735 6589:0.115891 6590:0.105682 6591:0.12644 6592:0.127994 6593:0.014313 6594:0 6595:0 6596:0 6597:0 6598:0 6599:0 6600:0 6601:0 6602:0 6603:0 6604:0 6605:0 6606:0 6607:0 6608:0 6609:0 6610:0 6611:0 6612:0 6613:0 6614:0 6615:0 6616:0 6617:0 6618:0 6619:0 6620:0 6621:0 6622:0 6623:0 6624:0 6625:0 6626:0 6627:0 6628:0 6629:0 6630:0 6631:0 6632:0 6633:0 6634:0 6635:0 6636:0 6637:0 6638:0 6639:0 6640:0 6641:0 6642:0 6643:0 6644:0 6645:0 6646:0 6647:0 6648:0 6649:0 6650:0 6651:0 6652:0 6653:0 6654:0 6655:0 6656:0 6657:0 6658:0 6659:0 6660:0 6661:0 6662:0 6663:0 6664:0 6665:0 6666:0 6667:0 6668:0 6669:0 6670:0 6671:0 6672:0 6673:0 6674:0 6675:0 6676:0 6677:0 6678:0 6679:0 6680:0 6681:0 6682:0 6683:0 6684:0 6685:0 6686:0 6687:0 6688:0 6689:0 6690:0 6691:0 6692:0 6693:0 6694:0 6695:0 6696:0 6697:0 6698:0 6699:0 6700:0 6701:0 6702:0 6703:0 6704:0 6705:0 6706:0 6707:0 6708:0 6709:0 6710:0 6711:0 6712:0 6713:0 6714:0 6715:0 6716:0 6717:0 6718:0 6719:0 6720:0 6721:0 6722:0 6723:0 6724:0 6725:0 6726:0 6727:0 6728:0 6729:0 6730:0 6731:0 6732:0 6733:0 6734:0 6735:0 6736:0 6737:0 6738:0 6739:0 6740:0 6741:0 6742:0 6743:0 6744:0 6745:0 6746:0 6747:0 6748:0 6749:0 6750:0 6751:0 6752:0 6753:0 6754:0 6755:0 6756:0 6757:0 6758:0 6759:0 6760:0 6761:0 6762:0 6763:0 6764:0 6765:0 6766:0 6767:0 6768:0 6769:0 6770:0 6771:0 6772:0 6773:0 6774:0 6775:0 6776:0 6777:0 6778:0 6779:0 6780:0 6781:0 6782:0 6783:0 6784:0 6785:0 6786:0 6787:0 6788:0 6789:0 6790:0 6791:0 6792:0 6793:0 6794:0 6795:0 6796:0 6797:0 6798:0 6799:0 6800:0 6801:0 6802:0 6803:0 6804:0 6805:0 6806:0 6807:0 6808:0 6809:0 6810:0 6811:0 6812:0 6813:0 6814:0 6815:0 6816:0 6817:0 6818:0 6819:0 6820:0 6821:0 6822:0 6823:0 6824:0 6825:0 6826:0 6827:0 6828:0 6829:0 6830:0 6831:0 6832:0 6833:0 6834:0 6835:0 6836:0 6837:0 6838:0 6839:0 6840:0 6841:0 6842:0 6843:0 6844:0 6845:0 6846:0 6847:0 6848:0 6849:0 6850:0 6851:0 6852:0 6853:0 6854:0 6855:0 6856:0 6857:0 6858:0 6859:0 6860:0 6861:0 6862:0 6863:0 6864:0 6865:0 6866:0 6867:0 6868:0 6869:0 6870:0 6871:0 6872:0 6873:0 6874:0 6875:0 6876:0 6877:0 6878:0 6879:0 6880:0 6881:0 6882:0 6883:0 6884:0 6885:0 6886:0 6887:0 6888:0 6889:0 6890:0 6891:0 6892:0 6893:0 6894:0 6895:0 6896:0 6897:0 6898:0 6899:0 6900:0 6901:0 6902:0 6903:0 6904:0 6905:0 6906:0 6907:0 6908:0 6909:0 6910:0 6911:0 6912:0 6913:0.0971469 6914:0.114217 6915:0.122925 6916:0.110319 6917:0.115722 6918:0.112635 6919:0.127723 6920:0.123174 6921:0.090137 6922:0.0936468 6923:0.0777073 6924:0.09618 6925:0.0942042 6926:0.0993721 6927:0.0848509 6928:0.067761 6929:0.0911452 6930:0.109484 6931:0.0935993 6932:0.105522 6933:0.0893019 6934:0.0886598 6935:0.092245 6936:0.0690229 6937:0.0813511 6938:0.108487 6939:0.109653 6940:0.160672 6941:0.102039 6942:0.124156 6943:0.100303 6944:0.067307 6945:0.0763775 6946:0.135351 6947:0.107761 6948:0.132044 6949:0.132095 6950:0.102011 6951:0.0567974 6952:0.0661187 6953:0.0889471 6954:0.155583 6955:0.106612 6956:0.0811958 6957:0.0807955 6958:0.0998082 6959:0.0692423 6960:0.0486365 6961:0.0722611 6962:0.0958511 6963:0.10857 6964:0.109631 6965:0.0841075 6966:0.0984559 6967:0.0987643 6968:0.101726 6969:0.109992 6970:0.109312 6971:0.114628 6972:0.104811 6973:0.10442 6974:0.0892458 6975:0.0955098 6976:0.0777647 6977:0 6978:0 6979:0 6980:0 6981:0 6982:0 6983:0 6984:0 6985:0 6986:0 6987:0 6988:0 6989:0 6990:0 6991:0 6992:0 6993:0 6994:0 6995:0 6996:0 6997:0 6998:0 6999:0 7000:0 7001:0 7002:0 7003:0 7004:0 7005:0 7006:0 7007:0 7008:0 7009:0 7010:0 7011:0 7012:0 7013:0 7014:0 7015:0 7016:0 7017:0 7018:0 7019:0 7020:0 7021:0 7022:0 7023:0 7024:0 7025:0 7026:0 7027:0 7028:0 7029:0 7030:0 7031:0 7032:0 7033:0 7034:0 7035:0 7036:0 7037:0 7038:0 7039:0 7040:0 7041:0 7042:0 7043:0 7044:0 7045:0 7046:0 7047:0 7048:0 7049:0 7050:0 7051:0 7052:0 7053:0 7054:0 7055:0 7056:0 7057:0 7058:0 7059:0 7060:0 7061:0 7062:0 7063:0 7064:0 7065:0 7066:0 7067:0 7068:0 7069:0 7070:0 7071:0 7072:0 7073:0 7074:0 7075:0 7076:0 7077:0 7078:0 7079:0 7080:0 7081:0 7082:0 7083:0 7084:0 7085:0 7086:0 7087:0 7088:0 7089:0 7090:0 7091:0 7092:0 7093:0 7094:0 7095:0 7096:0 7097:0 7098:0 7099:0 7100:0 7101:0 7102:0 7103:0 7104:0 7105:0.0726115 7106:0.0563457 7107:0.0491946 7108:0.0465409 7109:0.0532184 7110:0.0634006 7111:0.0472764 7112:0.0567517 7113:0.0657243 7114:0.0350066 7115:0.034545 7116:0.0646483 7117:0.0600808 7118:0.0598112 7119:0.0411533 7120:0.0304268 7121:0.0718546 7122:0.0492237 7123:0.0702406 7124:0.0733327 7125:0.0529926 7126:0.0314375 7127:0.0474776 7128:0.03908 7129:0.0624292 7130:0.0678106 7131:0.0764585 7132:0.0881061 7133:0.0631565 7134:0.0687986 7135:0.000181625 7136:0.0336667 7137:0.0573572 7138:0.0888946 7139:0.0441209 7140:0.0459824 7141:0.0762931 7142:0.116506 7143:0.0339725 7144:0.047677 7145:0.0620794 7146:0.0622977 7147:0.097799 7148:0.0484581 7149:0.0589334 7150:0.0726537 7151:0.0818053 7152:0.0643163 7153:0.0551967 7154:0.053318 7155:0.058675 7156:0.061147 7157:0.0508941 7158:0.0533602 7159:0.0820773 7160:0.104503 7161:0.0818308 7162:0.0380043 7163:0.0547363 7164:0.0526888 7165:0.0458566 7166:0.0370568 7167:0.0910164 7168:0.0572974 7169:0 7170:0 7171:0 7172:0 7173:0 7174:0 7175:0 7176:0 7177:0 7178:0 7179:0 7180:0 7181:0 7182:0 7183:0 7184:0 7185:0 7186:0 7187:0 7188:0 7189:0 7190:0 7191:0 7192:0 7193:0 7194:0 7195:0 7196:0 7197:0 7198:0 7199:0 7200:0 7201:0 7202:0 7203:0 7204:0 7205:0.0125439 7206:0 7207:0 7208:0 7209:0 7210:0 7211:0 7212:0 7213:0 7214:0 7215:0 7216:0 7217:0 7218:0 7219:0.00762457 7220:0 7221:0 7222:0 7223:0 7224:0 7225:0.0145041 7226:0.011618 7227:0.0188236 7228:0.0203963 7229:0.016163 7230:0.0106284 7231:0.0230094 7232:0.0432165 7233:0.0329048 7234:0.0509118 7235:0.0498962 7236:0.0563022 7237:0.0527757 7238:0.0611532 7239:0.0688147 7240:0.0557249 7241:0.0411643 7242:0.0561054 7243:0.0495332 7244:0.0466088 7245:0.0576563 7246:0.0745219 7247:0.0558074 7248:0.0313449 7249:0.0611684 7250:0.0610971 7251:0.0439171 7252:0.04614 7253:0.0851581 7254:0.0415812 7255:0.0724037 7256:0.0490943 7257:0.0605745 7258:0.0577211 7259:0.0976171 7260:0.173203 7261:0.163688 7262:0.132272 7263:0.0661016 7264:0.0518631 7265:0.0644648 7266:0.113053 7267:0.107777 7268:0.13152 7269:0.126039 7270:0.13052 7271:0.108567 7272:0.047188 7273:0.0642995 7274:0.0911841 7275:0.123827 7276:0.0912457 7277:0.0680876 7278:0.0983246 7279:0.0720719 7280:0.062708 7281:0.0481832 7282:0.0683355 7283:0.0881595 7284:0.0950771 7285:0.0652919 7286:0.0719722 7287:0.0676363 7288:0.0920857 7289:0.0769629 7290:0.0762379 7291:0.0707617 7292:0.071612 7293:0.0725873 7294:0.0591199 7295:0.0647793 7296:0.0735641 7297:0.0256018 7298:0.00267041 7299:0.0108636 7300:0.015526 7301:0.0134894 7302:0.00666984 7303:0.0112562 7304:0.0375407 7305:0 7306:0 7307:0 7308:0 7309:0 7310:0 7311:0 7312:0 7313:0 7314:0 7315:0 7316:0 7317:0 7318:0 7319:0 7320:0 7321:0 7322:0 7323:0 7324:0 7325:0 7326:0 7327:0 7328:0 7329:0 7330:0 7331:0 7332:0 7333:0 7334:0 7335:0 7336:0 7337:0 7338:0 7339:0 7340:0 7341:0 7342:0 7343:0 7344:0 7345:0 7346:0 7347:0 7348:0 7349:0 7350:0 7351:0 7352:0 7353:0 7354:0 7355:0 7356:0 7357:0 7358:0 7359:0 7360:0.0364884 7361:0.0585765 7362:0.0495106 7363:0.0487567 7364:0.0522736 7365:0.0493855 7366:0.0549632 7367:0.0564666 7368:0.0695815 7369:0.0422245 7370:0.0339355 7371:0.0380561 7372:0.0466245 7373:0.0400576 7374:0.036382 7375:0.0329515 7376:0.0445306 7377:0.0352737 7378:0.0314139 7379:0.039871 7380:0.0365906 7381:0.0439016 7382:0.0234709 7383:0.0212588 7384:0.0437126 7385:0.0280555 7386:0.0382161 7387:0.0275012 7388:0.0162833 7389:0.0316947 7390:0.0258557 7391:0.0276422 7392:0.0423763 7393:0.0335094 7394:0.0391376 7395:0.0482766 7396:0.0615982 7397:0.0389371 7398:0.0390024 7399:0.0426919 7400:0.0405702 7401:0.0365788 7402:0.0513731 7403:0.0319805 7404:0.0442605 7405:0.0493956 7406:0.0626964 7407:0.0261681 7408:0.054876 7409:0.0488566 7410:0.0355538 7411:0.0515539 7412:0.0491946 7413:0.0331747 7414:0.0335202 7415:0.0467171 7416:0.0638407 7417:0.120851 7418:0.113412 7419:0.103978 7420:0.0952667 7421:0.0943812 7422:0.0969225 7423:0.108829 7424:0.102584 7425:0.101274 7426:0.122363 7427:0.108934 7428:0.109583 7429:0.113331 7430:0.112152 7431:0.130454 7432:0.113665 7433:0.10925 7434:0.0933226 7435:0.0918728 7436:0.0874539 7437:0.0974862 7438:0.100346 7439:0.082911 7440:0.0981232 7441:0.136698 7442:0.0834908 7443:0.086715 7444:0.0829127 7445:0.101455 7446:0.101853 7447:0.0840428 7448:0.100627 7449:0.150247 7450:0.0864519 7451:0.106812 7452:0.119147 7453:0.100441 7454:0.0858966 7455:0.0904418 7456:0.100476 7457:0.140248 7458:0.110538 7459:0.122043 7460:0.177912 7461:0.145732 7462:0.155945 7463:0.122527 7464:0.110074 7465:0.139851 7466:0.183016 7467:0.135806 7468:0.119384 7469:0.112486 7470:0.15958 7471:0.0995535 7472:0.112931 7473:0.159786 7474:0.111291 7475:0.123272 7476:0.117882 7477:0.0892305 7478:0.0981636 7479:0.0930899 7480:0.117496 7481:0.157633 7482:0.136872 7483:0.103443 7484:0.0989781 7485:0.104692 7486:0.108233 7487:0.101547 7488:0.115559 7489:0.0640264 7490:0.0844032 7491:0.0870282 7492:0.0762001 7493:0.0854431 7494:0.0889789 7495:0.0952302 7496:0.0654409 7497:0.0857874 7498:0.0829328 7499:0.0794751 7500:0.095652 7501:0.1052 7502:0.102932 7503:0.0912498 7504:0.0817147 7505:0.0998465 7506:0.0825242 7507:0.101736 7508:0.128848 7509:0.143147 7510:0.112854 7511:0.104745 7512:0.0773799 7513:0.10097 7514:0.122524 7515:0.133416 7516:0.146999 7517:0.152759 7518:0.1493 7519:0.118916 7520:0.0925701 7521:0.107588 7522:0.124541 7523:0.129689 7524:0.129471 7525:0.129073 7526:0.15579 7527:0.109707 7528:0.096715 7529:0.10781 7530:0.122526 7531:0.157062 7532:0.134473 7533:0.104532 7534:0.114091 7535:0.100302 7536:0.101499 7537:0.126849 7538:0.109789 7539:0.104744 7540:0.109726 7541:0.100417 7542:0.0918645 7543:0.101546 7544:0.0575392 7545:0.126822 7546:0.115565 7547:0.100624 7548:0.100856 7549:0.0983176 7550:0.0931435 7551:0.118309 7552:0.0711283 7553:0.0860106 7554:0.108359 7555:0.116019 7556:0.12185 7557:0.116844 7558:0.122309 7559:0.129136 7560:0.11625 7561:0.121488 7562:0.118157 7563:0.124754 7564:0.0994906 7565:0.146177 7566:0.127133 7567:0.126415 7568:0.102321 7569:0.142404 7570:0.128629 7571:0.13836 7572:0.161077 7573:0.164665 7574:0.15073 7575:0.138533 7576:0.10634 7577:0.150468 7578:0.123779 7579:0.173636 7580:0.191698 7581:0.192057 7582:0.181786 7583:0.185283 7584:0.124695 7585:0.135244 7586:0.136096 7587:0.204616 7588:0.2 7589:0.166897 7590:0.185856 7591:0.179589 7592:0.123658 7593:0.152281 7594:0.159864 7595:0.171753 7596:0.190383 7597:0.156271 7598:0.139612 7599:0.144881 7600:0.127714 7601:0.17857 7602:0.156534 7603:0.145588 7604:0.156069 7605:0.138345 7606:0.140629 7607:0.135825 7608:0.162063 7609:0.187301 7610:0.180969 7611:0.169399 7612:0.149296 7613:0.144968 7614:0.141909 7615:0.116954 7616:0.15331 7617:0 7618:0 7619:0 7620:0 7621:0 7622:0 7623:0 7624:0 7625:0 7626:0 7627:0 7628:0 7629:0 7630:0 7631:0 7632:0.0174203 7633:0.00285869 7634:0 7635:0 7636:0 7637:0.00177727 7638:0.0103153 7639:0 7640:0.0251736 7641:0.000743201 7642:0 7643:0.00825046 7644:0.0344923 7645:0.023022 7646:0.0365986 7647:0.0152688 7648:0.0248587 7649:0 7650:0.0115317 7651:0.0464272 7652:0 7653:0 7654:0 7655:0 7656:0.0253093 7657:0 7658:0 7659:0 7660:0 7661:0 7662:0 7663:0 7664:0.0361873 7665:0.0142127 7666:0 7667:0 7668:0 7669:0 7670:0 7671:0.00594307 7672:0.0394268 7673:0 7674:0 7675:0 7676:0 7677:0 7678:0 7679:0 7680:0 7681:0.0312478 7682:0.00054287 7683:0.00326953 7684:0 7685:0 7686:0.00516529 7687:0.00116584 7688:0.00163588 7689:0.025567 7690:0 7691:0 7692:0.00516693 7693:0 7694:0 7695:0 7696:0 7697:0.0219663 7698:0 7699:0.000841346 7700:0.0113652 7701:0.00903534 7702:0.00611137 7703:0.0113716 7704:0 7705:0.0232228 7706:0.0179129 7707:0.00866083 7708:0.0203433 7709:0.0234703 7710:0.037822 7711:0.016235 7712:0 7713:0.0277601 7714:0.0157737 7715:0.0436299 7716:0.0093427 7717:0 7718:0 7719:0.00665127 7720:0 7721:0.0282361 7722:0 7723:0.000235634 7724:0 7725:0 7726:0 7727:0 7728:0 7729:0.0237757 7730:0 7731:0 7732:0 7733:0 7734:0 7735:0 7736:0.0149004 7737:0.0486713 7738:0.025048 7739:0.0278972 7740:0.0281603 7741:0.0246323 7742:0.0234594 7743:0.0226935 7744:0.034173 7745:0.0244967 7746:0.0288884 7747:0.0302875 7748:0.0330142 7749:0.0305393 7750:0.0336759 7751:0.0310572 7752:0.029703 7753:0.0159088 7754:0 7755:0.00115781 7756:0.00422623 7757:0 7758:0 7759:0.000266802 7760:0.00700462 7761:0.0210127 7762:0 7763:0.0260558 7764:0.0210296 7765:0 7766:0 7767:0 7768:0.00837935 7769:0.0158616 7770:0 7771:0 7772:0.01216 7773:0 7774:0.00220534 7775:0 7776:0 7777:0.0144388 7778:0 7779:0 7780:0 7781:0 7782:0 7783:0 7784:0 7785:0.0173849 7786:0 7787:0 7788:0 7789:0 7790:0 7791:0 7792:0 7793:0.0239717 7794:0 7795:0 7796:0 7797:0 7798:0 7799:0.00443114 7800:0 7801:0.0668592 7802:0.0217263 7803:0.00966828 7804:0.0115061 7805:0.00745703 7806:0 7807:0 7808:0.0309497 7809:0 7810:0 7811:0 7812:0 7813:0 7814:0 7815:0 7816:0 7817:0 7818:0 7819:0 7820:0 7821:0.0105353 7822:0 7823:0 7824:0 7825:0 7826:0 7827:0 7828:0 7829:0 7830:0 7831:0 7832:0 7833:0 7834:0 7835:0 7836:0 7837:0 7838:0 7839:0 7840:0 7841:0 7842:0 7843:0 7844:0 7845:0 7846:0 7847:0 7848:0 7849:0 7850:0 7851:0 7852:0 7853:0 7854:0 7855:0 7856:0 7857:0 7858:0 7859:0 7860:0 7861:0 7862:0 7863:0 7864:0 7865:0 7866:0 7867:0 7868:0 7869:0 7870:0 7871:0 7872:0.00712376 7873:0.0814956 7874:0.0801955 7875:0.0805611 7876:0.0829838 7877:0.0899436 7878:0.0896921 7879:0.0876846 7880:0.074279 7881:0.0968845 7882:0.0668828 7883:0.0589459 7884:0.0889655 7885:0.0602843 7886:0.0826111 7887:0.066556 7888:0.0510983 7889:0.130685 7890:0.0746939 7891:0.0813016 7892:0.0957391 7893:0.0944223 7894:0.0637777 7895:0.0680998 7896:0.0624589 7897:0.140432 7898:0.103587 7899:0.147445 7900:0.160415 7901:0.144468 7902:0.16414 7903:0.0632784 7904:0.068272 7905:0.136085 7906:0.152041 7907:0.146225 7908:0.170957 7909:0.179163 7910:0.182896 7911:0.087621 7912:0.0720821 7913:0.130705 7914:0.165467 7915:0.158807 7916:0.104855 7917:0.10693 7918:0.131992 7919:0.0821134 7920:0.0858286 7921:0.148893 7922:0.112878 7923:0.119978 7924:0.1025 7925:0.0773332 7926:0.077678 7927:0.11665 7928:0.0820446 7929:0.183802 7930:0.114074 7931:0.10986 7932:0.0970772 7933:0.0968722 7934:0.0906801 7935:0.116094 7936:0.103087 7937:0 7938:0 7939:0 7940:0 7941:0 7942:0 7943:0 7944:0 7945:0 7946:0 7947:0 7948:0 7949:0 7950:0 7951:0 7952:0 7953:0 7954:0 7955:0 7956:0 7957:0 7958:0 7959:0 7960:0 7961:0 7962:0 7963:0 7964:0 7965:0 7966:0 7967:0 7968:0 7969:0 7970:0 7971:0 7972:0 7973:0 7974:0 7975:0 7976:0 7977:0 7978:0 7979:0 7980:0 7981:0 7982:0 7983:0 7984:0 7985:0 7986:0 7987:0 7988:0 7989:0 7990:0 7991:0 7992:0 7993:0 7994:0 7995:0 7996:0 7997:0 7998:0 7999:0 8000:0.00225578 8001:0.0830583 8002:0.0809764 8003:0.0796697 8004:0.0772056 8005:0.0738629 8006:0.0826528 8007:0.0744401 8008:0.0734024 8009:0.0926571 8010:0.0831302 8011:0.076372 8012:0.124572 8013:0.0867671 8014:0.10179 8015:0.0969717 8016:0.0983737 8017:0.0784903 8018:0.0810742 8019:0.120521 8020:0.120203 8021:0.0888101 8022:0.0887092 8023:0.0870354 8024:0.0819701 8025:0.0714107 8026:0.13784 8027:0.115788 8028:0.0865704 8029:0.0572596 8030:0.0765627 8031:0.0584802 8032:0.0905845 8033:0.085927 8034:0.117671 8035:0.0725961 8036:0.0891586 8037:0.106769 8038:0.112939 8039:0.0926663 8040:0.0925265 8041:0.0814346 8042:0.113395 8043:0.123986 8044:0.112259 8045:0.086051 8046:0.109651 8047:0.125465 8048:0.121448 8049:0.0888815 8050:0.0937725 8051:0.105545 8052:0.0826612 8053:0.090274 8054:0.0996649 8055:0.0853714 8056:0.0977995 8057:0.078125 8058:0.07413 8059:0.075358 8060:0.0759051 8061:0.0699739 8062:0.08143 8063:0.114882 8064:0.0971311 8065:0.0704812 8066:0.0762108 8067:0.0655289 8068:0.0627461 8069:0.0643374 8070:0.0773782 8071:0.0747798 8072:0.0558111 8073:0.0821469 8074:0.0685799 8075:0.0550284 8076:0.0530085 8077:0.0839283 8078:0.0739084 8079:0.0685387 8080:0.0386265 8081:0.10315 8082:0.0595092 8083:0.0777971 8084:0.102075 8085:0.111613 8086:0.0672482 8087:0.0636792 8088:0.0606139 8089:0.10311 8090:0.0736006 8091:0.146452 8092:0.126409 8093:0.0950832 8094:0.090175 8095:0.0536647 8096:0.0548048 8097:0.100445 8098:0.10945 8099:0.110277 8100:0.132547 8101:0.117112 8102:0.144391 8103:0.0666493 8104:0.0548325 8105:0.0929115 8106:0.161154 8107:0.138369 8108:0.117292 8109:0.0917148 8110:0.115867 8111:0.0737925 8112:0.0697871 8113:0.126 8114:0.0858901 8115:0.0933141 8116:0.0953632 8117:0.0748877 8118:0.0803385 8119:0.0771285 8120:0.0770303 8121:0.129319 8122:0.0606467 8123:0.0462899 8124:0.0530142 8125:0.0542364 8126:0.0515843 8127:0.0542745 8128:0.102818 8129:0.0417721 8130:0.0444605 8131:0.0482766 8132:0.047739 8133:0.0485101 8134:0.0472288 8135:0.0565546 8136:0.0510701 8137:0.026578 8138:0 8139:0 8140:0 8141:0 8142:0 8143:0 8144:0 8145:0.0419238 8146:0 8147:0 8148:0.00825878 8149:0 8150:0.00575138 8151:0 8152:0 8153:0.0454889 8154:0 8155:0.0333303 8156:0.0558778 8157:0.0292891 8158:0.0185456 8159:0.00238411 8160:0 8161:0.0323009 8162:0.0817086 8163:0.0571009 8164:0.0735744 8165:0.0533863 8166:0.0562964 8167:0 8168:0 8169:0.0262979 8170:0.0727769 8171:0.0288682 8172:0.00271337 8173:0.00793839 8174:0.0444693 8175:0 8176:0 8177:0.0383585 8178:0.00399247 8179:0.0257712 8180:0 8181:0 8182:0 8183:0 8184:0 8185:0.0144282 8186:0 8187:0.00559719 8188:0.00832054 8189:0.00935875 8190:0.00882936 8191:0.0182082 8192:0.0227297\n"]}]}]}