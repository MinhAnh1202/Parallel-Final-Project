{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO0yFWBwOAgtbrO9GDTQVvb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Demo GPU optimazation version 2"],"metadata":{"id":"ON9j-BYXIEFF"}},{"cell_type":"markdown","source":["## 1) Huấn luyện Autoencoder"],"metadata":{"id":"-kRTQdeyIECK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGAwRD5mviAz","executionInfo":{"status":"ok","timestamp":1766672858577,"user_tz":-420,"elapsed":12,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"e0e0ed95-9040-4ef6-d95f-45405089d021"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.h\n"]}],"source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"]},{"cell_type":"code","source":["%%writefile load_data.cu\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","    // for (int i = 0; i < 2; i++) {\n","    //     printf(\"Label: %d\\n\", data->test_labels[i]);\n","    //     for (int j = 0; j < IMG_SIZE; j++) {\n","    //         printf(\"%f \", data->test_images[i*IMG_SIZE + j]);\n","    //     }\n","    // }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ieT5YXa_4tQB","executionInfo":{"status":"ok","timestamp":1766672858594,"user_tz":-420,"elapsed":8,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"f6aa6b82-340b-410e-b124-b28124dee764"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt2.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward_opt2(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight_opt2(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXBMdEbi41O5","executionInfo":{"status":"ok","timestamp":1766672858605,"user_tz":-420,"elapsed":10,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"546901e5-14fb-4ac3-df63-2938bad915d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt2.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt2.cu\n","// ============================================================================\n","// OPTIMIZED CONV2D - FIX CRITICAL ISSUES\n","// Target: 12.83ms → 7-8ms (forward), similar for backward\n","// ============================================================================\n","\n","#include <cuda_runtime.h>\n","#include \"gpu_layers_opt2.h\"\n","\n","\n","// Optimal configurations\n","#define TILE_W 16\n","#define TILE_H 16\n","#define K 3\n","\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","// ============================================================================\n","// FORWARD PASS - FIXED\n","// Key fix: Efficient cooperative loading\n","// ============================================================================\n","__global__ void conv2d_forward_opt2(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","    int tid = ty * blockDim.x + tx;\n","    int num_threads = blockDim.x * blockDim.y;  // 256\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    // Base position for loading (với padding)\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    // Loop over input channels\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // ✅ OPTIMIZED: Cooperative loading\n","        // Total elements = 18x18 = 324\n","        // 256 threads → mỗi thread load 1-2 elements\n","        int total_elements = BLOCK_H * BLOCK_W;\n","\n","        for (int idx = tid; idx < total_elements; idx += num_threads) {\n","            int i = idx / BLOCK_W;  // row\n","            int j = idx % BLOCK_W;  // col\n","\n","            int h_in = row_start + i;\n","            int w_in = col_start + j;\n","\n","            // Load with bounds checking\n","            if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                smem[i][j] = input[idx4(n, c_in, h_in, w_in, C_in, H, W)];\n","            } else {\n","                smem[i][j] = 0.0f;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // ✅ OPTIMIZED: Compute convolution with unrolling\n","        if (h_out < H_out && w_out < W_out && n < N) {\n","            int smem_row = ty * stride;\n","            int smem_col = tx * stride;\n","\n","            #pragma unroll\n","            for (int i = 0; i < K; ++i) {\n","                #pragma unroll\n","                for (int j = 0; j < K; ++j) {\n","                    float in_val = smem[smem_row + i][smem_col + j];\n","                    float w_val = weight[idx4(c_out, c_in, i, j, C_in, K, K)];\n","                    value += in_val * w_val;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    // Write output with bias from constant memory\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","// ============================================================================\n","// BACKWARD INPUT - FIXED\n","// Key fix: Parallel halo loading\n","// ============================================================================\n","\n","__global__ void conv2d_backward_input_opt2(\n","    float* __restrict__ dY,     // [N, C_out, H, W]\n","    float* __restrict__ weight, // [C_out, C_in, K, K]\n","    float* __restrict__ dX,     // [N, C_in, H, W]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    // Only support K=3, pad=1, stride=1\n","    if (stride != 1 || pad != 1 || K != 3) return;\n","\n","    const int H_out = H;\n","    const int W_out = W;\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int base_h = blockIdx.y * TILE_H;\n","    int base_w = blockIdx.x * TILE_W;\n","\n","    int h_in = base_h + ty;\n","    int w_in = base_w + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n    = nc / C_in;\n","\n","    if (n >= N || c_in >= C_in) return;\n","\n","    // Shared memory for tile + halo\n","    __shared__ float s_dY[TILE_H + 2][TILE_W + 2];\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // ---- Load dY tile (n, c_out, :, :) into shared memory ----\n","        for (int sy = ty; sy < TILE_H + 2; sy += blockDim.y) {\n","            for (int sx = tx; sx < TILE_W + 2; sx += blockDim.x) {\n","                int ho = base_h + sy - 1;  // -pad\n","                int wo = base_w + sx - 1;  // -pad\n","\n","                float v = 0.0f;\n","                if (ho >= 0 && ho < H_out && wo >= 0 && wo < W_out) {\n","                    v = dY[idx4(n, c_out, ho, wo, C_out, H_out, W_out)];\n","                }\n","                s_dY[sy][sx] = v;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // ---- Compute gradient for (n, c_in, h_in, w_in) ----\n","        if (h_in < H && w_in < W) {\n","            #pragma unroll\n","            for (int kh = 0; kh < K; ++kh) {\n","                #pragma unroll\n","                for (int kw = 0; kw < K; ++kw) {\n","                    // Shared memory indices\n","                    int sy = ty + 2 - kh;\n","                    int sx = tx + 2 - kw;\n","\n","                    float dy = s_dY[sy][sx];\n","\n","                    // CRITICAL FIX: Flip the kernel indices\n","                    int kh_flip = K - 1 - kh;  // 2, 1, 0\n","                    int kw_flip = K - 1 - kw;  // 2, 1, 0\n","\n","                    // Weight layout [C_out, C_in, K, K]\n","                    float w = weight[idx4(c_out, c_in, kh_flip, kw_flip, C_in, K, K)];\n","                    value += dy * w;\n","                }\n","            }\n","        }\n","\n","        __syncthreads();\n","    }\n","\n","    if (h_in < H && w_in < W) {\n","        dX[idx4(n, c_in, h_in, w_in, C_in, H, W)] = value;\n","    }\n","}\n","// ============================================================================\n","// BACKWARD WEIGHT - FIXED\n","// Key fix: Block-level reduction BEFORE atomic\n","// ============================================================================\n","__global__ void conv2d_backward_weight_opt2(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","    __shared__ float s_dw[K * K][256];  // Reduction buffer\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","    int tid = ty * blockDim.x + tx;\n","    int num_threads = blockDim.x * blockDim.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    // Local accumulator\n","    float dw[K][K];\n","    #pragma unroll\n","    for (int i = 0; i < K; ++i)\n","        #pragma unroll\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // ✅ FIX: Cooperative loading cho dY\n","                int h_out = block_h * TILE_H;\n","                int w_out = block_w * TILE_W;\n","\n","                for (int idx = tid; idx < TILE_H * TILE_W; idx += num_threads) {\n","                    int i = idx / TILE_W;\n","                    int j = idx % TILE_W;\n","                    int h = h_out + i;\n","                    int w = w_out + j;\n","\n","                    if (h < H_out && w < W_out) {\n","                        s_dY[i][j] = dY[idx4(n, c_out, h, w, C_out, H_out, W_out)];\n","                    } else {\n","                        s_dY[i][j] = 0.0f;\n","                    }\n","                }\n","\n","                // ✅ FIX: Cooperative loading cho input với halo\n","                int h_in_base = block_h * TILE_H - pad;\n","                int w_in_base = block_w * TILE_W - pad;\n","\n","                for (int idx = tid; idx < BLOCK_H * BLOCK_W; idx += num_threads) {\n","                    int i = idx / BLOCK_W;\n","                    int j = idx % BLOCK_W;\n","                    int h = h_in_base + i;\n","                    int w = w_in_base + j;\n","\n","                    if (h >= 0 && h < H && w >= 0 && w < W) {\n","                        s_in[i][j] = input[idx4(n, c_in, h, w, C_in, H, W)];\n","                    } else {\n","                        s_in[i][j] = 0.0f;\n","                    }\n","                }\n","                __syncthreads();\n","\n","                // Compute local dW\n","                if (ty < TILE_H && tx < TILE_W) {\n","                    float val_dy = s_dY[ty][tx];\n","                    #pragma unroll\n","                    for (int kh = 0; kh < K; ++kh) {\n","                        #pragma unroll\n","                        for (int kw = 0; kw < K; ++kw) {\n","                            dw[kh][kw] += s_in[ty * stride + kh][tx * stride + kw] * val_dy;\n","                        }\n","                    }\n","                }\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    // ✅ FIX: Block-level reduction TRƯỚC atomic\n","    // Mỗi kernel element có reduction riêng\n","    #pragma unroll\n","    for (int kh = 0; kh < K; ++kh) {\n","        #pragma unroll\n","        for (int kw = 0; kw < K; ++kw) {\n","            int k_idx = kh * K + kw;\n","\n","            // Store to shared memory\n","            s_dw[k_idx][tid] = dw[kh][kw];\n","            __syncthreads();\n","\n","            // Tree reduction\n","            for (int s = num_threads / 2; s > 0; s >>= 1) {\n","                if (tid < s) {\n","                    s_dw[k_idx][tid] += s_dw[k_idx][tid + s];\n","                }\n","                __syncthreads();\n","            }\n","\n","            // ONLY thread 0 does atomic\n","            if (tid == 0) {\n","                size_t dw_idx = idx4(c_out, c_in, kh, kw, C_in, K, K);\n","                atomicAdd(&dW[dw_idx], s_dw[k_idx][0]);\n","            }\n","        }\n","    }\n","}\n","\n","// ============================================================================\n","// BACKWARD BIAS - OPTIMIZED (warp-level reduction)\n","// ============================================================================\n","__global__ void conv2d_backward_bias_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","\n","    int tid = threadIdx.x;\n","    int lane = tid % 32;\n","\n","    float sum = 0.0f;\n","\n","    // Grid-stride loop\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","\n","    // Warp-level reduction\n","    #pragma unroll\n","    for (int offset = 16; offset > 0; offset >>= 1) {\n","        sum += __shfl_down_sync(0xffffffff, sum, offset);\n","    }\n","\n","    // First thread in each warp writes to shared memory\n","    __shared__ float warp_sums[32];\n","    int warp_id = tid / 32;\n","\n","    if (lane == 0) {\n","        warp_sums[warp_id] = sum;\n","    }\n","    __syncthreads();\n","\n","    // Final reduction by first warp\n","    if (warp_id == 0) {\n","        sum = (lane < (blockDim.x / 32)) ? warp_sums[lane] : 0.0f;\n","\n","        #pragma unroll\n","        for (int offset = 16; offset > 0; offset >>= 1) {\n","            sum += __shfl_down_sync(0xffffffff, sum, offset);\n","        }\n","\n","        if (lane == 0) {\n","            dB[c] = sum;\n","        }\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gbxZSPD44I2","executionInfo":{"status":"ok","timestamp":1766672858747,"user_tz":-420,"elapsed":103,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"0d05a1ca-ea7b-4168-9114-bbc8f5a6f0f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt2.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt2.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers_opt2.h\"\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0; ///\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights, biases ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjqRl4ks45pt","executionInfo":{"status":"ok","timestamp":1766672858754,"user_tz":-420,"elapsed":6,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"3a890494-645e-497d-ed3b-5e7626ab8b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt2.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt2.cu\n","#include \"gpu_autoencoder_opt2.h\"\n","#include <cmath>\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","// find max weight bytes\n","size_t max_w_bytes = w1_bytes;\n","if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","// find max bias bytes\n","size_t max_b_bytes = b1_bytes;\n","if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","float *h_w = (float*)malloc(max_w_bytes);\n","float *h_b = (float*)malloc(max_b_bytes);\n","\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ---------- [batch_size, channels, height, width]\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Copy input to device\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);  // 256 threads per block\n","\n","    // ========= ENCODER =========\n","\n","    // Layer 1: conv1: 3 -> 256, 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        // ✅ Grid configuration cho optimized kernel\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,  // Ceiling division\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        // Update constant memory bias\n","        update_dc_bias(ae->d_b1, C_out);\n","\n","        // ✅ Launch optimized kernel\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // Layer 2: conv2: 256 -> 128, 16x16, pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // Pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT: ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","\n","    // Layer 3: conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b3, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // Upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // Layer 4: conv4: 128 -> 256, 16x16, upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b4, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // Upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // Layer 5: conv5: 256 -> 3, 32x32 (output layer)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b5, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","    }\n","\n","    // ========= COMPUTE LOSS (optional) =========\n","    float loss_value = 0.0f;\n","    if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;\n","    }\n","\n","    // Copy output back to host\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H;\n","    const int W0 = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ✅ OPTIMIZATION: Zero gradients asynchronously in streams\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        // dU2 - ✅ OPTIMIZED KERNEL\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,  // Use TILE_W instead of block2d.x\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW5 - ✅ OPTIMIZED KERNEL\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_u2, ae->d_gout, ae->d_gw5,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB5 - ✅ OPTIMIZED KERNEL\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gout, ae->d_gb5, N, C_out, H, W);\n","\n","        // SGD update\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;\n","        int Hu = 32, Wu = 32;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4, N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int C = 256, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward: 128->256, 16x16 =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_u1, ae->d_gh4, ae->d_gw4,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh4, ae->d_gb4, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;\n","        int Hu = 16, Wu = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3, N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int C = 128, H = 8, W = 8;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward: 128->128, 8x8 =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED (note: cudaMemset removed, already done at start)\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_p2, ae->d_gh3, ae->d_gw3,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh3, ae->d_gb3, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16;\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2, N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int C = 128, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward: 256->128, 16x16 =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_p1, ae->d_gh2, ae->d_gw2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh2, ae->d_gb2, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32;\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1, N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int C = 256, H = 32, W = 32;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward: 3->256, 32x32 =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_x0, ae->d_gh1, ae->d_gw1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh1, ae->d_gb1, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // copy input [N_batch, 3, 32, 32] lên GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, rồi ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b1, 256 * sizeof(float)));\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, rồi ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b2, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","\n","         cudaError_t err = cudaGetLastError();\n","   if (err != cudaSuccess) {\n","       fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n","   }\n","   cudaDeviceSynchronize();\n","    }\n","    cudaDeviceSynchronize();\n","\n","size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6gIv8AT47P8","executionInfo":{"status":"ok","timestamp":1766672858828,"user_tz":-420,"elapsed":73,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"e8ad3e77-175b-4882-fb4d-6276c5404d92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt2.cu\n"]}]},{"cell_type":"code","source":["%%writefile main_gpu.cu\n","// the UPDATED main() code with argc/argv\n","// main_gpu.cu (DEBUG VERSION)\n","#include <cstdio>\n","#include <ctime>\n","#include <cuda_runtime.h>\n","#include <cstdlib>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt2.h\"\n","\n","// GpuTimer dùng cudaEvent để đo time (đúng spec Phase 2.5)\n","struct GpuTimer {\n","    cudaEvent_t start, stop;\n","    GpuTimer() {\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","    }\n","    ~GpuTimer() {\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","    void tic() {\n","        cudaEventRecord(start, 0);\n","    }\n","    float toc() {\n","        cudaEventRecord(stop, 0);\n","        cudaEventSynchronize(stop);\n","        float ms = 0.0f;\n","        cudaEventElapsedTime(&ms, start, stop);\n","        return ms;\n","    }\n","};\n","\n","void print_gpu_memory() {\n","    size_t free_byte, total_byte;\n","    cudaError_t status = cudaMemGetInfo(&free_byte, &total_byte);\n","\n","    if (status == cudaSuccess) {\n","        double total_mb = (double)total_byte / (1024.0 * 1024.0);\n","        double free_mb = (double)free_byte / (1024.0 * 1024.0);\n","        double used_mb = total_mb - free_mb;\n","\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB / Total: %.2f MB\\n\", used_mb, total_mb);\n","    }\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","\n","    printf(\"[MAIN] Start program\\n\");\n","    fflush(stdout);\n","\n","    // ---- Check GPU device ----\n","    int deviceCount = 0;\n","    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n","    if (err != cudaSuccess) {\n","        fprintf(stderr, \"[MAIN] cudaGetDeviceCount error: %s\\n\",\n","                cudaGetErrorString(err));\n","        return 1;\n","    }\n","    printf(\"[MAIN] Num CUDA devices = %d\\n\", deviceCount);\n","    fflush(stdout);\n","\n","    // ---- Load CIFAR-10 on CPU ----\n","    Cifar10 data;\n","    load_cifar10(&data);   // câu lệnh này in: CIFAR-10 loaded successfully ...\n","    printf(\"[MAIN] After load_cifar10\\n\");\n","    fflush(stdout);\n","\n","    normalize_cifar10(&data);\n","    printf(\"[MAIN] After normalize_cifar10\\n\");\n","    fflush(stdout);\n","\n","    GpuTimer epoch_timer;\n","\n","    int batch_size = 64;\n","    int epochs     = 2;\n","    float lr       = 0.001f;\n","    float total_time = 0.0f;\n","\n","    int num_batches = TRAIN_NUM / batch_size;\n","\n","    printf(\"[MAIN] Start training loop (epochs=%d, num_batches=%d)\\n\",\n","       epochs, num_batches);\n","    fflush(stdout);\n","\n","\n","    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float)); // dùng làm buffer tạm\n","\n","    // ---- Init GPU autoencoder ----\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","\n","\n","    for (int epoch = 0; epoch < epochs; ++epoch) {\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","\n","        epoch_timer.tic();\n","\n","        for (int b = 0; b < num_batches; ++b) {\n","            get_next_batch(&data, batch_size, b, h_batch);\n","\n","            float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n","            gpu_autoencoder_backward(&ae, lr);\n","\n","            epoch_loss += loss;\n","\n","            if ((b + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\",\n","                       epoch, b + 1, num_batches, loss);\n","                fflush(stdout);\n","            }\n","        }\n","\n","        float ms = epoch_timer.toc();\n","        total_time += ms;\n","        printf(\"==> Epoch %d done. Avg loss = %f, time = %.3f ms (%.3f s)\\n\",\n","               epoch, epoch_loss / num_batches, ms, ms / 1000.0f);\n","        fflush(stdout);\n","    }\n","\n","    printf(\"[MAIN] Training finished\\n\");\n","    printf(\"[MAIN] Total training time = %.3f s\\n\", total_time / 1000.0f);\n","    print_gpu_memory();\n","    fflush(stdout);\n","\n","    // save weights\n","    gpu_autoencoder_save_weights(&ae, \"ae_weights_gpu_opt2.bin\");\n","\n","    // ---- cleanup ----\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_output);\n","    free_cifar10(&data);\n","\n","    printf(\"[MAIN] Program finished\\n\");\n","    fflush(stdout);\n","\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHf28RHr4_Ow","executionInfo":{"status":"ok","timestamp":1766672858838,"user_tz":-420,"elapsed":9,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"6f6c3c21-baca-4264-d198-765d0f9f3ab1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main_gpu.cu\n"]}]},{"cell_type":"code","source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","!tar -xzvf cifar-10-binary.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9uPq3mS5Ba9","executionInfo":{"status":"ok","timestamp":1766672866344,"user_tz":-420,"elapsed":7505,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"f3c59329-dda8-441a-fe3c-6b5ef0d4a076"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-25 14:27:38--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  34.7MB/s    in 5.0s    \n","\n","2025-12-25 14:27:43 (32.7 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n","\n","cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 main_gpu.cu gpu_autoencoder_opt2.cu gpu_layers_opt2.cu load_data.cu -o autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYMAZ0zj5DIu","executionInfo":{"status":"ok","timestamp":1766672872676,"user_tz":-420,"elapsed":6330,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"2075e567-697a-43cc-eea1-b07cf09dc502"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(462)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StxUE1DR5FDi","executionInfo":{"status":"ok","timestamp":1766673156761,"user_tz":-420,"elapsed":284082,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"7f251f44-3244-42b8-ec0f-807a46c1e9e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=2, num_batches=781)\n","[TRAIN] Epoch 0, batch 100/781, loss = 0.094373\n","[TRAIN] Epoch 0, batch 200/781, loss = 0.061521\n","[TRAIN] Epoch 0, batch 300/781, loss = 0.058019\n","[TRAIN] Epoch 0, batch 400/781, loss = 0.057471\n","[TRAIN] Epoch 0, batch 500/781, loss = 0.056909\n","[TRAIN] Epoch 0, batch 600/781, loss = 0.059528\n","[TRAIN] Epoch 0, batch 700/781, loss = 0.049340\n","==> Epoch 0 done. Avg loss = 0.073496, time = 140048.406 ms (140.048 s)\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.047542\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.044592\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.045123\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.045073\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.048167\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.044558\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.046562\n","==> Epoch 1 done. Avg loss = 0.046386, time = 142994.641 ms (142.995 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 283.043 s\n","[SYSTEM] Memory Usage: 474.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights_gpu_opt2.bin\n","[MAIN] Program finished\n"]}]},{"cell_type":"markdown","source":["## 2) Trích xuất đặc trưng"],"metadata":{"id":"zJ1UIAefIPFH"}},{"cell_type":"code","source":["%%writefile extract_svm_features.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cstring>\n","#include <cuda_runtime.h>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt2.h\"\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    if (argc < 2) {\n","        fprintf(stderr,\n","                \"Usage: %s <ae_weights.bin>\\n\",\n","                argv[0]);\n","        return 1;\n","    }\n","    const char* weight_file = argv[1];\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","    gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = TRAIN_NUM; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = TEST_NUM; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9nVATOtn5GtL","executionInfo":{"status":"ok","timestamp":1766673156771,"user_tz":-420,"elapsed":7,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"e107c966-5997-4807-dc1b-d49a0acfe0b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing extract_svm_features.cu\n"]}]},{"cell_type":"code","source":["!nvcc  -arch=sm_75 -O2 -o extract_svm_features \\\n","    extract_svm_features.cu gpu_autoencoder_opt2.cu gpu_layers_opt2.cu load_data.cu \\\n","    -lcudart"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WInuObDx5kl5","executionInfo":{"status":"ok","timestamp":1766673162303,"user_tz":-420,"elapsed":5531,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"f8328bef-b200-4900-87f1-d2b21bd47f60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(462)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./extract_svm_features ae_weights_gpu_opt2.bin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rr6iJE2j5mNL","executionInfo":{"status":"ok","timestamp":1766673381070,"user_tz":-420,"elapsed":218765,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"7c6c18ac-75a0-4be7-dbd7-4ed6f5809dc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","Loaded weights from ae_weights_gpu_opt2.bin\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/782 done\n","[SVM][TRAIN] Batch 2/782 done\n","[SVM][TRAIN] Batch 3/782 done\n","[SVM][TRAIN] Batch 4/782 done\n","[SVM][TRAIN] Batch 5/782 done\n","[SVM][TRAIN] Batch 6/782 done\n","[SVM][TRAIN] Batch 7/782 done\n","[SVM][TRAIN] Batch 8/782 done\n","[SVM][TRAIN] Batch 9/782 done\n","[SVM][TRAIN] Batch 10/782 done\n","[SVM][TRAIN] Batch 11/782 done\n","[SVM][TRAIN] Batch 12/782 done\n","[SVM][TRAIN] Batch 13/782 done\n","[SVM][TRAIN] Batch 14/782 done\n","[SVM][TRAIN] Batch 15/782 done\n","[SVM][TRAIN] Batch 16/782 done\n","[SVM][TRAIN] Batch 17/782 done\n","[SVM][TRAIN] Batch 18/782 done\n","[SVM][TRAIN] Batch 19/782 done\n","[SVM][TRAIN] Batch 20/782 done\n","[SVM][TRAIN] Batch 21/782 done\n","[SVM][TRAIN] Batch 22/782 done\n","[SVM][TRAIN] Batch 23/782 done\n","[SVM][TRAIN] Batch 24/782 done\n","[SVM][TRAIN] Batch 25/782 done\n","[SVM][TRAIN] Batch 26/782 done\n","[SVM][TRAIN] Batch 27/782 done\n","[SVM][TRAIN] Batch 28/782 done\n","[SVM][TRAIN] Batch 29/782 done\n","[SVM][TRAIN] Batch 30/782 done\n","[SVM][TRAIN] Batch 31/782 done\n","[SVM][TRAIN] Batch 32/782 done\n","[SVM][TRAIN] Batch 33/782 done\n","[SVM][TRAIN] Batch 34/782 done\n","[SVM][TRAIN] Batch 35/782 done\n","[SVM][TRAIN] Batch 36/782 done\n","[SVM][TRAIN] Batch 37/782 done\n","[SVM][TRAIN] Batch 38/782 done\n","[SVM][TRAIN] Batch 39/782 done\n","[SVM][TRAIN] Batch 40/782 done\n","[SVM][TRAIN] Batch 41/782 done\n","[SVM][TRAIN] Batch 42/782 done\n","[SVM][TRAIN] Batch 43/782 done\n","[SVM][TRAIN] Batch 44/782 done\n","[SVM][TRAIN] Batch 45/782 done\n","[SVM][TRAIN] Batch 46/782 done\n","[SVM][TRAIN] Batch 47/782 done\n","[SVM][TRAIN] Batch 48/782 done\n","[SVM][TRAIN] Batch 49/782 done\n","[SVM][TRAIN] Batch 50/782 done\n","[SVM][TRAIN] Batch 51/782 done\n","[SVM][TRAIN] Batch 52/782 done\n","[SVM][TRAIN] Batch 53/782 done\n","[SVM][TRAIN] Batch 54/782 done\n","[SVM][TRAIN] Batch 55/782 done\n","[SVM][TRAIN] Batch 56/782 done\n","[SVM][TRAIN] Batch 57/782 done\n","[SVM][TRAIN] Batch 58/782 done\n","[SVM][TRAIN] Batch 59/782 done\n","[SVM][TRAIN] Batch 60/782 done\n","[SVM][TRAIN] Batch 61/782 done\n","[SVM][TRAIN] Batch 62/782 done\n","[SVM][TRAIN] Batch 63/782 done\n","[SVM][TRAIN] Batch 64/782 done\n","[SVM][TRAIN] Batch 65/782 done\n","[SVM][TRAIN] Batch 66/782 done\n","[SVM][TRAIN] Batch 67/782 done\n","[SVM][TRAIN] Batch 68/782 done\n","[SVM][TRAIN] Batch 69/782 done\n","[SVM][TRAIN] Batch 70/782 done\n","[SVM][TRAIN] Batch 71/782 done\n","[SVM][TRAIN] Batch 72/782 done\n","[SVM][TRAIN] Batch 73/782 done\n","[SVM][TRAIN] Batch 74/782 done\n","[SVM][TRAIN] Batch 75/782 done\n","[SVM][TRAIN] Batch 76/782 done\n","[SVM][TRAIN] Batch 77/782 done\n","[SVM][TRAIN] Batch 78/782 done\n","[SVM][TRAIN] Batch 79/782 done\n","[SVM][TRAIN] Batch 80/782 done\n","[SVM][TRAIN] Batch 81/782 done\n","[SVM][TRAIN] Batch 82/782 done\n","[SVM][TRAIN] Batch 83/782 done\n","[SVM][TRAIN] Batch 84/782 done\n","[SVM][TRAIN] Batch 85/782 done\n","[SVM][TRAIN] Batch 86/782 done\n","[SVM][TRAIN] Batch 87/782 done\n","[SVM][TRAIN] Batch 88/782 done\n","[SVM][TRAIN] Batch 89/782 done\n","[SVM][TRAIN] Batch 90/782 done\n","[SVM][TRAIN] Batch 91/782 done\n","[SVM][TRAIN] Batch 92/782 done\n","[SVM][TRAIN] Batch 93/782 done\n","[SVM][TRAIN] Batch 94/782 done\n","[SVM][TRAIN] Batch 95/782 done\n","[SVM][TRAIN] Batch 96/782 done\n","[SVM][TRAIN] Batch 97/782 done\n","[SVM][TRAIN] Batch 98/782 done\n","[SVM][TRAIN] Batch 99/782 done\n","[SVM][TRAIN] Batch 100/782 done\n","[SVM][TRAIN] Batch 101/782 done\n","[SVM][TRAIN] Batch 102/782 done\n","[SVM][TRAIN] Batch 103/782 done\n","[SVM][TRAIN] Batch 104/782 done\n","[SVM][TRAIN] Batch 105/782 done\n","[SVM][TRAIN] Batch 106/782 done\n","[SVM][TRAIN] Batch 107/782 done\n","[SVM][TRAIN] Batch 108/782 done\n","[SVM][TRAIN] Batch 109/782 done\n","[SVM][TRAIN] Batch 110/782 done\n","[SVM][TRAIN] Batch 111/782 done\n","[SVM][TRAIN] Batch 112/782 done\n","[SVM][TRAIN] Batch 113/782 done\n","[SVM][TRAIN] Batch 114/782 done\n","[SVM][TRAIN] Batch 115/782 done\n","[SVM][TRAIN] Batch 116/782 done\n","[SVM][TRAIN] Batch 117/782 done\n","[SVM][TRAIN] Batch 118/782 done\n","[SVM][TRAIN] Batch 119/782 done\n","[SVM][TRAIN] Batch 120/782 done\n","[SVM][TRAIN] Batch 121/782 done\n","[SVM][TRAIN] Batch 122/782 done\n","[SVM][TRAIN] Batch 123/782 done\n","[SVM][TRAIN] Batch 124/782 done\n","[SVM][TRAIN] Batch 125/782 done\n","[SVM][TRAIN] Batch 126/782 done\n","[SVM][TRAIN] Batch 127/782 done\n","[SVM][TRAIN] Batch 128/782 done\n","[SVM][TRAIN] Batch 129/782 done\n","[SVM][TRAIN] Batch 130/782 done\n","[SVM][TRAIN] Batch 131/782 done\n","[SVM][TRAIN] Batch 132/782 done\n","[SVM][TRAIN] Batch 133/782 done\n","[SVM][TRAIN] Batch 134/782 done\n","[SVM][TRAIN] Batch 135/782 done\n","[SVM][TRAIN] Batch 136/782 done\n","[SVM][TRAIN] Batch 137/782 done\n","[SVM][TRAIN] Batch 138/782 done\n","[SVM][TRAIN] Batch 139/782 done\n","[SVM][TRAIN] Batch 140/782 done\n","[SVM][TRAIN] Batch 141/782 done\n","[SVM][TRAIN] Batch 142/782 done\n","[SVM][TRAIN] Batch 143/782 done\n","[SVM][TRAIN] Batch 144/782 done\n","[SVM][TRAIN] Batch 145/782 done\n","[SVM][TRAIN] Batch 146/782 done\n","[SVM][TRAIN] Batch 147/782 done\n","[SVM][TRAIN] Batch 148/782 done\n","[SVM][TRAIN] Batch 149/782 done\n","[SVM][TRAIN] Batch 150/782 done\n","[SVM][TRAIN] Batch 151/782 done\n","[SVM][TRAIN] Batch 152/782 done\n","[SVM][TRAIN] Batch 153/782 done\n","[SVM][TRAIN] Batch 154/782 done\n","[SVM][TRAIN] Batch 155/782 done\n","[SVM][TRAIN] Batch 156/782 done\n","[SVM][TRAIN] Batch 157/782 done\n","[SVM][TRAIN] Batch 158/782 done\n","[SVM][TRAIN] Batch 159/782 done\n","[SVM][TRAIN] Batch 160/782 done\n","[SVM][TRAIN] Batch 161/782 done\n","[SVM][TRAIN] Batch 162/782 done\n","[SVM][TRAIN] Batch 163/782 done\n","[SVM][TRAIN] Batch 164/782 done\n","[SVM][TRAIN] Batch 165/782 done\n","[SVM][TRAIN] Batch 166/782 done\n","[SVM][TRAIN] Batch 167/782 done\n","[SVM][TRAIN] Batch 168/782 done\n","[SVM][TRAIN] Batch 169/782 done\n","[SVM][TRAIN] Batch 170/782 done\n","[SVM][TRAIN] Batch 171/782 done\n","[SVM][TRAIN] Batch 172/782 done\n","[SVM][TRAIN] Batch 173/782 done\n","[SVM][TRAIN] Batch 174/782 done\n","[SVM][TRAIN] Batch 175/782 done\n","[SVM][TRAIN] Batch 176/782 done\n","[SVM][TRAIN] Batch 177/782 done\n","[SVM][TRAIN] Batch 178/782 done\n","[SVM][TRAIN] Batch 179/782 done\n","[SVM][TRAIN] Batch 180/782 done\n","[SVM][TRAIN] Batch 181/782 done\n","[SVM][TRAIN] Batch 182/782 done\n","[SVM][TRAIN] Batch 183/782 done\n","[SVM][TRAIN] Batch 184/782 done\n","[SVM][TRAIN] Batch 185/782 done\n","[SVM][TRAIN] Batch 186/782 done\n","[SVM][TRAIN] Batch 187/782 done\n","[SVM][TRAIN] Batch 188/782 done\n","[SVM][TRAIN] Batch 189/782 done\n","[SVM][TRAIN] Batch 190/782 done\n","[SVM][TRAIN] Batch 191/782 done\n","[SVM][TRAIN] Batch 192/782 done\n","[SVM][TRAIN] Batch 193/782 done\n","[SVM][TRAIN] Batch 194/782 done\n","[SVM][TRAIN] Batch 195/782 done\n","[SVM][TRAIN] Batch 196/782 done\n","[SVM][TRAIN] Batch 197/782 done\n","[SVM][TRAIN] Batch 198/782 done\n","[SVM][TRAIN] Batch 199/782 done\n","[SVM][TRAIN] Batch 200/782 done\n","[SVM][TRAIN] Batch 201/782 done\n","[SVM][TRAIN] Batch 202/782 done\n","[SVM][TRAIN] Batch 203/782 done\n","[SVM][TRAIN] Batch 204/782 done\n","[SVM][TRAIN] Batch 205/782 done\n","[SVM][TRAIN] Batch 206/782 done\n","[SVM][TRAIN] Batch 207/782 done\n","[SVM][TRAIN] Batch 208/782 done\n","[SVM][TRAIN] Batch 209/782 done\n","[SVM][TRAIN] Batch 210/782 done\n","[SVM][TRAIN] Batch 211/782 done\n","[SVM][TRAIN] Batch 212/782 done\n","[SVM][TRAIN] Batch 213/782 done\n","[SVM][TRAIN] Batch 214/782 done\n","[SVM][TRAIN] Batch 215/782 done\n","[SVM][TRAIN] Batch 216/782 done\n","[SVM][TRAIN] Batch 217/782 done\n","[SVM][TRAIN] Batch 218/782 done\n","[SVM][TRAIN] Batch 219/782 done\n","[SVM][TRAIN] Batch 220/782 done\n","[SVM][TRAIN] Batch 221/782 done\n","[SVM][TRAIN] Batch 222/782 done\n","[SVM][TRAIN] Batch 223/782 done\n","[SVM][TRAIN] Batch 224/782 done\n","[SVM][TRAIN] Batch 225/782 done\n","[SVM][TRAIN] Batch 226/782 done\n","[SVM][TRAIN] Batch 227/782 done\n","[SVM][TRAIN] Batch 228/782 done\n","[SVM][TRAIN] Batch 229/782 done\n","[SVM][TRAIN] Batch 230/782 done\n","[SVM][TRAIN] Batch 231/782 done\n","[SVM][TRAIN] Batch 232/782 done\n","[SVM][TRAIN] Batch 233/782 done\n","[SVM][TRAIN] Batch 234/782 done\n","[SVM][TRAIN] Batch 235/782 done\n","[SVM][TRAIN] Batch 236/782 done\n","[SVM][TRAIN] Batch 237/782 done\n","[SVM][TRAIN] Batch 238/782 done\n","[SVM][TRAIN] Batch 239/782 done\n","[SVM][TRAIN] Batch 240/782 done\n","[SVM][TRAIN] Batch 241/782 done\n","[SVM][TRAIN] Batch 242/782 done\n","[SVM][TRAIN] Batch 243/782 done\n","[SVM][TRAIN] Batch 244/782 done\n","[SVM][TRAIN] Batch 245/782 done\n","[SVM][TRAIN] Batch 246/782 done\n","[SVM][TRAIN] Batch 247/782 done\n","[SVM][TRAIN] Batch 248/782 done\n","[SVM][TRAIN] Batch 249/782 done\n","[SVM][TRAIN] Batch 250/782 done\n","[SVM][TRAIN] Batch 251/782 done\n","[SVM][TRAIN] Batch 252/782 done\n","[SVM][TRAIN] Batch 253/782 done\n","[SVM][TRAIN] Batch 254/782 done\n","[SVM][TRAIN] Batch 255/782 done\n","[SVM][TRAIN] Batch 256/782 done\n","[SVM][TRAIN] Batch 257/782 done\n","[SVM][TRAIN] Batch 258/782 done\n","[SVM][TRAIN] Batch 259/782 done\n","[SVM][TRAIN] Batch 260/782 done\n","[SVM][TRAIN] Batch 261/782 done\n","[SVM][TRAIN] Batch 262/782 done\n","[SVM][TRAIN] Batch 263/782 done\n","[SVM][TRAIN] Batch 264/782 done\n","[SVM][TRAIN] Batch 265/782 done\n","[SVM][TRAIN] Batch 266/782 done\n","[SVM][TRAIN] Batch 267/782 done\n","[SVM][TRAIN] Batch 268/782 done\n","[SVM][TRAIN] Batch 269/782 done\n","[SVM][TRAIN] Batch 270/782 done\n","[SVM][TRAIN] Batch 271/782 done\n","[SVM][TRAIN] Batch 272/782 done\n","[SVM][TRAIN] Batch 273/782 done\n","[SVM][TRAIN] Batch 274/782 done\n","[SVM][TRAIN] Batch 275/782 done\n","[SVM][TRAIN] Batch 276/782 done\n","[SVM][TRAIN] Batch 277/782 done\n","[SVM][TRAIN] Batch 278/782 done\n","[SVM][TRAIN] Batch 279/782 done\n","[SVM][TRAIN] Batch 280/782 done\n","[SVM][TRAIN] Batch 281/782 done\n","[SVM][TRAIN] Batch 282/782 done\n","[SVM][TRAIN] Batch 283/782 done\n","[SVM][TRAIN] Batch 284/782 done\n","[SVM][TRAIN] Batch 285/782 done\n","[SVM][TRAIN] Batch 286/782 done\n","[SVM][TRAIN] Batch 287/782 done\n","[SVM][TRAIN] Batch 288/782 done\n","[SVM][TRAIN] Batch 289/782 done\n","[SVM][TRAIN] Batch 290/782 done\n","[SVM][TRAIN] Batch 291/782 done\n","[SVM][TRAIN] Batch 292/782 done\n","[SVM][TRAIN] Batch 293/782 done\n","[SVM][TRAIN] Batch 294/782 done\n","[SVM][TRAIN] Batch 295/782 done\n","[SVM][TRAIN] Batch 296/782 done\n","[SVM][TRAIN] Batch 297/782 done\n","[SVM][TRAIN] Batch 298/782 done\n","[SVM][TRAIN] Batch 299/782 done\n","[SVM][TRAIN] Batch 300/782 done\n","[SVM][TRAIN] Batch 301/782 done\n","[SVM][TRAIN] Batch 302/782 done\n","[SVM][TRAIN] Batch 303/782 done\n","[SVM][TRAIN] Batch 304/782 done\n","[SVM][TRAIN] Batch 305/782 done\n","[SVM][TRAIN] Batch 306/782 done\n","[SVM][TRAIN] Batch 307/782 done\n","[SVM][TRAIN] Batch 308/782 done\n","[SVM][TRAIN] Batch 309/782 done\n","[SVM][TRAIN] Batch 310/782 done\n","[SVM][TRAIN] Batch 311/782 done\n","[SVM][TRAIN] Batch 312/782 done\n","[SVM][TRAIN] Batch 313/782 done\n","[SVM][TRAIN] Batch 314/782 done\n","[SVM][TRAIN] Batch 315/782 done\n","[SVM][TRAIN] Batch 316/782 done\n","[SVM][TRAIN] Batch 317/782 done\n","[SVM][TRAIN] Batch 318/782 done\n","[SVM][TRAIN] Batch 319/782 done\n","[SVM][TRAIN] Batch 320/782 done\n","[SVM][TRAIN] Batch 321/782 done\n","[SVM][TRAIN] Batch 322/782 done\n","[SVM][TRAIN] Batch 323/782 done\n","[SVM][TRAIN] Batch 324/782 done\n","[SVM][TRAIN] Batch 325/782 done\n","[SVM][TRAIN] Batch 326/782 done\n","[SVM][TRAIN] Batch 327/782 done\n","[SVM][TRAIN] Batch 328/782 done\n","[SVM][TRAIN] Batch 329/782 done\n","[SVM][TRAIN] Batch 330/782 done\n","[SVM][TRAIN] Batch 331/782 done\n","[SVM][TRAIN] Batch 332/782 done\n","[SVM][TRAIN] Batch 333/782 done\n","[SVM][TRAIN] Batch 334/782 done\n","[SVM][TRAIN] Batch 335/782 done\n","[SVM][TRAIN] Batch 336/782 done\n","[SVM][TRAIN] Batch 337/782 done\n","[SVM][TRAIN] Batch 338/782 done\n","[SVM][TRAIN] Batch 339/782 done\n","[SVM][TRAIN] Batch 340/782 done\n","[SVM][TRAIN] Batch 341/782 done\n","[SVM][TRAIN] Batch 342/782 done\n","[SVM][TRAIN] Batch 343/782 done\n","[SVM][TRAIN] Batch 344/782 done\n","[SVM][TRAIN] Batch 345/782 done\n","[SVM][TRAIN] Batch 346/782 done\n","[SVM][TRAIN] Batch 347/782 done\n","[SVM][TRAIN] Batch 348/782 done\n","[SVM][TRAIN] Batch 349/782 done\n","[SVM][TRAIN] Batch 350/782 done\n","[SVM][TRAIN] Batch 351/782 done\n","[SVM][TRAIN] Batch 352/782 done\n","[SVM][TRAIN] Batch 353/782 done\n","[SVM][TRAIN] Batch 354/782 done\n","[SVM][TRAIN] Batch 355/782 done\n","[SVM][TRAIN] Batch 356/782 done\n","[SVM][TRAIN] Batch 357/782 done\n","[SVM][TRAIN] Batch 358/782 done\n","[SVM][TRAIN] Batch 359/782 done\n","[SVM][TRAIN] Batch 360/782 done\n","[SVM][TRAIN] Batch 361/782 done\n","[SVM][TRAIN] Batch 362/782 done\n","[SVM][TRAIN] Batch 363/782 done\n","[SVM][TRAIN] Batch 364/782 done\n","[SVM][TRAIN] Batch 365/782 done\n","[SVM][TRAIN] Batch 366/782 done\n","[SVM][TRAIN] Batch 367/782 done\n","[SVM][TRAIN] Batch 368/782 done\n","[SVM][TRAIN] Batch 369/782 done\n","[SVM][TRAIN] Batch 370/782 done\n","[SVM][TRAIN] Batch 371/782 done\n","[SVM][TRAIN] Batch 372/782 done\n","[SVM][TRAIN] Batch 373/782 done\n","[SVM][TRAIN] Batch 374/782 done\n","[SVM][TRAIN] Batch 375/782 done\n","[SVM][TRAIN] Batch 376/782 done\n","[SVM][TRAIN] Batch 377/782 done\n","[SVM][TRAIN] Batch 378/782 done\n","[SVM][TRAIN] Batch 379/782 done\n","[SVM][TRAIN] Batch 380/782 done\n","[SVM][TRAIN] Batch 381/782 done\n","[SVM][TRAIN] Batch 382/782 done\n","[SVM][TRAIN] Batch 383/782 done\n","[SVM][TRAIN] Batch 384/782 done\n","[SVM][TRAIN] Batch 385/782 done\n","[SVM][TRAIN] Batch 386/782 done\n","[SVM][TRAIN] Batch 387/782 done\n","[SVM][TRAIN] Batch 388/782 done\n","[SVM][TRAIN] Batch 389/782 done\n","[SVM][TRAIN] Batch 390/782 done\n","[SVM][TRAIN] Batch 391/782 done\n","[SVM][TRAIN] Batch 392/782 done\n","[SVM][TRAIN] Batch 393/782 done\n","[SVM][TRAIN] Batch 394/782 done\n","[SVM][TRAIN] Batch 395/782 done\n","[SVM][TRAIN] Batch 396/782 done\n","[SVM][TRAIN] Batch 397/782 done\n","[SVM][TRAIN] Batch 398/782 done\n","[SVM][TRAIN] Batch 399/782 done\n","[SVM][TRAIN] Batch 400/782 done\n","[SVM][TRAIN] Batch 401/782 done\n","[SVM][TRAIN] Batch 402/782 done\n","[SVM][TRAIN] Batch 403/782 done\n","[SVM][TRAIN] Batch 404/782 done\n","[SVM][TRAIN] Batch 405/782 done\n","[SVM][TRAIN] Batch 406/782 done\n","[SVM][TRAIN] Batch 407/782 done\n","[SVM][TRAIN] Batch 408/782 done\n","[SVM][TRAIN] Batch 409/782 done\n","[SVM][TRAIN] Batch 410/782 done\n","[SVM][TRAIN] Batch 411/782 done\n","[SVM][TRAIN] Batch 412/782 done\n","[SVM][TRAIN] Batch 413/782 done\n","[SVM][TRAIN] Batch 414/782 done\n","[SVM][TRAIN] Batch 415/782 done\n","[SVM][TRAIN] Batch 416/782 done\n","[SVM][TRAIN] Batch 417/782 done\n","[SVM][TRAIN] Batch 418/782 done\n","[SVM][TRAIN] Batch 419/782 done\n","[SVM][TRAIN] Batch 420/782 done\n","[SVM][TRAIN] Batch 421/782 done\n","[SVM][TRAIN] Batch 422/782 done\n","[SVM][TRAIN] Batch 423/782 done\n","[SVM][TRAIN] Batch 424/782 done\n","[SVM][TRAIN] Batch 425/782 done\n","[SVM][TRAIN] Batch 426/782 done\n","[SVM][TRAIN] Batch 427/782 done\n","[SVM][TRAIN] Batch 428/782 done\n","[SVM][TRAIN] Batch 429/782 done\n","[SVM][TRAIN] Batch 430/782 done\n","[SVM][TRAIN] Batch 431/782 done\n","[SVM][TRAIN] Batch 432/782 done\n","[SVM][TRAIN] Batch 433/782 done\n","[SVM][TRAIN] Batch 434/782 done\n","[SVM][TRAIN] Batch 435/782 done\n","[SVM][TRAIN] Batch 436/782 done\n","[SVM][TRAIN] Batch 437/782 done\n","[SVM][TRAIN] Batch 438/782 done\n","[SVM][TRAIN] Batch 439/782 done\n","[SVM][TRAIN] Batch 440/782 done\n","[SVM][TRAIN] Batch 441/782 done\n","[SVM][TRAIN] Batch 442/782 done\n","[SVM][TRAIN] Batch 443/782 done\n","[SVM][TRAIN] Batch 444/782 done\n","[SVM][TRAIN] Batch 445/782 done\n","[SVM][TRAIN] Batch 446/782 done\n","[SVM][TRAIN] Batch 447/782 done\n","[SVM][TRAIN] Batch 448/782 done\n","[SVM][TRAIN] Batch 449/782 done\n","[SVM][TRAIN] Batch 450/782 done\n","[SVM][TRAIN] Batch 451/782 done\n","[SVM][TRAIN] Batch 452/782 done\n","[SVM][TRAIN] Batch 453/782 done\n","[SVM][TRAIN] Batch 454/782 done\n","[SVM][TRAIN] Batch 455/782 done\n","[SVM][TRAIN] Batch 456/782 done\n","[SVM][TRAIN] Batch 457/782 done\n","[SVM][TRAIN] Batch 458/782 done\n","[SVM][TRAIN] Batch 459/782 done\n","[SVM][TRAIN] Batch 460/782 done\n","[SVM][TRAIN] Batch 461/782 done\n","[SVM][TRAIN] Batch 462/782 done\n","[SVM][TRAIN] Batch 463/782 done\n","[SVM][TRAIN] Batch 464/782 done\n","[SVM][TRAIN] Batch 465/782 done\n","[SVM][TRAIN] Batch 466/782 done\n","[SVM][TRAIN] Batch 467/782 done\n","[SVM][TRAIN] Batch 468/782 done\n","[SVM][TRAIN] Batch 469/782 done\n","[SVM][TRAIN] Batch 470/782 done\n","[SVM][TRAIN] Batch 471/782 done\n","[SVM][TRAIN] Batch 472/782 done\n","[SVM][TRAIN] Batch 473/782 done\n","[SVM][TRAIN] Batch 474/782 done\n","[SVM][TRAIN] Batch 475/782 done\n","[SVM][TRAIN] Batch 476/782 done\n","[SVM][TRAIN] Batch 477/782 done\n","[SVM][TRAIN] Batch 478/782 done\n","[SVM][TRAIN] Batch 479/782 done\n","[SVM][TRAIN] Batch 480/782 done\n","[SVM][TRAIN] Batch 481/782 done\n","[SVM][TRAIN] Batch 482/782 done\n","[SVM][TRAIN] Batch 483/782 done\n","[SVM][TRAIN] Batch 484/782 done\n","[SVM][TRAIN] Batch 485/782 done\n","[SVM][TRAIN] Batch 486/782 done\n","[SVM][TRAIN] Batch 487/782 done\n","[SVM][TRAIN] Batch 488/782 done\n","[SVM][TRAIN] Batch 489/782 done\n","[SVM][TRAIN] Batch 490/782 done\n","[SVM][TRAIN] Batch 491/782 done\n","[SVM][TRAIN] Batch 492/782 done\n","[SVM][TRAIN] Batch 493/782 done\n","[SVM][TRAIN] Batch 494/782 done\n","[SVM][TRAIN] Batch 495/782 done\n","[SVM][TRAIN] Batch 496/782 done\n","[SVM][TRAIN] Batch 497/782 done\n","[SVM][TRAIN] Batch 498/782 done\n","[SVM][TRAIN] Batch 499/782 done\n","[SVM][TRAIN] Batch 500/782 done\n","[SVM][TRAIN] Batch 501/782 done\n","[SVM][TRAIN] Batch 502/782 done\n","[SVM][TRAIN] Batch 503/782 done\n","[SVM][TRAIN] Batch 504/782 done\n","[SVM][TRAIN] Batch 505/782 done\n","[SVM][TRAIN] Batch 506/782 done\n","[SVM][TRAIN] Batch 507/782 done\n","[SVM][TRAIN] Batch 508/782 done\n","[SVM][TRAIN] Batch 509/782 done\n","[SVM][TRAIN] Batch 510/782 done\n","[SVM][TRAIN] Batch 511/782 done\n","[SVM][TRAIN] Batch 512/782 done\n","[SVM][TRAIN] Batch 513/782 done\n","[SVM][TRAIN] Batch 514/782 done\n","[SVM][TRAIN] Batch 515/782 done\n","[SVM][TRAIN] Batch 516/782 done\n","[SVM][TRAIN] Batch 517/782 done\n","[SVM][TRAIN] Batch 518/782 done\n","[SVM][TRAIN] Batch 519/782 done\n","[SVM][TRAIN] Batch 520/782 done\n","[SVM][TRAIN] Batch 521/782 done\n","[SVM][TRAIN] Batch 522/782 done\n","[SVM][TRAIN] Batch 523/782 done\n","[SVM][TRAIN] Batch 524/782 done\n","[SVM][TRAIN] Batch 525/782 done\n","[SVM][TRAIN] Batch 526/782 done\n","[SVM][TRAIN] Batch 527/782 done\n","[SVM][TRAIN] Batch 528/782 done\n","[SVM][TRAIN] Batch 529/782 done\n","[SVM][TRAIN] Batch 530/782 done\n","[SVM][TRAIN] Batch 531/782 done\n","[SVM][TRAIN] Batch 532/782 done\n","[SVM][TRAIN] Batch 533/782 done\n","[SVM][TRAIN] Batch 534/782 done\n","[SVM][TRAIN] Batch 535/782 done\n","[SVM][TRAIN] Batch 536/782 done\n","[SVM][TRAIN] Batch 537/782 done\n","[SVM][TRAIN] Batch 538/782 done\n","[SVM][TRAIN] Batch 539/782 done\n","[SVM][TRAIN] Batch 540/782 done\n","[SVM][TRAIN] Batch 541/782 done\n","[SVM][TRAIN] Batch 542/782 done\n","[SVM][TRAIN] Batch 543/782 done\n","[SVM][TRAIN] Batch 544/782 done\n","[SVM][TRAIN] Batch 545/782 done\n","[SVM][TRAIN] Batch 546/782 done\n","[SVM][TRAIN] Batch 547/782 done\n","[SVM][TRAIN] Batch 548/782 done\n","[SVM][TRAIN] Batch 549/782 done\n","[SVM][TRAIN] Batch 550/782 done\n","[SVM][TRAIN] Batch 551/782 done\n","[SVM][TRAIN] Batch 552/782 done\n","[SVM][TRAIN] Batch 553/782 done\n","[SVM][TRAIN] Batch 554/782 done\n","[SVM][TRAIN] Batch 555/782 done\n","[SVM][TRAIN] Batch 556/782 done\n","[SVM][TRAIN] Batch 557/782 done\n","[SVM][TRAIN] Batch 558/782 done\n","[SVM][TRAIN] Batch 559/782 done\n","[SVM][TRAIN] Batch 560/782 done\n","[SVM][TRAIN] Batch 561/782 done\n","[SVM][TRAIN] Batch 562/782 done\n","[SVM][TRAIN] Batch 563/782 done\n","[SVM][TRAIN] Batch 564/782 done\n","[SVM][TRAIN] Batch 565/782 done\n","[SVM][TRAIN] Batch 566/782 done\n","[SVM][TRAIN] Batch 567/782 done\n","[SVM][TRAIN] Batch 568/782 done\n","[SVM][TRAIN] Batch 569/782 done\n","[SVM][TRAIN] Batch 570/782 done\n","[SVM][TRAIN] Batch 571/782 done\n","[SVM][TRAIN] Batch 572/782 done\n","[SVM][TRAIN] Batch 573/782 done\n","[SVM][TRAIN] Batch 574/782 done\n","[SVM][TRAIN] Batch 575/782 done\n","[SVM][TRAIN] Batch 576/782 done\n","[SVM][TRAIN] Batch 577/782 done\n","[SVM][TRAIN] Batch 578/782 done\n","[SVM][TRAIN] Batch 579/782 done\n","[SVM][TRAIN] Batch 580/782 done\n","[SVM][TRAIN] Batch 581/782 done\n","[SVM][TRAIN] Batch 582/782 done\n","[SVM][TRAIN] Batch 583/782 done\n","[SVM][TRAIN] Batch 584/782 done\n","[SVM][TRAIN] Batch 585/782 done\n","[SVM][TRAIN] Batch 586/782 done\n","[SVM][TRAIN] Batch 587/782 done\n","[SVM][TRAIN] Batch 588/782 done\n","[SVM][TRAIN] Batch 589/782 done\n","[SVM][TRAIN] Batch 590/782 done\n","[SVM][TRAIN] Batch 591/782 done\n","[SVM][TRAIN] Batch 592/782 done\n","[SVM][TRAIN] Batch 593/782 done\n","[SVM][TRAIN] Batch 594/782 done\n","[SVM][TRAIN] Batch 595/782 done\n","[SVM][TRAIN] Batch 596/782 done\n","[SVM][TRAIN] Batch 597/782 done\n","[SVM][TRAIN] Batch 598/782 done\n","[SVM][TRAIN] Batch 599/782 done\n","[SVM][TRAIN] Batch 600/782 done\n","[SVM][TRAIN] Batch 601/782 done\n","[SVM][TRAIN] Batch 602/782 done\n","[SVM][TRAIN] Batch 603/782 done\n","[SVM][TRAIN] Batch 604/782 done\n","[SVM][TRAIN] Batch 605/782 done\n","[SVM][TRAIN] Batch 606/782 done\n","[SVM][TRAIN] Batch 607/782 done\n","[SVM][TRAIN] Batch 608/782 done\n","[SVM][TRAIN] Batch 609/782 done\n","[SVM][TRAIN] Batch 610/782 done\n","[SVM][TRAIN] Batch 611/782 done\n","[SVM][TRAIN] Batch 612/782 done\n","[SVM][TRAIN] Batch 613/782 done\n","[SVM][TRAIN] Batch 614/782 done\n","[SVM][TRAIN] Batch 615/782 done\n","[SVM][TRAIN] Batch 616/782 done\n","[SVM][TRAIN] Batch 617/782 done\n","[SVM][TRAIN] Batch 618/782 done\n","[SVM][TRAIN] Batch 619/782 done\n","[SVM][TRAIN] Batch 620/782 done\n","[SVM][TRAIN] Batch 621/782 done\n","[SVM][TRAIN] Batch 622/782 done\n","[SVM][TRAIN] Batch 623/782 done\n","[SVM][TRAIN] Batch 624/782 done\n","[SVM][TRAIN] Batch 625/782 done\n","[SVM][TRAIN] Batch 626/782 done\n","[SVM][TRAIN] Batch 627/782 done\n","[SVM][TRAIN] Batch 628/782 done\n","[SVM][TRAIN] Batch 629/782 done\n","[SVM][TRAIN] Batch 630/782 done\n","[SVM][TRAIN] Batch 631/782 done\n","[SVM][TRAIN] Batch 632/782 done\n","[SVM][TRAIN] Batch 633/782 done\n","[SVM][TRAIN] Batch 634/782 done\n","[SVM][TRAIN] Batch 635/782 done\n","[SVM][TRAIN] Batch 636/782 done\n","[SVM][TRAIN] Batch 637/782 done\n","[SVM][TRAIN] Batch 638/782 done\n","[SVM][TRAIN] Batch 639/782 done\n","[SVM][TRAIN] Batch 640/782 done\n","[SVM][TRAIN] Batch 641/782 done\n","[SVM][TRAIN] Batch 642/782 done\n","[SVM][TRAIN] Batch 643/782 done\n","[SVM][TRAIN] Batch 644/782 done\n","[SVM][TRAIN] Batch 645/782 done\n","[SVM][TRAIN] Batch 646/782 done\n","[SVM][TRAIN] Batch 647/782 done\n","[SVM][TRAIN] Batch 648/782 done\n","[SVM][TRAIN] Batch 649/782 done\n","[SVM][TRAIN] Batch 650/782 done\n","[SVM][TRAIN] Batch 651/782 done\n","[SVM][TRAIN] Batch 652/782 done\n","[SVM][TRAIN] Batch 653/782 done\n","[SVM][TRAIN] Batch 654/782 done\n","[SVM][TRAIN] Batch 655/782 done\n","[SVM][TRAIN] Batch 656/782 done\n","[SVM][TRAIN] Batch 657/782 done\n","[SVM][TRAIN] Batch 658/782 done\n","[SVM][TRAIN] Batch 659/782 done\n","[SVM][TRAIN] Batch 660/782 done\n","[SVM][TRAIN] Batch 661/782 done\n","[SVM][TRAIN] Batch 662/782 done\n","[SVM][TRAIN] Batch 663/782 done\n","[SVM][TRAIN] Batch 664/782 done\n","[SVM][TRAIN] Batch 665/782 done\n","[SVM][TRAIN] Batch 666/782 done\n","[SVM][TRAIN] Batch 667/782 done\n","[SVM][TRAIN] Batch 668/782 done\n","[SVM][TRAIN] Batch 669/782 done\n","[SVM][TRAIN] Batch 670/782 done\n","[SVM][TRAIN] Batch 671/782 done\n","[SVM][TRAIN] Batch 672/782 done\n","[SVM][TRAIN] Batch 673/782 done\n","[SVM][TRAIN] Batch 674/782 done\n","[SVM][TRAIN] Batch 675/782 done\n","[SVM][TRAIN] Batch 676/782 done\n","[SVM][TRAIN] Batch 677/782 done\n","[SVM][TRAIN] Batch 678/782 done\n","[SVM][TRAIN] Batch 679/782 done\n","[SVM][TRAIN] Batch 680/782 done\n","[SVM][TRAIN] Batch 681/782 done\n","[SVM][TRAIN] Batch 682/782 done\n","[SVM][TRAIN] Batch 683/782 done\n","[SVM][TRAIN] Batch 684/782 done\n","[SVM][TRAIN] Batch 685/782 done\n","[SVM][TRAIN] Batch 686/782 done\n","[SVM][TRAIN] Batch 687/782 done\n","[SVM][TRAIN] Batch 688/782 done\n","[SVM][TRAIN] Batch 689/782 done\n","[SVM][TRAIN] Batch 690/782 done\n","[SVM][TRAIN] Batch 691/782 done\n","[SVM][TRAIN] Batch 692/782 done\n","[SVM][TRAIN] Batch 693/782 done\n","[SVM][TRAIN] Batch 694/782 done\n","[SVM][TRAIN] Batch 695/782 done\n","[SVM][TRAIN] Batch 696/782 done\n","[SVM][TRAIN] Batch 697/782 done\n","[SVM][TRAIN] Batch 698/782 done\n","[SVM][TRAIN] Batch 699/782 done\n","[SVM][TRAIN] Batch 700/782 done\n","[SVM][TRAIN] Batch 701/782 done\n","[SVM][TRAIN] Batch 702/782 done\n","[SVM][TRAIN] Batch 703/782 done\n","[SVM][TRAIN] Batch 704/782 done\n","[SVM][TRAIN] Batch 705/782 done\n","[SVM][TRAIN] Batch 706/782 done\n","[SVM][TRAIN] Batch 707/782 done\n","[SVM][TRAIN] Batch 708/782 done\n","[SVM][TRAIN] Batch 709/782 done\n","[SVM][TRAIN] Batch 710/782 done\n","[SVM][TRAIN] Batch 711/782 done\n","[SVM][TRAIN] Batch 712/782 done\n","[SVM][TRAIN] Batch 713/782 done\n","[SVM][TRAIN] Batch 714/782 done\n","[SVM][TRAIN] Batch 715/782 done\n","[SVM][TRAIN] Batch 716/782 done\n","[SVM][TRAIN] Batch 717/782 done\n","[SVM][TRAIN] Batch 718/782 done\n","[SVM][TRAIN] Batch 719/782 done\n","[SVM][TRAIN] Batch 720/782 done\n","[SVM][TRAIN] Batch 721/782 done\n","[SVM][TRAIN] Batch 722/782 done\n","[SVM][TRAIN] Batch 723/782 done\n","[SVM][TRAIN] Batch 724/782 done\n","[SVM][TRAIN] Batch 725/782 done\n","[SVM][TRAIN] Batch 726/782 done\n","[SVM][TRAIN] Batch 727/782 done\n","[SVM][TRAIN] Batch 728/782 done\n","[SVM][TRAIN] Batch 729/782 done\n","[SVM][TRAIN] Batch 730/782 done\n","[SVM][TRAIN] Batch 731/782 done\n","[SVM][TRAIN] Batch 732/782 done\n","[SVM][TRAIN] Batch 733/782 done\n","[SVM][TRAIN] Batch 734/782 done\n","[SVM][TRAIN] Batch 735/782 done\n","[SVM][TRAIN] Batch 736/782 done\n","[SVM][TRAIN] Batch 737/782 done\n","[SVM][TRAIN] Batch 738/782 done\n","[SVM][TRAIN] Batch 739/782 done\n","[SVM][TRAIN] Batch 740/782 done\n","[SVM][TRAIN] Batch 741/782 done\n","[SVM][TRAIN] Batch 742/782 done\n","[SVM][TRAIN] Batch 743/782 done\n","[SVM][TRAIN] Batch 744/782 done\n","[SVM][TRAIN] Batch 745/782 done\n","[SVM][TRAIN] Batch 746/782 done\n","[SVM][TRAIN] Batch 747/782 done\n","[SVM][TRAIN] Batch 748/782 done\n","[SVM][TRAIN] Batch 749/782 done\n","[SVM][TRAIN] Batch 750/782 done\n","[SVM][TRAIN] Batch 751/782 done\n","[SVM][TRAIN] Batch 752/782 done\n","[SVM][TRAIN] Batch 753/782 done\n","[SVM][TRAIN] Batch 754/782 done\n","[SVM][TRAIN] Batch 755/782 done\n","[SVM][TRAIN] Batch 756/782 done\n","[SVM][TRAIN] Batch 757/782 done\n","[SVM][TRAIN] Batch 758/782 done\n","[SVM][TRAIN] Batch 759/782 done\n","[SVM][TRAIN] Batch 760/782 done\n","[SVM][TRAIN] Batch 761/782 done\n","[SVM][TRAIN] Batch 762/782 done\n","[SVM][TRAIN] Batch 763/782 done\n","[SVM][TRAIN] Batch 764/782 done\n","[SVM][TRAIN] Batch 765/782 done\n","[SVM][TRAIN] Batch 766/782 done\n","[SVM][TRAIN] Batch 767/782 done\n","[SVM][TRAIN] Batch 768/782 done\n","[SVM][TRAIN] Batch 769/782 done\n","[SVM][TRAIN] Batch 770/782 done\n","[SVM][TRAIN] Batch 771/782 done\n","[SVM][TRAIN] Batch 772/782 done\n","[SVM][TRAIN] Batch 773/782 done\n","[SVM][TRAIN] Batch 774/782 done\n","[SVM][TRAIN] Batch 775/782 done\n","[SVM][TRAIN] Batch 776/782 done\n","[SVM][TRAIN] Batch 777/782 done\n","[SVM][TRAIN] Batch 778/782 done\n","[SVM][TRAIN] Batch 779/782 done\n","[SVM][TRAIN] Batch 780/782 done\n","[SVM][TRAIN] Batch 781/782 done\n","[SVM][TRAIN] Batch 782/782 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/157 done\n","[SVM][TEST] Batch 2/157 done\n","[SVM][TEST] Batch 3/157 done\n","[SVM][TEST] Batch 4/157 done\n","[SVM][TEST] Batch 5/157 done\n","[SVM][TEST] Batch 6/157 done\n","[SVM][TEST] Batch 7/157 done\n","[SVM][TEST] Batch 8/157 done\n","[SVM][TEST] Batch 9/157 done\n","[SVM][TEST] Batch 10/157 done\n","[SVM][TEST] Batch 11/157 done\n","[SVM][TEST] Batch 12/157 done\n","[SVM][TEST] Batch 13/157 done\n","[SVM][TEST] Batch 14/157 done\n","[SVM][TEST] Batch 15/157 done\n","[SVM][TEST] Batch 16/157 done\n","[SVM][TEST] Batch 17/157 done\n","[SVM][TEST] Batch 18/157 done\n","[SVM][TEST] Batch 19/157 done\n","[SVM][TEST] Batch 20/157 done\n","[SVM][TEST] Batch 21/157 done\n","[SVM][TEST] Batch 22/157 done\n","[SVM][TEST] Batch 23/157 done\n","[SVM][TEST] Batch 24/157 done\n","[SVM][TEST] Batch 25/157 done\n","[SVM][TEST] Batch 26/157 done\n","[SVM][TEST] Batch 27/157 done\n","[SVM][TEST] Batch 28/157 done\n","[SVM][TEST] Batch 29/157 done\n","[SVM][TEST] Batch 30/157 done\n","[SVM][TEST] Batch 31/157 done\n","[SVM][TEST] Batch 32/157 done\n","[SVM][TEST] Batch 33/157 done\n","[SVM][TEST] Batch 34/157 done\n","[SVM][TEST] Batch 35/157 done\n","[SVM][TEST] Batch 36/157 done\n","[SVM][TEST] Batch 37/157 done\n","[SVM][TEST] Batch 38/157 done\n","[SVM][TEST] Batch 39/157 done\n","[SVM][TEST] Batch 40/157 done\n","[SVM][TEST] Batch 41/157 done\n","[SVM][TEST] Batch 42/157 done\n","[SVM][TEST] Batch 43/157 done\n","[SVM][TEST] Batch 44/157 done\n","[SVM][TEST] Batch 45/157 done\n","[SVM][TEST] Batch 46/157 done\n","[SVM][TEST] Batch 47/157 done\n","[SVM][TEST] Batch 48/157 done\n","[SVM][TEST] Batch 49/157 done\n","[SVM][TEST] Batch 50/157 done\n","[SVM][TEST] Batch 51/157 done\n","[SVM][TEST] Batch 52/157 done\n","[SVM][TEST] Batch 53/157 done\n","[SVM][TEST] Batch 54/157 done\n","[SVM][TEST] Batch 55/157 done\n","[SVM][TEST] Batch 56/157 done\n","[SVM][TEST] Batch 57/157 done\n","[SVM][TEST] Batch 58/157 done\n","[SVM][TEST] Batch 59/157 done\n","[SVM][TEST] Batch 60/157 done\n","[SVM][TEST] Batch 61/157 done\n","[SVM][TEST] Batch 62/157 done\n","[SVM][TEST] Batch 63/157 done\n","[SVM][TEST] Batch 64/157 done\n","[SVM][TEST] Batch 65/157 done\n","[SVM][TEST] Batch 66/157 done\n","[SVM][TEST] Batch 67/157 done\n","[SVM][TEST] Batch 68/157 done\n","[SVM][TEST] Batch 69/157 done\n","[SVM][TEST] Batch 70/157 done\n","[SVM][TEST] Batch 71/157 done\n","[SVM][TEST] Batch 72/157 done\n","[SVM][TEST] Batch 73/157 done\n","[SVM][TEST] Batch 74/157 done\n","[SVM][TEST] Batch 75/157 done\n","[SVM][TEST] Batch 76/157 done\n","[SVM][TEST] Batch 77/157 done\n","[SVM][TEST] Batch 78/157 done\n","[SVM][TEST] Batch 79/157 done\n","[SVM][TEST] Batch 80/157 done\n","[SVM][TEST] Batch 81/157 done\n","[SVM][TEST] Batch 82/157 done\n","[SVM][TEST] Batch 83/157 done\n","[SVM][TEST] Batch 84/157 done\n","[SVM][TEST] Batch 85/157 done\n","[SVM][TEST] Batch 86/157 done\n","[SVM][TEST] Batch 87/157 done\n","[SVM][TEST] Batch 88/157 done\n","[SVM][TEST] Batch 89/157 done\n","[SVM][TEST] Batch 90/157 done\n","[SVM][TEST] Batch 91/157 done\n","[SVM][TEST] Batch 92/157 done\n","[SVM][TEST] Batch 93/157 done\n","[SVM][TEST] Batch 94/157 done\n","[SVM][TEST] Batch 95/157 done\n","[SVM][TEST] Batch 96/157 done\n","[SVM][TEST] Batch 97/157 done\n","[SVM][TEST] Batch 98/157 done\n","[SVM][TEST] Batch 99/157 done\n","[SVM][TEST] Batch 100/157 done\n","[SVM][TEST] Batch 101/157 done\n","[SVM][TEST] Batch 102/157 done\n","[SVM][TEST] Batch 103/157 done\n","[SVM][TEST] Batch 104/157 done\n","[SVM][TEST] Batch 105/157 done\n","[SVM][TEST] Batch 106/157 done\n","[SVM][TEST] Batch 107/157 done\n","[SVM][TEST] Batch 108/157 done\n","[SVM][TEST] Batch 109/157 done\n","[SVM][TEST] Batch 110/157 done\n","[SVM][TEST] Batch 111/157 done\n","[SVM][TEST] Batch 112/157 done\n","[SVM][TEST] Batch 113/157 done\n","[SVM][TEST] Batch 114/157 done\n","[SVM][TEST] Batch 115/157 done\n","[SVM][TEST] Batch 116/157 done\n","[SVM][TEST] Batch 117/157 done\n","[SVM][TEST] Batch 118/157 done\n","[SVM][TEST] Batch 119/157 done\n","[SVM][TEST] Batch 120/157 done\n","[SVM][TEST] Batch 121/157 done\n","[SVM][TEST] Batch 122/157 done\n","[SVM][TEST] Batch 123/157 done\n","[SVM][TEST] Batch 124/157 done\n","[SVM][TEST] Batch 125/157 done\n","[SVM][TEST] Batch 126/157 done\n","[SVM][TEST] Batch 127/157 done\n","[SVM][TEST] Batch 128/157 done\n","[SVM][TEST] Batch 129/157 done\n","[SVM][TEST] Batch 130/157 done\n","[SVM][TEST] Batch 131/157 done\n","[SVM][TEST] Batch 132/157 done\n","[SVM][TEST] Batch 133/157 done\n","[SVM][TEST] Batch 134/157 done\n","[SVM][TEST] Batch 135/157 done\n","[SVM][TEST] Batch 136/157 done\n","[SVM][TEST] Batch 137/157 done\n","[SVM][TEST] Batch 138/157 done\n","[SVM][TEST] Batch 139/157 done\n","[SVM][TEST] Batch 140/157 done\n","[SVM][TEST] Batch 141/157 done\n","[SVM][TEST] Batch 142/157 done\n","[SVM][TEST] Batch 143/157 done\n","[SVM][TEST] Batch 144/157 done\n","[SVM][TEST] Batch 145/157 done\n","[SVM][TEST] Batch 146/157 done\n","[SVM][TEST] Batch 147/157 done\n","[SVM][TEST] Batch 148/157 done\n","[SVM][TEST] Batch 149/157 done\n","[SVM][TEST] Batch 150/157 done\n","[SVM][TEST] Batch 151/157 done\n","[SVM][TEST] Batch 152/157 done\n","[SVM][TEST] Batch 153/157 done\n","[SVM][TEST] Batch 154/157 done\n","[SVM][TEST] Batch 155/157 done\n","[SVM][TEST] Batch 156/157 done\n","[SVM][TEST] Batch 157/157 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}]},{"cell_type":"code","source":["!head -n 1 /content/train_svm.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTziK9fPb7ai","executionInfo":{"status":"ok","timestamp":1766673381605,"user_tz":-420,"elapsed":531,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"289f48af-e2e6-4a81-d11d-f1bf1f7bd049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6 1:0.0494394 2:0.0385242 3:0.0181939 4:0.0175647 5:0.0106431 6:0.0207396 7:0.0173481 8:0.0229307 9:0 10:0 11:0 12:0 13:0.00612822 14:0 15:0 16:0 17:0 18:0 19:0 20:0.0149309 21:0 22:0 23:0 24:0 25:0 26:0 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:0 35:0 36:0 37:0 38:0 39:0 40:0 41:0 42:0 43:0 44:0 45:0 46:0 47:0 48:0 49:0 50:0 51:0 52:0 53:0 54:0 55:0 56:0 57:0 58:0 59:0 60:0 61:0 62:0 63:0 64:0 65:0.0782026 66:0.0335943 67:0.0147023 68:0.0121475 69:0.020677 70:0.00596453 71:0 72:0 73:0.0838593 74:0.0115385 75:0 76:0.0642626 77:0.00736496 78:0.0322956 79:0.00341198 80:0 81:0.0812883 82:0.000403218 83:0.0246191 84:0.0193821 85:0 86:0 87:0 88:0 89:0.0850586 90:0.0301853 91:0.0445718 92:0 93:0 94:0 95:0 96:0 97:0.0867667 98:0.0650555 99:0 100:0 101:0 102:0 103:0 104:0 105:0.0921887 106:0.0120452 107:0 108:0 109:0 110:0 111:0 112:0.00875467 113:0.0856992 114:0 115:0 116:0 117:0 118:0 119:0.0167814 120:0 121:0.107366 122:0 123:0 124:0 125:0 126:0 127:0.027344 128:0 129:0 130:0 131:0 132:0 133:0 134:0 135:0 136:0 137:0 138:0 139:0 140:0 141:0 142:0 143:0 144:0 145:0 146:0 147:0 148:0 149:0 150:0 151:0 152:0 153:0 154:0 155:0 156:0 157:0 158:0 159:0 160:0 161:0 162:0 163:0 164:0 165:0 166:0 167:0 168:0 169:0 170:0 171:0 172:0 173:0 174:0 175:0 176:0 177:0 178:0 179:0 180:0 181:0 182:0 183:0 184:0 185:0 186:0 187:0 188:0 189:0 190:0 191:0 192:0 193:0 194:0 195:0.00514212 196:0.00399484 197:0.0135307 198:0.00223909 199:0.00103347 200:0.0176574 201:0 202:0 203:0 204:0.00844258 205:0.0123692 206:0.0166071 207:0 208:0 209:0 210:0.00802409 211:0 212:0.0269139 213:0.0132426 214:0.00138875 215:0 216:0 217:0 218:0.00230433 219:0.0175939 220:0.0466183 221:0.0332867 222:0.0224379 223:0.00861238 224:0 225:0.00847762 226:0.0601056 227:0.0365977 228:0.0352716 229:0.0389874 230:0.0472242 231:0.0263632 232:0.00405011 233:0.0110155 234:0.0360703 235:0.0488717 236:0.0117425 237:0.021091 238:0.0361397 239:0.0284368 240:0 241:0.00633441 242:0.00577471 243:0.0361545 244:0.0339125 245:0.00458085 246:0 247:0.0144733 248:0.00174537 249:0.0304089 250:0.0333692 251:0.0405116 252:0.0271633 253:0.0279461 254:0.0307456 255:0.067421 256:0.0260409 257:0 258:0 259:0 260:0 261:0 262:0 263:0 264:0.00823753 265:0 266:0 267:0 268:0 269:0 270:0 271:0 272:0 273:0 274:0 275:0 276:0 277:0 278:0 279:0.00714593 280:0 281:0 282:0 283:0 284:0 285:0 286:0 287:0.0180543 288:0 289:0 290:0 291:0 292:0 293:0 294:0 295:0 296:0 297:0 298:0 299:0 300:0 301:0 302:0 303:0 304:0 305:0 306:0 307:0 308:0 309:0 310:0 311:0 312:0.0125617 313:0 314:0 315:0 316:0 317:0 318:0 319:0 320:0 321:0.126844 322:0.17074 323:0.178785 324:0.171658 325:0.173688 326:0.189509 327:0.188165 328:0.178679 329:0.176205 330:0.174135 331:0.171799 332:0.168894 333:0.169524 334:0.178938 335:0.18625 336:0.183203 337:0.21775 338:0.187844 339:0.19466 340:0.242814 341:0.247636 342:0.235639 343:0.199846 344:0.192894 345:0.222707 346:0.210755 347:0.23208 348:0.344542 349:0.354034 350:0.344502 351:0.277304 352:0.212471 353:0.221261 354:0.30516 355:0.303974 356:0.349858 357:0.32097 358:0.353787 359:0.285552 360:0.214535 361:0.204059 362:0.255401 363:0.334137 364:0.285809 365:0.244385 366:0.238334 367:0.241795 368:0.214259 369:0.262531 370:0.224601 371:0.240131 372:0.239919 373:0.214275 374:0.209434 375:0.203764 376:0.243865 377:0.26029 378:0.227622 379:0.219783 380:0.206989 381:0.200195 382:0.189008 383:0.193642 384:0.217196 385:0.0602241 386:0.0955773 387:0.098182 388:0.100804 389:0.0992504 390:0.108838 391:0.126765 392:0.0890989 393:0.0937496 394:0.0889782 395:0.0932976 396:0.095907 397:0.109293 398:0.0933165 399:0.0979357 400:0.0739018 401:0.0953933 402:0.0968128 403:0.110592 404:0.156984 405:0.179234 406:0.138869 407:0.114338 408:0.0920968 409:0.0975268 410:0.128954 411:0.170623 412:0.217587 413:0.245106 414:0.217801 415:0.15363 416:0.0913797 417:0.0838037 418:0.197501 419:0.219915 420:0.219337 421:0.199194 422:0.232351 423:0.173753 424:0.0891476 425:0.0986854 426:0.154451 427:0.19534 428:0.19107 429:0.124114 430:0.141032 431:0.111932 432:0.132711 433:0.0960222 434:0.123005 435:0.137529 436:0.132395 437:0.109002 438:0.103352 439:0.113519 440:0.144983 441:0.131154 442:0.129921 443:0.113856 444:0.110356 445:0.106368 446:0.100072 447:0.103916 448:0.135611 449:0.0380016 450:0.0356653 451:0.032994 452:0.0330224 453:0.0352676 454:0.0398652 455:0.0353025 456:0.0317142 457:0.043734 458:0.0198796 459:0.00778285 460:0.0199031 461:0.0300182 462:0.0177107 463:0.016182 464:0.0184133 465:0.0501132 466:0.0121008 467:0.0259351 468:0.0501809 469:0.0234951 470:0.0252925 471:0.0108937 472:0.0124161 473:0.0397805 474:0.00467073 475:0.0132187 476:0.0370901 477:0.0134239 478:0.0109195 479:0 480:0.00996375 481:0.0372023 482:0.0334242 483:0.0251736 484:0 485:0 486:0.00970033 487:0 488:0.00987834 489:0.0414715 490:0 491:0.00140724 492:0 493:0.00133797 494:0 495:1.41999e-05 496:0.01614 497:0.0654202 498:0.00798656 499:0 500:0 501:0 502:0.0125891 503:0.0219836 504:0.0101583 505:0.0530933 506:0.0255458 507:0.00837933 508:0.0010548 509:0.00479046 510:0.00201009 511:0.00131973 512:0.0324207 513:0.0149249 514:0.0126701 515:0.00673899 516:0.00620854 517:0.00161732 518:0.00381988 519:0.016693 520:0.00669092 521:0.0312656 522:0.018065 523:0.0254313 524:0.0219351 525:0.0455902 526:0.0171657 527:0.0187157 528:0.0100016 529:0.0261606 530:0.0120537 531:0.00830995 532:0.030098 533:0.0311411 534:0.0431303 535:0.0296198 536:0.000502937 537:0.0262302 538:0.0250819 539:0.0678498 540:0.0405374 541:0.0336772 542:0.014426 543:0.0304986 544:0 545:0.0265925 546:0.0212512 547:0.0411537 548:0.0486068 549:0.0528264 550:0.038732 551:0.0396037 552:0.00987288 553:0.0457584 554:0.056337 555:0.0423088 556:0.0430771 557:0.053409 558:0.0565501 559:0.0469604 560:0.00108757 561:0.0409494 562:0.0316201 563:0.0450429 564:0.037169 565:0.0151146 566:0.0269165 567:0.0228788 568:0.00600432 569:0.0478623 570:0.0334965 571:0.0249884 572:0.016492 573:0.01122 574:0.0167254 575:0.0273914 576:0.0318701 577:0.0861294 578:0.0962291 579:0.0882309 580:0.0879215 581:0.0848312 582:0.0878392 583:0.105256 584:0.0882262 585:0.102807 586:0.0970301 587:0.105422 588:0.0839465 589:0.11879 590:0.107765 591:0.111405 592:0.102841 593:0.111072 594:0.109183 595:0.092447 596:0.116378 597:0.12135 598:0.130682 599:0.133009 600:0.0982037 601:0.113346 602:0.110889 603:0.137646 604:0.149938 605:0.1467 606:0.123758 607:0.149288 608:0.0991906 609:0.0763727 610:0.100238 611:0.150135 612:0.163103 613:0.130977 614:0.128702 615:0.112093 616:0.109048 617:0.120057 618:0.128853 619:0.123878 620:0.139261 621:0.141745 622:0.138632 623:0.0757988 624:0.102153 625:0.118917 626:0.113113 627:0.124411 628:0.106229 629:0.10889 630:0.110529 631:0.107437 632:0.125773 633:0.117651 634:0.0976283 635:0.0951291 636:0.0923233 637:0.0902024 638:0.0989777 639:0.112663 640:0.129627 641:0.0198865 642:0.0409594 643:0.0434626 644:0.0415584 645:0.0392063 646:0.0470905 647:0.0419593 648:0.0664439 649:0.0437117 650:0.0419575 651:0.0447728 652:0.0365238 653:0.0723226 654:0.0557424 655:0.0548254 656:0.0505695 657:0.0599798 658:0.0547072 659:0.0502886 660:0.0684471 661:0.075063 662:0.0640143 663:0.0551774 664:0.0502139 665:0.0531883 666:0.0513196 667:0.0837945 668:0.111735 669:0.123481 670:0.10686 671:0.0942457 672:0.0531717 673:0.0535224 674:0.0999267 675:0.108153 676:0.108315 677:0.110386 678:0.110678 679:0.0960921 680:0.0505777 681:0.0495874 682:0.0861525 683:0.102371 684:0.0801096 685:0.0512123 686:0.0839063 687:0.0717172 688:0.058669 689:0.0322862 690:0.0447255 691:0.0841418 692:0.0828292 693:0.0545225 694:0.0476497 695:0.0472443 696:0.0683172 697:0.0574793 698:0.0494938 699:0.0475307 700:0.0536623 701:0.0482387 702:0.0498765 703:0.0538376 704:0.0602527 705:0.0950903 706:0.152391 707:0.169292 708:0.171 709:0.166281 710:0.17142 711:0.183606 712:0.180905 713:0.139284 714:0.171554 715:0.162851 716:0.16994 717:0.174708 718:0.183157 719:0.183745 720:0.164051 721:0.187136 722:0.195663 723:0.189897 724:0.18561 725:0.236999 726:0.23275 727:0.191415 728:0.181212 729:0.190643 730:0.205289 731:0.224386 732:0.338118 733:0.365757 734:0.360552 735:0.266044 736:0.206209 737:0.18589 738:0.26429 739:0.338668 740:0.338193 741:0.381324 742:0.357987 743:0.322473 744:0.20663 745:0.190766 746:0.259218 747:0.327966 748:0.285762 749:0.233646 750:0.281338 751:0.291115 752:0.196373 753:0.213647 754:0.219557 755:0.276634 756:0.28122 757:0.23507 758:0.202541 759:0.221387 760:0.213445 761:0.262003 762:0.24336 763:0.217891 764:0.20233 765:0.198645 766:0.200218 767:0.19853 768:0.195905 769:0 770:0 771:0 772:0 773:0 774:0 775:0 776:0 777:0 778:0 779:0 780:0 781:0 782:0 783:0 784:0 785:0 786:0 787:0 788:0 789:0 790:0 791:0 792:0 793:0 794:0 795:0 796:0 797:0 798:0 799:0 800:0 801:0 802:0 803:0 804:0 805:0 806:0 807:0 808:0 809:0 810:0 811:0 812:0 813:0 814:0 815:0 816:0 817:0 818:0 819:0 820:0 821:0 822:0 823:0 824:0 825:0 826:0 827:0 828:0 829:0 830:0 831:0 832:0 833:0.0830995 834:0.0861149 835:0.0624677 836:0.0660692 837:0.0591709 838:0.05727 839:0.0622126 840:0.0514782 841:0.0880814 842:0.0794211 843:0.0758419 844:0.078972 845:0.0906814 846:0.0865964 847:0.0697974 848:0.0659167 849:0.0767353 850:0.0825222 851:0.0669971 852:0.101154 853:0.10468 854:0.0891883 855:0.0961578 856:0.0656237 857:0.0640441 858:0.0855462 859:0.125405 860:0.131032 861:0.0799923 862:0.0752902 863:0.0926229 864:0.0667007 865:0.0781222 866:0.0918674 867:0.102347 868:0.0855303 869:0.0483411 870:0.0491773 871:0.04036 872:0.0657269 873:0.0756614 874:0.0860645 875:0.0879243 876:0.0628713 877:0.0762835 878:0.0794642 879:0.0785281 880:0.0797667 881:0.0927358 882:0.0881989 883:0.0759532 884:0.0634683 885:0.069564 886:0.0802029 887:0.0950047 888:0.0589283 889:0.0693444 890:0.0677629 891:0.0770022 892:0.0815962 893:0.0826023 894:0.0762068 895:0.0890708 896:0.022616 897:0.056931 898:0.079442 899:0.0958827 900:0.0947184 901:0.0871586 902:0.0895563 903:0.101483 904:0.150916 905:0.0145885 906:0.0141054 907:0.0238189 908:0.0139687 909:0.0295882 910:0.0112958 911:0.0277916 912:0.0859925 913:0.0250149 914:0.0184822 915:0.017594 916:0.0200601 917:0.0152521 918:0.0437124 919:0.022679 920:0.0940979 921:0.0349394 922:0.0178552 923:0.00321254 924:0.0228257 925:0.0474212 926:0.0953807 927:0.0661989 928:0.0927755 929:0.0130665 930:0 931:0.0508821 932:0.0301636 933:0.0359435 934:0.0529551 935:0.0652273 936:0.104527 937:0.00532352 938:0.00163203 939:0.0485617 940:0.0274585 941:0.0187012 942:0.0179837 943:0.0358029 944:0.126998 945:0.0296323 946:0.0154107 947:0.00645725 948:0.0418828 949:0.0218632 950:0.0127619 951:0.0198027 952:0.127532 953:0.0447434 954:0.0519131 955:0.0380169 956:0.0145445 957:0.0271031 958:0.0245126 959:0.0224979 960:0.114588 961:0.0555519 962:0.05897 963:0.0575729 964:0.0509607 965:0.0524167 966:0.0664609 967:0.0583566 968:0.0492955 969:0.0477582 970:0.0431307 971:0.0446889 972:0.0484113 973:0.0551345 974:0.038058 975:0.053598 976:0.0595628 977:0.0595536 978:0.0429823 979:0.0703346 980:0.0887177 981:0.0871997 982:0.0820663 983:0.0512555 984:0.0623443 985:0.0569321 986:0.0864922 987:0.0884269 988:0.119611 989:0.0936708 990:0.103103 991:0.0612878 992:0.0484739 993:0.0525351 994:0.0899419 995:0.0995256 996:0.0623074 997:0.0305016 998:0.0792674 999:0.0531613 1000:0.0420156 1001:0.0534301 1002:0.0537675 1003:0.065296 1004:0.0693932 1005:0.0411845 1006:0.0307051 1007:0.0410645 1008:0.0590094 1009:0.0561983 1010:0.0537724 1011:0.0439874 1012:0.0411908 1013:0.0414589 1014:0.0542979 1015:0.0668656 1016:0.0617813 1017:0.0626607 1018:0.0496167 1019:0.0339541 1020:0.042823 1021:0.0428319 1022:0.0298626 1023:0.0270744 1024:0.0447503 1025:0 1026:0 1027:0 1028:0.000573359 1029:0 1030:0.000659231 1031:0 1032:0.00536905 1033:0 1034:0 1035:0 1036:0 1037:0 1038:0 1039:0.00382602 1040:0.0143146 1041:0 1042:0 1043:0.0162313 1044:0.000702575 1045:0 1046:0.00520746 1047:0 1048:0.0103799 1049:0 1050:0 1051:0.012604 1052:0 1053:0 1054:0 1055:0 1056:0.0104685 1057:0 1058:0 1059:0 1060:0 1061:0 1062:0 1063:0 1064:0.0281823 1065:0 1066:0 1067:0 1068:0 1069:0 1070:0 1071:0 1072:0.0354388 1073:0 1074:0 1075:0 1076:0 1077:0 1078:0 1079:0 1080:0.0183019 1081:0 1082:0 1083:0 1084:0 1085:0 1086:0 1087:0 1088:0.0439702 1089:0.0112379 1090:0.0178384 1091:0.0205411 1092:0.0115394 1093:0.0159766 1094:0.0162211 1095:0.0221688 1096:0.0148347 1097:0 1098:0 1099:0 1100:0 1101:0 1102:0 1103:0 1104:0 1105:0 1106:0 1107:0 1108:0 1109:0 1110:0 1111:0 1112:0 1113:0 1114:0 1115:0 1116:0 1117:0 1118:0 1119:0 1120:0 1121:0 1122:0 1123:0 1124:0 1125:0 1126:0 1127:0 1128:0 1129:0 1130:0 1131:0 1132:0 1133:0 1134:0 1135:0 1136:0 1137:0 1138:0 1139:0 1140:0 1141:0 1142:0 1143:0 1144:0 1145:0 1146:0 1147:0 1148:0 1149:0 1150:0 1151:0 1152:0 1153:0 1154:0 1155:0 1156:0 1157:0 1158:0 1159:0 1160:0 1161:0 1162:0 1163:0 1164:0 1165:0 1166:0 1167:0 1168:0 1169:0 1170:0 1171:0 1172:0 1173:0 1174:0 1175:0 1176:0 1177:0 1178:0 1179:0 1180:0 1181:0 1182:0 1183:0 1184:0 1185:0 1186:0 1187:0 1188:0 1189:0 1190:0 1191:0 1192:0 1193:0 1194:0 1195:0 1196:0 1197:0 1198:0 1199:0 1200:0 1201:0 1202:0 1203:0 1204:0 1205:0 1206:0 1207:0 1208:0 1209:0 1210:0 1211:0 1212:0 1213:0 1214:0 1215:0 1216:0 1217:0.0356614 1218:0 1219:0 1220:0 1221:0 1222:0 1223:0 1224:0 1225:0 1226:0 1227:0 1228:0 1229:0 1230:0 1231:0 1232:0 1233:0 1234:0 1235:0 1236:0 1237:0 1238:0 1239:0 1240:0 1241:0 1242:0 1243:0 1244:0 1245:0 1246:0 1247:0 1248:0 1249:0 1250:0 1251:0 1252:0 1253:0 1254:0 1255:0 1256:0 1257:0 1258:0 1259:0 1260:0 1261:0 1262:0 1263:0 1264:0 1265:0 1266:0 1267:0 1268:0 1269:0 1270:0 1271:0 1272:0 1273:0 1274:0 1275:0.000486519 1276:0 1277:0 1278:0 1279:0 1280:0.0116179 1281:0.0118914 1282:0.0171388 1283:0.0235292 1284:0.0130789 1285:0.0201715 1286:0.0225442 1287:0.0174932 1288:0.0702981 1289:0.00732378 1290:0.0257104 1291:0.0286597 1292:0.0186776 1293:0.0385853 1294:0.024061 1295:0.0268348 1296:0.0694646 1297:0.0269946 1298:0.0263829 1299:0.0315461 1300:0.0376788 1301:0.0380147 1302:0.0208258 1303:0.0257742 1304:0.0743933 1305:0.0272458 1306:0.0411723 1307:0 1308:0.00981494 1309:0.00887717 1310:0.0228827 1311:0.050436 1312:0.0863263 1313:0.0195003 1314:0.0400433 1315:0.00765013 1316:0.0163741 1317:0.0396199 1318:0.0211887 1319:0.0523805 1320:0.0862225 1321:0.0200816 1322:0.00756143 1323:0.0258821 1324:0.0300513 1325:0.0215598 1326:0.0226539 1327:0.0709909 1328:0.0892382 1329:0.040204 1330:0.0407456 1331:0.0323839 1332:0.04626 1333:0.0335426 1334:0.0237569 1335:0.0229617 1336:0.110981 1337:0.0393637 1338:0.0469139 1339:0.0379238 1340:0.0256474 1341:0.0204882 1342:0.029398 1343:0.0281979 1344:0.0913322 1345:0.0501984 1346:0.0645513 1347:0.0590122 1348:0.0610689 1349:0.0603246 1350:0.0559069 1351:0.0543382 1352:0.0582681 1353:0 1354:0.005342 1355:0 1356:0.00926105 1357:0.019725 1358:0 1359:0 1360:0.0386794 1361:0 1362:0 1363:0.011624 1364:0.0133629 1365:0.00795845 1366:0 1367:0 1368:0.0322254 1369:0 1370:0 1371:0 1372:0 1373:0 1374:0 1375:0 1376:0.0333915 1377:0 1378:0.00735037 1379:0 1380:0 1381:0 1382:0 1383:0 1384:0.0490666 1385:0 1386:0 1387:0 1388:0 1389:0 1390:0 1391:0 1392:0.0356935 1393:0.0172126 1394:0 1395:0 1396:0 1397:0 1398:0.00139914 1399:0 1400:0.0551809 1401:0.0125704 1402:0.00817756 1403:0 1404:0 1405:0 1406:0 1407:0 1408:0.0524436 1409:0.0742821 1410:0.097155 1411:0.106589 1412:0.112914 1413:0.108103 1414:0.113123 1415:0.116841 1416:0.12066 1417:0.122603 1418:0.118252 1419:0.114495 1420:0.103486 1421:0.126514 1422:0.121428 1423:0.132834 1424:0.11937 1425:0.145246 1426:0.127961 1427:0.151995 1428:0.193992 1429:0.176109 1430:0.206709 1431:0.153325 1432:0.129455 1433:0.146739 1434:0.150767 1435:0.185149 1436:0.240513 1437:0.257033 1438:0.278946 1439:0.244351 1440:0.141287 1441:0.140684 1442:0.174187 1443:0.229879 1444:0.179189 1445:0.211919 1446:0.261874 1447:0.232952 1448:0.146768 1449:0.12284 1450:0.150413 1451:0.204697 1452:0.185425 1453:0.144849 1454:0.154098 1455:0.177467 1456:0.140088 1457:0.148601 1458:0.135631 1459:0.128541 1460:0.173337 1461:0.147131 1462:0.13714 1463:0.126521 1464:0.164145 1465:0.169249 1466:0.158255 1467:0.151163 1468:0.131218 1469:0.134318 1470:0.121084 1471:0.128278 1472:0.181141 1473:0.0971939 1474:0.12007 1475:0.10423 1476:0.106177 1477:0.105556 1478:0.112839 1479:0.111448 1480:0.103276 1481:0.0976317 1482:0.0936151 1483:0.0999389 1484:0.109503 1485:0.10669 1486:0.08855 1487:0.101702 1488:0.104368 1489:0.0873878 1490:0.0897523 1491:0.110843 1492:0.118347 1493:0.110632 1494:0.114505 1495:0.0955846 1496:0.0981452 1497:0.0802741 1498:0.0892732 1499:0.120747 1500:0.118011 1501:0.096357 1502:0.115448 1503:0.0910112 1504:0.0853568 1505:0.0794324 1506:0.111704 1507:0.0903166 1508:0.0590125 1509:0.0499663 1510:0.0737483 1511:0.0697984 1512:0.0913949 1513:0.0850203 1514:0.0684075 1515:0.0782556 1516:0.084312 1517:0.0771924 1518:0.0603602 1519:0.095569 1520:0.105823 1521:0.102219 1522:0.0883675 1523:0.0701468 1524:0.0844608 1525:0.0972013 1526:0.0893384 1527:0.0941765 1528:0.0951439 1529:0.100007 1530:0.100803 1531:0.095862 1532:0.0864215 1533:0.0814507 1534:0.0756574 1535:0.0720372 1536:0.105474 1537:0.0207229 1538:0.0665407 1539:0.072141 1540:0.0647917 1541:0.0660241 1542:0.0677974 1543:0.085971 1544:0.0929042 1545:0.0202039 1546:0.0358846 1547:0.0389594 1548:0.0535831 1549:0.035238 1550:0.0579269 1551:0.0505521 1552:0.0575642 1553:0.0493244 1554:0.0545334 1555:0.0470729 1556:0.0494368 1557:0.0739699 1558:0.0433625 1559:0.0504543 1560:0.0522568 1561:0.0655266 1562:0.0602356 1563:0.0747927 1564:0.120811 1565:0.138353 1566:0.131565 1567:0.0690626 1568:0.0524792 1569:0.0685626 1570:0.0802632 1571:0.154453 1572:0.138307 1573:0.166128 1574:0.149526 1575:0.121924 1576:0.042861 1577:0.0468313 1578:0.119959 1579:0.156372 1580:0.124433 1581:0.102345 1582:0.121759 1583:0.105835 1584:0.0528564 1585:0.0108611 1586:0.0698631 1587:0.0987798 1588:0.10352 1589:0.0669017 1590:0.0566188 1591:0.0575911 1592:0.077191 1593:0.116184 1594:0.0861932 1595:0.0725237 1596:0.0743071 1597:0.0774654 1598:0.0793342 1599:0.088902 1600:0.119947 1601:0 1602:0 1603:0 1604:0 1605:0 1606:0 1607:0 1608:0 1609:0 1610:0 1611:0 1612:0 1613:0 1614:0 1615:0 1616:0 1617:0 1618:0 1619:0 1620:0 1621:0 1622:0 1623:0 1624:0 1625:0 1626:0 1627:0 1628:0 1629:0 1630:0 1631:0 1632:0 1633:0 1634:0 1635:0 1636:0 1637:0 1638:0 1639:0 1640:0 1641:0 1642:0 1643:0 1644:0 1645:0 1646:0 1647:0 1648:0 1649:0 1650:0 1651:0 1652:0 1653:0 1654:0 1655:0 1656:0 1657:0 1658:0 1659:0 1660:0 1661:0 1662:0 1663:0 1664:0 1665:0 1666:0 1667:0.00216569 1668:0 1669:0 1670:0 1671:0 1672:0.0301373 1673:0 1674:0 1675:0 1676:0 1677:0 1678:0 1679:0 1680:0 1681:0 1682:0 1683:0 1684:0.00508879 1685:0 1686:0 1687:0 1688:0 1689:0 1690:0 1691:0 1692:0 1693:0 1694:0 1695:0.0279125 1696:0 1697:0 1698:0 1699:0 1700:0 1701:0 1702:0 1703:0 1704:0 1705:0 1706:0 1707:0 1708:0 1709:0 1710:0 1711:0 1712:0 1713:0 1714:0 1715:0 1716:0 1717:0 1718:0 1719:0 1720:0 1721:0 1722:0 1723:0 1724:0 1725:0 1726:0 1727:0 1728:0 1729:0.0238795 1730:0.0926033 1731:0.0948793 1732:0.0994762 1733:0.0985057 1734:0.115817 1735:0.113397 1736:0.110564 1737:0.0923186 1738:0.101138 1739:0.103088 1740:0.0900928 1741:0.0965801 1742:0.108462 1743:0.107804 1744:0.0821329 1745:0.132438 1746:0.139241 1747:0.104814 1748:0.131666 1749:0.156214 1750:0.129176 1751:0.125739 1752:0.10212 1753:0.131075 1754:0.123352 1755:0.218358 1756:0.265938 1757:0.343267 1758:0.295838 1759:0.277148 1760:0.122077 1761:0.107388 1762:0.177481 1763:0.293383 1764:0.287709 1765:0.309925 1766:0.283819 1767:0.318367 1768:0.114831 1769:0.108625 1770:0.153877 1771:0.263727 1772:0.217347 1773:0.158145 1774:0.197524 1775:0.216566 1776:0.102233 1777:0.138052 1778:0.133542 1779:0.183369 1780:0.20079 1781:0.154181 1782:0.122834 1783:0.160138 1784:0.139532 1785:0.17298 1786:0.16648 1787:0.166733 1788:0.134573 1789:0.139278 1790:0.141351 1791:0.151881 1792:0.160374 1793:0.155989 1794:0.19511 1795:0.199672 1796:0.202816 1797:0.198946 1798:0.219633 1799:0.216493 1800:0.198889 1801:0.187608 1802:0.190657 1803:0.18457 1804:0.168371 1805:0.156946 1806:0.189413 1807:0.193824 1808:0.175692 1809:0.229361 1810:0.20765 1811:0.198712 1812:0.282326 1813:0.273984 1814:0.255801 1815:0.218214 1816:0.204568 1817:0.220207 1818:0.217177 1819:0.286294 1820:0.392614 1821:0.399645 1822:0.366634 1823:0.297107 1824:0.212962 1825:0.20799 1826:0.318699 1827:0.352608 1828:0.366706 1829:0.369274 1830:0.376646 1831:0.327585 1832:0.213572 1833:0.209261 1834:0.30524 1835:0.341147 1836:0.288167 1837:0.257716 1838:0.271964 1839:0.268965 1840:0.229538 1841:0.223849 1842:0.231689 1843:0.267507 1844:0.275245 1845:0.234964 1846:0.221544 1847:0.23182 1848:0.251567 1849:0.238793 1850:0.240895 1851:0.238317 1852:0.227422 1853:0.23268 1854:0.220244 1855:0.218666 1856:0.21244 1857:0.0822824 1858:0.0808645 1859:0.0625025 1860:0.0624696 1861:0.0731415 1862:0.066889 1863:0.0773214 1864:0.0514531 1865:0.077402 1866:0.072062 1867:0.0578924 1868:0.0716436 1869:0.0645121 1870:0.0732115 1871:0.0564902 1872:0.0635883 1873:0.076166 1874:0.0679035 1875:0.0704851 1876:0.0734124 1877:0.0950006 1878:0.0786508 1879:0.0743054 1880:0.0575002 1881:0.0731962 1882:0.102084 1883:0.102293 1884:0.15943 1885:0.105288 1886:0.125452 1887:0.0665343 1888:0.0603776 1889:0.0984565 1890:0.11931 1891:0.113266 1892:0.132223 1893:0.12744 1894:0.158426 1895:0.0651608 1896:0.0755901 1897:0.087519 1898:0.154679 1899:0.117742 1900:0.0926748 1901:0.0960132 1902:0.123386 1903:0.06264 1904:0.0878864 1905:0.0827465 1906:0.0883242 1907:0.108898 1908:0.0959973 1909:0.0701842 1910:0.0644701 1911:0.0799638 1912:0.0759834 1913:0.0972544 1914:0.0812543 1915:0.0765324 1916:0.0749548 1917:0.0733209 1918:0.0772832 1919:0.123249 1920:0.0858702 1921:0 1922:0 1923:0 1924:0 1925:0 1926:0 1927:0 1928:0 1929:0 1930:0 1931:0 1932:0 1933:0 1934:0 1935:0 1936:0 1937:0 1938:0 1939:0 1940:0 1941:0 1942:0 1943:0 1944:0 1945:0 1946:0 1947:0 1948:0 1949:0 1950:0 1951:0 1952:0 1953:0 1954:0 1955:0 1956:0 1957:0 1958:0 1959:0 1960:0 1961:0 1962:0 1963:0 1964:0 1965:0 1966:0 1967:0 1968:0 1969:0 1970:0 1971:0 1972:0 1973:0 1974:0 1975:0 1976:0 1977:0 1978:0 1979:0 1980:0 1981:0 1982:0 1983:0 1984:0.00995774 1985:0 1986:0 1987:0 1988:0 1989:0 1990:0 1991:0 1992:0 1993:0 1994:0 1995:0 1996:0 1997:0 1998:0 1999:0 2000:0 2001:0 2002:0 2003:0 2004:0 2005:0 2006:0 2007:0 2008:0 2009:0 2010:0 2011:0 2012:0 2013:0 2014:0 2015:0 2016:0 2017:0 2018:0 2019:0 2020:0 2021:0 2022:0 2023:0 2024:0 2025:0 2026:0 2027:0.00775048 2028:0 2029:0 2030:0 2031:0.0203785 2032:0 2033:0 2034:0 2035:0 2036:0 2037:0 2038:0 2039:0 2040:0 2041:0.0991987 2042:0.0797814 2043:0.0907681 2044:0.0811089 2045:0.0765803 2046:0.0734008 2047:0.0766092 2048:0.0761221 2049:0.00477481 2050:0.00375959 2051:0.00797749 2052:0.0114988 2053:0.0132734 2054:0.0170212 2055:0.0113956 2056:0.00615158 2057:0.0226085 2058:0 2059:0 2060:0.00812416 2061:0.00543666 2062:0.00548717 2063:0.00690672 2064:0.00430434 2065:0.0229243 2066:0 2067:0.0382541 2068:0.0669664 2069:0.0498969 2070:0.0431612 2071:0.00953509 2072:0.00386402 2073:0.0240454 2074:0.0364108 2075:0.0552214 2076:0.0751935 2077:0.100822 2078:0.0947972 2079:0.0672436 2080:0.0150891 2081:0.0135624 2082:0.0701675 2083:0.0672657 2084:0.0729498 2085:0.095701 2086:0.104243 2087:0.0622278 2088:0.00745169 2089:0.0211562 2090:0.028293 2091:0.0888721 2092:0.0764132 2093:0.0354575 2094:0.0283878 2095:0.0340042 2096:0.0224184 2097:0.0359014 2098:0.0199911 2099:0.020207 2100:0.0322766 2101:0.0276415 2102:0.0132665 2103:0.0211777 2104:0.0392609 2105:0.0178915 2106:0.0320314 2107:0.022912 2108:0.0257423 2109:0.0150022 2110:0.00388861 2111:0 2112:0.0120256 2113:0 2114:0 2115:0 2116:0 2117:0 2118:0 2119:0 2120:0.047213 2121:0 2122:0 2123:0 2124:0 2125:0 2126:0 2127:0 2128:0.0345405 2129:0 2130:0 2131:0 2132:0 2133:0 2134:0 2135:0 2136:0.0257501 2137:0 2138:0 2139:0 2140:0 2141:0 2142:0 2143:0 2144:0.0292606 2145:0 2146:0 2147:0 2148:0 2149:0 2150:0 2151:0 2152:0.030542 2153:0 2154:0 2155:0 2156:0 2157:0 2158:0 2159:0 2160:0.0396982 2161:0 2162:0 2163:0 2164:0 2165:0 2166:0 2167:0 2168:0.0525896 2169:0 2170:0 2171:0 2172:0 2173:0 2174:0 2175:0 2176:0.0333114 2177:0.016285 2178:0.0297778 2179:0.0199941 2180:0.0256469 2181:0.0172823 2182:0.0310797 2183:0.0273994 2184:0.049306 2185:0.0100152 2186:0.00669708 2187:0.0253191 2188:0.0202111 2189:0.0460599 2190:0.0116727 2191:0.0229934 2192:0.0172059 2193:0.0282741 2194:0.0225266 2195:0.0257386 2196:0 2197:0.0163705 2198:0.0160038 2199:0.0165363 2200:0.0194595 2201:0.0365763 2202:0.0315524 2203:0.0628693 2204:0.0519256 2205:0.0749578 2206:0.0802563 2207:0.0474369 2208:0.0239887 2209:0.0273679 2210:0.0322811 2211:0.113008 2212:0.0845724 2213:0.101208 2214:0.0811336 2215:0.0994142 2216:0.0371253 2217:0.0246862 2218:0.0406229 2219:0.0752285 2220:0.0446827 2221:0.041385 2222:0.0553147 2223:0.0582529 2224:0.0349789 2225:0.0378723 2226:0.0164781 2227:0.0520545 2228:0.0531945 2229:0.0143477 2230:0.0229931 2231:0.0427233 2232:0.0605722 2233:0.104173 2234:0.097643 2235:0.0736799 2236:0.0697105 2237:0.0760452 2238:0.073451 2239:0.0886578 2240:0.0696692 2241:0.109231 2242:0.142704 2243:0.146778 2244:0.148333 2245:0.152494 2246:0.145329 2247:0.161007 2248:0.155723 2249:0.114922 2250:0.124515 2251:0.12467 2252:0.133291 2253:0.132575 2254:0.135028 2255:0.118662 2256:0.12391 2257:0.117449 2258:0.119909 2259:0.128145 2260:0.125177 2261:0.125466 2262:0.116103 2263:0.13249 2264:0.118348 2265:0.128557 2266:0.137918 2267:0.131753 2268:0.144624 2269:0.136566 2270:0.125874 2271:0.117319 2272:0.128376 2273:0.13991 2274:0.116281 2275:0.158532 2276:0.181423 2277:0.196745 2278:0.174172 2279:0.191296 2280:0.141901 2281:0.129965 2282:0.175636 2283:0.188468 2284:0.182696 2285:0.169572 2286:0.182059 2287:0.17113 2288:0.133911 2289:0.129273 2290:0.141592 2291:0.162301 2292:0.166677 2293:0.131545 2294:0.128675 2295:0.126579 2296:0.144921 2297:0.105325 2298:0.132095 2299:0.14176 2300:0.142143 2301:0.129374 2302:0.130195 2303:0.131171 2304:0.119602 2305:0.0643654 2306:0.103573 2307:0.110373 2308:0.104285 2309:0.108735 2310:0.109205 2311:0.11759 2312:0.111493 2313:0.0619581 2314:0.0746138 2315:0.0760191 2316:0.0748005 2317:0.0805588 2318:0.0739605 2319:0.0863884 2320:0.0799819 2321:0.0821152 2322:0.0841164 2323:0.111963 2324:0.099823 2325:0.11795 2326:0.12368 2327:0.0928362 2328:0.0746171 2329:0.0874742 2330:0.101477 2331:0.102708 2332:0.126482 2333:0.166578 2334:0.151337 2335:0.158195 2336:0.0785858 2337:0.0847972 2338:0.107803 2339:0.122125 2340:0.140548 2341:0.155294 2342:0.146938 2343:0.159955 2344:0.0937904 2345:0.0693008 2346:0.0803492 2347:0.147908 2348:0.137764 2349:0.103527 2350:0.109665 2351:0.14595 2352:0.0786157 2353:0.0815377 2354:0.0744018 2355:0.0999378 2356:0.130605 2357:0.108833 2358:0.0876114 2359:0.081709 2360:0.0875272 2361:0.112274 2362:0.107757 2363:0.101754 2364:0.0952449 2365:0.0881421 2366:0.0860702 2367:0.0877131 2368:0.122747 2369:0.0966627 2370:0.142914 2371:0.149512 2372:0.143406 2373:0.139484 2374:0.157822 2375:0.170019 2376:0.145516 2377:0.142798 2378:0.147617 2379:0.142114 2380:0.134431 2381:0.180345 2382:0.16406 2383:0.151174 2384:0.146499 2385:0.176493 2386:0.16458 2387:0.169201 2388:0.195481 2389:0.258058 2390:0.212244 2391:0.184529 2392:0.164475 2393:0.175855 2394:0.17329 2395:0.285213 2396:0.378261 2397:0.351904 2398:0.323363 2399:0.234022 2400:0.166196 2401:0.158558 2402:0.273691 2403:0.327864 2404:0.392009 2405:0.380079 2406:0.358843 2407:0.283829 2408:0.171498 2409:0.165396 2410:0.306745 2411:0.31479 2412:0.28868 2413:0.241133 2414:0.285502 2415:0.216253 2416:0.197184 2417:0.1794 2418:0.208555 2419:0.261497 2420:0.249692 2421:0.187142 2422:0.1718 2423:0.189894 2424:0.200303 2425:0.148026 2426:0.183581 2427:0.1953 2428:0.185316 2429:0.173376 2430:0.163422 2431:0.186882 2432:0.172474 2433:0.0593771 2434:0.0632831 2435:0.0702099 2436:0.0602909 2437:0.0611282 2438:0.0804982 2439:0.0700708 2440:0.0589181 2441:0.0661736 2442:0.0551735 2443:0.0471843 2444:0.0703719 2445:0.0641768 2446:0.0567046 2447:0.0569571 2448:0.0619619 2449:0.0734094 2450:0.0558524 2451:0.111995 2452:0.161206 2453:0.128114 2454:0.126992 2455:0.0576183 2456:0.0594193 2457:0.068405 2458:0.13021 2459:0.134785 2460:0.171396 2461:0.182151 2462:0.171098 2463:0.129296 2464:0.0649701 2465:0.0761398 2466:0.151852 2467:0.16683 2468:0.136519 2469:0.130706 2470:0.162614 2471:0.102016 2472:0.0663279 2473:0.069252 2474:0.111817 2475:0.151045 2476:0.126676 2477:0.0669517 2478:0.0652527 2479:0.0814998 2480:0.0816233 2481:0.101946 2482:0.0722448 2483:0.0690558 2484:0.0771874 2485:0.0747911 2486:0.0754544 2487:0.092867 2488:0.0729244 2489:0.105367 2490:0.0999283 2491:0.0763838 2492:0.0665362 2493:0.0644575 2494:0.0606173 2495:0.0580742 2496:0.0937107 2497:0 2498:0 2499:0 2500:0 2501:0 2502:0 2503:0 2504:0 2505:0 2506:0 2507:0 2508:0 2509:0 2510:0 2511:0 2512:0 2513:0 2514:0 2515:0 2516:0 2517:0 2518:0 2519:0 2520:0 2521:0 2522:0 2523:0 2524:0 2525:0 2526:0 2527:0 2528:0 2529:0 2530:0 2531:0 2532:0 2533:0 2534:0 2535:0 2536:0 2537:0 2538:0 2539:0 2540:0 2541:0 2542:0 2543:0 2544:0 2545:0 2546:0 2547:0 2548:0 2549:0 2550:0 2551:0 2552:0 2553:0 2554:0 2555:0 2556:0 2557:0 2558:0 2559:0 2560:0 2561:0.15357 2562:0.195474 2563:0.194706 2564:0.190811 2565:0.195942 2566:0.200053 2567:0.215382 2568:0.196355 2569:0.194442 2570:0.194573 2571:0.191428 2572:0.184403 2573:0.198994 2574:0.195453 2575:0.203307 2576:0.176618 2577:0.23121 2578:0.202535 2579:0.194432 2580:0.217431 2581:0.231335 2582:0.218809 2583:0.207025 2584:0.197196 2585:0.23719 2586:0.20743 2587:0.295883 2588:0.34274 2589:0.337927 2590:0.313319 2591:0.264133 2592:0.198998 2593:0.220835 2594:0.263745 2595:0.329296 2596:0.369623 2597:0.358538 2598:0.348578 2599:0.314766 2600:0.200278 2601:0.237085 2602:0.30792 2603:0.329772 2604:0.298945 2605:0.275754 2606:0.31305 2607:0.265947 2608:0.196199 2609:0.242841 2610:0.243954 2611:0.279647 2612:0.277095 2613:0.229706 2614:0.212868 2615:0.21916 2616:0.227559 2617:0.247008 2618:0.220731 2619:0.219383 2620:0.219408 2621:0.215118 2622:0.215699 2623:0.235586 2624:0.225804 2625:0.0556966 2626:0.0711041 2627:0.0648894 2628:0.0689386 2629:0.0707784 2630:0.0729598 2631:0.0678312 2632:0.0725948 2633:0.0631596 2634:0.0636733 2635:0.0681869 2636:0.0821413 2637:0.0901967 2638:0.0813848 2639:0.0787238 2640:0.0725682 2641:0.0758851 2642:0.0761321 2643:0.0877796 2644:0.076656 2645:0.0781638 2646:0.0832229 2647:0.0806907 2648:0.0803904 2649:0.0757478 2650:0.0800277 2651:0.104039 2652:0.0969136 2653:0.0904019 2654:0.0864351 2655:0.0739832 2656:0.0631183 2657:0.0917601 2658:0.0905544 2659:0.101547 2660:0.105201 2661:0.123427 2662:0.123379 2663:0.0876443 2664:0.084073 2665:0.07968 2666:0.101552 2667:0.105641 2668:0.100394 2669:0.0845803 2670:0.101909 2671:0.0994726 2672:0.0779241 2673:0.0809791 2674:0.0675563 2675:0.101576 2676:0.0932191 2677:0.0848401 2678:0.0726094 2679:0.0837598 2680:0.0797566 2681:0.0928261 2682:0.0844298 2683:0.0576718 2684:0.0642925 2685:0.0697422 2686:0.071405 2687:0.0872602 2688:0.0934156 2689:0.0272417 2690:0.00695375 2691:0.00477966 2692:0.010668 2693:0.0158726 2694:0.000952623 2695:0.0131621 2696:0.0290805 2697:0.0168047 2698:0 2699:0.00516025 2700:0.00607634 2701:0 2702:0.00279983 2703:0 2704:0 2705:0.000591166 2706:0 2707:0 2708:0 2709:0 2710:0 2711:0 2712:0 2713:0.0114768 2714:0 2715:0 2716:0 2717:0 2718:0 2719:0 2720:0 2721:0.0195375 2722:0 2723:0 2724:0 2725:0 2726:0 2727:0 2728:0 2729:0.0119209 2730:0.0180972 2731:0 2732:0 2733:0 2734:0.008188 2735:0 2736:0 2737:0 2738:0 2739:0.00278563 2740:0 2741:0 2742:0 2743:0 2744:0.00161035 2745:0.0742477 2746:0.0553444 2747:0.0529388 2748:0.0518785 2749:0.050675 2750:0.0519786 2751:0.0613193 2752:0.0704894 2753:0.0627872 2754:0.0809999 2755:0.0829177 2756:0.087617 2757:0.0835358 2758:0.0901407 2759:0.0997419 2760:0.101943 2761:0.0665608 2762:0.0497696 2763:0.0389018 2764:0.0199788 2765:0.0468603 2766:0.0564118 2767:0.0538158 2768:0.0352568 2769:0.0693455 2770:0.0485711 2771:0.0382041 2772:0.110784 2773:0.144712 2774:0.124691 2775:0.0922204 2776:0.0510963 2777:0.0622873 2778:0.0376908 2779:0.135939 2780:0.157137 2781:0.166313 2782:0.165569 2783:0.122835 2784:0.0664392 2785:0.056754 2786:0.158694 2787:0.188695 2788:0.162927 2789:0.117333 2790:0.15458 2791:0.112776 2792:0.0486092 2793:0.064343 2794:0.122745 2795:0.102219 2796:0.110086 2797:0.0789414 2798:0.0859982 2799:0.0306899 2800:0.0566031 2801:0.0647646 2802:0.0627848 2803:0.0738459 2804:0.0779603 2805:0.0497987 2806:0.0510811 2807:0.0446922 2808:0.0964719 2809:0.0705149 2810:0.0778562 2811:0.0629597 2812:0.0550888 2813:0.056309 2814:0.0383939 2815:0.0233332 2816:0.081233 2817:0.221283 2818:0.256089 2819:0.257278 2820:0.258829 2821:0.257923 2822:0.279889 2823:0.27829 2824:0.261345 2825:0.250763 2826:0.259634 2827:0.246138 2828:0.25697 2829:0.273664 2830:0.284921 2831:0.28441 2832:0.238606 2833:0.299922 2834:0.282463 2835:0.278037 2836:0.31925 2837:0.357404 2838:0.334463 2839:0.281516 2840:0.269152 2841:0.295474 2842:0.30429 2843:0.411011 2844:0.557953 2845:0.546253 2846:0.534129 2847:0.379081 2848:0.276431 2849:0.287637 2850:0.471205 2851:0.50419 2852:0.550308 2853:0.561534 2854:0.568021 2855:0.420572 2856:0.286796 2857:0.310292 2858:0.494481 2859:0.47782 2860:0.405804 2861:0.381858 2862:0.434887 2863:0.380598 2864:0.314811 2865:0.319419 2866:0.34509 2867:0.437542 2868:0.421549 2869:0.326883 2870:0.298761 2871:0.317791 2872:0.331411 2873:0.338349 2874:0.319184 2875:0.290969 2876:0.288163 2877:0.28716 2878:0.283209 2879:0.320995 2880:0.28498 2881:0 2882:0 2883:0 2884:0 2885:0 2886:0 2887:0 2888:0.0106866 2889:0 2890:0 2891:0 2892:0 2893:0 2894:0 2895:0 2896:0 2897:0 2898:0 2899:0 2900:0 2901:0 2902:0 2903:0 2904:0 2905:0 2906:0 2907:0 2908:0 2909:0 2910:0 2911:0 2912:0 2913:0 2914:0 2915:0 2916:0 2917:0 2918:0 2919:0 2920:0 2921:0 2922:0 2923:0 2924:0 2925:0 2926:0 2927:0 2928:0 2929:0 2930:0 2931:0 2932:0 2933:0 2934:0 2935:0 2936:0.00430845 2937:0 2938:0 2939:0 2940:0 2941:0 2942:0 2943:0 2944:0 2945:0.0639378 2946:0.101442 2947:0.0934842 2948:0.100679 2949:0.0986837 2950:0.110672 2951:0.114326 2952:0.125618 2953:0.115681 2954:0.114415 2955:0.116 2956:0.104063 2957:0.116747 2958:0.119229 2959:0.114475 2960:0.109403 2961:0.123984 2962:0.126697 2963:0.122414 2964:0.170488 2965:0.191769 2966:0.161146 2967:0.138468 2968:0.12814 2969:0.127948 2970:0.125748 2971:0.220906 2972:0.246591 2973:0.258386 2974:0.232176 2975:0.196557 2976:0.137834 2977:0.120639 2978:0.17727 2979:0.232376 2980:0.236841 2981:0.212976 2982:0.223085 2983:0.202074 2984:0.13967 2985:0.12503 2986:0.183687 2987:0.209529 2988:0.189339 2989:0.158817 2990:0.182617 2991:0.151753 2992:0.159198 2993:0.136375 2994:0.149658 2995:0.16796 2996:0.154407 2997:0.133946 2998:0.135082 2999:0.145099 3000:0.181928 3001:0.176645 3002:0.179833 3003:0.155645 3004:0.147274 3005:0.141134 3006:0.141364 3007:0.154691 3008:0.168229 3009:0.0775502 3010:0.064812 3011:0.0378052 3012:0.0393672 3013:0.0438033 3014:0.0519087 3015:0.0411145 3016:0.0271289 3017:0.116086 3018:0.0619506 3019:0.0530749 3020:0.0724213 3021:0.0755984 3022:0.0682382 3023:0.06061 3024:0.0310562 3025:0.139617 3026:0.0615269 3027:0.079252 3028:0.105476 3029:0.0939848 3030:0.0691824 3031:0.0514718 3032:0.0412278 3033:0.147544 3034:0.077043 3035:0.0977193 3036:0.118757 3037:0.0761073 3038:0.0773564 3039:0.0350343 3040:0.0402965 3041:0.133563 3042:0.138747 3043:0.0951786 3044:0.0841521 3045:0.0357965 3046:0.0984683 3047:0.00137786 3048:0.0414377 3049:0.129003 3050:0.104388 3051:0.0803858 3052:0.0647221 3053:0.0422685 3054:0.0669975 3055:0.0329005 3056:0.0483948 3057:0.146469 3058:0.0635884 3059:0.0750549 3060:0.0496733 3061:0.0508578 3062:0.0556469 3063:0.0721639 3064:0.0494962 3065:0.163957 3066:0.0694925 3067:0.0670298 3068:0.0791663 3069:0.0754896 3070:0.0549736 3071:0.0685887 3072:0.0559561 3073:0.0327054 3074:0 3075:0 3076:0 3077:0 3078:0 3079:0 3080:0.00468772 3081:0.0106561 3082:0 3083:0 3084:0 3085:0 3086:0 3087:0 3088:0.0202477 3089:0 3090:0 3091:0 3092:0 3093:0 3094:0 3095:0 3096:0 3097:0 3098:0 3099:0 3100:0 3101:0 3102:0 3103:0 3104:0 3105:0 3106:0 3107:0 3108:0 3109:0 3110:0 3111:0 3112:0.00540012 3113:0 3114:0 3115:0 3116:0 3117:0 3118:0 3119:0 3120:0 3121:0 3122:0 3123:0 3124:0 3125:0 3126:0 3127:0 3128:0 3129:0.0322909 3130:0 3131:0 3132:0 3133:0 3134:0 3135:0.0202278 3136:0.000399027 3137:0 3138:0.0307755 3139:0.0274559 3140:0.0294468 3141:0.0228567 3142:0.0338192 3143:0.0458347 3144:0.054049 3145:0.0271502 3146:0 3147:0 3148:0 3149:0 3150:0 3151:0 3152:0.0019385 3153:0.0431049 3154:0 3155:0 3156:0 3157:0.000818808 3158:0 3159:0 3160:0.0049494 3161:0.0412589 3162:0 3163:0.0428308 3164:0.0417874 3165:0.012128 3166:0 3167:0 3168:0.00226254 3169:0.028889 3170:0.0408722 3171:0.0446864 3172:0.000701835 3173:0 3174:0 3175:0 3176:0.0109647 3177:0.024969 3178:0.0173116 3179:0 3180:0 3181:0 3182:0 3183:0 3184:0.0308737 3185:0.0476673 3186:0 3187:0 3188:0 3189:0 3190:0 3191:0.00220464 3192:0.0248955 3193:0.0797789 3194:0.0285617 3195:0.00390123 3196:0 3197:0.00397568 3198:0 3199:0 3200:0.0102262 3201:0.10255 3202:0.175975 3203:0.174136 3204:0.170623 3205:0.174315 3206:0.194224 3207:0.193442 3208:0.168856 3209:0.148127 3210:0.170563 3211:0.163733 3212:0.164506 3213:0.171225 3214:0.183995 3215:0.187578 3216:0.159574 3217:0.192288 3218:0.18696 3219:0.193412 3220:0.205855 3221:0.223875 3222:0.208788 3223:0.181613 3224:0.174321 3225:0.198389 3226:0.201219 3227:0.247583 3228:0.342744 3229:0.364729 3230:0.365371 3231:0.273057 3232:0.18289 3233:0.200458 3234:0.277803 3235:0.338499 3236:0.366069 3237:0.39737 3238:0.395561 3239:0.324428 3240:0.188364 3241:0.197345 3242:0.296322 3243:0.340317 3244:0.321943 3245:0.270619 3246:0.303057 3247:0.275972 3248:0.188277 3249:0.196819 3250:0.23457 3251:0.266692 3252:0.258394 3253:0.213902 3254:0.216291 3255:0.214476 3256:0.198264 3257:0.200921 3258:0.195861 3259:0.212424 3260:0.204119 3261:0.195131 3262:0.198915 3263:0.1986 3264:0.200211 3265:0.0150961 3266:0.0265414 3267:0.0407291 3268:0.0379001 3269:0.0296009 3270:0.0348414 3271:0.0276892 3272:0.0498935 3273:0.0261942 3274:0.044938 3275:0.0522669 3276:0.0579577 3277:0.0577036 3278:0.0507995 3279:0.0672908 3280:0.054386 3281:0.0720427 3282:0.0541591 3283:0.0652857 3284:0.0699366 3285:0.0715641 3286:0.0695562 3287:0.065193 3288:0.0399617 3289:0.0830218 3290:0.0890979 3291:0.0294717 3292:0.0451716 3293:0.0931562 3294:0.0686989 3295:0.110517 3296:0.0515973 3297:0.064671 3298:0.0796787 3299:0.125502 3300:0.131561 3301:0.158479 3302:0.143219 3303:0.163783 3304:0.0497428 3305:0.0617117 3306:0.0502274 3307:0.15813 3308:0.148362 3309:0.110549 3310:0.0871555 3311:0.137443 3312:0.0398703 3313:0.0858372 3314:0.0677906 3315:0.0816987 3316:0.119344 3317:0.0912016 3318:0.0668297 3319:0.0581245 3320:0.0919004 3321:0.0878655 3322:0.0752888 3323:0.0930445 3324:0.0826638 3325:0.0667558 3326:0.0667541 3327:0.0667128 3328:0.137134 3329:0.0150665 3330:0.0313539 3331:0.0487368 3332:0.0495844 3333:0.0501404 3334:0.0442184 3335:0.0561459 3336:0.0569216 3337:0.0323667 3338:0.0426878 3339:0.0503712 3340:0.0395255 3341:0.0461355 3342:0.0294054 3343:0.0409383 3344:0.0388238 3345:0.045276 3346:0.0675406 3347:0.0373682 3348:0.0268786 3349:0.0752198 3350:0.0682949 3351:0.0752571 3352:0.0442271 3353:0.0372657 3354:0.0571112 3355:0.0634155 3356:0.0782709 3357:0.123318 3358:0.0910429 3359:0.137977 3360:0.0485296 3361:0.015607 3362:0.0261788 3363:0.0924551 3364:0.10052 3365:0.094171 3366:0.0911143 3367:0.135548 3368:0.0556631 3369:0.0282943 3370:0.0453928 3371:0.0697034 3372:0.0932033 3373:0.0767693 3374:0.0593255 3375:0.0677151 3376:0.0413273 3377:0.0415899 3378:0.0475608 3379:0.0620857 3380:0.0722938 3381:0.0648966 3382:0.0570941 3383:0.0640779 3384:0.0520384 3385:0.0469622 3386:0.0732122 3387:0.0514067 3388:0.0433423 3389:0.0518385 3390:0.0590473 3391:0.0477238 3392:0.0451953 3393:0.137719 3394:0.165133 3395:0.162679 3396:0.163747 3397:0.165625 3398:0.175765 3399:0.186648 3400:0.183593 3401:0.173765 3402:0.16254 3403:0.161505 3404:0.144332 3405:0.165242 3406:0.160135 3407:0.171929 3408:0.154093 3409:0.208134 3410:0.186868 3411:0.155522 3412:0.205751 3413:0.233794 3414:0.218672 3415:0.179092 3416:0.173508 3417:0.208471 3418:0.182963 3419:0.270411 3420:0.332716 3421:0.358955 3422:0.345925 3423:0.280571 3424:0.194499 3425:0.174759 3426:0.312926 3427:0.33505 3428:0.354797 3429:0.346097 3430:0.332935 3431:0.281811 3432:0.171023 3433:0.197611 3434:0.276414 3435:0.290401 3436:0.261004 3437:0.208871 3438:0.244896 3439:0.201926 3440:0.20365 3441:0.211465 3442:0.204567 3443:0.237491 3444:0.237336 3445:0.183153 3446:0.187067 3447:0.209579 3448:0.247671 3449:0.198226 3450:0.192333 3451:0.187348 3452:0.174273 3453:0.179585 3454:0.173344 3455:0.183013 3456:0.184506 3457:0.100635 3458:0.130223 3459:0.128331 3460:0.125837 3461:0.125813 3462:0.139132 3463:0.149228 3464:0.134186 3465:0.128749 3466:0.138285 3467:0.131951 3468:0.129711 3469:0.15843 3470:0.137578 3471:0.155132 3472:0.140401 3473:0.143356 3474:0.151975 3475:0.146998 3476:0.149998 3477:0.175193 3478:0.173354 3479:0.138877 3480:0.155514 3481:0.145488 3482:0.157313 3483:0.196551 3484:0.206431 3485:0.254356 3486:0.234676 3487:0.23139 3488:0.153455 3489:0.132009 3490:0.207519 3491:0.247452 3492:0.238251 3493:0.24477 3494:0.25883 3495:0.236005 3496:0.147816 3497:0.15311 3498:0.204874 3499:0.243919 3500:0.208539 3501:0.190657 3502:0.206353 3503:0.182461 3504:0.173896 3505:0.156938 3506:0.170361 3507:0.207448 3508:0.199511 3509:0.179939 3510:0.153299 3511:0.155451 3512:0.184693 3513:0.165445 3514:0.178055 3515:0.169107 3516:0.157293 3517:0.155047 3518:0.154208 3519:0.173856 3520:0.142601 3521:0 3522:0 3523:0 3524:0 3525:0 3526:0 3527:0 3528:0 3529:0 3530:0 3531:0 3532:0 3533:0 3534:0 3535:0 3536:0 3537:0 3538:0 3539:0 3540:0 3541:0 3542:0 3543:0 3544:0 3545:0 3546:0 3547:0 3548:0 3549:0 3550:0 3551:0 3552:0 3553:0 3554:0 3555:0 3556:0 3557:0 3558:0 3559:0 3560:0 3561:0 3562:0 3563:0 3564:0 3565:0 3566:0 3567:0 3568:0 3569:0 3570:0 3571:0 3572:0 3573:0 3574:0 3575:0 3576:0 3577:0 3578:0 3579:0 3580:0 3581:0 3582:0 3583:0 3584:0 3585:0.0512733 3586:0.0425569 3587:0.043344 3588:0.043387 3589:0.0444805 3590:0.0469129 3591:0.0377213 3592:0.022723 3593:0.0578105 3594:0.0421649 3595:0.0426677 3596:0.0518113 3597:0.0458313 3598:0.0455045 3599:0.0348512 3600:0.025288 3601:0.0559132 3602:0.0346343 3603:0.039627 3604:0.034445 3605:0.0242653 3606:0.0134034 3607:0.0374488 3608:0.0235363 3609:0.0653673 3610:0.0425532 3611:0.0433018 3612:0.0216107 3613:0 3614:0 3615:0.012587 3616:0.0177335 3617:0.0622337 3618:0.035335 3619:0 3620:0.0145417 3621:0.0328568 3622:0.0163654 3623:0.0308948 3624:0.0515989 3625:0.0651558 3626:0.0508721 3627:0.037242 3628:0.00679848 3629:0.0450306 3630:0.039102 3631:0.0555772 3632:0.03392 3633:0.0974045 3634:0.0518057 3635:0.0497237 3636:0.0525493 3637:0.0474311 3638:0.0554185 3639:0.0604579 3640:0.0257218 3641:0.0747317 3642:0.0421998 3643:0.00964545 3644:0.0167303 3645:0.0261494 3646:0.0334012 3647:0.0267409 3648:0 3649:0.193106 3650:0.211099 3651:0.222657 3652:0.229921 3653:0.231277 3654:0.236473 3655:0.248359 3656:0.239803 3657:0.240912 3658:0.224859 3659:0.218827 3660:0.198626 3661:0.240315 3662:0.221051 3663:0.239911 3664:0.203859 3665:0.252376 3666:0.239917 3667:0.256809 3668:0.379115 3669:0.370505 3670:0.343508 3671:0.298889 3672:0.234353 3673:0.226992 3674:0.272696 3675:0.369835 3676:0.416317 3677:0.437247 3678:0.407141 3679:0.355089 3680:0.251962 3681:0.225278 3682:0.391725 3683:0.416038 3684:0.421045 3685:0.346777 3686:0.394928 3687:0.310281 3688:0.230987 3689:0.244167 3690:0.335175 3691:0.360086 3692:0.33591 3693:0.274351 3694:0.266418 3695:0.23624 3696:0.264669 3697:0.27408 3698:0.26957 3699:0.275124 3700:0.256091 3701:0.256789 3702:0.251754 3703:0.267745 3704:0.30283 3705:0.287464 3706:0.306151 3707:0.287151 3708:0.250456 3709:0.260367 3710:0.252055 3711:0.246918 3712:0.235591 3713:0 3714:0 3715:0 3716:0 3717:0 3718:0 3719:0 3720:0 3721:0 3722:0 3723:0 3724:0 3725:0 3726:0 3727:0 3728:0 3729:0 3730:0 3731:0 3732:0 3733:0 3734:0 3735:0 3736:0 3737:0 3738:0 3739:0 3740:0 3741:0 3742:0 3743:0 3744:0 3745:0 3746:0 3747:0 3748:0 3749:0 3750:0 3751:0 3752:0 3753:0 3754:0 3755:0 3756:0 3757:0 3758:0 3759:0 3760:0 3761:0 3762:0 3763:0 3764:0 3765:0 3766:0 3767:0 3768:0 3769:0 3770:0 3771:0 3772:0 3773:0 3774:0 3775:0 3776:0 3777:0.0345477 3778:0.0831249 3779:0.0934756 3780:0.0938681 3781:0.0990028 3782:0.10114 3783:0.108163 3784:0.100819 3785:0.0307606 3786:0.0527674 3787:0.0341348 3788:0.0509524 3789:0.0436083 3790:0.0470725 3791:0.052706 3792:0.0404734 3793:0.0668044 3794:0.0605759 3795:0.0845187 3796:0.082966 3797:0.108578 3798:0.106733 3799:0.059146 3800:0.0476176 3801:0.049279 3802:0.0867672 3803:0.115484 3804:0.163352 3805:0.210886 3806:0.214703 3807:0.161728 3808:0.051956 3809:0.0571187 3810:0.111485 3811:0.182704 3812:0.17759 3813:0.189252 3814:0.189873 3815:0.174849 3816:0.0694223 3817:0.0584031 3818:0.0718345 3819:0.135221 3820:0.124136 3821:0.0923973 3822:0.063856 3823:0.136094 3824:0.0717088 3825:0.112302 3826:0.0773218 3827:0.0875541 3828:0.116583 3829:0.103656 3830:0.0737907 3831:0.0756469 3832:0.0625779 3833:0.123207 3834:0.106892 3835:0.0927571 3836:0.0680848 3837:0.0626959 3838:0.0672398 3839:0.0643184 3840:0.0803126 3841:0 3842:0 3843:0 3844:0 3845:0 3846:0 3847:0 3848:0 3849:0 3850:0 3851:0 3852:0 3853:0 3854:0 3855:0 3856:0 3857:0 3858:0 3859:0 3860:0 3861:0 3862:0 3863:0 3864:0 3865:0 3866:0 3867:0 3868:0 3869:0 3870:0 3871:0 3872:0 3873:0 3874:0 3875:0 3876:0 3877:0 3878:0 3879:0 3880:0 3881:0 3882:0 3883:0 3884:0 3885:0 3886:0 3887:0 3888:0 3889:0 3890:0 3891:0 3892:0 3893:0 3894:0 3895:0 3896:0 3897:0 3898:0 3899:0 3900:0 3901:0 3902:0 3903:0 3904:0 3905:0.0046282 3906:0 3907:0 3908:0 3909:0 3910:0 3911:0 3912:0 3913:0 3914:0 3915:0 3916:0 3917:0 3918:0 3919:0 3920:0 3921:0 3922:0 3923:0 3924:0 3925:0 3926:0 3927:0 3928:0 3929:0 3930:0 3931:0 3932:0 3933:0 3934:0 3935:0 3936:0 3937:0 3938:0 3939:0 3940:0 3941:0 3942:0 3943:0 3944:0 3945:0 3946:0 3947:0 3948:0 3949:0 3950:0 3951:0 3952:0 3953:0 3954:0 3955:0 3956:0 3957:0 3958:0 3959:0 3960:0 3961:0 3962:0 3963:0 3964:0 3965:0 3966:0 3967:0 3968:0 3969:0 3970:0 3971:0.00440664 3972:0.00129073 3973:0 3974:0.000665266 3975:0.000701195 3976:0.0874494 3977:0 3978:0 3979:0.00396189 3980:0 3981:0 3982:0.00101958 3983:0 3984:0.0727394 3985:0 3986:0 3987:0 3988:0 3989:0 3990:0.00957625 3991:0.00106679 3992:0.0813529 3993:0 3994:0.00620884 3995:0 3996:0 3997:0.00255981 3998:0.0118563 3999:0.0396994 4000:0.0924413 4001:0 4002:0 4003:0.0159206 4004:0.0250881 4005:0.0288982 4006:0.0193174 4007:0.0558292 4008:0.0938962 4009:0 4010:0 4011:0 4012:0.0326236 4013:0.0145132 4014:0.0129701 4015:0.0219424 4016:0.111179 4017:0 4018:0.0165543 4019:0.011092 4020:0.00715763 4021:0 4022:0 4023:0.00132233 4024:0.112211 4025:0 4026:0.00117942 4027:0.00215763 4028:0 4029:0.00191992 4030:0.00931192 4031:0.013491 4032:0.0858179 4033:0 4034:0 4035:0 4036:0 4037:0 4038:0 4039:0 4040:0 4041:0.0133144 4042:0 4043:0 4044:0 4045:0 4046:0 4047:0 4048:0 4049:0.0318633 4050:0 4051:0 4052:0 4053:0 4054:0 4055:0 4056:0 4057:0.0237841 4058:0 4059:0 4060:0 4061:0 4062:0 4063:0 4064:0 4065:0.0163485 4066:0 4067:0 4068:0 4069:0 4070:0 4071:0 4072:0 4073:0.0149819 4074:0 4075:0 4076:0 4077:0 4078:0 4079:0 4080:0 4081:0.0127507 4082:0 4083:0 4084:0 4085:0 4086:0 4087:0 4088:0 4089:0.0899063 4090:0.0183291 4091:0.00841428 4092:0.014318 4093:0.0167722 4094:0.0147176 4095:0.0286606 4096:0.0484478 4097:0.0101213 4098:0.00270227 4099:0.0105875 4100:0.0129191 4101:0.00168521 4102:0 4103:0.0130003 4104:0.0328156 4105:0 4106:0 4107:0 4108:0 4109:0 4110:0.00962483 4111:0 4112:0.0327039 4113:0 4114:0 4115:0 4116:0 4117:0.00153153 4118:0.00885079 4119:0.0174435 4120:0.0240264 4121:0 4122:0 4123:0 4124:0 4125:0 4126:0 4127:0.0256924 4128:0.0190902 4129:0 4130:0 4131:0 4132:0 4133:0 4134:0 4135:0.018455 4136:0.0253358 4137:0 4138:0 4139:0 4140:0.00451948 4141:0.00724375 4142:0.0022564 4143:0.0137453 4144:0.0274735 4145:0 4146:0 4147:0 4148:0 4149:0 4150:0 4151:0 4152:0.0252741 4153:0 4154:0 4155:0 4156:0 4157:0 4158:0 4159:0 4160:0.0362564 4161:0 4162:0 4163:0 4164:0 4165:0 4166:0 4167:0 4168:0 4169:0 4170:0 4171:0 4172:0 4173:0 4174:0 4175:0 4176:0 4177:0 4178:0 4179:0 4180:0 4181:0 4182:0 4183:0 4184:0 4185:0 4186:0 4187:0 4188:0 4189:0 4190:0 4191:0 4192:0 4193:0 4194:0 4195:0 4196:0 4197:0 4198:0 4199:0 4200:0 4201:0 4202:0 4203:0 4204:0 4205:0 4206:0 4207:0 4208:0 4209:0 4210:0 4211:0 4212:0 4213:0 4214:0 4215:0 4216:0 4217:0.0316645 4218:0.0294022 4219:0.020927 4220:0.0114548 4221:0.0145742 4222:0.0167205 4223:0.00222573 4224:0.037496 4225:0.0578555 4226:0.0678307 4227:0.0600305 4228:0.0660674 4229:0.0727358 4230:0.0729071 4231:0.0823147 4232:0.0454916 4233:0.102231 4234:0.0905227 4235:0.0801716 4236:0.0869615 4237:0.0902491 4238:0.0816233 4239:0.0903944 4240:0.0628819 4241:0.097207 4242:0.093296 4243:0.0979641 4244:0.133063 4245:0.143296 4246:0.126918 4247:0.131277 4248:0.0809278 4249:0.0771146 4250:0.095947 4251:0.156772 4252:0.166209 4253:0.184669 4254:0.172593 4255:0.15451 4256:0.086178 4257:0.0909101 4258:0.103302 4259:0.15522 4260:0.156605 4261:0.133019 4262:0.131302 4263:0.136376 4264:0.0910835 4265:0.100468 4266:0.0991989 4267:0.115581 4268:0.12389 4269:0.124601 4270:0.10038 4271:0.0952776 4272:0.0842518 4273:0.126953 4274:0.112579 4275:0.103651 4276:0.0989178 4277:0.108368 4278:0.0997081 4279:0.105128 4280:0.0925393 4281:0.120366 4282:0.121596 4283:0.0850071 4284:0.079508 4285:0.0903463 4286:0.088624 4287:0.0845732 4288:0.0694759 4289:0.0403131 4290:0 4291:0 4292:0 4293:0 4294:0 4295:0 4296:0 4297:0.0287356 4298:0 4299:0 4300:0 4301:0 4302:0 4303:0 4304:0 4305:0.0292558 4306:0 4307:0.0226008 4308:0.00207468 4309:0.00483256 4310:0 4311:0 4312:0 4313:0.0316717 4314:0.0402242 4315:0.00917589 4316:0 4317:0 4318:0 4319:0 4320:0 4321:0.0249334 4322:0.0304051 4323:0 4324:0 4325:0 4326:0 4327:0 4328:0 4329:0.0317085 4330:0 4331:0 4332:0 4333:0 4334:0 4335:0 4336:0 4337:0.0366771 4338:0 4339:0 4340:0 4341:0 4342:0 4343:0.000905847 4344:0 4345:0.0259612 4346:0 4347:0 4348:0 4349:0 4350:0 4351:0 4352:0 4353:0.0426887 4354:0.0427649 4355:0.0423792 4356:0.0442854 4357:0.0525698 4358:0.043743 4359:0.0447509 4360:0.0427338 4361:0.0610119 4362:0.0269261 4363:0.0396448 4364:0.0445627 4365:0.0468196 4366:0.0567906 4367:0.0402218 4368:0.0292633 4369:0.078692 4370:0.0330532 4371:0.0335476 4372:0.029418 4373:0.00619354 4374:0.0109073 4375:0.0362726 4376:0.0214445 4377:0.0902362 4378:0.0383466 4379:0.0653793 4380:0.0602973 4381:0.0245673 4382:0.000104556 4383:0 4384:0.0245056 4385:0.0886492 4386:0.0694348 4387:0.0510492 4388:0.0908554 4389:0.128195 4390:0.140837 4391:0.0553628 4392:0.0326492 4393:0.0745082 4394:0.108883 4395:0.0962319 4396:0.0520927 4397:0.087197 4398:0.113309 4399:0.0852349 4400:0.0423602 4401:0.105354 4402:0.0690254 4403:0.0807609 4404:0.0962369 4405:0.0478117 4406:0.0396972 4407:0.0545514 4408:0.0550185 4409:0.138858 4410:0.106709 4411:0.0858493 4412:0.0839327 4413:0.0895007 4414:0.086611 4415:0.1097 4416:0.0953633 4417:0.0812301 4418:0.0690002 4419:0.0569756 4420:0.0620537 4421:0.0545854 4422:0.0637714 4423:0.0631041 4424:0.0599691 4425:0.0876152 4426:0.0596009 4427:0.0545914 4428:0.0790165 4429:0.0664874 4430:0.080087 4431:0.0753062 4432:0.0423724 4433:0.0956982 4434:0.0636464 4435:0.0739601 4436:0.0823826 4437:0.0742676 4438:0.0736409 4439:0.0699234 4440:0.0630247 4441:0.0910981 4442:0.068223 4443:0.104934 4444:0.146536 4445:0.144239 4446:0.114532 4447:0.0805397 4448:0.0616991 4449:0.0988572 4450:0.118591 4451:0.13132 4452:0.118609 4453:0.121565 4454:0.115243 4455:0.0866217 4456:0.0427979 4457:0.0870079 4458:0.110732 4459:0.117775 4460:0.0948868 4461:0.0579104 4462:0.0857224 4463:0.0752482 4464:0.0608225 4465:0.096089 4466:0.05849 4467:0.0841621 4468:0.0903953 4469:0.0630672 4470:0.0642709 4471:0.0835901 4472:0.0724884 4473:0.109032 4474:0.0534095 4475:0.0603699 4476:0.0804621 4477:0.0856689 4478:0.0743876 4479:0.0900458 4480:0.0848619 4481:0 4482:0 4483:0 4484:0 4485:0 4486:0 4487:0 4488:0 4489:0 4490:0 4491:0 4492:0 4493:0 4494:0 4495:0 4496:0 4497:0 4498:0 4499:0 4500:0 4501:0 4502:0 4503:0 4504:0 4505:0 4506:0 4507:0 4508:0 4509:0 4510:0 4511:0 4512:0 4513:0 4514:0 4515:0 4516:0 4517:0 4518:0 4519:0 4520:0 4521:0 4522:0 4523:0 4524:0 4525:0 4526:0 4527:0 4528:0 4529:0 4530:0 4531:0 4532:0 4533:0 4534:0 4535:0 4536:0 4537:0 4538:0 4539:0 4540:0 4541:0 4542:0 4543:0 4544:0 4545:0 4546:0 4547:0 4548:0 4549:0 4550:0 4551:0 4552:0 4553:0 4554:0 4555:0 4556:0 4557:0 4558:0 4559:0 4560:0 4561:0 4562:0 4563:0 4564:0 4565:0 4566:0 4567:0 4568:0 4569:0 4570:0 4571:0 4572:0 4573:0 4574:0 4575:0 4576:0 4577:0 4578:0 4579:0 4580:0 4581:0 4582:0 4583:0 4584:0 4585:0 4586:0 4587:0 4588:0 4589:0 4590:0 4591:0 4592:0 4593:0 4594:0 4595:0 4596:0 4597:0 4598:0 4599:0 4600:0 4601:0 4602:0 4603:0 4604:0 4605:0 4606:0 4607:0 4608:0 4609:0.0787306 4610:0.0696269 4611:0.0536069 4612:0.0569649 4613:0.0481181 4614:0.0517679 4615:0.0546673 4616:0.052402 4617:0.00852606 4618:0.00915778 4619:0.0105037 4620:0.0305933 4621:0.0379206 4622:0.0296167 4623:0.0257656 4624:0.0444423 4625:0.0278254 4626:0.0143519 4627:0.0453034 4628:0.0149303 4629:0.0549387 4630:0.0194435 4631:0.0334453 4632:0.0314717 4633:0.025447 4634:0.0309254 4635:0.0411843 4636:0.0302547 4637:0.024832 4638:0.00868663 4639:0.017791 4640:0.0309687 4641:0.0390508 4642:0.0588401 4643:0.00900075 4644:0.00794571 4645:0.0285526 4646:0.0255498 4647:0.00817101 4648:0.041711 4649:0.0313046 4650:0.0154886 4651:0.0413614 4652:0.027131 4653:0.00252527 4654:0.0054146 4655:0.0526304 4656:0.0407724 4657:0.0194598 4658:0.012418 4659:0.00784099 4660:0.0375537 4661:0.0179379 4662:0.0189425 4663:0.0121664 4664:0.0472178 4665:0.0688302 4666:0.0646058 4667:0.0718626 4668:0.0539975 4669:0.0511925 4670:0.0522028 4671:0.0470417 4672:0.121235 4673:0.100008 4674:0.104688 4675:0.0895772 4676:0.0909992 4677:0.0913173 4678:0.0925537 4679:0.0992878 4680:0.0936281 4681:0.123211 4682:0.0607676 4683:0.0508722 4684:0.071721 4685:0.0698743 4686:0.0714214 4687:0.062818 4688:0.0456032 4689:0.136511 4690:0.0659426 4691:0.0827243 4692:0.088062 4693:0.0672349 4694:0.07788 4695:0.0534746 4696:0.0702436 4697:0.137106 4698:0.0905163 4699:0.109993 4700:0.1366 4701:0.123842 4702:0.124901 4703:0.0852231 4704:0.0689644 4705:0.126503 4706:0.149983 4707:0.119946 4708:0.12279 4709:0.137141 4710:0.136302 4711:0.0678833 4712:0.0762891 4713:0.128605 4714:0.148788 4715:0.122278 4716:0.087561 4717:0.0671299 4718:0.0990513 4719:0.0784188 4720:0.0921839 4721:0.147299 4722:0.073142 4723:0.103953 4724:0.0883928 4725:0.0769027 4726:0.0742779 4727:0.109662 4728:0.0765359 4729:0.148839 4730:0.0887379 4731:0.0581661 4732:0.0602046 4733:0.0659357 4734:0.0744569 4735:0.110324 4736:0.0664032 4737:0.000193769 4738:0.00720171 4739:0.00264626 4740:0.00173229 4741:0.00391299 4742:0.000760233 4743:0.00498352 4744:0.0283969 4745:0 4746:0 4747:0 4748:0.00858131 4749:0.015594 4750:0.00796193 4751:0.00265328 4752:0.0147679 4753:0.00756026 4754:0 4755:0.0127743 4756:0.0189048 4757:0.0310614 4758:0.0208803 4759:0 4760:0.0224602 4761:0.0126451 4762:0.0225722 4763:0.0177541 4764:0.03026 4765:0.00510045 4766:0.0139306 4767:0 4768:0.0155987 4769:0.00858141 4770:0.0194956 4771:0.048949 4772:0.02896 4773:0.0148228 4774:0.030369 4775:0 4776:0.018717 4777:0.00248825 4778:0.0614471 4779:0.0481133 4780:0.0415147 4781:0.00202101 4782:0.0334501 4783:0.0175112 4784:0.0334063 4785:0.023382 4786:0.0110409 4787:0.0121782 4788:0.0129713 4789:0 4790:0.00104125 4791:0.0079793 4792:0.041246 4793:0.0198932 4794:0 4795:0 4796:0 4797:0 4798:0 4799:0 4800:0.0340833 4801:0.0473669 4802:0.0441213 4803:0.055565 4804:0.0528255 4805:0.0528433 4806:0.0580876 4807:0.0511547 4808:0.0545683 4809:0.0570084 4810:0.0589217 4811:0.0645226 4812:0.0602093 4813:0.0689022 4814:0.0443641 4815:0.0622633 4816:0.0628889 4817:0.0480505 4818:0.0632317 4819:0.0601697 4820:0.0633143 4821:0.0609332 4822:0.0747299 4823:0.0604812 4824:0.0713624 4825:0.0337673 4826:0.05738 4827:0.0726116 4828:0.0311325 4829:0.0644873 4830:0.0656815 4831:0.0810175 4832:0.0693519 4833:0.03978 4834:0.0437634 4835:0.0342547 4836:0.0302759 4837:0 4838:0.0146962 4839:0.0490904 4840:0.0686479 4841:0.0478995 4842:0.0391145 4843:0.0159571 4844:0.0651112 4845:0.0539628 4846:0.0306025 4847:0.0409732 4848:0.0762796 4849:0.0572124 4850:0.0613002 4851:0.0434213 4852:0.0434403 4853:0.055747 4854:0.0600207 4855:0.0567573 4856:0.0601927 4857:0.0348806 4858:0.055234 4859:0.0327363 4860:0.0348486 4861:0.0425145 4862:0.0505761 4863:0.0391206 4864:0.014383 4865:0 4866:0 4867:0 4868:0 4869:0 4870:0 4871:0 4872:0 4873:0 4874:0 4875:0 4876:0 4877:0 4878:0 4879:0 4880:0 4881:0 4882:0 4883:0 4884:0 4885:0 4886:0 4887:0 4888:0 4889:0 4890:0 4891:0 4892:0 4893:0 4894:0 4895:0 4896:0 4897:0 4898:0 4899:0 4900:0 4901:0 4902:0 4903:0 4904:0 4905:0 4906:0 4907:0 4908:0 4909:0 4910:0 4911:0 4912:0 4913:0 4914:0 4915:0 4916:0 4917:0 4918:0 4919:0 4920:0 4921:0 4922:0 4923:0 4924:0 4925:0 4926:0 4927:0 4928:0 4929:0 4930:0.0166766 4931:0.00444945 4932:0.00805675 4933:0.00663946 4934:0.00172015 4935:0.00616585 4936:0.064274 4937:0.00294683 4938:0.00642737 4939:0.0111194 4940:0.0027604 4941:0.0115706 4942:0 4943:0.00281606 4944:0.0225301 4945:0.00277665 4946:0.0024489 4947:0.0140954 4948:0 4949:0.00523679 4950:0 4951:0.00698222 4952:0.0273501 4953:0 4954:0.00785357 4955:0.00911994 4956:0 4957:0.00246831 4958:0 4959:0.0133985 4960:0.0218038 4961:0.00270973 4962:0 4963:0.00946417 4964:0.00262973 4965:0.000252399 4966:0 4967:0.053475 4968:0.0256785 4969:0 4970:0.00378205 4971:0.00808925 4972:0.0160602 4973:0.00622176 4974:0.00656918 4975:0.0196797 4976:0.0397308 4977:0.0152858 4978:0 4979:0.00878565 4980:0.00098037 4981:0.00153779 4982:0.00326262 4983:0.00226141 4984:0.0467173 4985:0.0373025 4986:0.0337674 4987:0.0334283 4988:0.0235434 4989:0.0218198 4990:0.0210904 4991:0.0260507 4992:0.0510671 4993:0.00309771 4994:0.000503085 4995:0 4996:0 4997:0 4998:0 4999:0 5000:0 5001:0 5002:0 5003:0 5004:0 5005:0 5006:0 5007:0 5008:0 5009:0 5010:0 5011:0 5012:0 5013:0 5014:0 5015:0 5016:0 5017:0 5018:0 5019:0 5020:0 5021:0 5022:0 5023:0 5024:0 5025:0 5026:0 5027:0 5028:0 5029:0 5030:0 5031:0 5032:0 5033:0 5034:0 5035:0 5036:0 5037:0 5038:0 5039:0 5040:0 5041:0 5042:0 5043:0 5044:0 5045:0 5046:0 5047:0 5048:0 5049:0 5050:0 5051:0 5052:0 5053:0 5054:0 5055:0 5056:0 5057:0 5058:0 5059:0 5060:0 5061:0 5062:0 5063:0 5064:0 5065:0.0141694 5066:0 5067:0 5068:0 5069:0 5070:0 5071:0 5072:0 5073:0.0204576 5074:0 5075:0 5076:0 5077:0 5078:0 5079:0 5080:0 5081:0.0144043 5082:0 5083:0 5084:0 5085:0 5086:0 5087:0 5088:0 5089:0.0133587 5090:0 5091:0 5092:0 5093:0 5094:0 5095:0 5096:0 5097:0.00976204 5098:0 5099:0 5100:0 5101:0 5102:0 5103:0 5104:0 5105:0.0283975 5106:0 5107:0 5108:0 5109:0 5110:0 5111:0 5112:0 5113:0.0380707 5114:0 5115:0 5116:0 5117:0 5118:0 5119:0 5120:0 5121:0.0918597 5122:0.115112 5123:0.124705 5124:0.120175 5125:0.122321 5126:0.122892 5127:0.13364 5128:0.165507 5129:0.014611 5130:0.0169537 5131:0.0103189 5132:0.0142891 5133:0.0318671 5134:0.0137208 5135:0.031368 5136:0.0888174 5137:0.0258554 5138:0.0253134 5139:0.029988 5140:0.0482385 5141:0.048429 5142:0.0705063 5143:0.0212978 5144:0.0800507 5145:0.0189201 5146:0.0263803 5147:0.00456782 5148:0.0421398 5149:0.0672724 5150:0.0853226 5151:0.105969 5152:0.084517 5153:0.0153719 5154:0.0452373 5155:0.046331 5156:0.0161041 5157:0.034062 5158:0.0117107 5159:0.0780374 5160:0.0827617 5161:0.0237887 5162:0 5163:0.0102819 5164:0.0460901 5165:0.0256261 5166:0 5167:0.0241774 5168:0.0920098 5169:0.0279016 5170:0.00806378 5171:0 5172:0.0382516 5173:0.0312407 5174:0.0194503 5175:0.00331347 5176:0.0829572 5177:0.00446818 5178:0.00389366 5179:0.00484306 5180:0.000178885 5181:0 5182:0 5183:0 5184:0.0763831 5185:0.0396342 5186:0 5187:0 5188:0 5189:0 5190:0 5191:0 5192:0.0317212 5193:0.0141043 5194:0 5195:0 5196:0.015542 5197:0 5198:0 5199:0 5200:0.00907668 5201:0.0155499 5202:0 5203:0 5204:0 5205:0 5206:0 5207:0 5208:0 5209:0.0194001 5210:0 5211:0 5212:0 5213:0 5214:0 5215:0 5216:0 5217:0.0233322 5218:0 5219:0 5220:0 5221:0 5222:0 5223:0 5224:0.00320914 5225:0.0236988 5226:0 5227:0 5228:0 5229:0 5230:0 5231:0 5232:0 5233:0.0240765 5234:0 5235:0 5236:0 5237:0 5238:0 5239:0 5240:0 5241:0.0259525 5242:0.0217934 5243:0.0197416 5244:0.0156673 5245:0.0267755 5246:0.032738 5247:0.0393636 5248:0.0724954 5249:0.019491 5250:0.0110846 5251:0.00144402 5252:0.00219786 5253:0.000281636 5254:0 5255:0 5256:0.0248266 5257:0 5258:0 5259:0 5260:0 5261:0 5262:0 5263:0 5264:0.0350725 5265:0 5266:0 5267:0 5268:0 5269:0 5270:0 5271:0 5272:0.0208115 5273:0 5274:0 5275:0 5276:0 5277:0 5278:0 5279:0 5280:0.0244528 5281:0 5282:0 5283:0 5284:0 5285:0 5286:0 5287:0 5288:0.0178204 5289:0 5290:0 5291:0 5292:0 5293:0 5294:0 5295:0 5296:0 5297:0 5298:0 5299:0 5300:0 5301:0 5302:0 5303:0 5304:0.0148974 5305:0.00632219 5306:0.00253711 5307:0 5308:0 5309:0 5310:0 5311:0.00535601 5312:0.0237308 5313:0.111046 5314:0.110629 5315:0.112498 5316:0.105905 5317:0.111636 5318:0.10954 5319:0.121968 5320:0.100913 5321:0.115861 5322:0.108155 5323:0.104225 5324:0.15031 5325:0.125623 5326:0.129276 5327:0.11297 5328:0.103353 5329:0.13504 5330:0.120828 5331:0.127809 5332:0.138378 5333:0.148389 5334:0.136543 5335:0.131668 5336:0.108444 5337:0.138691 5338:0.1474 5339:0.188 5340:0.227896 5341:0.188362 5342:0.220941 5343:0.137931 5344:0.111129 5345:0.152755 5346:0.186536 5347:0.221432 5348:0.251087 5349:0.224602 5350:0.214532 5351:0.166854 5352:0.126928 5353:0.14912 5354:0.194381 5355:0.197392 5356:0.181008 5357:0.175303 5358:0.192364 5359:0.160946 5360:0.163107 5361:0.146027 5362:0.143917 5363:0.177431 5364:0.15808 5365:0.13193 5366:0.135919 5367:0.160366 5368:0.14877 5369:0.147832 5370:0.109879 5371:0.12992 5372:0.128055 5373:0.122875 5374:0.126063 5375:0.137553 5376:0.135778 5377:0.0572824 5378:0.0590035 5379:0.0705619 5380:0.0706165 5381:0.0739619 5382:0.0629409 5383:0.068943 5384:0.088868 5385:0.0381457 5386:0.0499195 5387:0.0790994 5388:0.0804138 5389:0.058306 5390:0.0757716 5391:0.0616897 5392:0.0775961 5393:0.0420137 5394:0.0595993 5395:0.0594581 5396:0.0529919 5397:0.0432445 5398:0.0520807 5399:0.0524729 5400:0.0669763 5401:0.0478077 5402:0.063522 5403:0.0353488 5404:0 5405:0 5406:0 5407:0.0196937 5408:0.0645285 5409:0.0546218 5410:0.0327796 5411:0 5412:0.0373439 5413:0.0707147 5414:0.0725257 5415:0.0613165 5416:0.0764077 5417:0.0528685 5418:0.0472302 5419:0.0532799 5420:0.0627248 5421:0.0713512 5422:0.0788688 5423:0.0907312 5424:0.0598252 5425:0.0435074 5426:0.0438604 5427:0.0615533 5428:0.0701664 5429:0.0669152 5430:0.0684619 5431:0.0531056 5432:0.0690415 5433:0.128215 5434:0.144865 5435:0.142973 5436:0.133051 5437:0.139019 5438:0.146871 5439:0.151563 5440:0.163778 5441:0.0551182 5442:0.0676775 5443:0.0678076 5444:0.0644334 5445:0.0733184 5446:0.0711739 5447:0.0703264 5448:0.0470257 5449:0.0592144 5450:0.0793377 5451:0.0612502 5452:0.0914827 5453:0.067029 5454:0.0861556 5455:0.0646824 5456:0.0708023 5457:0.0674564 5458:0.0936413 5459:0.103042 5460:0.113657 5461:0.13653 5462:0.101276 5463:0.0994361 5464:0.0799501 5465:0.0694222 5466:0.158821 5467:0.149452 5468:0.224284 5469:0.167974 5470:0.215159 5471:0.127978 5472:0.0935492 5473:0.0998443 5474:0.190096 5475:0.190389 5476:0.202003 5477:0.17013 5478:0.19482 5479:0.0819437 5480:0.10348 5481:0.0891386 5482:0.161112 5483:0.14158 5484:0.0975569 5485:0.127405 5486:0.132552 5487:0.093914 5488:0.136886 5489:0.0902354 5490:0.111257 5491:0.133077 5492:0.0925998 5493:0.0871644 5494:0.0986142 5495:0.13241 5496:0.0881923 5497:0.105134 5498:0.132324 5499:0.105904 5500:0.108631 5501:0.105212 5502:0.0958506 5503:0.136874 5504:0.0261059 5505:0.0969523 5506:0.0950802 5507:0.0996456 5508:0.0953862 5509:0.0932419 5510:0.0949519 5511:0.111404 5512:0.0954461 5513:0.117696 5514:0.0744511 5515:0.0772162 5516:0.102926 5517:0.0839943 5518:0.0908439 5519:0.0763003 5520:0.0684317 5521:0.136133 5522:0.0816799 5523:0.0915507 5524:0.0819165 5525:0.0867647 5526:0.0927474 5527:0.0905453 5528:0.069612 5529:0.143451 5530:0.0861425 5531:0.118652 5532:0.100241 5533:0.109045 5534:0.107533 5535:0.110449 5536:0.0698805 5537:0.138259 5538:0.0971889 5539:0.120362 5540:0.131598 5541:0.132385 5542:0.136832 5543:0.11809 5544:0.0614373 5545:0.143105 5546:0.122208 5547:0.129753 5548:0.116439 5549:0.111634 5550:0.125108 5551:0.1092 5552:0.0643037 5553:0.151413 5554:0.0907499 5555:0.116446 5556:0.114904 5557:0.0901281 5558:0.0954347 5559:0.0850933 5560:0.104243 5561:0.139108 5562:0.114996 5563:0.107544 5564:0.101457 5565:0.103269 5566:0.105829 5567:0.124462 5568:0.104954 5569:0.13907 5570:0.146863 5571:0.161756 5572:0.15816 5573:0.153316 5574:0.153467 5575:0.16084 5576:0.156674 5577:0.164228 5578:0.171124 5579:0.166524 5580:0.181506 5581:0.175222 5582:0.17317 5583:0.18112 5584:0.159758 5585:0.178839 5586:0.173123 5587:0.210483 5588:0.20009 5589:0.243213 5590:0.2203 5591:0.194813 5592:0.174807 5593:0.180742 5594:0.211595 5595:0.236434 5596:0.242063 5597:0.238938 5598:0.25335 5599:0.215163 5600:0.171615 5601:0.170385 5602:0.21931 5603:0.224751 5604:0.203557 5605:0.204446 5606:0.236608 5607:0.228203 5608:0.202232 5609:0.178256 5610:0.173037 5611:0.228186 5612:0.23228 5613:0.204634 5614:0.16987 5615:0.216636 5616:0.190587 5617:0.204961 5618:0.184904 5619:0.192631 5620:0.199607 5621:0.193628 5622:0.194092 5623:0.180447 5624:0.182284 5625:0.215332 5626:0.215118 5627:0.200772 5628:0.204616 5629:0.196899 5630:0.180696 5631:0.178664 5632:0.202594 5633:0.0173849 5634:0.0372715 5635:0.0437889 5636:0.0417045 5637:0.0481796 5638:0.0454479 5639:0.0426202 5640:0.043364 5641:0.00876528 5642:0.0293386 5643:0.0182569 5644:0.0211147 5645:0.0330593 5646:0.0274502 5647:0.0436515 5648:0.0442902 5649:0.0281841 5650:0.030231 5651:0.0625705 5652:0.0644727 5653:0.0729722 5654:0.0662205 5655:0.0343599 5656:0.0370939 5657:0.019723 5658:0.0743412 5659:0.0434374 5660:0.0889875 5661:0.111648 5662:0.108903 5663:0.0806704 5664:0.0413785 5665:0.0325321 5666:0.0716705 5667:0.0567438 5668:0.0504538 5669:0.0975449 5670:0.10219 5671:0.0890973 5672:0.0450699 5673:0.0274051 5674:0.0349308 5675:0.0919723 5676:0.0902269 5677:0.0569384 5678:0.0275512 5679:0.0770163 5680:0.0239203 5681:0.0249823 5682:0.0287251 5683:0.0317183 5684:0.0555278 5685:0.0531056 5686:0.0554254 5687:0.0495702 5688:0.0306332 5689:0.069174 5690:0.0553733 5691:0.0543614 5692:0.0601853 5693:0.0555165 5694:0.039549 5695:0.0279679 5696:0.0542802 5697:0.0932099 5698:0.105014 5699:0.0901675 5700:0.0917874 5701:0.0890207 5702:0.0969705 5703:0.0975537 5704:0.0937154 5705:0.0960155 5706:0.0856406 5707:0.0807945 5708:0.0707702 5709:0.0842751 5710:0.0958258 5711:0.0932067 5712:0.0835731 5713:0.0975967 5714:0.0884312 5715:0.0878689 5716:0.151361 5717:0.132438 5718:0.130799 5719:0.130443 5720:0.0863466 5721:0.0972217 5722:0.102864 5723:0.142496 5724:0.149971 5725:0.169053 5726:0.170462 5727:0.142002 5728:0.0983635 5729:0.0910853 5730:0.151518 5731:0.151093 5732:0.156293 5733:0.158065 5734:0.17028 5735:0.130127 5736:0.0884497 5737:0.109553 5738:0.146381 5739:0.165935 5740:0.136612 5741:0.124715 5742:0.121861 5743:0.0953877 5744:0.0966936 5745:0.113137 5746:0.110454 5747:0.112889 5748:0.108011 5749:0.106349 5750:0.100165 5751:0.0952116 5752:0.122358 5753:0.109434 5754:0.119401 5755:0.120213 5756:0.111612 5757:0.112711 5758:0.105438 5759:0.106976 5760:0.108812 5761:0 5762:0 5763:0 5764:0 5765:0 5766:0 5767:0 5768:0 5769:0 5770:0 5771:0 5772:0 5773:0 5774:0 5775:0 5776:0 5777:0 5778:0 5779:0 5780:0 5781:0 5782:0 5783:0 5784:0 5785:0 5786:0 5787:0 5788:0 5789:0 5790:0 5791:0 5792:0 5793:0 5794:0 5795:0 5796:0 5797:0 5798:0 5799:0 5800:0 5801:0 5802:0 5803:0 5804:0 5805:0 5806:0 5807:0 5808:0 5809:0 5810:0 5811:0 5812:0 5813:0 5814:0 5815:0 5816:0 5817:0 5818:0 5819:0 5820:0 5821:0 5822:0 5823:0 5824:0 5825:0.0258173 5826:0.0567168 5827:0.0606823 5828:0.0557005 5829:0.0554989 5830:0.0522007 5831:0.0600498 5832:0.0993208 5833:0.0255846 5834:0.0310653 5835:0.0311718 5836:0.0497866 5837:0.0365614 5838:0.0446264 5839:0.0532925 5840:0.05107 5841:0.0604364 5842:0.0426098 5843:0.0580831 5844:0.0197464 5845:0.0676294 5846:0.0486081 5847:0.0439494 5848:0.0607594 5849:0.0598368 5850:0.038634 5851:0.0450352 5852:0.0806926 5853:0.0975635 5854:0.120995 5855:0.102082 5856:0.0619247 5857:0.0510211 5858:0.0504845 5859:0.112434 5860:0.0871967 5861:0.128951 5862:0.101775 5863:0.106709 5864:0.0635751 5865:0.0320565 5866:0.0778314 5867:0.123018 5868:0.0762545 5869:0.0440808 5870:0.0845595 5871:0.119561 5872:0.0671707 5873:0.0436882 5874:0.0426854 5875:0.0623769 5876:0.102459 5877:0.0641185 5878:0.0483843 5879:0.043879 5880:0.0985066 5881:0.0983059 5882:0.0998193 5883:0.0894084 5884:0.0841563 5885:0.0785159 5886:0.0771809 5887:0.0747347 5888:0.119735 5889:0 5890:0 5891:0 5892:0 5893:0 5894:0 5895:0 5896:0 5897:0 5898:0 5899:0 5900:0 5901:0 5902:0 5903:0 5904:0 5905:0 5906:0 5907:0 5908:0 5909:0 5910:0 5911:0 5912:0 5913:0 5914:0 5915:0 5916:0 5917:0 5918:0 5919:0 5920:0 5921:0 5922:0 5923:0 5924:0 5925:0 5926:0 5927:0 5928:0 5929:0 5930:0 5931:0 5932:0 5933:0 5934:0 5935:0 5936:0 5937:0 5938:0 5939:0 5940:0 5941:0 5942:0 5943:0 5944:0 5945:0 5946:0 5947:0 5948:0 5949:0 5950:0 5951:0 5952:0 5953:0.0476007 5954:0.00993547 5955:0.00731326 5956:0.00963445 5957:0.0125362 5958:0.00987796 5959:0.00844132 5960:0.00776511 5961:0.0403763 5962:0.0105976 5963:0.0157696 5964:0.0287853 5965:0.0122081 5966:0.0198274 5967:0.00558499 5968:0.019914 5969:0.0229992 5970:0.0172867 5971:0.0223281 5972:0.0588948 5973:0.0340508 5974:0.0310879 5975:0.0299942 5976:0.0188365 5977:0.0121558 5978:0.0332363 5979:0.0435281 5980:0.0161413 5981:0.0143329 5982:0 5983:0.0324418 5984:0.0199766 5985:0.0161084 5986:0.0477953 5987:0.017407 5988:0.0260914 5989:0 5990:0 5991:0.0350125 5992:0.00952767 5993:0.0207624 5994:0.0226149 5995:0.0048124 5996:0.00553509 5997:0.0186509 5998:0.0129159 5999:0.0215557 6000:0.0269848 6001:0.020596 6002:0.00632296 6003:0.00888717 6004:0 6005:0.0110716 6006:0.00643143 6007:0.0117669 6008:0 6009:0.0181643 6010:0 6011:0.00732543 6012:0 6013:0 6014:0 6015:0.0270577 6016:0 6017:0.0514988 6018:0.0802023 6019:0.0881994 6020:0.0837061 6021:0.092795 6022:0.0999718 6023:0.108753 6024:0.101189 6025:0.0803196 6026:0.0872902 6027:0.0853083 6028:0.0719375 6029:0.0902341 6030:0.0999818 6031:0.0963521 6032:0.0762842 6033:0.127132 6034:0.109146 6035:0.107647 6036:0.130504 6037:0.164392 6038:0.146977 6039:0.123672 6040:0.0966347 6041:0.112942 6042:0.103868 6043:0.166805 6044:0.305058 6045:0.302953 6046:0.284585 6047:0.173235 6048:0.107569 6049:0.11851 6050:0.21526 6051:0.246605 6052:0.273429 6053:0.279067 6054:0.301832 6055:0.20874 6056:0.100664 6057:0.1088 6058:0.195529 6059:0.225 6060:0.204639 6061:0.157152 6062:0.186484 6063:0.151054 6064:0.114911 6065:0.118816 6066:0.129542 6067:0.177493 6068:0.1743 6069:0.123209 6070:0.119228 6071:0.125045 6072:0.157351 6073:0.166215 6074:0.132451 6075:0.132482 6076:0.134287 6077:0.136716 6078:0.116573 6079:0.129495 6080:0.14295 6081:0.0636326 6082:0.0774995 6083:0.0606675 6084:0.061424 6085:0.0642512 6086:0.0718801 6087:0.0722908 6088:0.0767096 6089:0.0809085 6090:0.0526556 6091:0.0538947 6092:0.0475289 6093:0.0523392 6094:0.056892 6095:0.0535096 6096:0.0394039 6097:0.0636555 6098:0.0539322 6099:0.0523897 6100:0.084601 6101:0.068939 6102:0.0796741 6103:0.0623456 6104:0.051596 6105:0.0524681 6106:0.0499594 6107:0.0854817 6108:0.114895 6109:0.107381 6110:0.0989969 6111:0.0649285 6112:0.0548325 6113:0.0502246 6114:0.103981 6115:0.109136 6116:0.0840147 6117:0.0573231 6118:0.0755646 6119:0.0580283 6120:0.0528974 6121:0.067557 6122:0.073056 6123:0.0541154 6124:0.0514974 6125:0.0478611 6126:0.0422303 6127:0.0410788 6128:0.0725481 6129:0.0679452 6130:0.056701 6131:0.0557282 6132:0.0402149 6133:0.0452919 6134:0.0483243 6135:0.0688707 6136:0.0590502 6137:0.0848837 6138:0.061122 6139:0.0437551 6140:0.0417594 6141:0.0592995 6142:0.0473044 6143:0.0452113 6144:0.0411736 6145:0 6146:0 6147:0 6148:0 6149:0 6150:0 6151:0 6152:0 6153:0 6154:0 6155:0 6156:0 6157:0 6158:0 6159:0 6160:0 6161:0 6162:0 6163:0 6164:0 6165:0 6166:0 6167:0 6168:0 6169:0 6170:0 6171:0 6172:0 6173:0 6174:0 6175:0 6176:0 6177:0 6178:0 6179:0 6180:0 6181:0 6182:0 6183:0 6184:0 6185:0 6186:0 6187:0 6188:0 6189:0 6190:0 6191:0 6192:0 6193:0 6194:0 6195:0 6196:0 6197:0 6198:0 6199:0 6200:0 6201:0 6202:0 6203:0 6204:0 6205:0 6206:0 6207:0 6208:0 6209:0.0515832 6210:0.0626207 6211:0.0705085 6212:0.071644 6213:0.0717935 6214:0.0753387 6215:0.0822421 6216:0.0821 6217:0.0476841 6218:0.0704361 6219:0.0724433 6220:0.0609313 6221:0.0647877 6222:0.0687635 6223:0.0798516 6224:0.0777735 6225:0.0723617 6226:0.0750496 6227:0.0696888 6228:0.0694075 6229:0.0771016 6230:0.0731047 6231:0.0727265 6232:0.0797274 6233:0.0883644 6234:0.0806435 6235:0.055117 6236:0.110462 6237:0.120828 6238:0.106579 6239:0.0782356 6240:0.0844744 6241:0.0889033 6242:0.0855132 6243:0.121147 6244:0.146111 6245:0.164456 6246:0.159933 6247:0.115695 6248:0.095169 6249:0.0870982 6250:0.132881 6251:0.163626 6252:0.149769 6253:0.14537 6254:0.135939 6255:0.123996 6256:0.0743092 6257:0.0942664 6258:0.103218 6259:0.126616 6260:0.126694 6261:0.107765 6262:0.0906143 6263:0.0926475 6264:0.0762398 6265:0.0831241 6266:0.0838039 6267:0.0767771 6268:0.0817974 6269:0.0735563 6270:0.076296 6271:0.0760694 6272:0.100937 6273:0.0756782 6274:0.0791935 6275:0.0863567 6276:0.0816758 6277:0.0835285 6278:0.089788 6279:0.0886225 6280:0.083201 6281:0.0818561 6282:0.0737313 6283:0.0843685 6284:0.0878758 6285:0.0933935 6286:0.0801542 6287:0.08923 6288:0.0999348 6289:0.0835777 6290:0.077405 6291:0.0944489 6292:0.0988931 6293:0.0929667 6294:0.120523 6295:0.0870012 6296:0.083101 6297:0.0845276 6298:0.0825831 6299:0.0902789 6300:0.0958317 6301:0.090025 6302:0.0918221 6303:0.10872 6304:0.0884614 6305:0.0781211 6306:0.0699957 6307:0.111513 6308:0.109056 6309:0.0985562 6310:0.0964455 6311:0.119932 6312:0.104669 6313:0.0751746 6314:0.0813451 6315:0.114313 6316:0.119374 6317:0.0849615 6318:0.105128 6319:0.102243 6320:0.0941546 6321:0.0849942 6322:0.077618 6323:0.0888799 6324:0.0866496 6325:0.092707 6326:0.0845222 6327:0.095743 6328:0.105615 6329:0.0749023 6330:0.0764975 6331:0.0735359 6332:0.0753788 6333:0.0669118 6334:0.0739577 6335:0.078773 6336:0.12837 6337:0.0511619 6338:0 6339:0 6340:0 6341:0 6342:0 6343:0 6344:0 6345:0 6346:0 6347:0 6348:0 6349:0 6350:0 6351:0 6352:0 6353:0 6354:0 6355:0 6356:0 6357:0 6358:0 6359:0 6360:0 6361:0 6362:0 6363:0 6364:0 6365:0 6366:0 6367:0 6368:0 6369:0 6370:0 6371:0 6372:0 6373:0 6374:0 6375:0 6376:0 6377:0 6378:0 6379:0 6380:0 6381:0 6382:0 6383:0 6384:0 6385:0 6386:0 6387:0 6388:0 6389:0 6390:0 6391:0 6392:0 6393:0.00565835 6394:0 6395:0 6396:0 6397:0 6398:0 6399:0 6400:0 6401:0 6402:0 6403:0 6404:0 6405:0 6406:0 6407:0 6408:0 6409:0 6410:0 6411:0 6412:0 6413:0 6414:0 6415:0 6416:0 6417:0 6418:0 6419:0 6420:0 6421:0 6422:0 6423:0 6424:0 6425:0 6426:0 6427:0 6428:0 6429:0 6430:0 6431:0 6432:0 6433:0 6434:0 6435:0 6436:0 6437:0 6438:0 6439:0 6440:0 6441:0 6442:0 6443:0 6444:0 6445:0 6446:0 6447:0 6448:0 6449:0 6450:0 6451:0 6452:0 6453:0 6454:0 6455:0 6456:0 6457:0.00327981 6458:0 6459:0 6460:0 6461:0 6462:0 6463:0 6464:0.00269105 6465:0.0782189 6466:0.0907959 6467:0.0917801 6468:0.086379 6469:0.0844408 6470:0.100094 6471:0.105314 6472:0.0955629 6473:0.118114 6474:0.103569 6475:0.0944469 6476:0.0989609 6477:0.115364 6478:0.113393 6479:0.106387 6480:0.0962347 6481:0.128064 6482:0.108452 6483:0.102025 6484:0.153271 6485:0.175973 6486:0.138345 6487:0.121465 6488:0.122345 6489:0.132191 6490:0.114941 6491:0.18613 6492:0.238569 6493:0.227596 6494:0.212168 6495:0.163761 6496:0.122004 6497:0.135158 6498:0.163066 6499:0.216089 6500:0.217329 6501:0.210011 6502:0.241351 6503:0.185974 6504:0.124445 6505:0.116324 6506:0.184569 6507:0.196957 6508:0.205198 6509:0.158591 6510:0.186543 6511:0.127992 6512:0.115297 6513:0.128409 6514:0.131507 6515:0.154554 6516:0.146744 6517:0.122131 6518:0.121655 6519:0.114538 6520:0.132307 6521:0.13675 6522:0.149416 6523:0.152412 6524:0.150015 6525:0.134994 6526:0.113866 6527:0.137424 6528:0.141183 6529:0.057048 6530:0.0870436 6531:0.0781033 6532:0.085842 6533:0.0865392 6534:0.091864 6535:0.0909715 6536:0.0922718 6537:0.0882163 6538:0.0890312 6539:0.094163 6540:0.101383 6541:0.0916456 6542:0.0911535 6543:0.10166 6544:0.0950278 6545:0.104628 6546:0.0980937 6547:0.108864 6548:0.0953865 6549:0.11124 6550:0.112663 6551:0.097592 6552:0.0916045 6553:0.104893 6554:0.107672 6555:0.14499 6556:0.146268 6557:0.16776 6558:0.159403 6559:0.138859 6560:0.0987507 6561:0.11054 6562:0.1039 6563:0.166882 6564:0.170469 6565:0.179207 6566:0.175794 6567:0.1469 6568:0.0910046 6569:0.10114 6570:0.15341 6571:0.159486 6572:0.131308 6573:0.115535 6574:0.144317 6575:0.118944 6576:0.0775286 6577:0.0987604 6578:0.100965 6579:0.128809 6580:0.126184 6581:0.113062 6582:0.100345 6583:0.0963695 6584:0.105324 6585:0.0834293 6586:0.095356 6587:0.102594 6588:0.105171 6589:0.0948133 6590:0.0945617 6591:0.102918 6592:0.112979 6593:0 6594:0 6595:0 6596:0 6597:0 6598:0 6599:0 6600:0 6601:0 6602:0 6603:0 6604:0 6605:0 6606:0 6607:0 6608:0 6609:0 6610:0 6611:0 6612:0 6613:0 6614:0 6615:0 6616:0 6617:0 6618:0 6619:0 6620:0 6621:0 6622:0 6623:0 6624:0 6625:0 6626:0 6627:0 6628:0 6629:0 6630:0 6631:0 6632:0 6633:0 6634:0 6635:0 6636:0 6637:0 6638:0 6639:0 6640:0 6641:0 6642:0 6643:0 6644:0 6645:0 6646:0 6647:0 6648:0 6649:0 6650:0 6651:0 6652:0 6653:0 6654:0 6655:0 6656:0 6657:0 6658:0 6659:0 6660:0 6661:0 6662:0 6663:0 6664:0 6665:0 6666:0 6667:0 6668:0 6669:0 6670:0 6671:0 6672:0 6673:0 6674:0 6675:0 6676:0 6677:0 6678:0 6679:0 6680:0 6681:0 6682:0 6683:0 6684:0 6685:0 6686:0 6687:0 6688:0 6689:0 6690:0 6691:0 6692:0 6693:0 6694:0 6695:0 6696:0 6697:0 6698:0 6699:0 6700:0 6701:0 6702:0 6703:0 6704:0 6705:0 6706:0 6707:0 6708:0 6709:0 6710:0 6711:0 6712:0 6713:0 6714:0 6715:0 6716:0 6717:0 6718:0 6719:0 6720:0 6721:0.0138479 6722:0.00932247 6723:0.000977511 6724:0.00281655 6725:0.00285225 6726:0.00744465 6727:0.0121771 6728:0.0228296 6729:0.0386651 6730:0 6731:0 6732:0.0224925 6733:0.00668962 6734:0.0117314 6735:0.00333416 6736:0 6737:0.0515467 6738:0.00469535 6739:0.0103599 6740:0 6741:0.00722588 6742:0 6743:0.00556426 6744:0 6745:0.0456625 6746:0 6747:0.0463937 6748:0.0711735 6749:0.0648813 6750:0.0651438 6751:0.0148637 6752:0 6753:0.0433818 6754:0.0460323 6755:0.0259287 6756:0.055105 6757:0.0646704 6758:0.0776038 6759:0.0351337 6760:0.00902491 6761:0.0507269 6762:0.0253841 6763:0.061722 6764:0.0329804 6765:0.0262533 6766:0.0486667 6767:0.0415065 6768:0.0257058 6769:0.050317 6770:0.0109727 6771:0.0306314 6772:0.0302844 6773:0.0158925 6774:0.0056058 6775:0.0144318 6776:0.0240032 6777:0.075215 6778:0.00670156 6779:0 6780:0.00350648 6781:0.00628525 6782:0 6783:0.00345062 6784:0.0267618 6785:0.0609783 6786:0.054929 6787:0.0579423 6788:0.0549786 6789:0.0606407 6790:0.0609223 6791:0.0622348 6792:0.0656119 6793:0.0414808 6794:0.042949 6795:0.0603919 6796:0.0713456 6797:0.0586258 6798:0.0633417 6799:0.0531707 6800:0.0613495 6801:0.0448529 6802:0.0556661 6803:0.0587094 6804:0.0419432 6805:0.0176173 6806:0.0136017 6807:0.038855 6808:0.0429687 6809:0.0577686 6810:0.0703179 6811:0.0384673 6812:0.0073519 6813:0 6814:0 6815:0.0110844 6816:0.039005 6817:0.0620905 6818:0.0281042 6819:0.00645163 6820:0.0657583 6821:0.0461579 6822:0.0423133 6823:0.0143006 6824:0.0412202 6825:0.057559 6826:0.0805121 6827:0.0524893 6828:0.0467455 6829:0.0685012 6830:0.0803784 6831:0.0621038 6832:0.0499184 6833:0.0213865 6834:0.0486133 6835:0.0668697 6836:0.0590246 6837:0.042768 6838:0.0411971 6839:0.0567003 6840:0.0411644 6841:0.129336 6842:0.117876 6843:0.102946 6844:0.107796 6845:0.118325 6846:0.117292 6847:0.133759 6848:0.115128 6849:0.027898 6850:0.020759 6851:0.0271441 6852:0.0240196 6853:0.0272288 6854:0.0293282 6855:0.0380187 6856:0.0325208 6857:0.0155835 6858:0.0143719 6859:0.0161818 6860:0.0440586 6861:0.0198197 6862:0.0340267 6863:0.0259134 6864:0 6865:0.0372776 6866:0.0142537 6867:0.020869 6868:0.0168468 6869:0.0219784 6870:0.00967877 6871:0.01393 6872:0.0170165 6873:0.0400625 6874:0.0368705 6875:0.0466786 6876:0.0679973 6877:0.0394752 6878:0.0364644 6879:0 6880:0.0141623 6881:0.0320781 6882:0.0634863 6883:0.0853 6884:0.0514516 6885:0.069602 6886:0.0772816 6887:0.0114089 6888:0.0109396 6889:0.0229096 6890:0.0767713 6891:0.0878367 6892:0.0424757 6893:0.0267855 6894:0.0618416 6895:0.0340381 6896:0.0296615 6897:0.0202567 6898:0.0214226 6899:0.0378196 6900:0.0407691 6901:0.018767 6902:0.0146917 6903:0.0442248 6904:0.0386979 6905:0.0502812 6906:0.0362513 6907:0.0341738 6908:0.0269542 6909:0.0288201 6910:0.0194877 6911:0.0428855 6912:0.0304568 6913:0.0820043 6914:0.111784 6915:0.10248 6916:0.107564 6917:0.115453 6918:0.118803 6919:0.118592 6920:0.114834 6921:0.129388 6922:0.128747 6923:0.132818 6924:0.134792 6925:0.13836 6926:0.143304 6927:0.134737 6928:0.125754 6929:0.139269 6930:0.141595 6931:0.132982 6932:0.145117 6933:0.165137 6934:0.14049 6935:0.139738 6936:0.137244 6937:0.13689 6938:0.149888 6939:0.17449 6940:0.233428 6941:0.224258 6942:0.221265 6943:0.157787 6944:0.141103 6945:0.148515 6946:0.208489 6947:0.207907 6948:0.22219 6949:0.220386 6950:0.222326 6951:0.168748 6952:0.140664 6953:0.142719 6954:0.203487 6955:0.209464 6956:0.185766 6957:0.158653 6958:0.200819 6959:0.165826 6960:0.143571 6961:0.146572 6962:0.164237 6963:0.183548 6964:0.170917 6965:0.148146 6966:0.147048 6967:0.165562 6968:0.154228 6969:0.161433 6970:0.167401 6971:0.168283 6972:0.160143 6973:0.151305 6974:0.151204 6975:0.184543 6976:0.184933 6977:0.0290578 6978:0.0208096 6979:0.0147043 6980:0.0220797 6981:0.0188204 6982:0.0238941 6983:0.0207517 6984:0.0312138 6985:0.0407434 6986:0.0125067 6987:0.0164812 6988:0.00982494 6989:0.0289043 6990:0.00957804 6991:0.0164154 6992:0.0221358 6993:0.0608562 6994:0.010226 6995:0.0164821 6996:0.00882001 6997:0.0352707 6998:0.033428 6999:0.0116037 7000:0.0442006 7001:0.0646856 7002:0.0152199 7003:0.0198423 7004:0.0355428 7005:0.0493974 7006:0.0317366 7007:0.0576869 7008:0.0476698 7009:0.0519604 7010:0.0197777 7011:0.069768 7012:0.0593454 7013:0.056679 7014:0.048384 7015:0.0596078 7016:0.0485611 7017:0.0530394 7018:0.0541165 7019:0.0468757 7020:0.0316824 7021:0.0130969 7022:0.038187 7023:0 7024:0.0603356 7025:0.0625236 7026:0.0178041 7027:0.0221879 7028:0.0190331 7029:0.00950184 7030:0.00997935 7031:0.0141062 7032:0.0563423 7033:0.0543758 7034:0.0270165 7035:0.0218492 7036:0.0123226 7037:0.00766959 7038:0.01233 7039:0 7040:0.0462563 7041:0.170761 7042:0.173544 7043:0.176375 7044:0.176486 7045:0.185768 7046:0.191759 7047:0.193641 7048:0.163255 7049:0.197695 7050:0.187462 7051:0.18081 7052:0.16772 7053:0.198294 7054:0.191889 7055:0.188972 7056:0.169754 7057:0.219766 7058:0.201716 7059:0.221361 7060:0.272731 7061:0.299155 7062:0.274661 7063:0.222228 7064:0.195693 7065:0.21641 7066:0.222464 7067:0.311628 7068:0.384745 7069:0.394626 7070:0.380014 7071:0.293772 7072:0.200264 7073:0.214971 7074:0.360949 7075:0.35843 7076:0.381794 7077:0.357837 7078:0.352698 7079:0.296615 7080:0.207847 7081:0.220149 7082:0.304466 7083:0.306566 7084:0.275144 7085:0.246116 7086:0.270463 7087:0.22477 7088:0.216293 7089:0.258097 7090:0.226888 7091:0.265653 7092:0.262387 7093:0.213663 7094:0.219885 7095:0.235961 7096:0.246652 7097:0.277106 7098:0.247143 7099:0.223024 7100:0.218645 7101:0.220624 7102:0.21163 7103:0.219858 7104:0.195079 7105:0.095117 7106:0.13911 7107:0.148729 7108:0.147913 7109:0.144775 7110:0.146279 7111:0.158924 7112:0.161455 7113:0.0931231 7114:0.103332 7115:0.10887 7116:0.0920147 7117:0.101384 7118:0.0886563 7119:0.108107 7120:0.157095 7121:0.120683 7122:0.10745 7123:0.128329 7124:0.111497 7125:0.149116 7126:0.1638 7127:0.122471 7128:0.168313 7129:0.127888 7130:0.101351 7131:0.0961183 7132:0.127716 7133:0.166364 7134:0.194682 7135:0.178798 7136:0.180056 7137:0.122388 7138:0.0966031 7139:0.15037 7140:0.127704 7141:0.155865 7142:0.1695 7143:0.205097 7144:0.190599 7145:0.101897 7146:0.0819131 7147:0.139439 7148:0.166842 7149:0.129116 7150:0.143172 7151:0.180573 7152:0.193095 7153:0.15057 7154:0.107033 7155:0.106915 7156:0.147617 7157:0.121054 7158:0.114132 7159:0.114061 7160:0.213305 7161:0.138226 7162:0.122468 7163:0.116156 7164:0.0907836 7165:0.0849818 7166:0.0897174 7167:0.0771387 7168:0.199524 7169:0.0403894 7170:0.0411656 7171:0.0364728 7172:0.0411381 7173:0.0360096 7174:0.0408381 7175:0.0445108 7176:0.049938 7177:0.0286341 7178:0.0336731 7179:0.034238 7180:0.059568 7181:0.0524868 7182:0.0401308 7183:0.0424444 7184:0.0300299 7185:0.0501506 7186:0.025925 7187:0.0794781 7188:0.051026 7189:0.0444136 7190:0.0457688 7191:0.0274786 7192:0.0356453 7193:0.0454401 7194:0.0725616 7195:0.0570194 7196:0.0908347 7197:0.0355791 7198:0.048251 7199:0.00069966 7200:0.0300227 7201:0.0670101 7202:0.0891829 7203:0.051797 7204:0.0614998 7205:0.0798268 7206:0.0767371 7207:0.017566 7208:0.0417005 7209:0.0633506 7210:0.0791894 7211:0.0664284 7212:0.0286897 7213:0.0693954 7214:0.0520887 7215:0.0455126 7216:0.0665587 7217:0.0796498 7218:0.056277 7219:0.0583128 7220:0.0419338 7221:0.0404776 7222:0.0489983 7223:0.0709267 7224:0.0422459 7225:0.10596 7226:0.0879307 7227:0.0450401 7228:0.0484965 7229:0.0493504 7230:0.049032 7231:0.0663405 7232:0.0486064 7233:0.0239156 7234:0.0517752 7235:0.0537903 7236:0.0545464 7237:0.0486589 7238:0.0508797 7239:0.0591492 7240:0.0797037 7241:0.00329207 7242:0 7243:0 7244:0 7245:0.0027269 7246:0 7247:0 7248:0.0296799 7249:0.0197985 7250:0 7251:0.0130701 7252:0 7253:0.00238122 7254:0.00723385 7255:0 7256:0.0443833 7257:0.0219723 7258:0 7259:9.54946e-05 7260:0.0185406 7261:0 7262:0.023425 7263:0.0268592 7264:0.0431911 7265:0.014236 7266:0 7267:0.0317011 7268:0 7269:0 7270:0.0125492 7271:0.0201692 7272:0.0533519 7273:0 7274:0.00236932 7275:0.00979805 7276:0 7277:0 7278:0.0104279 7279:0 7280:0.0668287 7281:0.0276894 7282:0 7283:0 7284:0.0114369 7285:0 7286:0 7287:0 7288:0.0816238 7289:0.0283011 7290:0.0112537 7291:0 7292:0 7293:0 7294:0 7295:0 7296:0.0472235 7297:0 7298:0 7299:0 7300:0 7301:0 7302:0 7303:0 7304:0 7305:0 7306:0 7307:0 7308:0 7309:0 7310:0 7311:0 7312:0 7313:0 7314:0 7315:0 7316:0 7317:0 7318:0 7319:0 7320:0 7321:0 7322:0 7323:0 7324:0 7325:0 7326:0 7327:0 7328:0 7329:0 7330:0 7331:0 7332:0 7333:0 7334:0 7335:0 7336:0 7337:0 7338:0 7339:0 7340:0 7341:0 7342:0 7343:0 7344:0 7345:0 7346:0 7347:0 7348:0 7349:0 7350:0 7351:0 7352:0 7353:0 7354:0 7355:0 7356:0 7357:0 7358:0 7359:0 7360:0 7361:0.0379536 7362:0.0204763 7363:0.0284965 7364:0.0327486 7365:0.0295234 7366:0.0223088 7367:0.0169152 7368:0.0178624 7369:0.0185183 7370:0.0170081 7371:0.0124522 7372:0.0359005 7373:0.0283845 7374:0.0319959 7375:0.0159409 7376:0.0154555 7377:0.00966795 7378:0.0201549 7379:0.0236969 7380:0.0397599 7381:0.0162092 7382:0.0332912 7383:0.0242036 7384:0.00763039 7385:0.00282113 7386:0.0443499 7387:0.0408646 7388:0.0195827 7389:0 7390:0.00831658 7391:0.0153058 7392:0.0136146 7393:0.0187075 7394:0.0547276 7395:0.0308408 7396:0.0302245 7397:0.026487 7398:0.0296816 7399:0.0121617 7400:0.0257214 7401:0.0266745 7402:0.0498179 7403:0.0228553 7404:0.0177605 7405:0.0230334 7406:0.0328848 7407:0.00574287 7408:0.0223816 7409:0.015998 7410:0.0320313 7411:0.0345287 7412:0.00831436 7413:0.0146337 7414:0.0221904 7415:0.0284684 7416:0.011052 7417:0.0170013 7418:0.00643903 7419:0.0116041 7420:0.00808794 7421:0.00523154 7422:0.00568656 7423:0.0248011 7424:0 7425:0 7426:0 7427:0 7428:0 7429:0 7430:0 7431:0 7432:0 7433:0 7434:0 7435:0 7436:0 7437:0 7438:0 7439:0 7440:0 7441:0 7442:0 7443:0 7444:0.00706559 7445:0 7446:0 7447:0 7448:0 7449:0 7450:0 7451:0 7452:0 7453:0 7454:0 7455:0 7456:0 7457:0 7458:0 7459:0 7460:0 7461:0 7462:0 7463:0 7464:0 7465:0 7466:0 7467:0 7468:0 7469:0 7470:0 7471:0 7472:0 7473:0 7474:0 7475:0 7476:0 7477:0 7478:0 7479:0 7480:0 7481:0 7482:0 7483:0.00165318 7484:0 7485:0.00242857 7486:0.00317609 7487:0.0125171 7488:0.0264708 7489:0.0174245 7490:0.0157881 7491:0.0186879 7492:0.0222568 7493:0.0186135 7494:0.00739863 7495:0.0203274 7496:0.0405531 7497:0.00907131 7498:0.00689382 7499:0.0223719 7500:0.0148239 7501:0.0159127 7502:0.0230108 7503:0.0183903 7504:0.0359553 7505:0.0175477 7506:0.00929584 7507:0.00726957 7508:0 7509:0.0121548 7510:0.0195833 7511:0.0238736 7512:0.0255491 7513:0.0248468 7514:0.00707073 7515:0 7516:0 7517:0 7518:0 7519:0.00651058 7520:0.0281124 7521:0.019147 7522:0 7523:0 7524:0.00851203 7525:0 7526:0.0087681 7527:0 7528:0.035157 7529:0.0146347 7530:0.000527734 7531:0 7532:0.00725445 7533:0.0230976 7534:0.0115891 7535:0.0202835 7536:0.0446684 7537:0.0174849 7538:0.00767022 7539:0.0122162 7540:0.015921 7541:0.00802481 7542:0.0100504 7543:0.00565672 7544:0.035341 7545:0.0387081 7546:0.0134183 7547:0.0245487 7548:0.025362 7549:0.0330122 7550:0.033881 7551:0.0346153 7552:0.0475128 7553:0 7554:0 7555:0 7556:0 7557:0 7558:0 7559:0 7560:0 7561:0 7562:0 7563:0 7564:0 7565:0 7566:0 7567:0 7568:0 7569:0 7570:0 7571:0 7572:0 7573:0 7574:0 7575:0 7576:0 7577:0 7578:0 7579:0 7580:0.00262785 7581:0 7582:0 7583:0 7584:0 7585:0 7586:0 7587:0 7588:0.000592083 7589:0.00340756 7590:0.0105802 7591:0 7592:0 7593:0 7594:0 7595:0 7596:0 7597:0 7598:0 7599:0 7600:0 7601:0 7602:0 7603:0 7604:0 7605:0 7606:0 7607:0 7608:0 7609:0 7610:0 7611:0 7612:0 7613:0 7614:0 7615:0 7616:0 7617:0 7618:0 7619:0 7620:0 7621:0 7622:0 7623:0 7624:0.00844268 7625:0 7626:0 7627:0 7628:0 7629:0 7630:0 7631:0 7632:0 7633:0 7634:0 7635:0 7636:0 7637:0.0123184 7638:0 7639:0 7640:0 7641:0 7642:0 7643:0 7644:0 7645:0.00154969 7646:0 7647:0 7648:0 7649:0 7650:0 7651:0 7652:0 7653:0 7654:0 7655:0 7656:0 7657:0 7658:0 7659:0 7660:0 7661:0 7662:0 7663:0 7664:0 7665:0 7666:0 7667:0 7668:0 7669:0 7670:0 7671:0 7672:0 7673:0 7674:0 7675:0 7676:0 7677:0 7678:0 7679:0 7680:0 7681:0.131626 7682:0.146276 7683:0.151004 7684:0.153439 7685:0.16357 7686:0.173993 7687:0.165693 7688:0.152942 7689:0.155228 7690:0.148835 7691:0.141077 7692:0.162203 7693:0.174303 7694:0.169239 7695:0.157294 7696:0.163576 7697:0.19001 7698:0.164107 7699:0.218827 7700:0.243699 7701:0.233698 7702:0.213032 7703:0.154996 7704:0.160992 7705:0.190548 7706:0.218794 7707:0.244621 7708:0.352108 7709:0.348714 7710:0.329545 7711:0.241785 7712:0.169589 7713:0.194304 7714:0.287293 7715:0.314206 7716:0.303972 7717:0.330521 7718:0.342185 7719:0.266045 7720:0.175735 7721:0.179025 7722:0.245677 7723:0.322787 7724:0.269629 7725:0.206372 7726:0.235718 7727:0.242532 7728:0.198189 7729:0.211157 7730:0.185183 7731:0.227102 7732:0.235209 7733:0.192102 7734:0.180428 7735:0.210712 7736:0.216446 7737:0.243974 7738:0.228664 7739:0.204035 7740:0.198436 7741:0.185452 7742:0.179112 7743:0.197987 7744:0.242041 7745:0.0204759 7746:0.0643712 7747:0.0677353 7748:0.0586054 7749:0.0661495 7750:0.0743589 7751:0.0723751 7752:0.0571946 7753:0.0367344 7754:0.0420292 7755:0.0407722 7756:0.0352494 7757:0.0702256 7758:0.0396787 7759:0.0504355 7760:0.0352915 7761:0.0695337 7762:0.0563985 7763:0.0920458 7764:0.0950635 7765:0.0931167 7766:0.0902956 7767:0.0609383 7768:0.0440917 7769:0.0609501 7770:0.0815114 7771:0.130425 7772:0.13325 7773:0.167661 7774:0.15456 7775:0.134014 7776:0.0515505 7777:0.0738798 7778:0.111592 7779:0.159132 7780:0.13178 7781:0.152966 7782:0.164617 7783:0.158004 7784:0.0497258 7785:0.0709904 7786:0.0729174 7787:0.160451 7788:0.122442 7789:0.0936306 7790:0.0737682 7791:0.104079 7792:0.0589674 7793:0.0910415 7794:0.0672869 7795:0.0759604 7796:0.0901412 7797:0.0674853 7798:0.0658882 7799:0.0900535 7800:0.0805127 7801:0.0933516 7802:0.0832492 7803:0.0832897 7804:0.0669752 7805:0.0576942 7806:0.0557046 7807:0.051903 7808:0.0655315 7809:0 7810:0 7811:0 7812:0 7813:0 7814:0 7815:0 7816:0 7817:0 7818:0 7819:0 7820:0 7821:0 7822:0 7823:0 7824:0 7825:0 7826:0 7827:0 7828:0 7829:0 7830:0 7831:0 7832:0 7833:0 7834:0 7835:0 7836:0 7837:0 7838:0 7839:0 7840:0 7841:0 7842:0 7843:0 7844:0 7845:0 7846:0 7847:0 7848:0 7849:0 7850:0 7851:0 7852:0 7853:0 7854:0 7855:0 7856:0 7857:0 7858:0 7859:0 7860:0 7861:0 7862:0 7863:0 7864:0 7865:0 7866:0 7867:0 7868:0 7869:0 7870:0 7871:0 7872:0 7873:0.0696147 7874:0.0811325 7875:0.0961385 7876:0.10469 7877:0.0990362 7878:0.109749 7879:0.11659 7880:0.114198 7881:0.0796089 7882:0.0799688 7883:0.0912627 7884:0.0728442 7885:0.0814992 7886:0.096243 7887:0.0940657 7888:0.0670922 7889:0.114818 7890:0.103375 7891:0.076209 7892:0.0819706 7893:0.120293 7894:0.121527 7895:0.100188 7896:0.0917086 7897:0.114011 7898:0.0859042 7899:0.144768 7900:0.23177 7901:0.251603 7902:0.232879 7903:0.153207 7904:0.0999969 7905:0.107579 7906:0.18307 7907:0.204096 7908:0.246583 7909:0.263065 7910:0.225535 7911:0.192952 7912:0.0935422 7913:0.109569 7914:0.163824 7915:0.182014 7916:0.178576 7917:0.138491 7918:0.175424 7919:0.130248 7920:0.098736 7921:0.110488 7922:0.114564 7923:0.171 7924:0.168302 7925:0.104475 7926:0.106807 7927:0.110231 7928:0.129398 7929:0.141117 7930:0.111449 7931:0.0969956 7932:0.104092 7933:0.109871 7934:0.0957832 7935:0.0948138 7936:0.103186 7937:0.0475387 7938:0.0671333 7939:0.0806206 7940:0.0798451 7941:0.0876545 7942:0.083111 7943:0.0948624 7944:0.102781 7945:0.0528347 7946:0.0740183 7947:0.0740184 7948:0.100674 7949:0.0771359 7950:0.0935177 7951:0.0756048 7952:0.0839472 7953:0.0812849 7954:0.090521 7955:0.0810764 7956:0.0973364 7957:0.102718 7958:0.100678 7959:0.0702462 7960:0.0891513 7961:0.0851747 7962:0.0896127 7963:0.123128 7964:0.161304 7965:0.179003 7966:0.158414 7967:0.120088 7968:0.092693 7969:0.0875816 7970:0.158172 7971:0.168969 7972:0.181614 7973:0.180308 7974:0.201477 7975:0.145417 7976:0.119518 7977:0.0906927 7978:0.148316 7979:0.178853 7980:0.154739 7981:0.143695 7982:0.165815 7983:0.13822 7984:0.133296 7985:0.104553 7986:0.112602 7987:0.153199 7988:0.125216 7989:0.112407 7990:0.0956355 7991:0.128561 7992:0.146573 7993:0.159778 7994:0.150201 7995:0.113573 7996:0.100559 7997:0.10038 7998:0.10652 7999:0.158117 8000:0.122912 8001:0 8002:0 8003:0 8004:0 8005:0 8006:0 8007:0 8008:0 8009:0.00428084 8010:0 8011:0 8012:0.0116903 8013:0 8014:0 8015:0 8016:0 8017:0.0258849 8018:0 8019:0.0087373 8020:0.00211046 8021:0.0208235 8022:0 8023:0 8024:0 8025:0.0203598 8026:0.000124909 8027:0.0173338 8028:0.0159604 8029:0 8030:0.00612898 8031:0 8032:0 8033:0.0202377 8034:0.0100788 8035:0.0113544 8036:0.00835327 8037:0.0191164 8038:0 8039:0 8040:0.0196486 8041:0.0225443 8042:0.0210357 8043:0.000639144 8044:0 8045:0 8046:0.00736409 8047:0.0112296 8048:0.0190191 8049:0.0476827 8050:0.000102576 8051:0.000768166 8052:0 8053:0 8054:0 8055:0.000294968 8056:0 8057:0.0505133 8058:0 8059:0 8060:0 8061:0 8062:0 8063:0.0282269 8064:0.0200676 8065:0 8066:0.00212111 8067:0.00877746 8068:6.97561e-05 8069:0 8070:0 8071:0.00160508 8072:0.0673679 8073:0 8074:0.0122407 8075:0.0104995 8076:0 8077:0 8078:0 8079:0.010541 8080:0.0532611 8081:0.00829003 8082:0.0008086 8083:0.0183136 8084:0 8085:0.0347189 8086:0.0330478 8087:0.0264306 8088:0.0727716 8089:0.0214463 8090:0.023756 8091:0 8092:0 8093:0.0135256 8094:0.0559082 8095:0.0696582 8096:0.087351 8097:0.0136932 8098:0.00199591 8099:0.010414 8100:0.0314794 8101:0.0202127 8102:0.0246485 8103:0.0905577 8104:0.0857722 8105:0 8106:0 8107:0.0355151 8108:0.0425116 8109:0.0247282 8110:0.0185076 8111:0.0528738 8112:0.0913099 8113:0.0213784 8114:0.0234793 8115:0.0103467 8116:0.0336407 8117:0.0108827 8118:0.0092875 8119:0 8120:0.0988632 8121:0 8122:0.00227392 8123:0.0150923 8124:0 8125:0 8126:0 8127:0 8128:0.105772 8129:0.031854 8130:0.081796 8131:0.0820157 8132:0.0814837 8133:0.0842912 8134:0.0927239 8135:0.0897465 8136:0.0962142 8137:0.0649301 8138:0.0688128 8139:0.0618582 8140:0.097334 8141:0.0773802 8142:0.0959427 8143:0.0847647 8144:0.0751837 8145:0.0931862 8146:0.0853129 8147:0.0867591 8148:0.0841915 8149:0.109335 8150:0.0901851 8151:0.0816408 8152:0.0770815 8153:0.0926711 8154:0.0975926 8155:0.124271 8156:0.163951 8157:0.14565 8158:0.152448 8159:0.0940806 8160:0.0859701 8161:0.0863941 8162:0.113528 8163:0.154079 8164:0.172504 8165:0.186695 8166:0.168793 8167:0.132911 8168:0.0874652 8169:0.080026 8170:0.170846 8171:0.173987 8172:0.139394 8173:0.116913 8174:0.173602 8175:0.134066 8176:0.077091 8177:0.0983498 8178:0.0972675 8179:0.137479 8180:0.139925 8181:0.0920524 8182:0.0821664 8183:0.100341 8184:0.109808 8185:0.119942 8186:0.0987327 8187:0.102634 8188:0.0966405 8189:0.0955894 8190:0.0942249 8191:0.130785 8192:0.12856\n"]}]}]}