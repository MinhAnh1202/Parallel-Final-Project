{"cells":[{"cell_type":"markdown","source":["# CPU"],"metadata":{"id":"yML7sBgZAWFg"}},{"cell_type":"code","source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","!tar -xzvf cifar-10-binary.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1idnkCMcNPgB","executionInfo":{"status":"ok","timestamp":1766298308290,"user_tz":-420,"elapsed":4624,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"e5bfe6b0-b66b-4f3a-f420-da6b78afe27f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-21 06:25:03--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  66.0MB/s    in 2.5s    \n","\n","2025-12-21 06:25:06 (66.0 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n","\n","cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"markdown","source":["## 1) Huấn luyện Autoencoder:"],"metadata":{"id":"SDOf21TsAdH1"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvsCf0PFdQ07","outputId":"13ebe0ae-9e1a-4818-acb9-a83300c2f0ef","executionInfo":{"status":"ok","timestamp":1766298308338,"user_tz":-420,"elapsed":47,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_layers.c\n"]}],"source":["%%writefile cpu_layers.c\n","#include \"cpu_layers.h\"\n","\n","void Relu(float* input, int N, float* output) {\n","    for (int i = 0; i < N; i++) {\n","        output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n","    }\n","}\n","\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width)\n","{\n","    // // Tính toán kích thước output\n","    // int H_out = (input_height + 2 * padding - kernel_height) / stride + 1;\n","    // int W_out = (input_width + 2 * padding - kernel_width) / stride + 1;\n","    // int output_size = filter_count * H_out * W_out;\n","    // output = (float*)malloc(output_size * sizeof(float));\n","    // if (output == NULL) {\n","    //     fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","    //     return;\n","    // }\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua chiều cao output\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            // Lặp qua chiều rộng output\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float sum = 0.0f;\n","                // Lặp qua kênh đầu vào (c_in)\n","                for (int c_in = 0; c_in < input_channels; c_in++) {\n","                    // Lặp qua kernel height\n","                    for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                        // Lặp qua kernel width\n","                        for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                            // Vị trí input tương ứng\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float val = 0.0f;\n","                            // Kiểm tra zero padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int channel_size = input_width * input_height;\n","                                val = input[c_in * channel_size + h_in * input_width + w_in];\n","                            }\n","\n","                            int weight_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            k_h * kernel_width + k_w;\n","\n","                            sum += val * kernel[weight_idx];\n","                        }\n","                    }\n","                }\n","                sum += biases[c_out];  // Thêm bias\n","                int output_idx = h_out * output_width + w_out + c_out * output_width * output_height;\n","                output[output_idx] = sum;\n","            }\n","        }\n","    }\n","}\n","\n","\n","void MaxPool2D_Forward(float* input, int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width) {\n","    // int H_out = (input_height - filter_height) / stride + 1;\n","    // int W_out = (input_width - filter_width) / stride + 1;\n","\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float max_val = -FLT_MAX;\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = c * plane_size_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                        }\n","                    }\n","                }\n","                int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                output[output_idx] = max_val;\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","int scale_factor, int filter_count, float* output, int output_height, int output_width) {\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float val = input[c * plane_size_in + h_in * input_width + w_in];\n","                for (int sh = 0; sh < scale_factor; sh++) { // Gấp đôi hàng\n","                    for (int sw = 0; sw < scale_factor; sw++) { // Gấp đôi cột\n","                        int h_out = h_in * scale_factor + sh;\n","                        int w_out = w_in * scale_factor + sw;\n","                        int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                        output[output_idx] = val;\n","                    }\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","float MSE(float* input, float* output, int size) {\n","    float sum = 0.0f;\n","    for (int i = 0; i < size; i++) {\n","        sum += (output[i] - input[i]) * (output[i] - input[i]);\n","    }\n","    return sum / size;\n","}\n","\n","void Relu_Backward(float* d_output, float* input,int N) {\n","    for (int i = 0; i < N; i++) {\n","        d_output[i] = input[i] > 0.0f ? d_output[i] : 0.0f;\n","    }\n","}\n","\n","void MSE_Gradient(float* input, float* output, int size, float* d_output) {\n","    float sum = 0.0f;\n","    float factor = 2.0f / size;\n","    for (int i = 0; i < size; i++) {\n","        d_output[i] = factor * (output[i] - input[i]);\n","    }\n","}\n","\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* d_input)\n","{\n","    // Chỉ gán vị trí giá trị max của input ban đầu là gradient của lớp tiếp theo (d_output), còn lại là 0\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) { // Khởi tạo gradient của input ban đầu là 0\n","        d_input[i] = 0.0f;\n","    }\n","\n","    for (int c = 0; c < filter_count; c++) {\n","\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","\n","                float max_val = -FLT_MAX;\n","                int max_input_idx = -1;\n","\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = channel_offset_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                            max_input_idx = input_idx;\n","                        }\n","                    }\n","                }\n","                //Lấy gradient từ output\n","                if (max_input_idx != -1) {\n","                    int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                    d_input[max_input_idx] += d_output[output_idx];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width) {\n","\n","    int plane_size_in = d_input_height * d_input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) {\n","        d_input[i] = 0.0f;\n","    }\n","    for (int c = 0; c < filter_count; c++) {\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < d_input_height; h_in++) {\n","            for (int w_in = 0; w_in < d_input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                int h_start_out = h_in * scale_factor;\n","                int w_start_out = w_in * scale_factor;\n","                for (int sh = 0; sh < scale_factor; sh++) {\n","                    for (int sw = 0; sw < scale_factor; sw++) {\n","                        int h_out = h_start_out + sh;\n","                        int w_out = w_start_out + sw;\n","                        if (h_out < d_output_height && w_out < d_output_width) {\n","                            int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                            sum_gradient += d_output[output_idx];\n","                        }\n","                    }\n","                }\n","                int input_idx = channel_offset_in + h_in * d_input_width + w_in;\n","                d_input[input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input) {\n","    // Thực hiện tích chập giữa dE/dO và kernel (xoay 180 độ) để tính dE/dI\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh input (kênh output gradient)\n","    for (int c_in = 0; c_in < input_channels; c_in++) {\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                // Lặp qua kênh output (số lượng filter)\n","                for (int c_out = 0; c_out < filter_count; c_out++) {\n","                    // Lặp qua kernel (xoay 180 độ)\n","                    for (int kh = 0; kh < kernel_height; kh++) {\n","                        for (int kw = 0; kw < kernel_width; kw++) {\n","                            int h_out = h_in - kh + padding;\n","                            int w_out = w_in - kw + padding;\n","                            float d_output_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_out >= 0 && h_out < d_output_height && w_out >= 0 && w_out < d_output_width) {\n","                                int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                                d_output_val = d_output[d_output_idx];\n","                            }\n","                            // Tính chỉ số kernel (xoay 180 độ)\n","                            int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            (kernel_height - 1 - kh) * kernel_width + (kernel_width - 1 - kw);\n","\n","                            sum_gradient += d_output_val * kernel[kernel_idx];\n","                        }\n","                    }\n","                }\n","                int d_input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                d_input[d_input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights) {\n","    // Thực hiện tích chập giữa dE/dO và input để tính dE/dW\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua kênh đầu vào\n","        for (int c_in = 0; c_in < input_channels; c_in++) {\n","            // Lặp qua kích thước kernel\n","            for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                    float sum_gradient = 0.0f;\n","                    // Lặp qua output grid (d_output) để tích lũy\n","                    for (int h_out = 0; h_out < d_output_height; h_out++) {\n","                        for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float input_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                                input_val = input[input_idx];\n","                            }\n","                            int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                            float d_output_val = d_output[d_output_idx];\n","                            // Tính gradient\n","                            sum_gradient += input_val * d_output_val;\n","                        }\n","                    }\n","                    int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                    k_h * kernel_width + k_w;\n","                    d_weights[kernel_idx] += sum_gradient;\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height,\n","    int filter_count, float* d_biases) {\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua từng filter\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        float sum_gradient = 0.0f;\n","        // Lặp qua từng vị trí trong output\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                // Tính chỉ số trong mảng d_output\n","                int output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                // Cộng dồn gradient từ d_output\n","                sum_gradient += d_output[output_idx];\n","            }\n","        }\n","        d_biases[c_out] += sum_gradient;\n","    }\n","}\n","\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params) {\n","    for (int i = 0; i < N_params; i++) {\n","        weights[i] -= (learning_rate * d_weights[i]);\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"929dCgq1dZu3","outputId":"9259c512-e32e-4785-c957-165d866e399a","executionInfo":{"status":"ok","timestamp":1766298308394,"user_tz":-420,"elapsed":55,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_layers.h\n"]}],"source":["%%writefile cpu_layers.h\n","#pragma once\n","#include <stdio.h>\n","#include <float.h>\n","\n","void Relu(float* input, int N, float* output);\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void MaxPool2D_Forward(float* input, int input_width, int input_height,\n","    int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","    int scale_factor, int filter_count,\n","    float* output, int output_height, int output_width);\n","float MSE(float* input, float* output, int size);\n","void Relu_Backward(float* d_output, float* input,int N);\n","void MSE_Gradient(float* input, float* output, int size, float* d_output);\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width);\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights);\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height, int filter_count, float* d_biases);\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZDAw3eKdhov","outputId":"83ac47b7-1572-4820-a7d0-445ce1393c57","executionInfo":{"status":"ok","timestamp":1766298308413,"user_tz":-420,"elapsed":18,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_autoencoder.h\n"]}],"source":["%%writefile cpu_autoencoder.h\n","#pragma once\n","#include \"cpu_layers.h\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include <math.h>\n","#include <stdint.h>\n","#include <time.h>\n","#include <string.h>\n","// Định nghĩa kích thước Kernel/Stride/Padding\n","#define KERNEL_SIZE 3\n","#define POOL_SIZE 2\n","#define UPSAMPLE_SIZE 2\n","#define CONV_PADDING 1\n","#define CONV_STRIDE 1\n","#define POOL_STRIDE 2\n","\n","typedef struct {\n","    int batch_size;\n","    double learning_rate;\n","    // kích thước input\n","    int input_height;       // 32\n","    int input_width;        // 32\n","    int input_channels;     // 3\n","    // weight, bias và gradient của từng lớp Conv2D\n","    float* w1; float* b1; float* d_w1; float* d_b1;\n","    float* w2; float* b2; float* d_w2; float* d_b2;\n","    float* w3; float* b3; float* d_w3; float* d_b3;\n","    float* w4; float* b4; float* d_w4; float* d_b4;\n","    float* w5; float* b5; float* d_w5; float* d_b5;\n","\n","    float* batch_input;\n","    float* final_output;\n","    float* loss_gradient;\n","    // ouput và gradient của từng lớp Conv2D/MaxPool/UpSample\n","    float* conv1_output;   float* d_conv1_output;\n","    float* pool1_output;   float* d_pool1_output;\n","    float* conv2_output;   float* d_conv2_output;\n","    float* pool2_output;   float* d_pool2_output; // LATENT SPACE\n","    float* conv3_output;   float* d_conv3_output;\n","    float* upsample1_output; float* d_upsample1_output;\n","    float* conv4_output;   float* d_conv4_output;\n","    float* upsample2_output; float* d_upsample2_output;\n","} CPUAutoEncoder;\n","\n","void random_initialize(float* array, int size, float min, float max);\n","void initialize_autoencoder(CPUAutoEncoder* autoencoder, int batch_size, double learning_rate);\n","void free_autoencoder(CPUAutoEncoder* autoencoder);\n","void forward_autoencoder(CPUAutoEncoder* autoencoder);\n","void backward_autoencoder(CPUAutoEncoder* autoencoder);\n","void update_autoencoder_parameters(CPUAutoEncoder* autoencoder);\n","void save_weights(CPUAutoEncoder* autoencoder, const char* filename);\n","void cpu_extract_features(CPUAutoEncoder* autoencoder, float* input_data, int num_images, float* features_output);\n","void cpu_load_weights(CPUAutoEncoder* autoencoder, const char* filename);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4y5aXh1idtX2","outputId":"4feef3ba-9c30-4b56-ba8b-2abf274045b4","executionInfo":{"status":"ok","timestamp":1766298308476,"user_tz":-420,"elapsed":62,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_autoencoder.c\n"]}],"source":["%%writefile cpu_autoencoder.c\n","#include \"cpu_autoencoder.h\"\n","\n","// Hàm khởi tạo mảng trọng số với giá trị ngẫu nhiên trong khoảng [min, max]\n","void random_initialize(float* array, int size, float min, float max) {\n","    for (int i = 0; i < size; i++) {\n","        float scale = (float)rand() / (float)RAND_MAX;\n","        array[i] = min + scale * (max - min);\n","    }\n","}\n","\n","void zero_initialize(float* array, int size) {\n","    for (int i = 0; i < size; i++) {\n","        array[i] = 0.0f;\n","    }\n","}\n","\n","\n","void initialize_conv_layer(float** w, float** b, float** dw, float** db, int C_in, int C_out) {\n","    int size_W = C_out * C_in * KERNEL_SIZE * KERNEL_SIZE;\n","    *w = (float*)malloc(size_W * sizeof(float));\n","    *b = (float*)malloc(C_out * sizeof(float));\n","    *dw = (float*)malloc(size_W * sizeof(float));\n","    *db = (float*)malloc(C_out * sizeof(float));\n","\n","    random_initialize(*w, size_W, -0.05f, 0.05f);\n","    random_initialize(*b, C_out, -0.05f, 0.05f);\n","    zero_initialize(*dw, size_W);\n","    zero_initialize(*db, C_out);\n","}\n","\n","float* allocate_buffer(int batch_size, int H, int W, int C) {\n","    int size = batch_size * H * W * C;\n","    return (float*)malloc(size * sizeof(float));\n","}\n","\n","\n","void initialize_autoencoder(CPUAutoEncoder* autoencoder, int batch_size, double learning_rate) {\n","    // Tham số chung\n","    autoencoder->batch_size = batch_size;\n","    autoencoder->learning_rate = learning_rate;\n","    autoencoder->input_height = 32;\n","    autoencoder->input_width = 32;\n","    autoencoder->input_channels = 3;\n","\n","    // Output channels của các lớp\n","    int C_in = 3, C1 = 256, C2 = 128, C3 = 128, C4 = 256, C5 = 3;\n","    // Kích thước không gian (Pixel/kênh)\n","    int P1 = 32 * 32, P2 = 16 * 16, P3 = 8 * 8;\n","    // Khởi tạo trọng số, bias và gradient cho từng lớp Conv2D\n","    initialize_conv_layer(&autoencoder->w1, &autoencoder->b1, &autoencoder->d_w1, &autoencoder->d_b1, C_in, C1);\n","    initialize_conv_layer(&autoencoder->w2, &autoencoder->b2, &autoencoder->d_w2, &autoencoder->d_b2, C1, C2);\n","    initialize_conv_layer(&autoencoder->w3, &autoencoder->b3, &autoencoder->d_w3, &autoencoder->d_b3, C2, C3);\n","    initialize_conv_layer(&autoencoder->w4, &autoencoder->b4, &autoencoder->d_w4, &autoencoder->d_b4, C3, C4);\n","    initialize_conv_layer(&autoencoder->w5, &autoencoder->b5, &autoencoder->d_w5, &autoencoder->d_b5, C4, C5);\n","    // Khởi tạo Buffers cho activations và gradients\n","    int input_height = 32, input_width = 32;\n","    autoencoder->batch_input = allocate_buffer(batch_size, input_height, input_width, C_in);\n","    autoencoder->final_output = allocate_buffer(batch_size, input_height, input_width, C5); // Output size (32x32x3)\n","    autoencoder->loss_gradient = allocate_buffer(batch_size, input_height, input_width, C5);\n","    // Layer 1 (Conv1): 32x32x256\n","    autoencoder->conv1_output = allocate_buffer(batch_size, input_height, input_width, C1);\n","    autoencoder->d_conv1_output = allocate_buffer(batch_size, input_height, input_width, C1);\n","    // Layer 2 (Pool1): 16x16x256\n","    int H2 = 16, W2 = 16;\n","    autoencoder->pool1_output = allocate_buffer(batch_size, H2, W2, C1);\n","    autoencoder->d_pool1_output = allocate_buffer(batch_size, H2, W2, C1);\n","    // Layer 3 (Conv2): 16x16x128\n","    autoencoder->conv2_output = allocate_buffer(batch_size, H2, W2, C2);\n","    autoencoder->d_conv2_output = allocate_buffer(batch_size, H2, W2, C2);\n","    // Layer 4 (Pool2 - Latent): 8x8x128\n","    int H3 = 8, W3 = 8;\n","    autoencoder->pool2_output = allocate_buffer(batch_size, H3, W3, C2);\n","    autoencoder->d_pool2_output = allocate_buffer(batch_size, H3, W3, C2);\n","    // Layer 5 (Conv3): 8x8x128\n","    autoencoder->conv3_output = allocate_buffer(batch_size, H3, W3, C3);\n","    autoencoder->d_conv3_output = allocate_buffer(batch_size, H3, W3, C3);\n","    // Layer 6 (UpSample1): 16x16x128\n","    autoencoder->upsample1_output = allocate_buffer(batch_size, H2, W2, C3);\n","    autoencoder->d_upsample1_output = allocate_buffer(batch_size, H2, W2, C3);\n","    // Layer 7 (Conv4): 16x16x256\n","    autoencoder->conv4_output = allocate_buffer(batch_size, H2, W2, C4);\n","    autoencoder->d_conv4_output = allocate_buffer(batch_size, H2, W2, C4);\n","    // Layer 8 (UpSample2): 32x32x256\n","    autoencoder->upsample2_output = allocate_buffer(batch_size, input_height, input_width, C4);\n","    autoencoder->d_upsample2_output = allocate_buffer(batch_size, input_height, input_width, C4);\n","}\n","\n","void free_autoencoder(CPUAutoEncoder* autoencoder) {\n","    // Giải phóng trọng số và gradient\n","    free(autoencoder->w1); free(autoencoder->b1); free(autoencoder->d_w1); free(autoencoder->d_b1);\n","    free(autoencoder->w2); free(autoencoder->b2); free(autoencoder->d_w2); free(autoencoder->d_b2);\n","    free(autoencoder->w3); free(autoencoder->b3); free(autoencoder->d_w3); free(autoencoder->d_b3);\n","    free(autoencoder->w4); free(autoencoder->b4); free(autoencoder->d_w4); free(autoencoder->d_b4);\n","    free(autoencoder->w5); free(autoencoder->b5); free(autoencoder->d_w5); free(autoencoder->d_b5);\n","\n","    // Giải phóng buffers activation/gradient\n","    free(autoencoder->batch_input);\n","    free(autoencoder->final_output);\n","    free(autoencoder->loss_gradient);\n","    free(autoencoder->conv1_output); free(autoencoder->d_conv1_output);\n","    free(autoencoder->pool1_output); free(autoencoder->d_pool1_output);\n","    free(autoencoder->conv2_output); free(autoencoder->d_conv2_output);\n","    free(autoencoder->pool2_output); free(autoencoder->d_pool2_output);\n","    free(autoencoder->conv3_output); free(autoencoder->d_conv3_output);\n","    free(autoencoder->upsample1_output); free(autoencoder->d_upsample1_output);\n","    free(autoencoder->conv4_output); free(autoencoder->d_conv4_output);\n","    free(autoencoder->upsample2_output); free(autoencoder->d_upsample2_output);\n","}\n","\n","// Forward\n","void forward_autoencoder(CPUAutoEncoder* autoencoder) {\n","    int bs = autoencoder->batch_size;\n","\n","    // Kích thước activation của 1 ảnh tại các lớp\n","    int size_input = 32 * 32 * 3;\n","    int size_L1 = 32 * 32 * 256;\n","    int size_L2 = 16 * 16 * 256;\n","    int size_L3 = 16 * 16 * 128;\n","    int size_L4 = 8 * 8 * 128; // Latent\n","    // Decoder sizes\n","    int size_L5 = 8 * 8 * 128;\n","    int size_L6 = 16 * 16 * 128;\n","    int size_L7 = 16 * 16 * 256;\n","    int size_L8 = 32 * 32 * 256;\n","    int size_Out = 32 * 32 * 3;\n","    for (int b = 0; b < bs; b++) {\n","        // Tính offset con trỏ cho ảnh thứ b\n","        float* ptr_input = autoencoder->batch_input + b * size_input;\n","        float* ptr_L1 = autoencoder->conv1_output + b * size_L1;\n","        float* ptr_L2 = autoencoder->pool1_output + b * size_L2;\n","        float* ptr_L3 = autoencoder->conv2_output + b * size_L3;\n","        float* ptr_L4 = autoencoder->pool2_output + b * size_L4;\n","        float* ptr_L5 = autoencoder->conv3_output + b * size_L5;\n","        float* ptr_L6 = autoencoder->upsample1_output + b * size_L6;\n","        float* ptr_L7 = autoencoder->conv4_output + b * size_L7;\n","        float* ptr_L8 = autoencoder->upsample2_output + b * size_L8;\n","        float* ptr_Out = autoencoder->final_output + b * size_Out;\n","        // --- ENCODER ---\n","        // L1: Conv1 + ReLU\n","        Conv2D_Forward(ptr_input, 32, 32, 3, autoencoder->w1, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b1, CONV_PADDING, CONV_STRIDE, 256, ptr_L1, 32, 32);\n","        Relu(ptr_L1, size_L1, ptr_L1);\n","\n","        // L2: Pool1\n","        MaxPool2D_Forward(ptr_L1, 32, 32, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 256, ptr_L2, 16, 16);\n","\n","        // L3: Conv2 + ReLU\n","        Conv2D_Forward(ptr_L2, 16, 16, 256, autoencoder->w2, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b2, CONV_PADDING, CONV_STRIDE, 128, ptr_L3, 16, 16);\n","        Relu(ptr_L3, size_L3, ptr_L3);\n","\n","        // L4: Pool2 (Latent)\n","        MaxPool2D_Forward(ptr_L3, 16, 16, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 128, ptr_L4, 8, 8);\n","\n","        // --- DECODER ---\n","        // L5: Conv3 + ReLU\n","        Conv2D_Forward(ptr_L4, 8, 8, 128, autoencoder->w3, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b3, CONV_PADDING, CONV_STRIDE, 128, ptr_L5, 8, 8);\n","        Relu(ptr_L5, size_L5, ptr_L5);\n","\n","        // L6: UpSample1\n","        UpSample2D_Forward(ptr_L5, 8, 8, UPSAMPLE_SIZE, 128, ptr_L6, 16, 16);\n","\n","        // L7: Conv4 + ReLU\n","        Conv2D_Forward(ptr_L6, 16, 16, 128, autoencoder->w4, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b4, CONV_PADDING, CONV_STRIDE, 256, ptr_L7, 16, 16);\n","        Relu(ptr_L7, size_L7, ptr_L7);\n","\n","        // L8: UpSample2\n","        UpSample2D_Forward(ptr_L7, 16, 16, UPSAMPLE_SIZE, 256, ptr_L8, 32, 32);\n","\n","        // L9: Conv5 (Output)\n","        Conv2D_Forward(ptr_L8, 32, 32, 256, autoencoder->w5, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b5, CONV_PADDING, CONV_STRIDE, 3, ptr_Out, 32, 32);\n","    }\n","}\n","\n","\n","// Backward\n","void backward_autoencoder(CPUAutoEncoder* autoencoder) {\n","    int bs = autoencoder->batch_size;\n","    int total_elements = bs * 32 * 32 * 3;\n","    MSE_Gradient(autoencoder->batch_input, autoencoder->final_output, total_elements, autoencoder->loss_gradient);\n","    // Khởi tạo gradient về 0 trước khi cộng dồn\n","    zero_initialize(autoencoder->d_w1, 256*3*3*3); zero_initialize(autoencoder->d_b1, 256);\n","    zero_initialize(autoencoder->d_w2, 128*256*3*3); zero_initialize(autoencoder->d_b2, 128);\n","    zero_initialize(autoencoder->d_w3, 128*128*3*3); zero_initialize(autoencoder->d_b3, 128);\n","    zero_initialize(autoencoder->d_w4, 256*128*3*3); zero_initialize(autoencoder->d_b4, 256);\n","    zero_initialize(autoencoder->d_w5, 3*256*3*3); zero_initialize(autoencoder->d_b5, 3);\n","    // Kích thước 1 ảnh tại các lớp (như Forward)\n","    int size_Out = 32*32*3;\n","    int size_L8 = 32*32*256;\n","    int size_L7 = 16*16*256;\n","    int size_L6 = 16*16*128;\n","    int size_L5 = 8*8*128;\n","    int size_L4 = 8*8*128;\n","    int size_L3 = 16*16*128;\n","    int size_L2 = 16*16*256;\n","    int size_L1 = 32*32*256;\n","    int size_In = 32*32*3;\n","\n","    for (int b = 0; b < bs; b++) {\n","        // Offset pointers\n","        float* ptr_dOut = autoencoder->loss_gradient + b * size_Out;\n","        float* ptr_Upsample2_Out = autoencoder->upsample2_output + b * size_L8;\n","        float* ptr_d_Upsample2_Out = autoencoder->d_upsample2_output + b * size_L8;\n","        float* ptr_d_Conv4_Out = autoencoder->d_conv4_output + b * size_L7;\n","        float* ptr_Upsample1_Out = autoencoder->upsample1_output + b * size_L6;\n","        float* ptr_d_Upsample1_Out = autoencoder->d_upsample1_output + b * size_L6;\n","        float* ptr_d_Conv3_Out = autoencoder->d_conv3_output + b * size_L5;\n","        float* ptr_Pool2_Out = autoencoder->pool2_output + b * size_L4;\n","        float* ptr_d_Pool2_Out = autoencoder->d_pool2_output + b * size_L4;\n","        float* ptr_Conv2_Out = autoencoder->conv2_output + b * size_L3;\n","        float* ptr_d_Conv2_Out = autoencoder->d_conv2_output + b * size_L3;\n","        float* ptr_Pool1_Out = autoencoder->pool1_output + b * size_L2;\n","        float* ptr_d_Pool1_Out = autoencoder->d_pool1_output + b * size_L2;\n","        float* ptr_Conv1_Out = autoencoder->conv1_output + b * size_L1;\n","        float* ptr_d_Conv1_Out = autoencoder->d_conv1_output + b * size_L1;\n","        float* ptr_Input = autoencoder->batch_input + b * size_In;\n","\n","        // === L9 (Conv5) ===\n","        // dW5, dB5\n","        Conv2D_Backward_Kernel(ptr_dOut, 32, 32, ptr_Upsample2_Out, 32, 32, 256, 3, 3, 1, 1, 3, autoencoder->d_w5);\n","        Conv2D_Backward_Biases(ptr_dOut, 32, 32, 3, autoencoder->d_b5);\n","        // dInput cho L8\n","        Conv2D_Backward_Input(ptr_dOut, 32, 32, autoencoder->w5, 3, 3, 32, 32, 256, 1, 1, 3, ptr_d_Upsample2_Out);\n","\n","        // === L8 (Upsample2) ===\n","        UpSample2D_Backward(ptr_d_Upsample2_Out, 32, 32, UPSAMPLE_SIZE, 256, autoencoder->d_conv4_output + b * size_L7, 16, 16);\n","\n","        // === L7 (Conv4) ===\n","        // ReLU Backward\n","        Relu_Backward(ptr_d_Conv4_Out, autoencoder->conv4_output + b * size_L7, 16*16*256);\n","        Conv2D_Backward_Kernel(ptr_d_Conv4_Out, 16, 16, ptr_Upsample1_Out, 16, 16, 128, 3, 3, 1, 1, 256, autoencoder->d_w4);\n","        Conv2D_Backward_Biases(ptr_d_Conv4_Out, 16, 16, 256, autoencoder->d_b4);\n","        Conv2D_Backward_Input(ptr_d_Conv4_Out, 16, 16, autoencoder->w4, 3, 3, 16, 16, 128, 1, 1, 256, ptr_d_Upsample1_Out);\n","\n","        // === L6 (Upsample1) ===\n","        UpSample2D_Backward(ptr_d_Upsample1_Out, 16, 16, UPSAMPLE_SIZE, 128, ptr_d_Conv3_Out, 8, 8);\n","\n","        // === L5 (Conv3) ===\n","        Relu_Backward(ptr_d_Conv3_Out, autoencoder->conv3_output + b * size_L5, 8*8*128);\n","        Conv2D_Backward_Kernel(ptr_d_Conv3_Out, 8, 8, ptr_Pool2_Out, 8, 8, 128, 3, 3, 1, 1, 128, autoencoder->d_w3);\n","        Conv2D_Backward_Biases(ptr_d_Conv3_Out, 8, 8, 128, autoencoder->d_b3);\n","        Conv2D_Backward_Input(ptr_d_Conv3_Out, 8, 8, autoencoder->w3, 3, 3, 8, 8, 128, 1, 1, 128, ptr_d_Pool2_Out);\n","\n","        // === L4 (Pool2) ===\n","        MaxPool2D_Backward(ptr_d_Pool2_Out, 8, 8, ptr_Conv2_Out, 16, 16, 2, 2, 2, 128, ptr_d_Conv2_Out);\n","\n","        // === L3 (Conv2) ===\n","        Relu_Backward(ptr_d_Conv2_Out, ptr_Conv2_Out, 16*16*128);\n","        Conv2D_Backward_Kernel(ptr_d_Conv2_Out, 16, 16, ptr_Pool1_Out, 16, 16, 256, 3, 3, 1, 1, 128, autoencoder->d_w2);\n","        Conv2D_Backward_Biases(ptr_d_Conv2_Out, 16, 16, 128, autoencoder->d_b2);\n","        Conv2D_Backward_Input(ptr_d_Conv2_Out, 16, 16, autoencoder->w2, 3, 3, 16, 16, 256, 1, 1, 128, ptr_d_Pool1_Out);\n","\n","        // === L2 (Pool1) ===\n","        MaxPool2D_Backward(ptr_d_Pool1_Out, 16, 16, ptr_Conv1_Out, 32, 32, 2, 2, 2, 256, ptr_d_Conv1_Out);\n","\n","        // === L1 (Conv1) ===\n","        Relu_Backward(ptr_d_Conv1_Out, ptr_Conv1_Out, 32*32*256);\n","        Conv2D_Backward_Kernel(ptr_d_Conv1_Out, 32, 32, ptr_Input, 32, 32, 3, 3, 3, 1, 1, 256, autoencoder->d_w1);\n","        Conv2D_Backward_Biases(ptr_d_Conv1_Out, 32, 32, 256, autoencoder->d_b1);\n","    }\n","}\n","\n","void update_autoencoder_parameters(CPUAutoEncoder* autoencoder) {\n","    // Cập nhật tất cả 5 lớp Conv: W += -learning_rate * dW\n","    int size_W1 = 256 * 3 * 3 * 3;\n","    SGD_Update(autoencoder->w1, autoencoder->d_w1, autoencoder->learning_rate, size_W1);\n","    SGD_Update(autoencoder->b1, autoencoder->d_b1, autoencoder->learning_rate, 256);\n","    int size_W2 = 128 * 256 * 3 * 3;\n","    SGD_Update(autoencoder->w2, autoencoder->d_w2, autoencoder->learning_rate, size_W2);\n","    SGD_Update(autoencoder->b2, autoencoder->d_b2, autoencoder->learning_rate, 128);\n","    int size_W3  = 128 * 128 * 3 * 3;\n","    SGD_Update(autoencoder->w3, autoencoder->d_w3, autoencoder->learning_rate, size_W3);\n","    SGD_Update(autoencoder->b3, autoencoder->d_b3, autoencoder->learning_rate, 128);\n","    int size_W4 = 256 * 128 * 3 * 3;\n","    SGD_Update(autoencoder->w4, autoencoder->d_w4, autoencoder->learning_rate, size_W4);\n","    SGD_Update(autoencoder->b4, autoencoder->d_b4, autoencoder->learning_rate, 256);\n","    int size_W5 = 3 * 256 * 3 * 3;\n","    SGD_Update(autoencoder->w5, autoencoder->d_w5, autoencoder->learning_rate, size_W5);\n","    SGD_Update(autoencoder->b5, autoencoder->d_b5, autoencoder->learning_rate, 3);\n","}\n","\n","void save_weights(CPUAutoEncoder* autoencoder, const char* filename) {\n","    FILE* file = fopen(filename, \"wb\");\n","    if (file == NULL) {\n","        printf(\"Error opening file for writing weights.\\n\");\n","        return;\n","    }\n","    // Lưu trọng số và bias của từng lớp Conv2D\n","    fwrite(autoencoder->w1, sizeof(float), 256*3*3*3, file);\n","    fwrite(autoencoder->b1, sizeof(float), 256, file);\n","    fwrite(autoencoder->w2, sizeof(float), 128*256*3*3, file);\n","    fwrite(autoencoder->b2, sizeof(float), 128, file);\n","    fwrite(autoencoder->w3, sizeof(float), 128*128*3*3, file);\n","    fwrite(autoencoder->b3, sizeof(float), 128, file);\n","    fwrite(autoencoder->w4, sizeof(float), 256*128*3*3, file);\n","    fwrite(autoencoder->b4, sizeof(float), 256, file);\n","    fwrite(autoencoder->w5, sizeof(float), 3*256*3*3, file);\n","    fwrite(autoencoder->b5, sizeof(float), 3, file);\n","    fclose(file);\n","}\n","\n","\n","void cpu_extract_features(CPUAutoEncoder* autoencoder, float* input_data, int num_images, float* features_output) {\n","    // Chỉ có forward pass của encoder\n","    // Kích thước các lớp\n","    int size_input = 32 * 32 * 3;\n","    int size_L1 = 32 * 32 * 256;\n","    int size_L2 = 16 * 16 * 256;\n","    int size_L3 = 16 * 16 * 128;\n","    int size_L4 = 8 * 8 * 128;\n","    for (int i = 0; i < num_images; i++) {\n","        float* ptr_input = input_data + i * size_input;\n","        float* ptr_feature_dst = features_output + i * size_L4;\n","        float* ptr_L1 = autoencoder->conv1_output;\n","        float* ptr_L2 = autoencoder->pool1_output;\n","        float* ptr_L3 = autoencoder->conv2_output;\n","        float* ptr_L4 = autoencoder->pool2_output;\n","        //  ENCODER FORWARD PASS\n","        // L1: Conv1 + ReLU\n","        Conv2D_Forward(ptr_input, 32, 32, 3, autoencoder->w1, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b1, CONV_PADDING, CONV_STRIDE, 256, ptr_L1, 32, 32);\n","        Relu(ptr_L1, size_L1, ptr_L1);\n","        // L2: Pool1\n","        MaxPool2D_Forward(ptr_L1, 32, 32, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 256, ptr_L2, 16, 16);\n","        // L3: Conv2 + ReLU\n","        Conv2D_Forward(ptr_L2, 16, 16, 256, autoencoder->w2, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b2, CONV_PADDING, CONV_STRIDE, 128, ptr_L3, 16, 16);\n","        Relu(ptr_L3, size_L3, ptr_L3);\n","        // L4: Pool2 (Latent Space)\n","        MaxPool2D_Forward(ptr_L3, 16, 16, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 128, ptr_L4, 8, 8);\n","\n","        // Sao chép kết quả vào mảng output tổng\n","        memcpy(ptr_feature_dst, ptr_L4, size_L4 * sizeof(float));\n","    }\n","}\n","\n","void cpu_load_weights(CPUAutoEncoder* autoencoder, const char* filename) {\n","    FILE* file = fopen(filename, \"rb\");\n","    if (file == NULL) {\n","        printf(\"Error opening file for reading weights.\\n\");\n","        return;\n","    }\n","    // Đọc trọng số và bias của từng lớp Conv2D\n","    fread(autoencoder->w1, sizeof(float), 256*3*3*3, file);\n","    fread(autoencoder->b1, sizeof(float), 256, file);\n","    fread(autoencoder->w2, sizeof(float), 128*256*3*3, file);\n","    fread(autoencoder->b2, sizeof(float), 128, file);\n","    fread(autoencoder->w3, sizeof(float), 128*128*3*3, file);\n","    fread(autoencoder->b3, sizeof(float), 128, file);\n","    fread(autoencoder->w4, sizeof(float), 256*128*3*3, file);\n","    fread(autoencoder->b4, sizeof(float), 256, file);\n","    fread(autoencoder->w5, sizeof(float), 3*256*3*3, file);\n","    fread(autoencoder->b5, sizeof(float), 3, file);\n","    fclose(file);\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND_Smnn1d8GS","outputId":"15b5def6-7418-4fed-f03c-b67c97cd7abe","executionInfo":{"status":"ok","timestamp":1766298308484,"user_tz":-420,"elapsed":7,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.h\n"]}],"source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlE9ZxjUeFcy","outputId":"c2b33514-062b-4356-8d63-b8cc8d434ece","executionInfo":{"status":"ok","timestamp":1766298308499,"user_tz":-420,"elapsed":14,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.c\n"]}],"source":["%%writefile load_data.c\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","    // for (int i = 0; i < 2; i++) {\n","    //     printf(\"Label: %d\\n\", data->test_labels[i]);\n","    //     for (int j = 0; j < IMG_SIZE; j++) {\n","    //         printf(\"%f \", data->test_images[i*IMG_SIZE + j]);\n","    //     }\n","    // }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp-p2v8LeM0a","outputId":"f46d53b2-74d6-42ad-82c8-e8bd17b00171","executionInfo":{"status":"ok","timestamp":1766298308511,"user_tz":-420,"elapsed":11,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main.c\n"]}],"source":["%%writefile main.c\n","#include \"load_data.h\"\n","#include \"cpu_autoencoder.h\"\n","#include <time.h>\n","#include <sys/resource.h>\n","#include <stdint.h>\n","\n","void print_memory_usage() {\n","    struct rusage usage;\n","    if (getrusage(RUSAGE_SELF, &usage) == 0) {\n","        double memory_usage_mb = usage.ru_maxrss / 1024.0;\n","        double memory_usage_gb = memory_usage_mb / 1024.0;\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB (%.4f GB)\\n\", memory_usage_mb, memory_usage_gb);\n","    } else {\n","        printf(\"[SYSTEM] Error checking memory usage.\\n\");\n","    }\n","}\n","\n","// void save_summary(double total_time, double final_loss, FILE* file) {\n","//     fprintf(file, \"\\n*** Training Summary ***\\n\");\n","//     fprintf(file, \"Total training time: %.2f seconds.\\n\", total_time);\n","//     fprintf(file, \"Final reconstruction loss: %f\\n\", final_loss);\n","//     fclose(file);\n","// }\n","\n","uint8_t float_to_pixel(float val) {\n","    if (val < 0.0f) val = 0.0f;\n","    if (val > 1.0f) val = 1.0f;\n","    return (uint8_t)(val * 255.0f);\n","}\n","\n","void save_image_pnm(const char* filename, float* planar_data, int width, int height) {\n","    FILE* f = fopen(filename, \"wb\");\n","    if (!f) {\n","        printf(\"Error opening file %s for writing\\n\", filename);\n","        return;\n","    }\n","\n","    // Header PNM: P6 format (binary)\n","    fprintf(f, \"P6\\n%d %d\\n255\\n\", width, height);\n","\n","    int plane_size = width * height;\n","    uint8_t* pixel_buffer = (uint8_t*)malloc(width * height * 3 * sizeof(uint8_t));\n","    if (!pixel_buffer) {\n","        printf(\"Error allocating pixel buffer\\n\");\n","        fclose(f);\n","        return;\n","    }\n","\n","    // Convert float data to uint8_t and interleave RGB channels\n","    for (int h = 0; h < height; h++) {\n","        for (int w = 0; w < width; w++) {\n","            int pixel_idx = (h * width + w) * 3;\n","            int data_idx = h * width + w;\n","\n","            pixel_buffer[pixel_idx] = float_to_pixel(planar_data[data_idx]);\n","            pixel_buffer[pixel_idx + 1] = float_to_pixel(planar_data[plane_size + data_idx]);\n","            pixel_buffer[pixel_idx + 2] = float_to_pixel(planar_data[2 * plane_size + data_idx]);\n","        }\n","    }\n","\n","    // Write all pixel data at once\n","    fwrite(pixel_buffer, 1, width * height * 3, f);\n","    fclose(f);\n","    free(pixel_buffer);\n","}\n","\n","void sample_reconstructions(CPUAutoEncoder* ae, Cifar10* data, int num_samples) {\n","    printf(\"\\n*** Sampling Reconstructed Images ***\\n\");\n","    int batch_size = ae->batch_size;\n","\n","    float* sample_batch = (float*)malloc(batch_size * 32 * 32 * 3 * sizeof(float));\n","    if (!sample_batch) {\n","        printf(\"Error allocating sample batch\\n\");\n","        return;\n","    }\n","\n","    memcpy(ae->batch_input, data->test_images, batch_size * 32 * 32 * 3 * sizeof(float));\n","    forward_autoencoder(ae);\n","    char filename[64];\n","    int img_size = 32 * 32 * 3;\n","\n","    for (int i = 0; i < num_samples; i++) {\n","        // Save original image\n","        snprintf(filename, sizeof(filename), \"sample_%d_original.pnm\", i);\n","        save_image_pnm(filename, ae->batch_input + i * img_size, 32, 32);\n","\n","        // Save reconstructed image\n","        snprintf(filename, sizeof(filename), \"sample_%d_reconstructed.pnm\", i);\n","        save_image_pnm(filename, ae->final_output + i * img_size, 32, 32);\n","\n","        printf(\"Saved pair %d: %s vs %s\\n\", i, \"original\", \"reconstructed\");\n","    }\n","\n","    free(sample_batch);\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","    //Load Data\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","    printf(\"Data loaded and normalized.\\n\");\n","\n","    // Hyperparameters\n","    int train_subset_size = 1000;\n","    int batch_size = 32; // Can be changed\n","    int num_epochs = 1; // Can be changed\n","    int num_batches = train_subset_size / batch_size;\n","    float learning_rate = 0.001;\n","    float* batch_images = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    double total_time = 0.0;\n","    double final_loss = 0.0;\n","    // Initialize AutoEncoder\n","    CPUAutoEncoder autoencoder;\n","    initialize_autoencoder(&autoencoder, batch_size, learning_rate);\n","    printf(\"Autoencoder initialized (batch_size=%d, learning_rate=%f)\\n\", batch_size, learning_rate);\n","    printf(\"Start training...\\n\");\n","    // Training Loop\n","    for (int epoch = 0; epoch < num_epochs; epoch++) {\n","        // Shuffle the training indices at the beginning of each epoch\n","        clock_t start_time = clock();\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","        printf(\"Epoch %d/%d\\n\", epoch + 1, num_epochs);\n","        for (int batch_id = 0; batch_id < num_batches; batch_id++) {\n","            // Get the current batch data from the shuffled array\n","            get_next_batch(&data, batch_size, batch_id, batch_images);\n","            // forward + backward autoencoder on batch_images\n","            // copy into autoencoder input buffer\n","            for (int i = 0; i < batch_size * IMG_SIZE; i++) {\n","                autoencoder.batch_input[i] = batch_images[i];\n","            }\n","            // Training process\n","            forward_autoencoder(&autoencoder);\n","            // Calculate loss for display\n","            float current_loss = MSE(autoencoder.batch_input, autoencoder.final_output, batch_size * IMG_SIZE);\n","            epoch_loss += current_loss;\n","            backward_autoencoder(&autoencoder);\n","            update_autoencoder_parameters(&autoencoder);\n","            if ((batch_id + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\", epoch + 1, batch_id + 1, num_batches, current_loss);\n","            }\n","        }\n","        clock_t end_time = clock();\n","        double epoch_time = (double)(end_time - start_time) / CLOCKS_PER_SEC;\n","        total_time += epoch_time;\n","        printf(\"==> Epoch %d finished. Avg Loss: %f, time: %.2f seconds\\n\", epoch + 1, epoch_loss / num_batches, epoch_time);\n","        if(epoch + 1 == 20) final_loss = epoch_loss / num_batches;\n","    }\n","\n","    printf(\"\\n*** Training Summary ***\\n\");\n","    printf(\"Total training time: %.2f seconds.\\n\", total_time);\n","    printf(\"Final reconstruction loss: %f\\n\", final_loss);\n","    print_memory_usage();\n","\n","    //Save weights after training\n","    save_weights(&autoencoder, \"autoencoder_weights_cpu.bin\");\n","    //Save 5 pairs of original and reconstructed images\n","    //sample_reconstructions(&autoencoder, &data, 5);\n","    // Free memory\n","    free_autoencoder(&autoencoder);\n","    free(batch_images);\n","    free_cifar10(&data);\n","\n","    return 0;\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQ4Ryj3nGEk7"},"outputs":[],"source":["!gcc main.c load_data.c cpu_layers.c cpu_autoencoder.c -o run_model -lm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYKhfq9SGH56","outputId":"09648184-9deb-43a3-92dd-270a9e78552c","executionInfo":{"status":"ok","timestamp":1766298042146,"user_tz":-420,"elapsed":5858310,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CIFAR-10 loaded successfully\n","Data loaded and normalized.\n","Autoencoder initialized (batch_size=32, learning_rate=0.001000)\n","Start training...\n","Epoch 1/1\n","==> Epoch 1 finished. Avg Loss: 0.310920, time: 5819.46 seconds\n","\n","*** Training Summary ***\n","Total training time: 5819.46 seconds.\n","Final reconstruction loss: 0.000000\n","[SYSTEM] Memory Usage: 892.12 MB (0.8712 GB)\n"]}],"source":["!./run_model"]},{"cell_type":"markdown","source":["## 2) Trích xuất đặc trưng:"],"metadata":{"id":"TLbO2O5TAsM5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqXzjsgcebHi","outputId":"697258db-d6cf-4adc-86e0-b3b17e6cc66e","executionInfo":{"status":"ok","timestamp":1766298327751,"user_tz":-420,"elapsed":7,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing extract_svm_features_cpu.c\n"]}],"source":["%%writefile extract_svm_features_cpu.c\n","// # include <cstdio>\n","// # include <cstdlib>\n","// # include <cstring>\n","// # include <cuda_runtime.h>\n","\n","\n","  #include \"load_data.h\"\n","  //#include \"gpu_autoencoder.h\"\n","  #include \"cpu_autoencoder.h\"\n","\n","\n","#define AE_LATENT_DIM 128 * 8 * 8\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    //if (argc < 3) {\n","        //fprintf(stderr,\n","                //\"Usage: %s <path_to_cifar-10-batches-bin> <ae_weights.bin>\\n\",\n","                //argv[0]);\n","        //return 1;\n","    //}\n","    //const char* data_dir    = argv[1];\n","    //const char* weight_file = argv[2];\n","    char* weight_file_cpu = \"autoencoder_weights_cpu.bin\";\n","    float learning_rate = 0.001;\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    //GPUAutoencoder ae;\n","    //gpu_autoencoder_init(&ae, batch_size);\n","    //gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","    CPUAutoEncoder autoencoder;\n","    initialize_autoencoder(&autoencoder, batch_size, learning_rate);\n","    cpu_load_weights(&autoencoder, weight_file_cpu);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = 1000;//TRAIN_NUM; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        //gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","        cpu_extract_features(&autoencoder, h_batch, cur_bs, h_latent);\n","\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = 200;//TEST_NUM; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        //gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","        cpu_extract_features(&autoencoder, h_batch, cur_bs, h_latent);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    //gpu_autoencoder_free(&ae);\n","    free_autoencoder(&autoencoder);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rm8Xbxo14L-Y"},"outputs":[],"source":["!gcc extract_svm_features_cpu.c load_data.c cpu_layers.c cpu_autoencoder.c -o run_cpu -lm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nQIaQ_L4Opv","outputId":"b3ddf87b-420c-4fcf-ed5b-43e97802a63d","executionInfo":{"status":"ok","timestamp":1766299175132,"user_tz":-420,"elapsed":824746,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/16 done\n","[SVM][TRAIN] Batch 2/16 done\n","[SVM][TRAIN] Batch 3/16 done\n","[SVM][TRAIN] Batch 4/16 done\n","[SVM][TRAIN] Batch 5/16 done\n","[SVM][TRAIN] Batch 6/16 done\n","[SVM][TRAIN] Batch 7/16 done\n","[SVM][TRAIN] Batch 8/16 done\n","[SVM][TRAIN] Batch 9/16 done\n","[SVM][TRAIN] Batch 10/16 done\n","[SVM][TRAIN] Batch 11/16 done\n","[SVM][TRAIN] Batch 12/16 done\n","[SVM][TRAIN] Batch 13/16 done\n","[SVM][TRAIN] Batch 14/16 done\n","[SVM][TRAIN] Batch 15/16 done\n","[SVM][TRAIN] Batch 16/16 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/4 done\n","[SVM][TEST] Batch 2/4 done\n","[SVM][TEST] Batch 3/4 done\n","[SVM][TEST] Batch 4/4 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}],"source":["!./run_cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsQdXgn6xlDg","outputId":"7969689f-3270-44d6-914b-a5840d94b168","executionInfo":{"status":"ok","timestamp":1766299215966,"user_tz":-420,"elapsed":351,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["6 1:0.0982742 2:0.11629 3:0.11875 4:0.118918 5:0.11798 6:0.134471 7:0.130166 8:0.129019 9:0.101778 10:0.100332 11:0.107763 12:0.103453 13:0.130008 14:0.11007 15:0.120576 16:0.119509 17:0.129474 18:0.114856 19:0.141686 20:0.138824 21:0.141074 22:0.132544 23:0.120064 24:0.119202 25:0.13277 26:0.121556 27:0.154679 28:0.193619 29:0.18089 30:0.183575 31:0.15995 32:0.117418 33:0.124051 34:0.134411 35:0.218237 36:0.215222 37:0.187687 38:0.172826 39:0.212967 40:0.127942 41:0.129726 42:0.13831 43:0.194592 44:0.180277 45:0.141857 46:0.144128 47:0.166929 48:0.137269 49:0.130483 50:0.116519 51:0.145479 52:0.142247 53:0.126857 54:0.121549 55:0.116741 56:0.156044 57:0.15193 58:0.110514 59:0.131397 60:0.131495 61:0.121088 62:0.119606 63:0.1256 64:0.174724 65:0 66:0 67:0 68:0 69:0 70:0 71:0 72:0 73:0 74:0 75:0 76:0 77:0 78:0 79:0 80:0 81:0 82:0 83:0 84:0 85:0 86:0 87:0 88:0 89:0 90:0 91:0 92:0 93:0 94:0 95:0 96:0 97:0 98:0 99:0 100:0 101:0 102:0 103:0 104:0 105:0 106:0 107:0 108:0 109:0 110:0 111:0 112:0 113:0 114:0 115:0 116:0 117:0 118:0 119:0 120:0 121:0 122:0 123:0 124:0 125:0 126:0 127:0 128:0 129:0.0622158 130:0.0554071 131:0.0627861 132:0.0609482 133:0.0599579 134:0.0549296 135:0.0588261 136:0.0674402 137:0.0558927 138:0.05363 139:0.0639721 140:0.0562743 141:0.0652124 142:0.0658497 143:0.0587019 144:0.0636357 145:0.049391 146:0.0530797 147:0.0629512 148:0.0547229 149:0.0675499 150:0.0934409 151:0.0704608 152:0.0566014 153:0.0464768 154:0.0555954 155:0.0471542 156:0.0470972 157:0.065821 158:0.0630842 159:0.0946231 160:0.0575595 161:0.0455427 162:0.0387149 163:0.104742 164:0.0538174 165:0.0659643 166:0.0260818 167:0.11859 168:0.0528281 169:0.0508987 170:0.0352525 171:0.0529195 172:0.0778188 173:0.0609648 174:0.0553033 175:0.0825109 176:0.0557341 177:0.0656776 178:0.0575725 179:0.0481994 180:0.0460929 181:0.0450693 182:0.0521063 183:0.0497667 184:0.0818671 185:0.0517007 186:0.0441358 187:0.0474976 188:0.0460198 189:0.0458587 190:0.0477063 191:0.0498307 192:0.0630675 193:0.0897293 194:0.0555579 195:0.0592751 196:0.055782 197:0.0605901 198:0.0632842 199:0.0652847 200:0.0674114 201:0.0807848 202:0.0429081 203:0.0398833 204:0.0376006 205:0.0394069 206:0.0325496 207:0.0385176 208:0.0520007 209:0.0883019 210:0.0480474 211:0.0539973 212:0.0658437 213:0.0512027 214:0.0618227 215:0.0385135 216:0.056963 217:0.0843163 218:0.0711384 219:0.0555047 220:0.0868206 221:0.08575 222:0.0929939 223:0.0568203 224:0.0516552 225:0.0776959 226:0.098087 227:0.0829635 228:0.116344 229:0.0860158 230:0.105876 231:0.0688865 232:0.0535302 233:0.0742053 234:0.11158 235:0.0838519 236:0.0682997 237:0.0639057 238:0.0709368 239:0.0536759 240:0.070607 241:0.086054 242:0.0545728 243:0.0742525 244:0.0546578 245:0.0546633 246:0.0556257 247:0.0441803 248:0.0441989 249:0.0811382 250:0.0479943 251:0.0498482 252:0.0509516 253:0.0513347 254:0.0366259 255:0.0528408 256:0.0272883 257:0.0625362 258:0.0470905 259:0.0537197 260:0.0441606 261:0.0408166 262:0.0495563 263:0.0483743 264:0.0472356 265:0.0549019 266:0.0438223 267:0.0471041 268:0.0441395 269:0.0509601 270:0.0557577 271:0.0555934 272:0.0495857 273:0.0503018 274:0.0510904 275:0.0530602 276:0.0652981 277:0.062238 278:0.069767 279:0.0669909 280:0.0428568 281:0.0535572 282:0.0601031 283:0.062034 284:0.0569283 285:0.0768939 286:0.0831637 287:0.0893497 288:0.0448312 289:0.0494175 290:0.0684411 291:0.098809 292:0.0926709 293:0.125951 294:0.10727 295:0.11496 296:0.0391617 297:0.0575335 298:0.0668335 299:0.141281 300:0.102212 301:0.0931291 302:0.0735938 303:0.0913367 304:0.0394409 305:0.0532162 306:0.0592662 307:0.0681999 308:0.0787582 309:0.0710081 310:0.0658283 311:0.0491593 312:0.0618271 313:0.0324863 314:0.0411276 315:0.0501991 316:0.0492678 317:0.0439843 318:0.0446621 319:0.0493084 320:0.0811485 321:0.00016569 322:0 323:0 324:0 325:0 326:0 327:0 328:0 329:0.00671734 330:0 331:0 332:0 333:0 334:0 335:0 336:0 337:0.0213384 338:0 339:0 340:0 341:0.00953727 342:0 343:0 344:0 345:0.0251227 346:0.0062994 347:0.00495296 348:0.00921905 349:0 350:0 351:0 352:0 353:0.0227322 354:0.0105018 355:0 356:0 357:0 358:0 359:0 360:0 361:0.0148437 362:0.0017486 363:0 364:0 365:0 366:0 367:0 368:0.009223 369:0.0312677 370:0 371:0 372:0 373:0 374:0 375:0 376:0 377:0.0387344 378:0 379:0 380:0 381:0 382:0 383:0 384:0 385:0.0585881 386:0.05174 387:0.0390859 388:0.0387123 389:0.0392233 390:0.0409971 391:0.043778 392:0.0391752 393:0.0486952 394:0.0174364 395:0.0140837 396:0.030256 397:0.0141714 398:0.0284675 399:0.0254713 400:0.0122668 401:0.0533084 402:0.0144879 403:0.0178454 404:0.0414226 405:0.0245555 406:0.0151006 407:0.0222047 408:0.0202777 409:0.0521299 410:0.0589631 411:0.0592545 412:0.054928 413:0.0244412 414:0.0254488 415:0.00800731 416:0.013326 417:0.0485281 418:0.0662645 419:0.0314307 420:0.0291167 421:0.0506093 422:0.0524134 423:0.0123397 424:0.00111585 425:0.0477451 426:0.0471746 427:0.0319471 428:0.0099746 429:0.0280021 430:0.0306927 431:0.0245871 432:0.0108377 433:0.0333358 434:0.00941109 435:0.0213766 436:0.022288 437:0.0188414 438:0.0128695 439:0.0116544 440:0.0213458 441:0.0628591 442:0.0224014 443:0.0380611 444:0.0361168 445:0.0334268 446:0.0298071 447:0.0426936 448:0.0163096 449:0 450:0 451:0 452:0 453:0 454:0 455:0 456:0 457:0 458:0 459:0 460:0 461:0 462:0 463:0 464:0 465:0 466:0 467:0 468:0 469:0 470:0 471:0 472:0 473:0 474:0 475:0 476:0 477:0.00317923 478:0.0158829 479:0 480:0 481:0 482:0 483:0 484:0.00872699 485:0.0252388 486:0.0355158 487:0.0137844 488:0 489:0 490:0 491:0 492:0 493:0 494:0.0118422 495:0.0122964 496:0 497:0 498:0 499:0 500:0.00380374 501:0 502:0 503:0 504:0.0148385 505:0.0326328 506:0.0503754 507:0.0347729 508:0.0250476 509:0.0250091 510:0.0259788 511:0.0226214 512:0.0122589 513:0.0604649 514:0.0780372 515:0.0903198 516:0.0824025 517:0.0898329 518:0.0935922 519:0.100976 520:0.0838135 521:0.0986097 522:0.0990844 523:0.0947543 524:0.0933623 525:0.109255 526:0.0936747 527:0.110011 528:0.115342 529:0.117448 530:0.104524 531:0.106972 532:0.131413 533:0.138254 534:0.117778 535:0.126915 536:0.104856 537:0.113174 538:0.102755 539:0.123723 540:0.152867 541:0.183141 542:0.176349 543:0.168707 544:0.12651 545:0.113334 546:0.11941 547:0.128636 548:0.157327 549:0.150562 550:0.170678 551:0.173297 552:0.132103 553:0.114288 554:0.0990656 555:0.165242 556:0.156276 557:0.149633 558:0.105171 559:0.16534 560:0.148678 561:0.157994 562:0.131342 563:0.105858 564:0.127639 565:0.130767 566:0.127279 567:0.136029 568:0.136226 569:0.172997 570:0.155247 571:0.136518 572:0.118178 573:0.121134 574:0.116772 575:0.0957756 576:0.102992 577:0.0141929 578:0 579:0.000535534 580:0.000793839 581:0.00282451 582:0 583:0 584:0.00265807 585:6.58506e-05 586:0.00507447 587:0.0126845 588:0.0067014 589:0.00307991 590:0.00649184 591:0.00900631 592:0.0136083 593:0.00452213 594:0.00537337 595:0.00154424 596:0.0153487 597:0.00894005 598:0 599:0.0130943 600:0.0063454 601:0.00153309 602:0.0067546 603:0.00813251 604:0 605:0 606:0 607:0.00484036 608:0 609:0.0142252 610:0.0140425 611:0 612:0 613:0.00135166 614:0.00584739 615:0.0328683 616:0.00313244 617:0.0135987 618:0.00115284 619:0 620:0 621:0.0106874 622:0 623:0.0353966 624:0 625:0 626:0 627:0 628:0.00393731 629:0.0101844 630:0.0065771 631:0.00332305 632:0 633:0.0484385 634:0.0467582 635:0.0490573 636:0.0460076 637:0.0415247 638:0.0504676 639:0.043603 640:0.0660113 641:0.127921 642:0.1593 643:0.178954 644:0.175375 645:0.170988 646:0.172249 647:0.187259 648:0.183035 649:0.058193 650:0.0752261 651:0.0776675 652:0.0919478 653:0.0920918 654:0.0778177 655:0.08285 656:0.124638 657:0.0778416 658:0.0840601 659:0.104408 660:0.115508 661:0.112538 662:0.105303 663:0.091394 664:0.117756 665:0.0733612 666:0.120602 667:0.102723 668:0.134119 669:0.123002 670:0.136793 671:0.121476 672:0.11336 673:0.0719502 674:0.137584 675:0.132088 676:0.148404 677:0.110602 678:0.1355 679:0.122419 680:0.129092 681:0.0726414 682:0.080801 683:0.116955 684:0.105679 685:0.0845674 686:0.0743639 687:0.0911564 688:0.135127 689:0.0957458 690:0.0881862 691:0.0754156 692:0.0818311 693:0.100566 694:0.0970825 695:0.113955 696:0.149497 697:0.089545 698:0.0995123 699:0.0751127 700:0.0625153 701:0.0636459 702:0.0784984 703:0.0789611 704:0.143781 705:0 706:0 707:0 708:0 709:0 710:0 711:0 712:0 713:0 714:0 715:0 716:0 717:0 718:0 719:0 720:0 721:0 722:0 723:0 724:0 725:0 726:0 727:0 728:0 729:0 730:0 731:0 732:0 733:0 734:0 735:0 736:0 737:0 738:0 739:0 740:0 741:0 742:0 743:0 744:0 745:0 746:0 747:0 748:0 749:0 750:0 751:0 752:0 753:0 754:0 755:0 756:0 757:0 758:0 759:0 760:0 761:0.0604765 762:0 763:0 764:0 765:0 766:0 767:0 768:0.00743701 769:0.0637952 770:0.10388 771:0.09365 772:0.0906714 773:0.0875757 774:0.0984138 775:0.108707 776:0.0854134 777:0.103841 778:0.101391 779:0.0855089 780:0.0952339 781:0.0962524 782:0.0930504 783:0.104303 784:0.0895104 785:0.119825 786:0.102568 787:0.112772 788:0.154213 789:0.172049 790:0.131724 791:0.118569 792:0.0991963 793:0.127612 794:0.111985 795:0.151898 796:0.188378 797:0.170745 798:0.168288 799:0.129392 800:0.102378 801:0.122788 802:0.108646 803:0.205325 804:0.205718 805:0.188741 806:0.192319 807:0.150527 808:0.0913393 809:0.116452 810:0.18955 811:0.209303 812:0.201972 813:0.141538 814:0.162611 815:0.119152 816:0.11626 817:0.140317 818:0.125497 819:0.159026 820:0.142166 821:0.111491 822:0.110135 823:0.125212 824:0.0964453 825:0.118886 826:0.102516 827:0.109135 828:0.107793 829:0.0969594 830:0.0987392 831:0.1138 832:0.0921303 833:0.184735 834:0.186837 835:0.172522 836:0.170596 837:0.175102 838:0.184479 839:0.192185 840:0.171458 841:0.196339 842:0.171286 843:0.162242 844:0.171205 845:0.19326 846:0.183474 847:0.175221 848:0.149516 849:0.202577 850:0.175904 851:0.212792 852:0.247534 853:0.251135 854:0.232114 855:0.210261 856:0.163712 857:0.197559 858:0.193366 859:0.269026 860:0.313882 861:0.302062 862:0.281234 863:0.251804 864:0.161312 865:0.198421 866:0.290301 867:0.31965 868:0.314828 869:0.292615 870:0.308805 871:0.261211 872:0.171154 873:0.20324 874:0.258454 875:0.301597 876:0.265003 877:0.223758 878:0.216594 879:0.208554 880:0.207452 881:0.224469 882:0.206657 883:0.215589 884:0.215773 885:0.194596 886:0.184735 887:0.201299 888:0.216414 889:0.227957 890:0.198486 891:0.174958 892:0.176148 893:0.169813 894:0.1623 895:0.189638 896:0.191871 897:0.074807 898:0.131876 899:0.124522 900:0.124482 901:0.125085 902:0.136862 903:0.136981 904:0.131068 905:0.0811134 906:0.0794043 907:0.0712439 908:0.0450561 909:0.108714 910:0.0690851 911:0.0917185 912:0.10265 913:0.103528 914:0.0846087 915:0.11574 916:0.174006 917:0.183861 918:0.155842 919:0.0932291 920:0.0882368 921:0.105994 922:0.0930682 923:0.144223 924:0.199832 925:0.218782 926:0.200618 927:0.190416 928:0.0861823 929:0.0926764 930:0.137606 931:0.199894 932:0.169339 933:0.15822 934:0.169275 935:0.180013 936:0.09112 937:0.105431 938:0.0848709 939:0.148274 940:0.169352 941:0.106366 942:0.0743269 943:0.10217 944:0.115629 945:0.119751 946:0.0885811 947:0.0911741 948:0.110329 949:0.101136 950:0.0953574 951:0.102251 952:0.135285 953:0.143196 954:0.144454 955:0.107617 956:0.105235 957:0.0943445 958:0.0911294 959:0.0634355 960:0.116324 961:0 962:0 963:0 964:0 965:0 966:0 967:0 968:0 969:0 970:0 971:0 972:0 973:0 974:0 975:0 976:0 977:0 978:0 979:0 980:0 981:0 982:0 983:0 984:0 985:0 986:0 987:0 988:0 989:0 990:0 991:0 992:0 993:0 994:0 995:0 996:0 997:0 998:0 999:0 1000:0 1001:0 1002:0 1003:0 1004:0 1005:0 1006:0 1007:0 1008:0 1009:0 1010:0 1011:0 1012:0 1013:0 1014:0 1015:0 1016:0 1017:0 1018:0 1019:0 1020:0 1021:0 1022:0 1023:0 1024:0 1025:0.0433026 1026:0.0483941 1027:0.0173851 1028:0.0154376 1029:0.0174367 1030:0.0254491 1031:0.0252549 1032:0.020379 1033:0.0387138 1034:0.0306562 1035:0.0266443 1036:0.0248357 1037:0.0283375 1038:0.0477283 1039:0.0254859 1040:0.0191575 1041:0.0631626 1042:0.0304623 1043:0.0278035 1044:0.0427326 1045:0.0411739 1046:0.0398074 1047:0.0450677 1048:0.022146 1049:0.0684892 1050:0.0264703 1051:0.0415775 1052:0.0349916 1053:0.0178804 1054:0.0301345 1055:0.0395738 1056:0.0217375 1057:0.0641921 1058:0.0520195 1059:0.0573297 1060:0.0728767 1061:0.054124 1062:0.0519249 1063:0.00734331 1064:0.00865212 1065:0.0541666 1066:0.0798961 1067:0.0391749 1068:0.0264287 1069:0.0308014 1070:0.0704222 1071:0.000928605 1072:0.00936963 1073:0.0365586 1074:0.0324041 1075:0.0435329 1076:0.0387581 1077:0.0202298 1078:0.0252176 1079:0.0212769 1080:0.0247794 1081:0.0581131 1082:0.00875699 1083:0.0252459 1084:0.0351095 1085:0.0354363 1086:0.0248584 1087:0.028655 1088:0.0295256 1089:0.0968118 1090:0.139876 1091:0.143788 1092:0.149941 1093:0.13922 1094:0.141829 1095:0.153891 1096:0.155252 1097:0.14311 1098:0.13796 1099:0.138613 1100:0.135448 1101:0.173317 1102:0.132185 1103:0.14246 1104:0.162984 1105:0.169032 1106:0.142387 1107:0.147683 1108:0.163989 1109:0.19765 1110:0.187982 1111:0.159173 1112:0.174839 1113:0.163038 1114:0.15692 1115:0.191539 1116:0.197884 1117:0.224334 1118:0.211307 1119:0.247951 1120:0.184271 1121:0.173439 1122:0.154247 1123:0.198969 1124:0.219342 1125:0.206756 1126:0.209454 1127:0.23572 1128:0.20579 1129:0.163374 1130:0.157895 1131:0.184194 1132:0.203269 1133:0.171947 1134:0.185352 1135:0.18373 1136:0.235041 1137:0.214103 1138:0.170426 1139:0.161803 1140:0.171379 1141:0.155603 1142:0.148832 1143:0.154151 1144:0.220628 1145:0.235107 1146:0.197043 1147:0.1757 1148:0.17098 1149:0.160802 1150:0.166527 1151:0.158545 1152:0.191873 1153:0.00754893 1154:0 1155:0 1156:0 1157:0 1158:0 1159:0 1160:0 1161:0 1162:0 1163:0 1164:0 1165:0 1166:0 1167:0 1168:0 1169:0 1170:0 1171:0 1172:0 1173:0 1174:0 1175:0 1176:0 1177:0 1178:0 1179:0 1180:0 1181:0 1182:0 1183:0 1184:0 1185:0 1186:0 1187:0 1188:0 1189:0 1190:0 1191:0 1192:0 1193:0 1194:0 1195:0 1196:0 1197:0 1198:0 1199:0 1200:0 1201:0 1202:0 1203:0 1204:0 1205:0 1206:0 1207:0 1208:0 1209:0 1210:0 1211:0 1212:0 1213:0 1214:0 1215:0 1216:0 1217:0.0796963 1218:0.0880072 1219:0.0905215 1220:0.0912152 1221:0.0944728 1222:0.0966028 1223:0.104564 1224:0.0868055 1225:0.0904003 1226:0.0768535 1227:0.0624143 1228:0.0600127 1229:0.0520677 1230:0.0753404 1231:0.0744002 1232:0.0560969 1233:0.10257 1234:0.084708 1235:0.070113 1236:0.130463 1237:0.126765 1238:0.0976154 1239:0.0865094 1240:0.0648588 1241:0.0933817 1242:0.074674 1243:0.14554 1244:0.172736 1245:0.204661 1246:0.178912 1247:0.157853 1248:0.069553 1249:0.0987737 1250:0.12709 1251:0.222803 1252:0.176926 1253:0.165629 1254:0.153439 1255:0.162806 1256:0.0537171 1257:0.0868629 1258:0.119012 1259:0.169581 1260:0.148 1261:0.0977962 1262:0.119854 1263:0.119813 1264:0.0449467 1265:0.0934075 1266:0.0827871 1267:0.0970338 1268:0.0998916 1269:0.076947 1270:0.09773 1271:0.0987305 1272:0.0755154 1273:0.101848 1274:0.0786012 1275:0.0681081 1276:0.0746629 1277:0.071911 1278:0.069367 1279:0.0682386 1280:0.052338 1281:0.0181774 1282:0.00606458 1283:0.0253001 1284:0.0215762 1285:0.0198376 1286:0.0135054 1287:0 1288:0.0531811 1289:0.00546807 1290:0.0158331 1291:0.0164725 1292:0.0274143 1293:0.0122187 1294:0.00963056 1295:0.0202221 1296:0.0487312 1297:0.0189323 1298:0.0162727 1299:0.0353649 1300:0.0254605 1301:0 1302:0.00121735 1303:0.0109637 1304:0.0455275 1305:0.0121448 1306:0.0343047 1307:0.0135916 1308:0 1309:0 1310:0.00270368 1311:0 1312:0.0467404 1313:0.012709 1314:0.0457547 1315:0 1316:0 1317:0 1318:0 1319:0 1320:0.0491875 1321:0.0142449 1322:0 1323:0 1324:0 1325:0.000840668 1326:0 1327:0.0229431 1328:0.0411352 1329:0.0113113 1330:0.00621747 1331:0 1332:0 1333:0.0178747 1334:0.0197126 1335:0.0300673 1336:0.0303172 1337:0.0356665 1338:0 1339:0 1340:0 1341:0 1342:8.7725e-05 1343:0.00302421 1344:0.0258605 1345:0 1346:0 1347:0 1348:0 1349:0 1350:0 1351:0 1352:0 1353:0 1354:0 1355:0 1356:0 1357:0 1358:0 1359:0 1360:0 1361:0 1362:0 1363:0 1364:0 1365:0 1366:0 1367:0 1368:0 1369:0 1370:0 1371:0 1372:0 1373:0 1374:0 1375:0 1376:0 1377:0 1378:0 1379:0 1380:0 1381:0 1382:0 1383:0 1384:0 1385:0 1386:0 1387:0 1388:0 1389:0 1390:0 1391:0 1392:0 1393:0 1394:0 1395:0 1396:0 1397:0 1398:0 1399:0 1400:0 1401:0 1402:0 1403:0 1404:0 1405:0 1406:0 1407:0 1408:0 1409:0.0333388 1410:0.0039587 1411:0 1412:0 1413:0 1414:0 1415:0 1416:0 1417:0.0139158 1418:0 1419:0 1420:0 1421:0 1422:0 1423:0 1424:0 1425:0 1426:0 1427:0 1428:0 1429:0 1430:0 1431:0 1432:0 1433:0 1434:0 1435:0 1436:0 1437:0 1438:0 1439:0 1440:0 1441:0 1442:0 1443:0 1444:0 1445:0 1446:0 1447:0 1448:0 1449:0 1450:0 1451:0 1452:0 1453:0 1454:0 1455:0 1456:0 1457:0 1458:0 1459:0 1460:0 1461:0 1462:0 1463:0 1464:0 1465:0 1466:0 1467:0 1468:0 1469:0 1470:0 1471:0 1472:0 1473:0.00516982 1474:0 1475:0 1476:0 1477:0 1478:0 1479:0 1480:0 1481:0.00676186 1482:0 1483:0 1484:0 1485:0 1486:0 1487:0 1488:0 1489:0.0175926 1490:0 1491:0 1492:0 1493:0 1494:0 1495:0 1496:0 1497:0.0154965 1498:0 1499:0 1500:0 1501:0 1502:0 1503:0 1504:0 1505:0.0123847 1506:0 1507:0 1508:0 1509:0 1510:0 1511:0 1512:0 1513:0.0119591 1514:0 1515:0 1516:0 1517:0 1518:0 1519:0 1520:0 1521:0.0133792 1522:0 1523:0 1524:0 1525:0 1526:0 1527:0 1528:0 1529:0.030904 1530:0 1531:0 1532:0 1533:0 1534:0 1535:0 1536:0 1537:0 1538:0 1539:0 1540:0 1541:0 1542:0 1543:0 1544:0 1545:0 1546:0 1547:0 1548:0 1549:0 1550:0 1551:0 1552:0 1553:0 1554:0 1555:0 1556:0 1557:0 1558:0 1559:0 1560:0 1561:0 1562:0 1563:0 1564:0 1565:0 1566:0 1567:0 1568:0 1569:0 1570:0 1571:0 1572:0 1573:0 1574:0 1575:0 1576:0 1577:0 1578:0 1579:0 1580:0 1581:0 1582:0 1583:0 1584:0 1585:0 1586:0 1587:0 1588:0 1589:0 1590:0 1591:0 1592:0 1593:0 1594:0 1595:0 1596:0 1597:0 1598:0 1599:0 1600:0 1601:0 1602:0 1603:0 1604:0 1605:0 1606:0 1607:0 1608:0.00671003 1609:0.0129426 1610:0 1611:0 1612:0 1613:0 1614:0 1615:0 1616:0 1617:0.0380535 1618:0 1619:0 1620:0 1621:0.00156875 1622:0 1623:0 1624:0.0177431 1625:0.0384991 1626:0 1627:0 1628:0 1629:0 1630:0 1631:0 1632:0.0174279 1633:0.0375953 1634:0 1635:0.00447913 1636:0 1637:0 1638:0 1639:0 1640:0.0225602 1641:0.0302345 1642:0 1643:0 1644:0 1645:0 1646:0 1647:0 1648:0.0543586 1649:0.0761436 1650:0 1651:0 1652:0 1653:0 1654:0 1655:0 1656:0.0587825 1657:0.0833235 1658:0 1659:0 1660:0 1661:0 1662:0 1663:0 1664:0.0185124 1665:0.112868 1666:0.159319 1667:0.15629 1668:0.164114 1669:0.166026 1670:0.160365 1671:0.16812 1672:0.179843 1673:0.156326 1674:0.155274 1675:0.153398 1676:0.163574 1677:0.177476 1678:0.166049 1679:0.165539 1680:0.155464 1681:0.181439 1682:0.169313 1683:0.170402 1684:0.191555 1685:0.196103 1686:0.179471 1687:0.171458 1688:0.16077 1689:0.191061 1690:0.180468 1691:0.174895 1692:0.249727 1693:0.265091 1694:0.255896 1695:0.21138 1696:0.181395 1697:0.205715 1698:0.180761 1699:0.256094 1700:0.282497 1701:0.278431 1702:0.264019 1703:0.243918 1704:0.193902 1705:0.182758 1706:0.245909 1707:0.275085 1708:0.237151 1709:0.189482 1710:0.245405 1711:0.217507 1712:0.183121 1713:0.232368 1714:0.209589 1715:0.215372 1716:0.220168 1717:0.192973 1718:0.177883 1719:0.194961 1720:0.217547 1721:0.251026 1722:0.236114 1723:0.219486 1724:0.200749 1725:0.188923 1726:0.191424 1727:0.217406 1728:0.220559 1729:0.0880087 1730:0.111759 1731:0.104238 1732:0.113958 1733:0.105637 1734:0.111631 1735:0.116709 1736:0.125129 1737:0.100174 1738:0.103434 1739:0.103677 1740:0.11043 1741:0.109642 1742:0.117364 1743:0.107084 1744:0.102477 1745:0.112791 1746:0.116409 1747:0.109117 1748:0.103401 1749:0.117192 1750:0.101623 1751:0.108357 1752:0.12119 1753:0.118467 1754:0.107449 1755:0.144127 1756:0.133553 1757:0.14059 1758:0.136257 1759:0.116728 1760:0.12092 1761:0.115775 1762:0.135205 1763:0.14487 1764:0.151586 1765:0.145778 1766:0.155635 1767:0.126066 1768:0.130684 1769:0.113968 1770:0.140856 1771:0.142464 1772:0.131687 1773:0.140631 1774:0.150004 1775:0.13309 1776:0.144234 1777:0.13566 1778:0.122197 1779:0.129067 1780:0.126567 1781:0.114894 1782:0.118172 1783:0.12232 1784:0.160209 1785:0.181793 1786:0.167019 1787:0.145227 1788:0.12677 1789:0.126912 1790:0.130683 1791:0.145254 1792:0.146583 1793:0.00959802 1794:0.000890955 1795:0 1796:0 1797:0 1798:0 1799:0 1800:0 1801:0.0536164 1802:0 1803:0 1804:0 1805:0.0176676 1806:0.00157505 1807:0 1808:0 1809:0.0764599 1810:0.00146144 1811:0.0129234 1812:0.0855357 1813:0.0648952 1814:0.0427514 1815:0.0146227 1816:0 1817:0.0681518 1818:0.03108 1819:0.131265 1820:0.136294 1821:0.103035 1822:0.0958583 1823:0.0409464 1824:0.00462772 1825:0.0555282 1826:0.136059 1827:0.172371 1828:0.101554 1829:0.0406139 1830:0.0947784 1831:0.0131617 1832:0 1833:0.0615653 1834:0.0790504 1835:0.0527778 1836:0.043366 1837:0.0106787 1838:0.0378828 1839:0 1840:0.0397471 1841:0.0925428 1842:0.0207331 1843:0.0160837 1844:0.0066811 1845:0 1846:0 1847:0.0245086 1848:0.0731964 1849:0.100829 1850:0.00846276 1851:0.0130887 1852:0.0139831 1853:0.0144994 1854:0 1855:0.0234582 1856:0.0431591 1857:0.056275 1858:0.0686962 1859:0.0649161 1860:0.0693214 1861:0.0671095 1862:0.0773167 1863:0.0726688 1864:0.0626813 1865:0.0840817 1866:0.0743454 1867:0.0732917 1868:0.0847226 1869:0.0825355 1870:0.0700446 1871:0.0855861 1872:0.0610809 1873:0.0923628 1874:0.0821416 1875:0.090198 1876:0.114558 1877:0.100655 1878:0.105202 1879:0.0734879 1880:0.074411 1881:0.104742 1882:0.0844511 1883:0.107296 1884:0.144137 1885:0.141392 1886:0.134331 1887:0.113746 1888:0.0845088 1889:0.103961 1890:0.0974423 1891:0.104957 1892:0.152977 1893:0.153676 1894:0.139242 1895:0.112671 1896:0.0948282 1897:0.0943839 1898:0.118332 1899:0.13645 1900:0.110984 1901:0.0902586 1902:0.113872 1903:0.105843 1904:0.0999359 1905:0.117707 1906:0.0944025 1907:0.101478 1908:0.110058 1909:0.103009 1910:0.0868467 1911:0.08344 1912:0.0988499 1913:0.129668 1914:0.0957739 1915:0.0724704 1916:0.0778934 1917:0.0822178 1918:0.0853704 1919:0.092509 1920:0.0821474 1921:0.0601844 1922:0.0745053 1923:0.0669565 1924:0.0708834 1925:0.0715098 1926:0.080483 1927:0.0822026 1928:0.0966298 1929:0.0715059 1930:0.0680959 1931:0.0759403 1932:0.0638638 1933:0.0766237 1934:0.0684381 1935:0.0742125 1936:0.086033 1937:0.081496 1938:0.07528 1939:0.0696771 1940:0.108259 1941:0.116899 1942:0.115438 1943:0.0846987 1944:0.0865805 1945:0.0748602 1946:0.0746697 1947:0.166714 1948:0.126948 1949:0.152213 1950:0.156455 1951:0.152095 1952:0.0801108 1953:0.0670019 1954:0.0686326 1955:0.171187 1956:0.112532 1957:0.123041 1958:0.109458 1959:0.162625 1960:0.0753521 1961:0.0668346 1962:0.102059 1963:0.118676 1964:0.109645 1965:0.079446 1966:0.0891596 1967:0.0894199 1968:0.0721087 1969:0.0764263 1970:0.0703822 1971:0.083536 1972:0.0964008 1973:0.0701281 1974:0.0811327 1975:0.0754612 1976:0.123121 1977:0.095526 1978:0.0924942 1979:0.082265 1980:0.0721981 1981:0.0812298 1982:0.0846038 1983:0.092675 1984:0.132455 1985:0.0364567 1986:0.0612506 1987:0.0625974 1988:0.0616904 1989:0.0571227 1990:0.0619774 1991:0.0759036 1992:0.0781967 1993:0.0384438 1994:0.0559794 1995:0.0635738 1996:0.0457794 1997:0.0526201 1998:0.0459864 1999:0.0529126 2000:0.0543892 2001:0.0647196 2002:0.0674179 2003:0.0488833 2004:0.023757 2005:0.0573002 2006:0.056212 2007:0.069112 2008:0.0702858 2009:0.0687255 2010:0.0621206 2011:0.0422253 2012:0.0667578 2013:0.0937409 2014:0.0988867 2015:0.102205 2016:0.0657125 2017:0.0521911 2018:0.0450298 2019:0.106074 2020:0.108891 2021:0.125215 2022:0.115716 2023:0.131533 2024:0.070284 2025:0.0428655 2026:0.0975983 2027:0.127794 2028:0.0760517 2029:0.0568306 2030:0.126677 2031:0.10865 2032:0.0713693 2033:0.0462494 2034:0.0673968 2035:0.0854281 2036:0.109153 2037:0.066647 2038:0.0634593 2039:0.0637788 2040:0.0713603 2041:0.0496369 2042:0.0530476 2043:0.0466741 2044:0.0456566 2045:0.0524762 2046:0.0478209 2047:0.0551683 2048:0.0610862 2049:0.0805815 2050:0.097479 2051:0.0926544 2052:0.0928172 2053:0.0970664 2054:0.097166 2055:0.102097 2056:0.0760652 2057:0.102904 2058:0.112923 2059:0.100964 2060:0.123501 2061:0.124455 2062:0.12876 2063:0.109987 2064:0.0986747 2065:0.119106 2066:0.12912 2067:0.139557 2068:0.139374 2069:0.15124 2070:0.141639 2071:0.120727 2072:0.10259 2073:0.118259 2074:0.168103 2075:0.181048 2076:0.217341 2077:0.206782 2078:0.207934 2079:0.129866 2080:0.104986 2081:0.140317 2082:0.171743 2083:0.189701 2084:0.1938 2085:0.201295 2086:0.234166 2087:0.168193 2088:0.140728 2089:0.127501 2090:0.1854 2091:0.174212 2092:0.166132 2093:0.150329 2094:0.176919 2095:0.1553 2096:0.140308 2097:0.146721 2098:0.141557 2099:0.155473 2100:0.153283 2101:0.129256 2102:0.13436 2103:0.144148 2104:0.109161 2105:0.162901 2106:0.147762 2107:0.137263 2108:0.127004 2109:0.122091 2110:0.119512 2111:0.162683 2112:0.123761 2113:0 2114:0 2115:0.0104201 2116:0.00460486 2117:0.00631662 2118:0.00316164 2119:0 2120:0 2121:0.000235484 2122:0.00240581 2123:0.00874682 2124:0.000736734 2125:0.0097292 2126:0.00583649 2127:0 2128:0 2129:0.0116951 2130:0.00628263 2131:0 2132:0.0198193 2133:0.00616992 2134:0.00910286 2135:0.00121389 2136:0.00153156 2137:0.00639612 2138:0 2139:0.0227608 2140:0.0197278 2141:0.0223238 2142:0.00721876 2143:0.0121021 2144:0 2145:0.00953341 2146:0.0263585 2147:0.0450668 2148:0.00998802 2149:0.00472654 2150:0.00871761 2151:0.00451883 2152:0 2153:0.0228838 2154:0.0155071 2155:0.00850258 2156:0.00342731 2157:0.00793924 2158:0 2159:0.0262574 2160:0.0130419 2161:0.0185458 2162:0.00469851 2163:0.0103676 2164:0.000690474 2165:0.00562286 2166:0.0133135 2167:0.042651 2168:0 2169:0.0551663 2170:0.0254142 2171:0.00297192 2172:0.00829919 2173:0.0144836 2174:0.010824 2175:0.0372235 2176:0 2177:0.0273624 2178:0.0159521 2179:0.023795 2180:0.0239911 2181:0.0306372 2182:0.0239321 2183:0.0196087 2184:0.0267956 2185:0.0279178 2186:0.0153856 2187:0.0155218 2188:0.0418895 2189:0.012998 2190:0.0341475 2191:0.0173287 2192:0.0173292 2193:0.0218735 2194:0.0320616 2195:0.0210604 2196:0.0251629 2197:0 2198:0 2199:0.00914546 2200:0.0198011 2201:0.0245219 2202:0.0325881 2203:0.0234067 2204:0 2205:0 2206:0 2207:0 2208:0.0198317 2209:0.0245421 2210:0.0179808 2211:0 2212:0 2213:0.00330774 2214:0.0105197 2215:0.00272675 2216:0.034661 2217:0.0236347 2218:0.0280428 2219:0 2220:0 2221:0.0105775 2222:0.0111276 2223:0.00954811 2224:0.0378957 2225:0.0226299 2226:0.0160705 2227:0.0197201 2228:0.0112003 2229:0.014888 2230:0.0175607 2231:0.0439884 2232:0.00803734 2233:0.0542339 2234:0.0625282 2235:0.0478348 2236:0.0480978 2237:0.0444195 2238:0.0456949 2239:0.0490388 2240:0.0278897 2241:0.0433557 2242:0 2243:0 2244:0 2245:0 2246:0 2247:0 2248:0 2249:0.0756551 2250:0 2251:0 2252:0 2253:0 2254:0 2255:0 2256:0 2257:0.105843 2258:0 2259:0 2260:0 2261:0 2262:0 2263:0 2264:0 2265:0.103338 2266:0 2267:0.00188198 2268:0.0268948 2269:0 2270:0 2271:0 2272:0 2273:0.0865966 2274:0.056524 2275:0.0287574 2276:0.0423057 2277:0.0140923 2278:0.0349416 2279:0 2280:0 2281:0.084447 2282:0.0795579 2283:0.00844317 2284:0 2285:0.00454366 2286:0.0628691 2287:0 2288:0 2289:0.0759649 2290:7.62492e-05 2291:0.0249089 2292:0.00225566 2293:0 2294:0 2295:0.00818451 2296:0.0405075 2297:0.114759 2298:0 2299:0 2300:0.00205695 2301:0.0100571 2302:0 2303:0.0762987 2304:0.0441217 2305:0.0586512 2306:0.0145251 2307:0.0110007 2308:0.011026 2309:0.00397062 2310:0.000294242 2311:0.00705523 2312:0.0177253 2313:0.0613921 2314:0.00601014 2315:0.01085 2316:0.00585104 2317:0.00342907 2318:0 2319:0 2320:0.0126159 2321:0.0536429 2322:0 2323:0 2324:0.0364757 2325:0.0068889 2326:0.0131185 2327:0.000441615 2328:0.0183131 2329:0.0570032 2330:0.00115954 2331:0.0250257 2332:0 2333:0 2334:0 2335:0 2336:0.0146562 2337:0.0597894 2338:0.00454577 2339:0 2340:0 2341:0 2342:0 2343:0 2344:0.0136956 2345:0.0664301 2346:0.0263418 2347:0 2348:0 2349:0 2350:0 2351:0 2352:0.00961797 2353:0.071633 2354:0.00792874 2355:0.00206349 2356:0 2357:0 2358:0.0107964 2359:0.0102536 2360:0.00547114 2361:0.113294 2362:0.0158798 2363:0.0152892 2364:0.0189762 2365:0.0195098 2366:0.0156936 2367:0.028547 2368:0.000504307 2369:0.0498766 2370:0.0899729 2371:0.0993623 2372:0.0990901 2373:0.0988974 2374:0.0977128 2375:0.109009 2376:0.145669 2377:0.0693075 2378:0.084207 2379:0.0862844 2380:0.0899671 2381:0.0934649 2382:0.0939581 2383:0.0857648 2384:0.116825 2385:0.077947 2386:0.0921812 2387:0.09237 2388:0.0834571 2389:0.113508 2390:0.109344 2391:0.0899687 2392:0.120537 2393:0.0839087 2394:0.0995465 2395:0.0908324 2396:0.125219 2397:0.137315 2398:0.129779 2399:0.114486 2400:0.12118 2401:0.0908183 2402:0.101143 2403:0.132097 2404:0.140655 2405:0.164703 2406:0.148277 2407:0.148815 2408:0.13742 2409:0.0612758 2410:0.120497 2411:0.149278 2412:0.149048 2413:0.137231 2414:0.142361 2415:0.139584 2416:0.15252 2417:0.0843977 2418:0.0949455 2419:0.107994 2420:0.124659 2421:0.102785 2422:0.0906468 2423:0.0951572 2424:0.118045 2425:0.0898499 2426:0.110906 2427:0.114206 2428:0.105863 2429:0.0955374 2430:0.0957222 2431:0.120449 2432:0.127272 2433:0 2434:0 2435:0 2436:0 2437:0 2438:0 2439:0 2440:0 2441:0 2442:0 2443:0 2444:0 2445:0 2446:0 2447:0 2448:0 2449:0 2450:0 2451:0 2452:0 2453:0 2454:0 2455:0 2456:0 2457:0 2458:0 2459:0 2460:0 2461:0 2462:0 2463:0 2464:0 2465:0 2466:0 2467:0 2468:0 2469:0 2470:0 2471:0 2472:0 2473:0 2474:0 2475:0 2476:0 2477:0 2478:0 2479:0 2480:0 2481:0 2482:0 2483:0 2484:0 2485:0 2486:0 2487:0 2488:0 2489:0 2490:0 2491:0 2492:0 2493:0 2494:0 2495:0 2496:0 2497:0.011963 2498:0 2499:0 2500:0 2501:0 2502:0 2503:0 2504:0 2505:0 2506:0 2507:0 2508:0 2509:0 2510:0 2511:0 2512:0.00014173 2513:0 2514:0 2515:0 2516:0 2517:0 2518:0 2519:0 2520:0 2521:0 2522:0 2523:0.00556828 2524:0 2525:0 2526:0 2527:0 2528:0 2529:0 2530:0 2531:0 2532:0 2533:0 2534:0 2535:0 2536:0 2537:0 2538:0 2539:0 2540:0 2541:0 2542:0 2543:0 2544:0.00572045 2545:0 2546:0 2547:0 2548:0 2549:0 2550:0 2551:0 2552:0 2553:0.0311279 2554:0 2555:0 2556:0 2557:0 2558:0 2559:0 2560:0.0272995 2561:0 2562:0 2563:0 2564:0 2565:0 2566:0 2567:0 2568:0 2569:0 2570:0 2571:0 2572:0 2573:0 2574:0 2575:0 2576:0.00466839 2577:0 2578:0 2579:0.0277686 2580:0.049052 2581:0.0207864 2582:0.0203626 2583:0 2584:0.00947512 2585:0 2586:0.0080472 2587:0.0291464 2588:0.0325299 2589:0.0146256 2590:0.0378917 2591:0 2592:0.00463605 2593:0 2594:0.0296094 2595:0.0430476 2596:0 2597:0 2598:0 2599:0 2600:0.0211456 2601:0 2602:0 2603:0 2604:0 2605:0 2606:0 2607:0 2608:0.028028 2609:0.0142187 2610:0 2611:0 2612:0 2613:0 2614:0 2615:0.00278195 2616:0.00750383 2617:0.0491367 2618:0.0202897 2619:0.00371718 2620:0 2621:0 2622:0 2623:0 2624:0.0308841 2625:0 2626:0 2627:0 2628:0 2629:0 2630:0 2631:0 2632:0 2633:0 2634:0 2635:0 2636:0 2637:0 2638:0 2639:0 2640:0 2641:0 2642:0 2643:0 2644:0 2645:0 2646:0 2647:0 2648:0 2649:0 2650:0 2651:0 2652:0 2653:0 2654:0 2655:0 2656:0 2657:0 2658:0 2659:0 2660:0 2661:0 2662:0 2663:0 2664:0 2665:0 2666:0 2667:0 2668:0 2669:0 2670:0 2671:0 2672:0 2673:0 2674:0 2675:0 2676:0 2677:0 2678:0 2679:0 2680:0 2681:0 2682:0 2683:0 2684:0 2685:0 2686:0 2687:0 2688:0 2689:0.0579311 2690:0.0635315 2691:0.0587305 2692:0.0570674 2693:0.0586441 2694:0.0625902 2695:0.0642661 2696:0.053823 2697:0.0476988 2698:0.0449403 2699:0.0415443 2700:0.0372417 2701:0.0569932 2702:0.0502316 2703:0.0536523 2704:0.038898 2705:0.0627045 2706:0.041664 2707:0.0617141 2708:0.0748511 2709:0.0987783 2710:0.0954072 2711:0.0551149 2712:0.0480293 2713:0.0658983 2714:0.0587212 2715:0.0869382 2716:0.100599 2717:0.100631 2718:0.097918 2719:0.0993009 2720:0.0516106 2721:0.0671999 2722:0.0792451 2723:0.108519 2724:0.0761444 2725:0.0997114 2726:0.104552 2727:0.10004 2728:0.0542509 2729:0.0571685 2730:0.0580351 2731:0.097748 2732:0.0886788 2733:0.0619485 2734:0.0699083 2735:0.0707224 2736:0.0708818 2737:0.059099 2738:0.0462834 2739:0.0698016 2740:0.0800895 2741:0.0577383 2742:0.0527208 2743:0.0495714 2744:0.046386 2745:0.0422454 2746:0.0297297 2747:0.0391579 2748:0.0481254 2749:0.0436728 2750:0.0329873 2751:0.0374062 2752:0.040052 2753:0.00247764 2754:0.0148809 2755:0.0174194 2756:0.015108 2757:0.019167 2758:0.0322713 2759:0.0332232 2760:0.0189316 2761:0.0359167 2762:0.00423918 2763:0 2764:0 2765:0 2766:0 2767:0 2768:0 2769:0.0295349 2770:0 2771:0 2772:0.056269 2773:0.0831901 2774:0.037487 2775:0.046487 2776:0.00466454 2777:0.0094082 2778:0 2779:0.073075 2780:0.0938831 2781:0.104347 2782:0.0839058 2783:0.0665241 2784:0 2785:0 2786:0.0673093 2787:0.123365 2788:0.100544 2789:0.0781233 2790:0.0594398 2791:0.0519193 2792:0 2793:0.00367248 2794:0.0645489 2795:0.0643319 2796:0.0361863 2797:0.0225563 2798:0.0285654 2799:0 2800:0.00436921 2801:0.0221345 2802:0.00831411 2803:0.030059 2804:0.0198522 2805:0 2806:0 2807:0.0157868 2808:0.0211485 2809:0 2810:0.0193551 2811:0.00512619 2812:0 2813:0 2814:0 2815:0 2816:0 2817:0 2818:0 2819:0 2820:0 2821:0 2822:0 2823:0 2824:0 2825:0 2826:0 2827:0 2828:0 2829:0 2830:0 2831:0 2832:0 2833:0 2834:0 2835:0 2836:0 2837:0 2838:0 2839:0 2840:0 2841:0 2842:0 2843:0 2844:0 2845:0 2846:0 2847:0 2848:0 2849:0 2850:0 2851:0 2852:0 2853:0 2854:0 2855:0 2856:0 2857:0 2858:0 2859:0 2860:0 2861:0 2862:0 2863:0 2864:0 2865:0 2866:0 2867:0 2868:0 2869:0 2870:0 2871:0 2872:0 2873:0.00890715 2874:0 2875:0 2876:0 2877:0 2878:0 2879:0 2880:0 2881:0 2882:0 2883:0.00649838 2884:0 2885:0.0051508 2886:0.00981547 2887:0.00634178 2888:0.0201147 2889:0.00414979 2890:0.000262126 2891:0.000614114 2892:0.00181037 2893:0.00367444 2894:0.0106457 2895:0.0012788 2896:0.0137901 2897:0.00794017 2898:0.00838336 2899:0.0154239 2900:0 2901:0.0269975 2902:0 2903:0.00331986 2904:0.023666 2905:0.00103014 2906:0.00753529 2907:0.0340207 2908:0.0575413 2909:0.0475236 2910:0.0330526 2911:0.00741931 2912:0.0253839 2913:0.0155184 2914:0.0427668 2915:0.00630501 2916:0.0311259 2917:0.0280379 2918:0.0565906 2919:0.016004 2920:0.0398379 2921:0.00804782 2922:0.0300224 2923:0.0315952 2924:0.0298332 2925:0.0174657 2926:0.0336532 2927:0.0392566 2928:0.0402649 2929:0.0249483 2930:0.0133906 2931:0.0108112 2932:0.0134452 2933:0.0192234 2934:0.0169387 2935:0.027333 2936:0.0563967 2937:0.0503393 2938:0.0458417 2939:0.0221356 2940:0.0210842 2941:0.0207513 2942:0.0179742 2943:0.0333423 2944:0.0536289 2945:0 2946:0 2947:0 2948:0 2949:0 2950:0 2951:0 2952:0 2953:0 2954:0 2955:0 2956:0 2957:0 2958:0 2959:0 2960:0 2961:0 2962:0 2963:0 2964:0 2965:0 2966:0 2967:0 2968:0 2969:0 2970:0 2971:0 2972:0 2973:0 2974:0 2975:0 2976:0 2977:0 2978:0 2979:0 2980:0 2981:0 2982:0 2983:0 2984:0 2985:0 2986:0 2987:0 2988:0 2989:0 2990:0 2991:0 2992:0 2993:0 2994:0 2995:0 2996:0 2997:0 2998:0 2999:0 3000:0 3001:0 3002:0 3003:0 3004:0 3005:0 3006:0 3007:0 3008:0 3009:0.0515741 3010:0.100364 3011:0.102858 3012:0.104014 3013:0.0991094 3014:0.10411 3015:0.121542 3016:0.135599 3017:0.0651633 3018:0.0822323 3019:0.0930944 3020:0.0877783 3021:0.0964648 3022:0.0997647 3023:0.103658 3024:0.112554 3025:0.0949973 3026:0.100458 3027:0.0815232 3028:0.0605257 3029:0.0739981 3030:0.068862 3031:0.0904899 3032:0.118189 3033:0.100759 3034:0.107704 3035:0.0749521 3036:0.168642 3037:0.175178 3038:0.156721 3039:0.13296 3040:0.122027 3041:0.0974474 3042:0.11006 3043:0.183515 3044:0.220091 3045:0.243266 3046:0.192288 3047:0.198726 3048:0.119784 3049:0.0912582 3050:0.178347 3051:0.215144 3052:0.160426 3053:0.132279 3054:0.20848 3055:0.195885 3056:0.119463 3057:0.102807 3058:0.126035 3059:0.165962 3060:0.17618 3061:0.118133 3062:0.104542 3063:0.103086 3064:0.179717 3065:0.212289 3066:0.200504 3067:0.167342 3068:0.156447 3069:0.157195 3070:0.152182 3071:0.140705 3072:0.139123 3073:0.153349 3074:0.166216 3075:0.176012 3076:0.170205 3077:0.171378 3078:0.184329 3079:0.185448 3080:0.178006 3081:0.179519 3082:0.173299 3083:0.184203 3084:0.188459 3085:0.203296 3086:0.196368 3087:0.193773 3088:0.175491 3089:0.205348 3090:0.206343 3091:0.188462 3092:0.181655 3093:0.191193 3094:0.208869 3095:0.191231 3096:0.171252 3097:0.203337 3098:0.206427 3099:0.237274 3100:0.236777 3101:0.275576 3102:0.26321 3103:0.267479 3104:0.175968 3105:0.201758 3106:0.190917 3107:0.280762 3108:0.275813 3109:0.266931 3110:0.261111 3111:0.28646 3112:0.158764 3113:0.20504 3114:0.198192 3115:0.248088 3116:0.239367 3117:0.212024 3118:0.231678 3119:0.243767 3120:0.16526 3121:0.188158 3122:0.195989 3123:0.214162 3124:0.225881 3125:0.213838 3126:0.197888 3127:0.195743 3128:0.194386 3129:0.214022 3130:0.209136 3131:0.209247 3132:0.206907 3133:0.203199 3134:0.195472 3135:0.207249 3136:0.212405 3137:0.0552885 3138:0.00605091 3139:0.00425734 3140:0.00741557 3141:0.00640436 3142:0.0137573 3143:0.00252212 3144:0 3145:0.0723458 3146:0.000173861 3147:0 3148:0.0125914 3149:0 3150:0.0121383 3151:0.0127755 3152:0 3153:0.0685264 3154:0 3155:0.0145066 3156:0.014749 3157:0.00059795 3158:0 3159:0.00439908 3160:0 3161:0.068276 3162:0.00146075 3163:0.0218831 3164:0.0146281 3165:0.00244185 3166:0.00504111 3167:0 3168:0 3169:0.0647467 3170:0.00457619 3171:0.0131396 3172:0 3173:0.0143987 3174:0.00937873 3175:0.015007 3176:0 3177:0.0646749 3178:0.0019204 3179:0.0262945 3180:0 3181:0.00426448 3182:0.0127371 3183:0.0246252 3184:0 3185:0.0858457 3186:0 3187:0 3188:0.00780561 3189:0.000175927 3190:0.0027914 3191:0.0228508 3192:0 3193:0.116876 3194:0.0409172 3195:0.0167116 3196:0.0256791 3197:0.0280577 3198:0.0289397 3199:0.0261648 3200:0.0342386 3201:0.0523804 3202:0.0717947 3203:0.0692151 3204:0.0659988 3205:0.0770161 3206:0.0748925 3207:0.0707774 3208:0.0804486 3209:0.0558104 3210:0.0710184 3211:0.0713086 3212:0.0941479 3213:0.0737905 3214:0.0769669 3215:0.0834721 3216:0.0798105 3217:0.0754395 3218:0.0884108 3219:0.0915625 3220:0.0704975 3221:0.0586396 3222:0.0769449 3223:0.0770015 3224:0.0796629 3225:0.0847331 3226:0.101245 3227:0.0580572 3228:0.0873514 3229:0.0903797 3230:0.113572 3231:0.111204 3232:0.0925258 3233:0.0874238 3234:0.071256 3235:0.176382 3236:0.158366 3237:0.166771 3238:0.123189 3239:0.151824 3240:0.0883447 3241:0.0738403 3242:0.087463 3243:0.1574 3244:0.115693 3245:0.0929356 3246:0.121432 3247:0.133838 3248:0.0709471 3249:0.0635773 3250:0.0782602 3251:0.10418 3252:0.114603 3253:0.0938799 3254:0.0750554 3255:0.0718516 3256:0.0771026 3257:0.100654 3258:0.093316 3259:0.105019 3260:0.101828 3261:0.108564 3262:0.103469 3263:0.123983 3264:0.0857101 3265:0.0746828 3266:0.0782906 3267:0.0811277 3268:0.0792261 3269:0.0787752 3270:0.0785623 3271:0.0789596 3272:0.0970692 3273:0.0714545 3274:0.0813528 3275:0.0845569 3276:0.0982974 3277:0.102932 3278:0.113389 3279:0.100367 3280:0.0884973 3281:0.0926622 3282:0.0963569 3283:0.110533 3284:0.0980118 3285:0.0997597 3286:0.0994559 3287:0.0994507 3288:0.0838174 3289:0.0958173 3290:0.119799 3291:0.115778 3292:0.135627 3293:0.113914 3294:0.132677 3295:0.0853075 3296:0.0907446 3297:0.110913 3298:0.112952 3299:0.0975286 3300:0.0852708 3301:0.130456 3302:0.12186 3303:0.108215 3304:0.101205 3305:0.0839454 3306:0.0823193 3307:0.120761 3308:0.122228 3309:0.104013 3310:0.107842 3311:0.131941 3312:0.120688 3313:0.0963032 3314:0.0887394 3315:0.100301 3316:0.128094 3317:0.101912 3318:0.097361 3319:0.101868 3320:0.102426 3321:0.130552 3322:0.117002 3323:0.108454 3324:0.110247 3325:0.0994276 3326:0.0922502 3327:0.10726 3328:0.134203 3329:0 3330:0 3331:0 3332:0 3333:0 3334:0 3335:0 3336:0.007833 3337:0 3338:0 3339:0.00275125 3340:0.00870169 3341:0.00311706 3342:0.00512418 3343:0.00107185 3344:0.0194574 3345:0 3346:0 3347:0 3348:0 3349:0 3350:0.0213187 3351:0 3352:0.00877553 3353:0 3354:0.00407822 3355:0 3356:0 3357:0 3358:0 3359:0.00661319 3360:0.00905619 3361:0.00238262 3362:0 3363:0 3364:0.0129746 3365:0.0383933 3366:0 3367:0.0038693 3368:0.0139111 3369:0 3370:0.0075957 3371:0 3372:0.0143546 3373:0.0163116 3374:0.0237836 3375:0.00578521 3376:0.00551072 3377:0 3378:0 3379:0.0101935 3380:0.0142947 3381:0 3382:0 3383:0 3384:0.00397461 3385:0.0413101 3386:0.0224392 3387:0.0340051 3388:0.0349864 3389:0.0360495 3390:0.0421593 3391:0.0591668 3392:0.0400337 3393:0.0390351 3394:0.0081141 3395:0 3396:0 3397:0 3398:0 3399:0 3400:0 3401:0.0194977 3402:0 3403:0 3404:0.0176766 3405:0.0174868 3406:0.00461853 3407:0 3408:0 3409:0.00277224 3410:0 3411:0 3412:0.0378478 3413:0 3414:0 3415:0 3416:0 3417:0 3418:0.00586376 3419:0.0341154 3420:0 3421:0 3422:0 3423:0 3424:0 3425:0.00657925 3426:0.0329058 3427:0 3428:0 3429:0 3430:0 3431:0 3432:0 3433:0.0106463 3434:0.0141901 3435:0 3436:0 3437:0 3438:0 3439:0 3440:0 3441:0 3442:0 3443:0 3444:0 3445:0 3446:0 3447:0 3448:0 3449:0 3450:0 3451:0 3452:0 3453:0 3454:0 3455:0 3456:0 3457:0.0233599 3458:0.0892676 3459:0.105472 3460:0.101348 3461:0.101981 3462:0.108518 3463:0.116019 3464:0.120767 3465:0.079346 3466:0.084756 3467:0.0763771 3468:0.0672148 3469:0.0927647 3470:0.0703822 3471:0.0869475 3472:0.08366 3473:0.10582 3474:0.102182 3475:0.0840096 3476:0.127563 3477:0.16396 3478:0.140872 3479:0.136349 3480:0.104679 3481:0.100766 3482:0.0946213 3483:0.161909 3484:0.206464 3485:0.244374 3486:0.25184 3487:0.213986 3488:0.126934 3489:0.0888013 3490:0.149277 3491:0.228741 3492:0.228652 3493:0.222389 3494:0.189572 3495:0.190732 3496:0.122995 3497:0.0864364 3498:0.153588 3499:0.145866 3500:0.16 3501:0.14272 3502:0.141037 3503:0.0979166 3504:0.14735 3505:0.160793 3506:0.141587 3507:0.130027 3508:0.135171 3509:0.105919 3510:0.103568 3511:0.120883 3512:0.195149 3513:0.176085 3514:0.177789 3515:0.160342 3516:0.115052 3517:0.103008 3518:0.118578 3519:0.103353 3520:0.130961 3521:0 3522:0 3523:0 3524:0 3525:0 3526:0 3527:0 3528:0 3529:0 3530:0 3531:0 3532:0 3533:0 3534:0 3535:0 3536:0 3537:0 3538:0 3539:0 3540:0 3541:0 3542:0 3543:0 3544:0 3545:0 3546:0 3547:0 3548:0 3549:0 3550:0 3551:0 3552:0 3553:0 3554:0 3555:0 3556:0 3557:0 3558:0 3559:0 3560:0 3561:0 3562:0 3563:0 3564:0 3565:0 3566:0 3567:0 3568:0 3569:0 3570:0 3571:0 3572:0 3573:0 3574:0 3575:0 3576:0 3577:0 3578:0 3579:0 3580:0 3581:0 3582:0 3583:0 3584:0.0109747 3585:0.0502918 3586:0.0147439 3587:0.00294275 3588:0.00323713 3589:0.00491797 3590:0.0076118 3591:0.00309625 3592:0 3593:0.063418 3594:0.0125766 3595:0.012688 3596:0.0281545 3597:0.0324713 3598:0.0199157 3599:0.0175488 3600:0.0140182 3601:0.0635442 3602:0.0135836 3603:0.0471395 3604:0.057913 3605:0.0275555 3606:0.0209762 3607:0.0132377 3608:0.0175989 3609:0.0615872 3610:0.0633238 3611:0.0710794 3612:0.0459209 3613:0.0295607 3614:0.0287262 3615:0 3616:0.0164794 3617:0.0594369 3618:0.0707909 3619:0.0466955 3620:0.00428214 3621:0 3622:0.0178045 3623:0 3624:0.0259194 3625:0.0632752 3626:0.0314035 3627:0.027279 3628:0.00318571 3629:0.00708682 3630:0.0201387 3631:0 3632:0.0499691 3633:0.0843269 3634:0.0277291 3635:0.0227715 3636:0.00306842 3637:0.00581857 3638:0.016435 3639:0.0415891 3640:0.03813 3641:0.061885 3642:0.00293853 3643:0.0063947 3644:0.00586671 3645:0.00133741 3646:0 3647:0.0366625 3648:0.0222676 3649:0.101242 3650:0.111682 3651:0.118821 3652:0.117482 3653:0.119717 3654:0.120117 3655:0.114608 3656:0.150582 3657:0.0896305 3658:0.0834253 3659:0.104649 3660:0.117312 3661:0.123582 3662:0.0896301 3663:0.105679 3664:0.151756 3665:0.0987372 3666:0.10306 3667:0.120535 3668:0.106581 3669:0.111305 3670:0.127912 3671:0.11614 3672:0.13456 3673:0.101432 3674:0.115499 3675:0.0950827 3676:0.0533058 3677:0.0635708 3678:0.0868284 3679:0.13359 3680:0.148131 3681:0.100486 3682:0.0793169 3683:0.0894328 3684:0.106471 3685:0.109331 3686:0.0796743 3687:0.163001 3688:0.161377 3689:0.0821468 3690:0.0524823 3691:0.105776 3692:0.13658 3693:0.101059 3694:0.119737 3695:0.142945 3696:0.176614 3697:0.130632 3698:0.10452 3699:0.0801273 3700:0.113367 3701:0.0982687 3702:0.0951694 3703:0.0833501 3704:0.191106 3705:0.127273 3706:0.12257 3707:0.122382 3708:0.09654 3709:0.0913383 3710:0.0906016 3711:0.0966335 3712:0.205832 3713:0 3714:0 3715:0 3716:0 3717:0 3718:0 3719:0 3720:0 3721:0 3722:0 3723:0 3724:0 3725:0 3726:0 3727:0 3728:0 3729:0 3730:0 3731:0 3732:0 3733:0 3734:0 3735:0 3736:0 3737:0 3738:0 3739:0 3740:0 3741:0 3742:0 3743:0 3744:0 3745:0 3746:0 3747:0 3748:0 3749:0 3750:0 3751:0 3752:0 3753:0 3754:0 3755:0 3756:0 3757:0 3758:0 3759:0 3760:0 3761:0 3762:0 3763:0 3764:0 3765:0 3766:0 3767:0 3768:0 3769:0 3770:0 3771:0 3772:0 3773:0 3774:0 3775:0 3776:0 3777:0.0264475 3778:0 3779:0 3780:0 3781:0 3782:0 3783:0 3784:0 3785:0 3786:0 3787:0 3788:0 3789:0 3790:0 3791:0 3792:0.00101701 3793:0 3794:0 3795:0 3796:0 3797:0 3798:0 3799:0 3800:0 3801:0 3802:0 3803:0 3804:0 3805:0 3806:0 3807:0 3808:0 3809:0 3810:0 3811:0 3812:0 3813:0 3814:0 3815:0.00707052 3816:0 3817:0 3818:0 3819:0 3820:0 3821:0 3822:0 3823:0.00797983 3824:0 3825:0 3826:0 3827:0 3828:0 3829:0 3830:0 3831:0 3832:0 3833:0.00360528 3834:0 3835:0 3836:0 3837:0 3838:0 3839:0 3840:0.0196371 3841:0 3842:0 3843:0 3844:0 3845:0 3846:0 3847:0 3848:0 3849:0 3850:0 3851:0 3852:0 3853:0 3854:0 3855:0 3856:0.00418062 3857:0 3858:0.000178706 3859:0.0097367 3860:0.0279499 3861:0.0346249 3862:0.0206028 3863:0.0239136 3864:0.00193439 3865:0 3866:0.0282538 3867:0.0214502 3868:0.064113 3869:0.0528288 3870:0.0548272 3871:0.0486722 3872:0 3873:0 3874:0.0383546 3875:0.0472291 3876:0.0660426 3877:0.0111324 3878:0.038173 3879:0.0453459 3880:0 3881:0.000421777 3882:0.0122276 3883:0.0380143 3884:0.0306026 3885:0.0143149 3886:0.000107516 3887:0.0033177 3888:0 3889:0.00692115 3890:0.00326903 3891:0.0119759 3892:0.00502091 3893:0.00832675 3894:0.00768492 3895:0 3896:0.00256594 3897:0.0115072 3898:0.0114529 3899:0.00583628 3900:0.00215704 3901:0.00553396 3902:0 3903:0 3904:0.021676 3905:0.0241539 3906:0.0475476 3907:0.0525212 3908:0.0536087 3909:0.0531881 3910:0.0552174 3911:0.0671439 3912:0.0617209 3913:0.0396019 3914:0.0345837 3915:0.0387595 3916:0.0489585 3917:0.0431804 3918:0.0573043 3919:0.0414636 3920:0.0343198 3921:0.0655791 3922:0.0450989 3923:0.0428853 3924:0.0517129 3925:0.080647 3926:0.0561448 3927:0.054972 3928:0.0396294 3929:0.0715505 3930:0.0643246 3931:0.0584438 3932:0.124261 3933:0.0906168 3934:0.108303 3935:0.0833164 3936:0.0432368 3937:0.0635174 3938:0.0581132 3939:0.137572 3940:0.118544 3941:0.132457 3942:0.133607 3943:0.114606 3944:0.0525235 3945:0.0744946 3946:0.10333 3947:0.113003 3948:0.110432 3949:0.0848686 3950:0.104421 3951:0.0707643 3952:0.0531137 3953:0.0978663 3954:0.0727778 3955:0.0906511 3956:0.0825289 3957:0.064971 3958:0.0564937 3959:0.0564373 3960:0.0762466 3961:0.0964645 3962:0.0748757 3963:0.0713123 3964:0.0635267 3965:0.0625157 3966:0.0671574 3967:0.0535109 3968:0.101459 3969:0 3970:0 3971:0 3972:0 3973:0 3974:0 3975:0 3976:0 3977:0 3978:0 3979:0 3980:0 3981:0 3982:0 3983:0 3984:0 3985:0 3986:0 3987:0 3988:0 3989:0 3990:0 3991:0 3992:0 3993:0 3994:0 3995:0 3996:0 3997:0 3998:0 3999:0 4000:0 4001:0 4002:0 4003:0 4004:0 4005:0 4006:0 4007:0 4008:0.0180452 4009:0 4010:0 4011:0 4012:0 4013:0 4014:0 4015:0 4016:0.0141096 4017:0 4018:0 4019:0 4020:0 4021:0 4022:0 4023:0 4024:0 4025:0 4026:0 4027:0 4028:0 4029:0 4030:0 4031:0 4032:0 4033:0 4034:0 4035:0 4036:0 4037:0 4038:0 4039:0 4040:0 4041:0 4042:0 4043:0 4044:0 4045:0 4046:0 4047:0 4048:0 4049:0 4050:0 4051:0 4052:0 4053:0 4054:0 4055:0 4056:0 4057:0 4058:0 4059:0 4060:0 4061:0 4062:0 4063:0 4064:0 4065:0 4066:0 4067:0 4068:0 4069:0 4070:0 4071:0 4072:0 4073:0 4074:0 4075:0 4076:0 4077:0 4078:0 4079:0 4080:0 4081:0 4082:0 4083:0 4084:0 4085:0 4086:0 4087:0 4088:0.00922477 4089:0 4090:0 4091:0 4092:0 4093:0 4094:0 4095:0 4096:0.00852481 4097:0 4098:0 4099:0 4100:0 4101:0 4102:0 4103:0 4104:0 4105:0 4106:0 4107:0 4108:0 4109:0 4110:0 4111:0 4112:0 4113:0 4114:0 4115:0 4116:0 4117:0 4118:0 4119:0 4120:0 4121:0 4122:0 4123:0 4124:0 4125:0 4126:0 4127:0 4128:0 4129:0 4130:0 4131:0 4132:0 4133:0 4134:0 4135:0 4136:0 4137:0 4138:0 4139:0 4140:0 4141:0 4142:0 4143:0 4144:0 4145:0 4146:0 4147:0 4148:0 4149:0 4150:0 4151:0 4152:0 4153:0 4154:0 4155:0 4156:0 4157:0 4158:0 4159:0 4160:0 4161:0 4162:0 4163:0 4164:0 4165:0 4166:0 4167:0 4168:0 4169:0 4170:0 4171:0 4172:0 4173:0 4174:0 4175:0 4176:0 4177:0 4178:0 4179:0 4180:0 4181:0 4182:0 4183:0 4184:0 4185:0 4186:0 4187:0 4188:0 4189:0 4190:0 4191:0 4192:0 4193:0 4194:0 4195:0 4196:0 4197:0 4198:0 4199:0 4200:0 4201:0 4202:0 4203:0 4204:0 4205:0 4206:0 4207:0 4208:0 4209:0 4210:0 4211:0 4212:0 4213:0 4214:0 4215:0 4216:0 4217:0.0278566 4218:0.0143022 4219:0.00638293 4220:0 4221:0 4222:0 4223:0.00815006 4224:0.0142205 4225:0.0124989 4226:0.0201074 4227:0.0291167 4228:0.0270462 4229:0.0290767 4230:0.0308522 4231:0.0250031 4232:0.0852266 4233:0.0230262 4234:0.0260576 4235:0.0320535 4236:0.0330079 4237:0.0445056 4238:0.0283797 4239:0.0386067 4240:0.0989017 4241:0.0338795 4242:0.0278345 4243:0.0433451 4244:0.0178321 4245:0.0326514 4246:0.0455879 4247:0.0372364 4248:0.102439 4249:0.0360867 4250:0.0374054 4251:0.00760436 4252:0 4253:0.0300333 4254:0.0269446 4255:0.0890921 4256:0.106474 4257:0.026513 4258:0.00813335 4259:0.0619957 4260:0.0592586 4261:0.0711811 4262:0.0298911 4263:0.100871 4264:0.101377 4265:0.0210411 4266:0.0185976 4267:0.0726867 4268:0.0724098 4269:0.0521988 4270:0.0416499 4271:0.0548755 4272:0.118182 4273:0.0219893 4274:0.024655 4275:0.0382523 4276:0.0450767 4277:0.0472159 4278:0.0316799 4279:0.0106103 4280:0.137188 4281:0.0425466 4282:0.0427618 4283:0.0485292 4284:0.0392302 4285:0.0387593 4286:0.0440721 4287:0.065812 4288:0.127126 4289:0.12305 4290:0.0952825 4291:0.0877982 4292:0.0873221 4293:0.0870752 4294:0.0945991 4295:0.102289 4296:0.101716 4297:0.14354 4298:0.0802166 4299:0.0901068 4300:0.0859526 4301:0.0887576 4302:0.0876045 4303:0.0768175 4304:0.0863621 4305:0.143812 4306:0.0849716 4307:0.0796757 4308:0.151023 4309:0.121028 4310:0.111493 4311:0.105966 4312:0.0922797 4313:0.143484 4314:0.0941177 4315:0.128766 4316:0.145731 4317:0.148747 4318:0.126298 4319:0.118472 4320:0.100714 4321:0.13515 4322:0.155246 4323:0.15748 4324:0.136348 4325:0.114854 4326:0.133145 4327:0.0935877 4328:0.0932273 4329:0.137214 4330:0.140395 4331:0.118793 4332:0.123334 4333:0.0990422 4334:0.127618 4335:0.0921468 4336:0.118771 4337:0.158707 4338:0.108252 4339:0.101434 4340:0.0840672 4341:0.084888 4342:0.0886078 4343:0.118709 4344:0.130301 4345:0.147973 4346:0.123747 4347:0.109718 4348:0.0868422 4349:0.0901736 4350:0.0901491 4351:0.0958311 4352:0.0743994 4353:0.0166237 4354:0.0593018 4355:0.0629244 4356:0.062608 4357:0.0621923 4358:0.0674395 4359:0.0774008 4360:0.094204 4361:0.0142476 4362:0.0373229 4363:0.04308 4364:0.0271529 4365:0.0487825 4366:0.0419946 4367:0.054966 4368:0.0571601 4369:0.0416316 4370:0.0599603 4371:0.0556997 4372:0.075959 4373:0.105329 4374:0.0957038 4375:0.0651631 4376:0.0632338 4377:0.0412816 4378:0.0559288 4379:0.0951361 4380:0.166041 4381:0.186747 4382:0.174902 4383:0.144127 4384:0.0681447 4385:0.0249212 4386:0.128665 4387:0.130671 4388:0.165144 4389:0.152636 4390:0.17961 4391:0.145493 4392:0.0581105 4393:0.0180047 4394:0.0785843 4395:0.14032 4396:0.132478 4397:0.0869057 4398:0.0806211 4399:0.0991962 4400:0.0742616 4401:0.0242667 4402:0.0637866 4403:0.0804218 4404:0.0971227 4405:0.0747025 4406:0.0600353 4407:0.0679057 4408:0.0942454 4409:0.0466102 4410:0.0659379 4411:0.0636733 4412:0.0588596 4413:0.0588093 4414:0.0530928 4415:0.0485905 4416:0.0762841 4417:0.0462196 4418:0.0712033 4419:0.065372 4420:0.0680238 4421:0.0776965 4422:0.0792421 4423:0.0920327 4424:0.0823333 4425:0.0708907 4426:0.0651992 4427:0.074337 4428:0.070118 4429:0.0752824 4430:0.0839694 4431:0.078995 4432:0.06206 4433:0.079498 4434:0.0844086 4435:0.0700676 4436:0.0837681 4437:0.106264 4438:0.117116 4439:0.114306 4440:0.0720228 4441:0.0771604 4442:0.0805741 4443:0.175744 4444:0.146289 4445:0.176781 4446:0.151635 4447:0.163073 4448:0.085559 4449:0.0481095 4450:0.0996892 4451:0.19659 4452:0.195506 4453:0.198959 4454:0.163993 4455:0.156928 4456:0.0713469 4457:0.0707542 4458:0.142693 4459:0.129267 4460:0.140049 4461:0.128143 4462:0.135055 4463:0.0951003 4464:0.0881136 4465:0.0893266 4466:0.0949325 4467:0.118919 4468:0.119942 4469:0.10017 4470:0.0895758 4471:0.110619 4472:0.124428 4473:0.0882949 4474:0.106784 4475:0.106188 4476:0.0925013 4477:0.0875844 4478:0.0968173 4479:0.107249 4480:0.0897354 4481:0.0449865 4482:0.0743607 4483:0.0720505 4484:0.0738082 4485:0.0710742 4486:0.071532 4487:0.0730968 4488:0.0732213 4489:0.0702126 4490:0.0618661 4491:0.052978 4492:0.0625841 4493:0.0626787 4494:0.0642523 4495:0.0612206 4496:0.0482155 4497:0.084906 4498:0.06185 4499:0.0764258 4500:0.0804994 4501:0.0811799 4502:0.0772742 4503:0.0594637 4504:0.0571444 4505:0.0716918 4506:0.0755925 4507:0.131418 4508:0.133127 4509:0.125476 4510:0.102086 4511:0.128423 4512:0.0607943 4513:0.0710148 4514:0.0770817 4515:0.169287 4516:0.109054 4517:0.0998261 4518:0.114715 4519:0.134689 4520:0.059732 4521:0.0659244 4522:0.072884 4523:0.128209 4524:0.0943245 4525:0.0562901 4526:0.0781959 4527:0.0907367 4528:0.0667212 4529:0.0743951 4530:0.0490173 4531:0.0661433 4532:0.0759076 4533:0.069469 4534:0.0584195 4535:0.0798989 4536:0.107951 4537:0.104025 4538:0.0838517 4539:0.070742 4540:0.0688679 4541:0.0693367 4542:0.0676004 4543:0.0480387 4544:0.117272 4545:0.074751 4546:0.0986759 4547:0.102032 4548:0.102194 4549:0.098822 4550:0.102387 4551:0.11012 4552:0.112534 4553:0.0617687 4554:0.062886 4555:0.0650306 4556:0.0682476 4557:0.0765639 4558:0.0686036 4559:0.0748998 4560:0.0687862 4561:0.0696647 4562:0.0675809 4563:0.0760285 4564:0.0677938 4565:0.0914752 4566:0.0941162 4567:0.0694498 4568:0.0684516 4569:0.0656928 4570:0.0730845 4571:0.1189 4572:0.0982685 4573:0.137236 4574:0.118568 4575:0.147564 4576:0.0741101 4577:0.063049 4578:0.0926649 4579:0.143816 4580:0.118983 4581:0.111275 4582:0.0992332 4583:0.120724 4584:0.0720814 4585:0.0669192 4586:0.0872733 4587:0.0989816 4588:0.104504 4589:0.0736846 4590:0.0790431 4591:0.0698896 4592:0.0635918 4593:0.0798068 4594:0.0752409 4595:0.078973 4596:0.0878735 4597:0.0769395 4598:0.0696956 4599:0.0752633 4600:0.109345 4601:0.0763236 4602:0.089911 4603:0.0800715 4604:0.0673548 4605:0.068404 4606:0.0696622 4607:0.0826472 4608:0.0793987 4609:0 4610:0.0197614 4611:0.0175663 4612:0.0107091 4613:0.0122272 4614:0.018208 4615:0.030511 4616:0.0223016 4617:0 4618:0.00498445 4619:0.00691079 4620:0.0241793 4621:0.0318984 4622:0.0282416 4623:0.0109676 4624:0.0126582 4625:0 4626:0.0107656 4627:0.0106015 4628:0.00708874 4629:0 4630:0.0276725 4631:0.0208042 4632:0.012489 4633:0.00239238 4634:0.0291924 4635:0.0445198 4636:0.0217696 4637:0.045067 4638:0.0361634 4639:0.0431716 4640:0.018727 4641:0 4642:0 4643:0.10776 4644:0.109021 4645:0.112883 4646:0.0674777 4647:0.108809 4648:0.0076927 4649:0 4650:0.0467979 4651:0.087829 4652:0.0923469 4653:0.0617433 4654:0.0699497 4655:0.0792395 4656:0.0399646 4657:0 4658:0.0270106 4659:0.0637149 4660:0.055979 4661:0.024867 4662:0.00851206 4663:0 4664:0.0638229 4665:0.0372088 4666:0.0255042 4667:0.0182318 4668:0.0189825 4669:0.0252898 4670:0.0247468 4671:0.0419313 4672:0.0542777 4673:0.0182893 4674:0.0338955 4675:0.035999 4676:0.0418567 4677:0.0265866 4678:0.0324391 4679:0.0420421 4680:0.0783493 4681:0.0259836 4682:0.0222711 4683:0.0286175 4684:0.0106876 4685:0.0281025 4686:0.0106489 4687:0.0351272 4688:0.0805242 4689:0.0291549 4690:0.0249446 4691:0.0273505 4692:0 4693:0.0360122 4694:0.0350591 4695:0.0257374 4696:0.0908782 4697:0.0348053 4698:0.0364226 4699:0.00513486 4700:0 4701:0.00567841 4702:0.00304779 4703:0.0487245 4704:0.0867008 4705:0.0318842 4706:0 4707:0.052788 4708:0.0381228 4709:0.0586975 4710:0.0319444 4711:0.0705154 4712:0.0768431 4713:0.0302536 4714:0.0177558 4715:0.0446139 4716:0.0725414 4717:0.0381342 4718:0.0336566 4719:0.0491528 4720:0.0846618 4721:0.0483242 4722:0.0200675 4723:0.03184 4724:0.0460323 4725:0.0245778 4726:0.0207916 4727:0 4728:0.0903127 4729:0.00593569 4730:0 4731:0 4732:0.00773736 4733:0.00296824 4734:0.00695242 4735:0 4736:0.0618611 4737:0.0245017 4738:0.0136167 4739:0.0143858 4740:0.0102734 4741:0.0155805 4742:0.0112592 4743:0.018208 4744:0.00343375 4745:0 4746:0.00335061 4747:0 4748:0.0252827 4749:0.000383798 4750:0.0160621 4751:0.00476113 4752:0 4753:0 4754:0.00856115 4755:0.0162667 4756:0.0158626 4757:0.0225075 4758:0 4759:0.0121203 4760:0.0134133 4761:0 4762:0.0416444 4763:0.0407615 4764:0.0566927 4765:0.0241489 4766:0.0367147 4767:0 4768:0.0101067 4769:0.0273937 4770:0.0692854 4771:0 4772:0 4773:0.0509686 4774:0.0683515 4775:0 4776:0.029358 4777:0.0119054 4778:0.025216 4779:0.0489983 4780:0 4781:0 4782:0.0267764 4783:0.022105 4784:0.0333267 4785:0 4786:0.0222269 4787:0.0177013 4788:0.0258806 4789:0.0249696 4790:0.0222079 4791:0.0293278 4792:0 4793:0.0547885 4794:0.0654494 4795:0.0392372 4796:0.034081 4797:0.0327853 4798:0.0357231 4799:0.046852 4800:0.0394768 4801:0.0264831 4802:0.0223331 4803:0.0171674 4804:0.0238885 4805:0.0233452 4806:0.0190449 4807:0.0145901 4808:0.0103449 4809:0.00993826 4810:0 4811:0 4812:0 4813:0 4814:0 4815:0 4816:0 4817:0.0163031 4818:0 4819:0 4820:0 4821:0 4822:0 4823:0 4824:0 4825:0.0146341 4826:0 4827:0 4828:0 4829:0 4830:0 4831:0 4832:0 4833:0.013992 4834:0 4835:0 4836:0 4837:0 4838:0 4839:0 4840:0 4841:0.0217665 4842:0 4843:0 4844:0 4845:0 4846:0 4847:0 4848:0 4849:0.040112 4850:0 4851:0 4852:0 4853:0 4854:0 4855:0 4856:0 4857:0.0299698 4858:0 4859:0 4860:0 4861:0 4862:0 4863:0 4864:0 4865:0.0821205 4866:0.0855523 4867:0.0942862 4868:0.0909329 4869:0.0955027 4870:0.0948078 4871:0.0844152 4872:0.0755044 4873:0.0892225 4874:0.0917479 4875:0.0841081 4876:0.0908771 4877:0.0997552 4878:0.0671574 4879:0.0860369 4880:0.0864253 4881:0.0914944 4882:0.0915612 4883:0.102465 4884:0.145432 4885:0.133537 4886:0.10371 4887:0.0908711 4888:0.0976945 4889:0.0854018 4890:0.107884 4891:0.11945 4892:0.12543 4893:0.109482 4894:0.11495 4895:0.0931154 4896:0.093483 4897:0.105881 4898:0.127626 4899:0.0873927 4900:0.0751227 4901:0.0863527 4902:0.0959365 4903:0.102367 4904:0.110828 4905:0.105531 4906:0.0929871 4907:0.0863241 4908:0.0927258 4909:0.0874391 4910:0.0731338 4911:0.127117 4912:0.131267 4913:0.135119 4914:0.118109 4915:0.0758962 4916:0.0924529 4917:0.100076 4918:0.103934 4919:0.105406 4920:0.078558 4921:0.162334 4922:0.148794 4923:0.120858 4924:0.10166 4925:0.0980917 4926:0.0952816 4927:0.100643 4928:0.0708809 4929:0.0279023 4930:0.0321543 4931:0.0351353 4932:0.0439705 4933:0.0393041 4934:0.034281 4935:0.0394612 4936:0.0495353 4937:0.0327761 4938:0.0246995 4939:0.0326102 4940:0.017682 4941:0.0514479 4942:0.0264274 4943:0.0276566 4944:0.017884 4945:0.0274478 4946:0.0245399 4947:0.0424254 4948:0.0505574 4949:0.0464995 4950:0.0569117 4951:0.0436281 4952:0.0137882 4953:0.0320572 4954:0.0299051 4955:0.0984114 4956:0.057885 4957:0.046142 4958:0.0390483 4959:0.053161 4960:0.025435 4961:0.0117365 4962:0.0399342 4963:0.082658 4964:0.0778461 4965:0.0704829 4966:0.0559928 4967:0.0736685 4968:0.0278734 4969:0.045586 4970:0.0519893 4971:0.0526409 4972:0.0631602 4973:0.0625918 4974:0.0530254 4975:0.0452591 4976:0.0411831 4977:0.050509 4978:0.0561185 4979:0.0548836 4980:0.0418719 4981:0.0261226 4982:0.0325173 4983:0.0357357 4984:0.0395695 4985:0.0597766 4986:0.055726 4987:0.057357 4988:0.0366252 4989:0.036145 4990:0.0336822 4991:0.0288165 4992:0.0514455 4993:0 4994:0 4995:0 4996:0 4997:0 4998:0 4999:0 5000:0 5001:0 5002:0 5003:0 5004:0 5005:0 5006:0 5007:0 5008:0 5009:0 5010:0 5011:0 5012:0 5013:0 5014:0 5015:0 5016:0 5017:0 5018:0 5019:0 5020:0 5021:0 5022:0 5023:0 5024:0 5025:0 5026:0 5027:0 5028:0 5029:0 5030:0 5031:0 5032:0 5033:0 5034:0 5035:0 5036:0 5037:0 5038:0 5039:0 5040:0 5041:0 5042:0 5043:0 5044:0 5045:0 5046:0 5047:0 5048:0 5049:0 5050:0 5051:0 5052:0 5053:0 5054:0 5055:0 5056:0 5057:0.0672174 5058:0.10448 5059:0.120753 5060:0.115771 5061:0.121949 5062:0.112112 5063:0.114732 5064:0.147989 5065:0.0650287 5066:0.0931679 5067:0.103251 5068:0.114856 5069:0.0937508 5070:0.0970434 5071:0.0939347 5072:0.107693 5073:0.0726042 5074:0.0939125 5075:0.0908699 5076:0.0961719 5077:0.076085 5078:0.0742781 5079:0.100385 5080:0.103148 5081:0.101259 5082:0.0960534 5083:0.0774411 5084:0.0162815 5085:0.0278458 5086:0.0484378 5087:0.0859661 5088:0.111516 5089:0.10008 5090:0.0671501 5091:0.0703427 5092:0.0959062 5093:0.130714 5094:0.0743845 5095:0.114202 5096:0.131437 5097:0.0932463 5098:0.0773513 5099:0.101055 5100:0.0970209 5101:0.12082 5102:0.11521 5103:0.134855 5104:0.106975 5105:0.099262 5106:0.101382 5107:0.104361 5108:0.123759 5109:0.108841 5110:0.0939995 5111:0.0902909 5112:0.111093 5113:0.148845 5114:0.152925 5115:0.141979 5116:0.133388 5117:0.132993 5118:0.142123 5119:0.142921 5120:0.170969 5121:0.126291 5122:0.148678 5123:0.158919 5124:0.157383 5125:0.156894 5126:0.16835 5127:0.162799 5128:0.156552 5129:0.123818 5130:0.133519 5131:0.128832 5132:0.128861 5133:0.151064 5134:0.147452 5135:0.154335 5136:0.136183 5137:0.144767 5138:0.14434 5139:0.171878 5140:0.175472 5141:0.181667 5142:0.194161 5143:0.14107 5144:0.139194 5145:0.152158 5146:0.166773 5147:0.201711 5148:0.239853 5149:0.285992 5150:0.266568 5151:0.225478 5152:0.13958 5153:0.156806 5154:0.225976 5155:0.279953 5156:0.249987 5157:0.284099 5158:0.276693 5159:0.239036 5160:0.145698 5161:0.147009 5162:0.221938 5163:0.270156 5164:0.222767 5165:0.178129 5166:0.208379 5167:0.211491 5168:0.147817 5169:0.155868 5170:0.159039 5171:0.18689 5172:0.204172 5173:0.164608 5174:0.158268 5175:0.158075 5176:0.183575 5177:0.169018 5178:0.161702 5179:0.156911 5180:0.149598 5181:0.148528 5182:0.140729 5183:0.151553 5184:0.191262 5185:0 5186:0 5187:0 5188:0 5189:0 5190:0 5191:0 5192:0 5193:0 5194:0 5195:0 5196:0 5197:0 5198:0 5199:0 5200:0 5201:0 5202:0 5203:0 5204:0 5205:0 5206:0 5207:0 5208:0.00494105 5209:0 5210:0 5211:0 5212:0 5213:0 5214:0 5215:0 5216:0.00248694 5217:0 5218:0 5219:0 5220:0 5221:0 5222:0 5223:0 5224:0 5225:0 5226:0 5227:0 5228:0 5229:0 5230:0 5231:0 5232:0 5233:0 5234:0 5235:0 5236:0 5237:0 5238:0 5239:0 5240:0.002159 5241:0 5242:0 5243:0 5244:0 5245:0 5246:0 5247:0 5248:0 5249:0.0344943 5250:0.0573333 5251:0.0659987 5252:0.0592082 5253:0.0617769 5254:0.0600336 5255:0.0714369 5256:0.129563 5257:0.0227865 5258:0.0307715 5259:0.0436189 5260:0.0282774 5261:0.0425635 5262:0.0213973 5263:0.0320738 5264:0.0866956 5265:0.0442877 5266:0.0286241 5267:0.0328876 5268:0 5269:0.0309147 5270:0.0471953 5271:0.0269672 5272:0.113675 5273:0.041049 5274:0.0500243 5275:0.0274617 5276:0.0111484 5277:0.030855 5278:0.0169488 5279:0.0515103 5280:0.110573 5281:0.0318589 5282:0.0382178 5283:0.0699693 5284:0.0333742 5285:0.0532607 5286:0.0214721 5287:0.0686724 5288:0.112592 5289:0.0307836 5290:0.025101 5291:0.0717411 5292:0.0251492 5293:0.0182798 5294:0.0439665 5295:0.0612633 5296:0.111777 5297:0.0121126 5298:0.0121955 5299:0.0242402 5300:0.054139 5301:0.034479 5302:0.0249099 5303:0.0208294 5304:0.161281 5305:0.118094 5306:0.107123 5307:0.100374 5308:0.0939857 5309:0.086153 5310:0.0838569 5311:0.0800996 5312:0.131114 5313:0.0627449 5314:0.0581519 5315:0.059657 5316:0.0553959 5317:0.0581183 5318:0.0589565 5319:0.0558518 5320:0.0332036 5321:0.0504918 5322:0.0590235 5323:0.0473986 5324:0.0626483 5325:0.038681 5326:0.0560496 5327:0.0562735 5328:0.0333409 5329:0.060374 5330:0.0496979 5331:0.0538124 5332:0.0509799 5333:0.0393164 5334:0.0453147 5335:0.0544963 5336:0.0344996 5337:0.067108 5338:0.0577215 5339:0.0570229 5340:0.0741007 5341:0.0752766 5342:0.0728265 5343:0.049788 5344:0.0314539 5345:0.0686434 5346:0.0624142 5347:0.123139 5348:0.108546 5349:0.123503 5350:0.0952375 5351:0.101244 5352:0.0367315 5353:0.0733477 5354:0.090195 5355:0.139063 5356:0.0948365 5357:0.0733207 5358:0.0988701 5359:0.106236 5360:0.0434009 5361:0.0590997 5362:0.0635915 5363:0.0864949 5364:0.0878179 5365:0.0627348 5366:0.0619326 5367:0.0734898 5368:0.0766413 5369:0.100407 5370:0.0933752 5371:0.0890646 5372:0.0727766 5373:0.0722926 5374:0.0717499 5375:0.0816118 5376:0.0805043 5377:0.0985784 5378:0.1097 5379:0.104715 5380:0.105838 5381:0.10951 5382:0.119325 5383:0.119183 5384:0.130741 5385:0.133732 5386:0.117126 5387:0.10144 5388:0.104332 5389:0.129949 5390:0.102682 5391:0.111096 5392:0.11552 5393:0.143106 5394:0.11899 5395:0.140847 5396:0.150746 5397:0.171632 5398:0.162667 5399:0.138894 5400:0.12496 5401:0.13045 5402:0.105304 5403:0.176149 5404:0.189196 5405:0.202935 5406:0.191956 5407:0.188921 5408:0.113795 5409:0.132882 5410:0.12351 5411:0.187218 5412:0.169018 5413:0.157218 5414:0.159616 5415:0.182164 5416:0.121881 5417:0.129958 5418:0.104113 5419:0.135291 5420:0.157761 5421:0.133006 5422:0.110843 5423:0.110277 5424:0.144577 5425:0.168998 5426:0.117716 5427:0.116611 5428:0.130457 5429:0.126721 5430:0.110884 5431:0.12953 5432:0.149603 5433:0.19033 5434:0.147343 5435:0.123795 5436:0.11588 5437:0.117244 5438:0.119296 5439:0.0996527 5440:0.142048 5441:0 5442:0 5443:0 5444:0 5445:0 5446:0 5447:0 5448:0 5449:0 5450:0 5451:0 5452:0 5453:0 5454:0 5455:0 5456:0 5457:0 5458:0 5459:0 5460:0 5461:0 5462:0 5463:0 5464:0 5465:0 5466:0 5467:0 5468:0 5469:0 5470:0 5471:0 5472:0 5473:0 5474:0 5475:0 5476:0 5477:0 5478:0 5479:0 5480:0 5481:0 5482:0 5483:0 5484:0 5485:0 5486:0 5487:0 5488:0 5489:0 5490:0 5491:0 5492:0 5493:0 5494:0 5495:0 5496:0 5497:0 5498:0.000608843 5499:0 5500:0 5501:0 5502:0 5503:0 5504:0 5505:0.018472 5506:0 5507:0.00132484 5508:0 5509:0.00080879 5510:0 5511:0 5512:0.000749527 5513:0.00396356 5514:0 5515:0 5516:0.0044631 5517:0 5518:0.00199579 5519:0 5520:0.00878562 5521:0.00465843 5522:0 5523:0.00835996 5524:0.00392651 5525:0.00594037 5526:0 5527:0 5528:0 5529:0.00623883 5530:0.0163689 5531:0.00909131 5532:0.0341187 5533:0.0248117 5534:0.0287342 5535:0.0116451 5536:0 5537:0.0166642 5538:0.0157292 5539:0.0244383 5540:0.0236782 5541:0.00602152 5542:0.0103021 5543:0 5544:0 5545:0.00430144 5546:0.017454 5547:0 5548:0.00162131 5549:0 5550:0.00888865 5551:0 5552:0.00714935 5553:0.00666527 5554:0 5555:0 5556:0 5557:0 5558:0 5559:0 5560:0.0352876 5561:0 5562:0 5563:0 5564:0 5565:0 5566:0 5567:0.0178973 5568:0.0251495 5569:0 5570:0 5571:0 5572:0 5573:0 5574:0 5575:0 5576:0 5577:0 5578:0 5579:0 5580:0 5581:0 5582:0 5583:0 5584:0 5585:0 5586:0 5587:0 5588:0 5589:0 5590:0 5591:0 5592:0 5593:0 5594:0 5595:0 5596:0 5597:0 5598:0 5599:0 5600:0 5601:0 5602:0 5603:0 5604:0 5605:0 5606:0 5607:0 5608:0 5609:0 5610:0 5611:0 5612:0 5613:0 5614:0 5615:0 5616:0 5617:0 5618:0 5619:0 5620:0 5621:0 5622:0 5623:0 5624:0 5625:0 5626:0 5627:0 5628:0 5629:0 5630:0 5631:0 5632:0 5633:0.0165992 5634:0.0171818 5635:0.0166826 5636:0.0192317 5637:0.0177729 5638:0.0161432 5639:0.0156366 5640:0.0300682 5641:0.00775287 5642:0.0187824 5643:0.0297432 5644:0.00912222 5645:0.0251306 5646:0.0192093 5647:0.0170077 5648:0.0364631 5649:0.0111516 5650:0.0140405 5651:0.0164508 5652:0.0298619 5653:0.0453058 5654:0.019582 5655:0.0204832 5656:0.0356422 5657:0.0121499 5658:0.0196229 5659:0.035674 5660:0.00825156 5661:0.00814459 5662:0.00999284 5663:0.0263119 5664:0.0396008 5665:0.0174635 5666:0.0164469 5667:0.0165409 5668:0 5669:0 5670:0 5671:0.00950786 5672:0.0551404 5673:0.0118168 5674:0 5675:0.013165 5676:0.0211245 5677:0.0236694 5678:0 5679:0.0238299 5680:0.0611361 5681:0.0394528 5682:0.04324 5683:0.0130443 5684:0.015953 5685:0.0162847 5686:0.0146203 5687:0.0162421 5688:0.0244704 5689:0.0585201 5690:0.0485123 5691:0.0410869 5692:0.0256828 5693:0.0218978 5694:0.0258522 5695:0.0130072 5696:0.0519662 5697:0.000436227 5698:0 5699:0 5700:0 5701:0 5702:0 5703:0 5704:0 5705:0 5706:0 5707:0 5708:0 5709:0 5710:0 5711:0 5712:0 5713:0 5714:0 5715:0 5716:0 5717:0 5718:0 5719:0 5720:0 5721:0 5722:0 5723:0 5724:0 5725:0 5726:0 5727:0 5728:0 5729:0 5730:0 5731:0 5732:0 5733:0 5734:0 5735:0 5736:0 5737:0 5738:0 5739:0 5740:0 5741:0 5742:0 5743:0 5744:0 5745:0 5746:0 5747:0 5748:0 5749:0 5750:0 5751:0 5752:0 5753:0 5754:0 5755:0 5756:0 5757:0 5758:0 5759:0 5760:0 5761:0 5762:0 5763:0 5764:0 5765:0 5766:0 5767:0 5768:0.016671 5769:0 5770:0 5771:0 5772:0 5773:0 5774:0 5775:0 5776:0.0088096 5777:0 5778:0 5779:0 5780:0 5781:0 5782:0 5783:0 5784:0.00596759 5785:0 5786:0 5787:0 5788:0 5789:0 5790:0 5791:0 5792:0.000973614 5793:0 5794:0 5795:0 5796:0.0100631 5797:0 5798:0 5799:0 5800:0.00121704 5801:0 5802:0 5803:0 5804:0 5805:0 5806:0 5807:0 5808:0 5809:0 5810:0 5811:0 5812:0 5813:0 5814:0 5815:0 5816:0.0106143 5817:0 5818:0 5819:0 5820:0 5821:0 5822:0 5823:0 5824:0.0257627 5825:0.0996637 5826:0.0882442 5827:0.0825831 5828:0.08188 5829:0.0819439 5830:0.0771706 5831:0.0794938 5832:0.0724385 5833:0.080412 5834:0.0560303 5835:0.056557 5836:0.09368 5837:0.0572194 5838:0.0779235 5839:0.0424962 5840:0.0473904 5841:0.066057 5842:0.0581102 5843:0.0703596 5844:0.0677529 5845:0.0362225 5846:0.0506638 5847:0.0439793 5848:0.0472974 5849:0.0544786 5850:0.0971687 5851:0.0687664 5852:0.0591505 5853:0.0264262 5854:0.0153655 5855:0.0222932 5856:0.0321928 5857:0.0572149 5858:0.0880713 5859:0.035723 5860:0.0581765 5861:0.0435005 5862:0.0759291 5863:0.0312522 5864:0.0482493 5865:0.0618479 5866:0.0762585 5867:0.0525668 5868:0.0234449 5869:0.0616791 5870:0.0591405 5871:0.0684581 5872:0.0574212 5873:0.0508527 5874:0.0651035 5875:0.0552006 5876:0.0350905 5877:0.0501336 5878:0.0624472 5879:0.0743951 5880:0.0539831 5881:0.0688496 5882:0.0421719 5883:0.0471308 5884:0.041331 5885:0.0476137 5886:0.0445821 5887:0.0703317 5888:0.00670127 5889:0.0139495 5890:0.0292919 5891:0.0400252 5892:0.0277469 5893:0.0355305 5894:0.0399954 5895:0.0415393 5896:0.02553 5897:0.0337104 5898:0.0397789 5899:0.044759 5900:0.0555294 5901:0.0570836 5902:0.0463946 5903:0.0579015 5904:0.0362761 5905:0.0527537 5906:0.040852 5907:0.0632022 5908:0.0534661 5909:0.0570188 5910:0.0559726 5911:0.0601442 5912:0.0403367 5913:0.0547952 5914:0.0660141 5915:0.0672657 5916:0.0751372 5917:0.0612534 5918:0.0804456 5919:0.0800677 5920:0.0406292 5921:0.0497537 5922:0.0638238 5923:0.0626695 5924:0.071998 5925:0.0874718 5926:0.0886168 5927:0.0969976 5928:0.05403 5929:0.050244 5930:0.0542024 5931:0.088161 5932:0.0915067 5933:0.0620259 5934:0.0465038 5935:0.075486 5936:0.0519856 5937:0.0423751 5938:0.0469398 5939:0.0593937 5940:0.0668724 5941:0.0577427 5942:0.0477294 5943:0.0511743 5944:0.029602 5945:0.0574158 5946:0.0567079 5947:0.055215 5948:0.0457853 5949:0.0432205 5950:0.0490232 5951:0.041506 5952:0.0547801 5953:0.0524701 5954:0.0527171 5955:0.0500166 5956:0.0537218 5957:0.0535023 5958:0.0584039 5959:0.0525151 5960:0.0482433 5961:0.0551528 5962:0.0581614 5963:0.0593324 5964:0.0559483 5965:0.0589801 5966:0.0670816 5967:0.0606569 5968:0.0600921 5969:0.056613 5970:0.064582 5971:0.0615933 5972:0.0829295 5973:0.0868101 5974:0.0845536 5975:0.0649839 5976:0.0509841 5977:0.0605748 5978:0.0699562 5979:0.093631 5980:0.0881151 5981:0.106687 5982:0.0947999 5983:0.0949941 5984:0.0602196 5985:0.0524422 5986:0.092072 5987:0.114964 5988:0.0945562 5989:0.0945202 5990:0.108118 5991:0.0895764 5992:0.0633513 5993:0.0625023 5994:0.0978709 5995:0.0888054 5996:0.0877744 5997:0.103211 5998:0.084483 5999:0.0986604 6000:0.0672023 6001:0.075707 6002:0.0767109 6003:0.0816915 6004:0.080205 6005:0.0728472 6006:0.0657885 6007:0.0732027 6008:0.0668971 6009:0.0791308 6010:0.0867128 6011:0.0809987 6012:0.0761398 6013:0.0685395 6014:0.0666998 6015:0.0713376 6016:0.0860499 6017:0 6018:0 6019:0 6020:0 6021:0 6022:0 6023:0 6024:0.000325423 6025:0 6026:0 6027:0 6028:0 6029:0 6030:0 6031:0 6032:0 6033:0 6034:0 6035:0 6036:0 6037:0 6038:0 6039:0 6040:0 6041:0 6042:0 6043:0 6044:0 6045:0 6046:0 6047:0 6048:0 6049:0 6050:0 6051:0 6052:0 6053:0 6054:0 6055:0 6056:0 6057:0 6058:0 6059:0 6060:0 6061:0 6062:0 6063:0 6064:0 6065:0 6066:0 6067:0 6068:0 6069:0 6070:0 6071:0 6072:0 6073:0 6074:0 6075:0 6076:0 6077:0 6078:0 6079:0 6080:0.00539492 6081:0 6082:0 6083:0 6084:0 6085:0 6086:0 6087:0 6088:0 6089:0 6090:0 6091:0 6092:0 6093:0 6094:0 6095:0 6096:0 6097:0 6098:0 6099:0 6100:0 6101:0 6102:0 6103:0 6104:0 6105:0 6106:0 6107:0 6108:0 6109:0 6110:0 6111:0 6112:0 6113:0 6114:0 6115:0 6116:0 6117:0 6118:0 6119:0 6120:0 6121:0 6122:0 6123:0 6124:0 6125:0 6126:0 6127:0 6128:0 6129:0 6130:0 6131:0 6132:0 6133:0 6134:0 6135:0 6136:0 6137:0 6138:0 6139:0 6140:0 6141:0 6142:0 6143:0 6144:0 6145:0.0242309 6146:0 6147:0 6148:0 6149:0 6150:0 6151:0 6152:0 6153:0 6154:0 6155:0 6156:0 6157:0 6158:0 6159:0 6160:0 6161:0 6162:0 6163:0 6164:0 6165:0 6166:0 6167:0 6168:0 6169:0 6170:0 6171:0 6172:0 6173:0 6174:0 6175:0 6176:0 6177:0 6178:0 6179:0 6180:0 6181:0 6182:0 6183:0 6184:0 6185:0 6186:0 6187:0 6188:0 6189:0 6190:0 6191:0 6192:0 6193:0 6194:0 6195:0 6196:0 6197:0 6198:0 6199:0 6200:0 6201:0 6202:0 6203:0 6204:0 6205:0 6206:0 6207:0 6208:0 6209:0 6210:0 6211:0 6212:0 6213:0 6214:0 6215:0 6216:0 6217:0 6218:0 6219:0 6220:0 6221:0 6222:0 6223:0 6224:0 6225:0 6226:0 6227:0 6228:0 6229:0 6230:0 6231:0 6232:0 6233:0 6234:0 6235:0 6236:0 6237:0 6238:0 6239:0 6240:0 6241:0 6242:0 6243:0 6244:0 6245:0 6246:0 6247:0 6248:0 6249:0 6250:0 6251:0 6252:0 6253:0 6254:0 6255:0 6256:0 6257:0 6258:0 6259:0 6260:0 6261:0 6262:0 6263:0 6264:0 6265:0 6266:0 6267:0 6268:0 6269:0 6270:0 6271:0 6272:0 6273:0 6274:0.00903944 6275:0 6276:0 6277:0 6278:0.000481905 6279:0 6280:0.0142299 6281:0.00663316 6282:0 6283:0.000363539 6284:0 6285:0.000872632 6286:0 6287:0 6288:0 6289:0 6290:0 6291:0 6292:0.00494402 6293:0 6294:0 6295:0 6296:0 6297:0 6298:0 6299:0.000745467 6300:0.000503057 6301:0 6302:0 6303:0 6304:0 6305:0 6306:0.0290494 6307:0.00508656 6308:0.0109629 6309:0 6310:0 6311:0 6312:0 6313:0.0042175 6314:0.00365349 6315:0 6316:0 6317:0 6318:0 6319:0 6320:0.021821 6321:0 6322:0 6323:0 6324:0 6325:0 6326:0 6327:0 6328:0.0248572 6329:0.037131 6330:0.0333532 6331:0.0253622 6332:0.0227286 6333:0.022324 6334:0.0190235 6335:0.0176509 6336:0.0177485 6337:0.130594 6338:0.119887 6339:0.119001 6340:0.117976 6341:0.12354 6342:0.129554 6343:0.131691 6344:0.118231 6345:0.125749 6346:0.121251 6347:0.118207 6348:0.116764 6349:0.115524 6350:0.125335 6351:0.119104 6352:0.112172 6353:0.134199 6354:0.133815 6355:0.131706 6356:0.181027 6357:0.175104 6358:0.156006 6359:0.143134 6360:0.107497 6361:0.124332 6362:0.154662 6363:0.193843 6364:0.190354 6365:0.214336 6366:0.193673 6367:0.187385 6368:0.10732 6369:0.136232 6370:0.206185 6371:0.220476 6372:0.184668 6373:0.148762 6374:0.173396 6375:0.151616 6376:0.115095 6377:0.145502 6378:0.15002 6379:0.156565 6380:0.156493 6381:0.138806 6382:0.125849 6383:0.14526 6384:0.131982 6385:0.155602 6386:0.126021 6387:0.126921 6388:0.127623 6389:0.128571 6390:0.134767 6391:0.149839 6392:0.118676 6393:0.18722 6394:0.159774 6395:0.159638 6396:0.150323 6397:0.151739 6398:0.144314 6399:0.155065 6400:0.115698 6401:0 6402:0 6403:0 6404:0 6405:0 6406:0 6407:0 6408:0 6409:0 6410:0 6411:0 6412:0 6413:0 6414:0 6415:0 6416:0 6417:0 6418:0 6419:0 6420:0 6421:0 6422:0 6423:0 6424:0 6425:0 6426:0 6427:0 6428:0 6429:0 6430:0 6431:0 6432:0 6433:0 6434:0 6435:0 6436:0 6437:0 6438:0 6439:0 6440:0 6441:0 6442:0 6443:0 6444:0 6445:0 6446:0 6447:0 6448:0 6449:0 6450:0 6451:0 6452:0 6453:0 6454:0 6455:0 6456:0 6457:0 6458:0 6459:0 6460:0 6461:0 6462:0 6463:0 6464:0 6465:0.0593604 6466:0.0515176 6467:0.0643635 6468:0.0637965 6469:0.059995 6470:0.0601006 6471:0.0654592 6472:0.084258 6473:0.0555031 6474:0.0359118 6475:0.037313 6476:0.0439073 6477:0.0503063 6478:0.0505507 6479:0.0360509 6480:0.0327379 6481:0.0602585 6482:0.0438832 6483:0.0364568 6484:0.0636775 6485:0.060604 6486:0.0829542 6487:0.0788815 6488:0.0454491 6489:0.0483467 6490:0.028495 6491:0.0719347 6492:0.078565 6493:0.0859195 6494:0.0783219 6495:0.105553 6496:0.0410584 6497:0.0290144 6498:0.0446855 6499:0.091592 6500:0.069605 6501:0.053801 6502:0.0337658 6503:0.0789071 6504:0.0314352 6505:0.0432959 6506:0.0352321 6507:0.0227278 6508:0.0564958 6509:0.0508421 6510:0.0224075 6511:0.040242 6512:0.0451066 6513:0.0600321 6514:0.0314975 6515:0.0283369 6516:0.0271252 6517:0.039291 6518:0.0271621 6519:0.0459292 6520:0.0506939 6521:0.0431842 6522:0.0343522 6523:0.0299018 6524:0.0223044 6525:0.0294386 6526:0.0328604 6527:0.0346798 6528:0.0414264 6529:0 6530:0 6531:0 6532:0 6533:0 6534:0 6535:0 6536:0 6537:0 6538:0 6539:0 6540:0 6541:0 6542:0 6543:0 6544:0 6545:0 6546:0 6547:0 6548:0 6549:0 6550:0 6551:0 6552:0 6553:0 6554:0.0003229 6555:0 6556:0 6557:0 6558:0 6559:0 6560:0 6561:0 6562:0 6563:0 6564:0 6565:0 6566:0 6567:0 6568:0 6569:0 6570:0 6571:0 6572:0 6573:0 6574:0 6575:0 6576:0 6577:0 6578:0 6579:0 6580:0 6581:0 6582:0 6583:0 6584:0 6585:0 6586:0 6587:0 6588:0 6589:0 6590:0 6591:0 6592:0 6593:0.00767993 6594:0.0191951 6595:0.0311281 6596:0.0247437 6597:0.0228059 6598:0.0209939 6599:0.0319463 6600:0.0569618 6601:0.0194494 6602:0.0134452 6603:0.0259713 6604:0.0384598 6605:0.0144304 6606:0.0231917 6607:0.0182828 6608:0.0413755 6609:0.0214503 6610:0.0244079 6611:0.0281299 6612:0.0118876 6613:0.0231257 6614:0.0310237 6615:0.0265764 6616:0.0317218 6617:0.0168772 6618:0.0220779 6619:0.00139597 6620:0 6621:0.0167623 6622:0.0291509 6623:0.0415169 6624:0.0317371 6625:0.0150997 6626:0 6627:0.0155009 6628:0.0153416 6629:0.0233641 6630:0.031352 6631:0.0372518 6632:0.0343632 6633:0.0139161 6634:0.0130478 6635:0.0201751 6636:0.0188803 6637:0.0177117 6638:0.0330772 6639:0.0318561 6640:0.0355005 6641:0.028225 6642:0.0112299 6643:0.0176246 6644:0.0197075 6645:0.0179082 6646:0.0245159 6647:0.021266 6648:0.0506952 6649:0.0542246 6650:0.0312528 6651:0.0310926 6652:0.0263855 6653:0.0278354 6654:0.0291652 6655:0.0560698 6656:0.0698788 6657:0.109375 6658:0.117262 6659:0.127804 6660:0.138674 6661:0.132298 6662:0.129022 6663:0.144541 6664:0.145583 6665:0.122122 6666:0.11725 6667:0.122857 6668:0.116381 6669:0.129818 6670:0.114878 6671:0.124196 6672:0.114954 6673:0.151277 6674:0.128245 6675:0.122807 6676:0.146949 6677:0.162248 6678:0.165651 6679:0.121117 6680:0.134499 6681:0.153981 6682:0.142359 6683:0.168728 6684:0.262995 6685:0.251622 6686:0.240975 6687:0.164495 6688:0.138264 6689:0.141574 6690:0.267694 6691:0.249649 6692:0.275393 6693:0.268774 6694:0.287091 6695:0.170119 6696:0.142609 6697:0.140495 6698:0.233337 6699:0.225293 6700:0.199094 6701:0.198228 6702:0.210116 6703:0.139 6704:0.16286 6705:0.167049 6706:0.170661 6707:0.198515 6708:0.178233 6709:0.153242 6710:0.141868 6711:0.157995 6712:0.188901 6713:0.167415 6714:0.168614 6715:0.146901 6716:0.141009 6717:0.137045 6718:0.135207 6719:0.186293 6720:0.140148 6721:0.0578011 6722:0.0859883 6723:0.0853863 6724:0.0926701 6725:0.0992191 6726:0.0959015 6727:0.102373 6728:0.099269 6729:0.078539 6730:0.0967876 6731:0.10122 6732:0.104271 6733:0.0963046 6734:0.110913 6735:0.113363 6736:0.0901987 6737:0.113976 6738:0.115869 6739:0.102855 6740:0.102731 6741:0.123671 6742:0.108125 6743:0.113835 6744:0.0971178 6745:0.120608 6746:0.114407 6747:0.145369 6748:0.219107 6749:0.248437 6750:0.222719 6751:0.175834 6752:0.106609 6753:0.105715 6754:0.165744 6755:0.231417 6756:0.246965 6757:0.237993 6758:0.241442 6759:0.19711 6760:0.121528 6761:0.0967591 6762:0.152117 6763:0.209973 6764:0.181496 6765:0.15449 6766:0.185966 6767:0.179674 6768:0.143068 6769:0.110403 6770:0.133899 6771:0.170658 6772:0.166654 6773:0.118233 6774:0.121639 6775:0.161223 6776:0.149392 6777:0.158285 6778:0.169691 6779:0.15646 6780:0.133425 6781:0.129495 6782:0.120586 6783:0.112126 6784:0.155089 6785:0.0534225 6786:0.0821205 6787:0.0876628 6788:0.0850956 6789:0.0887503 6790:0.0980758 6791:0.101814 6792:0.0929127 6793:0.017485 6794:0.0198625 6795:0.0245735 6796:0.0233088 6797:0.0165943 6798:0.0187084 6799:0.0179212 6800:0.0153474 6801:0.0174708 6802:0.0249419 6803:0.0204608 6804:0.0379497 6805:0.0306101 6806:0.0589506 6807:0.0260745 6808:0.0227087 6809:0.0116679 6810:0.0155583 6811:0.0443785 6812:0.0777373 6813:0.0879188 6814:0.0991902 6815:0.0616207 6816:0.0233516 6817:0.00724343 6818:0.0419569 6819:0.0544353 6820:0.0625679 6821:0.0693977 6822:0.0910253 6823:0.0510801 6824:0.0279854 6825:0.00518402 6826:0.0104204 6827:0.0312829 6828:0.0413585 6829:0.0341649 6830:0.0345568 6831:0.0288929 6832:0.0450299 6833:0.0249034 6834:0.0207774 6835:0.0357307 6836:0.0351496 6837:0.0267807 6838:0.0247738 6839:0.0281821 6840:0.0398089 6841:0.0398966 6842:0.079582 6843:0.0724545 6844:0.0548595 6845:0.0559769 6846:0.0452545 6847:0.0321056 6848:0.0267356 6849:0.0346358 6850:0.00344628 6851:0 6852:0 6853:0 6854:0 6855:0 6856:0.00734926 6857:0 6858:0.00479423 6859:0 6860:0.0184072 6861:0.00522384 6862:0.0198124 6863:0.0014657 6864:0.0286952 6865:0 6866:0 6867:0.0218449 6868:0.0500918 6869:0.0011483 6870:0.0113367 6871:0.00324056 6872:0.0109103 6873:0 6874:0.040708 6875:0.0155531 6876:0 6877:0 6878:0 6879:0 6880:0.0165276 6881:0 6882:0.0206716 6883:0 6884:0 6885:0 6886:0 6887:0 6888:0.0152928 6889:0 6890:0 6891:0 6892:0 6893:0 6894:0.00355645 6895:0.00902094 6896:0.00445738 6897:0 6898:0 6899:0 6900:0 6901:0 6902:0.0108319 6903:0 6904:0 6905:0 6906:0 6907:0 6908:0.00686892 6909:0 6910:0 6911:0 6912:0.023251 6913:0.041176 6914:0.0453153 6915:0.0568007 6916:0.0533348 6917:0.0594954 6918:0.0552312 6919:0.0568172 6920:0.0531064 6921:0.0156009 6922:0.0102734 6923:0 6924:0.0185828 6925:0.00229706 6926:0.00680566 6927:0.00440663 6928:0.0156797 6929:0.000628843 6930:0.00483742 6931:0.0223421 6932:0.0209847 6933:0.0212531 6934:0.0199736 6935:0.0203567 6936:0.0197028 6937:0 6938:0.0255876 6939:0 6940:0 6941:0.0249592 6942:0.0490699 6943:0.0407746 6944:0.0183338 6945:0.00328488 6946:0.0239178 6947:0.00791139 6948:0.00488021 6949:0.0179976 6950:0.0066726 6951:0.0415387 6952:0.0151065 6953:0 6954:0 6955:0.00435014 6956:0 6957:0 6958:0.00873958 6959:0.0375887 6960:0 6961:0.00201736 6962:0 6963:0 6964:0.00652814 6965:0.00237216 6966:0.00933016 6967:0.00314052 6968:0.0344191 6969:0.0222935 6970:0 6971:0 6972:0 6973:0 6974:0 6975:0 6976:0.00776414 6977:0 6978:0 6979:0 6980:0 6981:0 6982:0 6983:0 6984:0 6985:0 6986:0 6987:0 6988:0 6989:0 6990:0 6991:0 6992:0 6993:0 6994:0 6995:0 6996:0 6997:0 6998:0 6999:0 7000:0 7001:0 7002:0 7003:0 7004:0 7005:0 7006:0 7007:0 7008:0 7009:0 7010:0 7011:0 7012:0 7013:0 7014:0 7015:0 7016:0 7017:0 7018:0 7019:0 7020:0 7021:0 7022:0 7023:0 7024:0 7025:0 7026:0 7027:0 7028:0 7029:0 7030:0 7031:0 7032:0 7033:0 7034:0 7035:0 7036:0 7037:0 7038:0 7039:0 7040:0 7041:0.0546727 7042:0.0543653 7043:0.0550792 7044:0.0469922 7045:0.0504924 7046:0.0492691 7047:0.0486867 7048:0.074496 7049:0.0386678 7050:0.0391241 7051:0.0379272 7052:0.0597353 7053:0.0473621 7054:0.0533156 7055:0.040806 7056:0.0747575 7057:0.0616273 7058:0.0519855 7059:0.0466441 7060:0.0388211 7061:0.0352953 7062:0.0330685 7063:0.0345139 7064:0.0724723 7065:0.0630081 7066:0.0569215 7067:0.0383221 7068:0.0666606 7069:0.0425591 7070:0.0497095 7071:0.0411193 7072:0.0752671 7073:0.0614125 7074:0.040661 7075:0.0746205 7076:0.104049 7077:0.104043 7078:0.0745867 7079:0.0897722 7080:0.0822837 7081:0.0620058 7082:0.0727548 7083:0.108306 7084:0.0582101 7085:0.0486286 7086:0.078137 7087:0.0775263 7088:0.0797929 7089:0.0514798 7090:0.0521244 7091:0.0617502 7092:0.0719621 7093:0.0453989 7094:0.0408536 7095:0.0412005 7096:0.0863583 7097:0.0642036 7098:0.0616083 7099:0.0657125 7100:0.0665227 7101:0.0638251 7102:0.0639872 7103:0.0772756 7104:0.0985058 7105:0.0236898 7106:0.0391042 7107:0.0338594 7108:0.0319322 7109:0.0294748 7110:0.0345571 7111:0.0333347 7112:0.0233675 7113:0 7114:0 7115:0 7116:0 7117:0 7118:0.00650426 7119:0 7120:0 7121:0 7122:0 7123:0 7124:0.0113204 7125:0 7126:0.00515506 7127:0 7128:0 7129:0 7130:0 7131:0.00270193 7132:0.0107127 7133:0 7134:0 7135:0 7136:0 7137:0 7138:0.0684384 7139:0.0243789 7140:0.0504228 7141:0.0195811 7142:0.0298839 7143:0 7144:0 7145:0 7146:0.0546659 7147:0.051476 7148:0 7149:0.0032329 7150:0.0421705 7151:0 7152:0 7153:0 7154:0.0197359 7155:0.0233983 7156:0.00208982 7157:0 7158:0 7159:0.00250733 7160:0 7161:0 7162:0.00154908 7163:0 7164:0 7165:0 7166:0 7167:0.00936166 7168:0 7169:0 7170:0 7171:0 7172:0 7173:0 7174:0 7175:0 7176:0 7177:0 7178:0 7179:0 7180:0 7181:0 7182:0 7183:0 7184:0 7185:0 7186:0 7187:0 7188:0 7189:0 7190:0 7191:0 7192:0 7193:0 7194:0 7195:0 7196:0 7197:0 7198:0 7199:0 7200:0 7201:0 7202:0 7203:0 7204:0 7205:0 7206:0 7207:0 7208:0 7209:0 7210:0 7211:0 7212:0 7213:0 7214:0 7215:0 7216:0 7217:0 7218:0 7219:0 7220:0 7221:0 7222:0 7223:0 7224:0 7225:0 7226:0 7227:0 7228:0 7229:0 7230:0 7231:0 7232:0 7233:0 7234:0 7235:0 7236:0 7237:0 7238:0 7239:0 7240:0 7241:0 7242:0 7243:0 7244:0 7245:0 7246:0 7247:0 7248:0 7249:0 7250:0 7251:0 7252:0 7253:0 7254:0 7255:0 7256:0 7257:0 7258:0 7259:0 7260:0 7261:0 7262:0 7263:0 7264:0 7265:0 7266:0 7267:0 7268:0 7269:0 7270:0 7271:0 7272:0 7273:0 7274:0 7275:0 7276:0 7277:0 7278:0 7279:0 7280:0 7281:0 7282:0 7283:0 7284:0 7285:0 7286:0 7287:0 7288:0 7289:0 7290:0 7291:0 7292:0 7293:0 7294:0 7295:0 7296:0 7297:0 7298:0 7299:0 7300:0 7301:0 7302:0 7303:0 7304:0 7305:0 7306:0 7307:0 7308:0 7309:0 7310:0 7311:0 7312:0 7313:0 7314:0 7315:0 7316:0 7317:0 7318:0 7319:0 7320:0 7321:0 7322:0 7323:0 7324:0 7325:0 7326:0 7327:0 7328:0 7329:0 7330:0 7331:0 7332:0 7333:0 7334:0 7335:0 7336:0 7337:0 7338:0 7339:0 7340:0 7341:0 7342:0 7343:0 7344:0 7345:0 7346:0 7347:0 7348:0 7349:0 7350:0 7351:0 7352:0 7353:0.075663 7354:0.0251329 7355:0.0257568 7356:0.032086 7357:0.0335027 7358:0.030973 7359:0.0598779 7360:0.0384215 7361:0.0901747 7362:0.0986722 7363:0.104875 7364:0.103224 7365:0.10282 7366:0.104991 7367:0.109712 7368:0.096642 7369:0.0838607 7370:0.0889195 7371:0.0823505 7372:0.110994 7373:0.0984399 7374:0.103806 7375:0.0937896 7376:0.0719701 7377:0.122147 7378:0.087879 7379:0.120108 7380:0.120886 7381:0.122118 7382:0.0972409 7383:0.0954545 7384:0.0742132 7385:0.122301 7386:0.168039 7387:0.142328 7388:0.187467 7389:0.162115 7390:0.172462 7391:0.105554 7392:0.0720487 7393:0.13305 7394:0.17084 7395:0.199743 7396:0.175276 7397:0.210735 7398:0.224931 7399:0.129948 7400:0.0802142 7401:0.13348 7402:0.152058 7403:0.198854 7404:0.152412 7405:0.138537 7406:0.137486 7407:0.131193 7408:0.0974972 7409:0.115497 7410:0.121525 7411:0.13694 7412:0.132962 7413:0.11534 7414:0.101731 7415:0.123525 7416:0.0883222 7417:0.159449 7418:0.117245 7419:0.113043 7420:0.116802 7421:0.116165 7422:0.105153 7423:0.129824 7424:0.114937 7425:0 7426:0 7427:0.00753275 7428:0.00350939 7429:0.00449257 7430:0.00471831 7431:0.0125584 7432:0.0102147 7433:0 7434:0 7435:0 7436:0 7437:0 7438:0 7439:0 7440:0 7441:0 7442:0 7443:0 7444:0 7445:0 7446:0 7447:0 7448:0 7449:0 7450:0 7451:0 7452:0 7453:0 7454:0 7455:0.0169399 7456:0 7457:0 7458:0 7459:0 7460:0 7461:0 7462:0 7463:0.0255145 7464:0 7465:0 7466:0 7467:0 7468:0 7469:0 7470:0 7471:0 7472:0 7473:0 7474:0 7475:0 7476:0 7477:0 7478:0 7479:0 7480:0 7481:0 7482:0 7483:0 7484:0 7485:0 7486:0 7487:0 7488:0 7489:0.0175171 7490:0.0481404 7491:0.0571165 7492:0.0535928 7493:0.0416182 7494:0.0530155 7495:0.065147 7496:0.0751813 7497:0.0390741 7498:0.0514727 7499:0.0501513 7500:0.0305362 7501:0.0612312 7502:0.0505386 7503:0.0611076 7504:0.0645194 7505:0.0660535 7506:0.0672259 7507:0.0614493 7508:0.0589558 7509:0.0887337 7510:0.0985432 7511:0.0605751 7512:0.0918986 7513:0.0645448 7514:0.063221 7515:0.10016 7516:0.144953 7517:0.178781 7518:0.167214 7519:0.175011 7520:0.0904424 7521:0.0391134 7522:0.098596 7523:0.152309 7524:0.169451 7525:0.172143 7526:0.186507 7527:0.199558 7528:0.0978344 7529:0.0449355 7530:0.112303 7531:0.149265 7532:0.147807 7533:0.0814766 7534:0.123668 7535:0.120713 7536:0.124715 7537:0.0579358 7538:0.0731913 7539:0.0948642 7540:0.12429 7541:0.0762694 7542:0.0649517 7543:0.0580958 7544:0.120548 7545:0.0546118 7546:0.0585813 7547:0.0431929 7548:0.0506367 7549:0.0568053 7550:0.0559688 7551:0.0393078 7552:0.0796771 7553:0 7554:0 7555:0 7556:0 7557:0 7558:0 7559:0 7560:0 7561:0 7562:0 7563:0 7564:0 7565:0 7566:0 7567:0 7568:0 7569:0 7570:0 7571:0 7572:0 7573:0 7574:0 7575:0 7576:0 7577:0 7578:0 7579:0 7580:0 7581:0 7582:0 7583:0 7584:0 7585:0 7586:0 7587:0 7588:0 7589:0 7590:0 7591:0 7592:0 7593:0 7594:0 7595:0 7596:0 7597:0 7598:0 7599:0 7600:0 7601:0 7602:0 7603:0 7604:0 7605:0 7606:0 7607:0 7608:0 7609:0 7610:0 7611:0 7612:0 7613:0 7614:0 7615:0 7616:0 7617:0.0373234 7618:0.0466357 7619:0.0399976 7620:0.0481288 7621:0.04051 7622:0.0481738 7623:0.0471525 7624:0.0499 7625:0.0483793 7626:0.0307898 7627:0.0324879 7628:0.0546209 7629:0.0472815 7630:0.0395345 7631:0.0382281 7632:0.0354834 7633:0.0570537 7634:0.0335578 7635:0.0643414 7636:0.08924 7637:0.064073 7638:0.0526392 7639:0.034139 7640:0.0277661 7641:0.0539796 7642:0.0558763 7643:0.0707425 7644:0.0853415 7645:0.063864 7646:0.0748833 7647:0.0390327 7648:0.0461803 7649:0.0475962 7650:0.0854222 7651:0.055073 7652:0.0835824 7653:0.0530123 7654:0.0860129 7655:0.0421533 7656:0.0473119 7657:0.0541565 7658:0.0411447 7659:0.0747708 7660:0.0499054 7661:0.0446129 7662:0.0385619 7663:0.0450844 7664:0.0579206 7665:0.0846054 7666:0.0569133 7667:0.035991 7668:0.0362616 7669:0.0368404 7670:0.0371289 7671:0.0546687 7672:0.0583487 7673:0.0759343 7674:0.0925936 7675:0.0611442 7676:0.0448323 7677:0.0461923 7678:0.0335186 7679:0.0451141 7680:0.0347491 7681:0 7682:0.0196555 7683:0.0148539 7684:0.0135144 7685:0.00853064 7686:0.0142128 7687:0.0281964 7688:0.0296092 7689:0.0216987 7690:0 7691:0 7692:0 7693:0 7694:0 7695:0 7696:0 7697:0.0511384 7698:0 7699:0 7700:0 7701:0.014192 7702:0.00925573 7703:0 7704:0 7705:0.0446676 7706:0 7707:0.00318859 7708:0.00824815 7709:0.0132236 7710:0.0490799 7711:0.0209354 7712:0 7713:0.0330726 7714:0 7715:0.0499718 7716:0 7717:0.0102989 7718:0.0512431 7719:0.0200298 7720:0.00924939 7721:0.0370473 7722:0 7723:6.27674e-05 7724:0.0117827 7725:0 7726:0.0180258 7727:0.00168615 7728:0.0273291 7729:0.0637091 7730:0 7731:0.00196415 7732:0.0319667 7733:0 7734:0 7735:0 7736:0.0495121 7737:0.0881324 7738:0.0343145 7739:0.0127057 7740:0.0166606 7741:0.0192631 7742:0 7743:0 7744:0.0438424 7745:0.00597178 7746:0 7747:0 7748:0 7749:0 7750:0 7751:0 7752:0 7753:0 7754:0 7755:0 7756:0 7757:0 7758:0 7759:0 7760:0 7761:0 7762:0 7763:0 7764:0 7765:0 7766:0 7767:0 7768:0 7769:0 7770:0 7771:0 7772:0 7773:0 7774:0 7775:0 7776:0 7777:0 7778:0 7779:0 7780:0 7781:0 7782:0 7783:0 7784:0 7785:0 7786:0 7787:0 7788:0 7789:0 7790:0 7791:0 7792:0 7793:0 7794:0 7795:0 7796:0 7797:0 7798:0 7799:0 7800:0 7801:0 7802:0 7803:0 7804:0 7805:0 7806:0 7807:0 7808:0 7809:0.105191 7810:0.0886338 7811:0.0945695 7812:0.0906321 7813:0.0958034 7814:0.0971248 7815:0.0972046 7816:0.0910581 7817:0.0951439 7818:0.0771166 7819:0.0657467 7820:0.0710141 7821:0.0876202 7822:0.0680946 7823:0.0799712 7824:0.0718108 7825:0.0884403 7826:0.0746647 7827:0.104593 7828:0.122153 7829:0.100422 7830:0.105166 7831:0.0846037 7832:0.0694746 7833:0.0816323 7834:0.0963478 7835:0.114232 7836:0.134115 7837:0.118515 7838:0.127924 7839:0.0890786 7840:0.0698381 7841:0.0814777 7842:0.126285 7843:0.110689 7844:0.124779 7845:0.0807392 7846:0.109502 7847:0.0826683 7848:0.0678165 7849:0.0878272 7850:0.0727814 7851:0.105253 7852:0.0896073 7853:0.0764378 7854:0.0596482 7855:0.0604917 7856:0.0822714 7857:0.0840406 7858:0.0730602 7859:0.0638687 7860:0.0632185 7861:0.0792775 7862:0.0691773 7863:0.0866207 7864:0.0924087 7865:0.0755014 7866:0.0491315 7867:0.050293 7868:0.0573279 7869:0.0578031 7870:0.0637653 7871:0.0447311 7872:0.0736249 7873:0 7874:0 7875:0 7876:0 7877:0 7878:0 7879:0 7880:0 7881:0 7882:0 7883:0 7884:0 7885:0 7886:0 7887:0 7888:0 7889:0 7890:0 7891:0 7892:0 7893:0 7894:0 7895:0 7896:0 7897:0 7898:0 7899:0 7900:0 7901:0 7902:0 7903:0 7904:0 7905:0 7906:0 7907:0 7908:0 7909:0 7910:0 7911:0 7912:0 7913:0 7914:0 7915:0 7916:0 7917:0 7918:0 7919:0 7920:0 7921:0 7922:0 7923:0 7924:0 7925:0 7926:0 7927:0 7928:0 7929:0 7930:0 7931:0 7932:0 7933:0 7934:0 7935:0 7936:0 7937:0.0170967 7938:0.00819221 7939:0.0177662 7940:0.0152268 7941:0.00736118 7942:0.0146843 7943:0.0276912 7944:0.0582781 7945:0.0108966 7946:0.00745744 7947:0.0111645 7948:0.00151117 7949:0.0327258 7950:0.0183628 7951:0.0181703 7952:0.0664986 7953:0.0177683 7954:0.00807203 7955:0.0101641 7956:0.00520317 7957:0.00955939 7958:0.0258938 7959:0.00825202 7960:0.0645071 7961:0.0280118 7962:0.00234867 7963:0.00264043 7964:0 7965:0.0110416 7966:0.0133975 7967:0.0344434 7968:0.0628102 7969:0.00296966 7970:0 7971:0.0349843 7972:0.0134591 7973:0.0281845 7974:0.0210755 7975:0.0517202 7976:0.0709478 7977:0.0112703 7978:0.0261459 7979:0.0266167 7980:0.0622829 7981:0.0159684 7982:0.0456802 7983:0.0169711 7984:0.0818675 7985:0.0203176 7986:0.00370745 7987:0.0132108 7988:0.0195424 7989:0.0119506 7990:0.0169191 7991:0.00745612 7992:0.0683633 7993:0.00897089 7994:0.0128586 7995:0.0108542 7996:0.0104697 7997:0.00433072 7998:0.00943949 7999:0.0074255 8000:0.0869912 8001:0.00118379 8002:0 8003:0 8004:0 8005:0 8006:0 8007:0 8008:0 8009:0 8010:0 8011:0 8012:0 8013:0 8014:0 8015:0 8016:0 8017:0.00454167 8018:0 8019:0 8020:0.00716358 8021:0 8022:0 8023:0 8024:0 8025:0 8026:0.0162273 8027:0.025273 8028:0.0442158 8029:0.0382454 8030:0.0389346 8031:0 8032:0 8033:0 8034:0.0813993 8035:0.0624803 8036:0.0652878 8037:0.0342014 8038:0.0463896 8039:0 8040:0 8041:0.00015204 8042:0.0586852 8043:0.0285114 8044:0 8045:0.0116801 8046:0.0291598 8047:0 8048:0 8049:0 8050:0 8051:0.00835105 8052:0 8053:0 8054:0 8055:0 8056:0 8057:0 8058:0 8059:0 8060:0 8061:0 8062:0 8063:0.00616556 8064:0 8065:0.100708 8066:0.1462 8067:0.13952 8068:0.144148 8069:0.13821 8070:0.157279 8071:0.157439 8072:0.160896 8073:0.0856684 8074:0.0624105 8075:0.0600151 8076:0.0456425 8077:0.077502 8078:0.0540782 8079:0.0704389 8080:0.0796125 8081:0.0904623 8082:0.0753397 8083:0.118045 8084:0.136205 8085:0.167671 8086:0.162429 8087:0.0933411 8088:0.0787321 8089:0.0846769 8090:0.0770463 8091:0.148537 8092:0.171224 8093:0.202161 8094:0.179523 8095:0.154491 8096:0.0693191 8097:0.0677764 8098:0.152783 8099:0.189608 8100:0.144796 8101:0.110148 8102:0.152648 8103:0.159139 8104:0.058713 8105:0.0773435 8106:0.0736985 8107:0.138259 8108:0.114408 8109:0.0749863 8110:0.0493924 8111:0.0774378 8112:0.085528 8113:0.0724674 8114:0.0675107 8115:0.0717197 8116:0.080646 8117:0.0673259 8118:0.0739298 8119:0.0842226 8120:0.115171 8121:0.0734361 8122:0.0597318 8123:0.0502671 8124:0.0573104 8125:0.0571021 8126:0.0548678 8127:0.0498572 8128:0.0758513 8129:0.0141998 8130:0.0176864 8131:0.0214967 8132:0.0219743 8133:0.0181907 8134:0.0163762 8135:0.0135787 8136:0.0611133 8137:0.00182465 8138:0.00625442 8139:0.0187172 8140:0.0326496 8141:0.0274983 8142:0.0210488 8143:0.00878643 8144:0.0624979 8145:0 8146:0.00471379 8147:0.0252507 8148:0.02014 8149:0 8150:0.00240589 8151:0.0142354 8152:0.0348385 8153:0 8154:0.00878545 8155:0 8156:0 8157:0 8158:0 8159:0.00963096 8160:0.044945 8161:0 8162:0 8163:0 8164:0 8165:0 8166:0 8167:0.0369998 8168:0.05506 8169:0 8170:0 8171:0 8172:0 8173:0.0190225 8174:0 8175:0.0417188 8176:0.0651644 8177:0 8178:0 8179:0 8180:0.00506484 8181:0.00405472 8182:0.00229974 8183:0.00182394 8184:0.0535018 8185:0 8186:0.0141546 8187:0.0188221 8188:0.0137404 8189:0.00853808 8190:0.0157171 8191:0.0258298 8192:0.0767865\n"]}],"source":["!head -1 train_svm.txt"]},{"cell_type":"markdown","source":["## 3) Huấn luyện SVM:"],"metadata":{"id":"2s_PtTW4pF3D"}},{"cell_type":"code","source":["!git clone https://github.com/cjlin1/libsvm.git\n","%cd libsvm\n","!make"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRrNeDzPpJjk","executionInfo":{"status":"ok","timestamp":1766299503541,"user_tz":-420,"elapsed":3468,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"042fe686-783b-4092-8f65-a9e32cc9d803"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'libsvm'...\n","remote: Enumerating objects: 4201, done.\u001b[K\n","remote: Counting objects: 100% (230/230), done.\u001b[K\n","remote: Compressing objects: 100% (112/112), done.\u001b[K\n","remote: Total 4201 (delta 140), reused 118 (delta 118), pack-reused 3971 (from 3)\u001b[K\n","Receiving objects: 100% (4201/4201), 9.92 MiB | 29.52 MiB/s, done.\n","Resolving deltas: 100% (2317/2317), done.\n","/content/libsvm/libsvm\n","g++ -Wall -Wconversion -O3 -fPIC -c svm.cpp\n","g++ -Wall -Wconversion -O3 -fPIC svm-train.c svm.o -o svm-train -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-predict.c svm.o -o svm-predict -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-scale.c -o svm-scale\n"]}]},{"cell_type":"code","source":["!./svm-train -s 0 -t 0 -c 1.0 \\\n","  \"/content/train_svm.txt\" \\\n","  /content/model_ae_svm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1ryjflbpUp3","executionInfo":{"status":"ok","timestamp":1766299527185,"user_tz":-420,"elapsed":22159,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"103a0b28-95c7-4519-f7a5-5cfefca6da93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["..*.*\n","optimization finished, #iter = 799\n","nu = 0.267285\n","obj = -33.717437, rho = -4.130477\n","nSV = 104, nBSV = 24\n","...*.*\n","optimization finished, #iter = 968\n","nu = 0.626444\n","obj = -88.503955, rho = 4.551423\n","nSV = 163, nBSV = 87\n","...*.*\n","optimization finished, #iter = 980\n","nu = 0.292281\n","obj = -36.667731, rho = -4.759696\n","nSV = 126, nBSV = 19\n","...*.*\n","optimization finished, #iter = 931\n","nu = 0.488704\n","obj = -66.294716, rho = 3.793940\n","nSV = 144, nBSV = 60\n","...*.*\n","optimization finished, #iter = 922\n","nu = 0.397528\n","obj = -51.160337, rho = -1.798178\n","nSV = 132, nBSV = 41\n",".*.*\n","optimization finished, #iter = 545\n","nu = 0.170049\n","obj = -21.662690, rho = -1.332393\n","nSV = 72, nBSV = 14\n","...*..*\n","optimization finished, #iter = 1028\n","nu = 0.550654\n","obj = -74.529883, rho = 3.604542\n","nSV = 148, nBSV = 66\n","...*..*\n","optimization finished, #iter = 996\n","nu = 0.476835\n","obj = -58.696541, rho = 0.612176\n","nSV = 146, nBSV = 47\n","..*\n","optimization finished, #iter = 605\n","nu = 0.213555\n","obj = -26.641866, rho = -0.158861\n","nSV = 79, nBSV = 21\n","..*.*\n","optimization finished, #iter = 752\n","nu = 0.254255\n","obj = -31.479992, rho = 5.680196\n","nSV = 98, nBSV = 22\n","....*.*\n","optimization finished, #iter = 1221\n","nu = 0.512563\n","obj = -67.448897, rho = 0.326462\n","nSV = 166, nBSV = 53\n","..*.*\n","optimization finished, #iter = 708\n","nu = 0.239304\n","obj = -29.530081, rho = 4.605639\n","nSV = 97, nBSV = 25\n","...*..*\n","optimization finished, #iter = 1102\n","nu = 0.339086\n","obj = -42.322653, rho = 2.108800\n","nSV = 130, nBSV = 30\n","..*..*\n","optimization finished, #iter = 824\n","nu = 0.331097\n","obj = -41.088246, rho = 3.884969\n","nSV = 116, nBSV = 33\n","...*..*\n","optimization finished, #iter = 1051\n","nu = 0.331700\n","obj = -39.265499, rho = 6.521427\n","nSV = 114, nBSV = 23\n","..*..*\n","optimization finished, #iter = 766\n","nu = 0.254230\n","obj = -28.375997, rho = 4.335140\n","nSV = 99, nBSV = 13\n","..*.*\n","optimization finished, #iter = 792\n","nu = 0.337837\n","obj = -43.064179, rho = 2.424331\n","nSV = 118, nBSV = 33\n","..*.*\n","optimization finished, #iter = 753\n","nu = 0.263790\n","obj = -31.406302, rho = -7.273963\n","nSV = 107, nBSV = 19\n","...*.*\n","optimization finished, #iter = 826\n","nu = 0.651189\n","obj = -95.703737, rho = 0.290629\n","nSV = 164, nBSV = 94\n","...*.*\n","optimization finished, #iter = 892\n","nu = 0.488567\n","obj = -63.444045, rho = -5.964219\n","nSV = 142, nBSV = 60\n",".*.*\n","optimization finished, #iter = 476\n","nu = 0.238612\n","obj = -29.907403, rho = -3.198603\n","nSV = 75, nBSV = 22\n","...*.*\n","optimization finished, #iter = 934\n","nu = 0.515393\n","obj = -65.882648, rho = -0.991731\n","nSV = 138, nBSV = 62\n","...*.*\n","optimization finished, #iter = 851\n","nu = 0.488385\n","obj = -60.279971, rho = -2.376049\n","nSV = 132, nBSV = 53\n","..*.*\n","optimization finished, #iter = 736\n","nu = 0.288471\n","obj = -38.782664, rho = -2.270129\n","nSV = 97, nBSV = 29\n","...*..*\n","optimization finished, #iter = 1062\n","nu = 0.258232\n","obj = -30.304100, rho = 6.206054\n","nSV = 105, nBSV = 16\n","...*.*\n","optimization finished, #iter = 969\n","nu = 0.318520\n","obj = -38.263716, rho = 3.727228\n","nSV = 126, nBSV = 25\n","..*..*\n","optimization finished, #iter = 844\n","nu = 0.305839\n","obj = -38.125060, rho = 4.741228\n","nSV = 111, nBSV = 28\n","...*..*\n","optimization finished, #iter = 1112\n","nu = 0.351980\n","obj = -41.148475, rho = 6.975843\n","nSV = 129, nBSV = 25\n","...*..*\n","optimization finished, #iter = 1010\n","nu = 0.297159\n","obj = -32.054063, rho = 4.677324\n","nSV = 113, nBSV = 13\n","...*.*\n","optimization finished, #iter = 996\n","nu = 0.309283\n","obj = -38.850686, rho = 3.998857\n","nSV = 117, nBSV = 25\n","...*..*\n","optimization finished, #iter = 1039\n","nu = 0.409990\n","obj = -49.958249, rho = -5.186564\n","nSV = 135, nBSV = 42\n","..*\n","optimization finished, #iter = 588\n","nu = 0.279785\n","obj = -34.394516, rho = -3.048724\n","nSV = 86, nBSV = 22\n","....*.*\n","optimization finished, #iter = 1099\n","nu = 0.500011\n","obj = -62.483677, rho = -0.768085\n","nSV = 135, nBSV = 52\n","...*..*\n","optimization finished, #iter = 972\n","nu = 0.505058\n","obj = -62.374989, rho = -2.134637\n","nSV = 132, nBSV = 56\n","...*.*\n","optimization finished, #iter = 897\n","nu = 0.362061\n","obj = -47.713115, rho = -1.227985\n","nSV = 115, nBSV = 38\n","..*.*\n","optimization finished, #iter = 789\n","nu = 0.222862\n","obj = -25.215127, rho = 0.999538\n","nSV = 93, nBSV = 15\n","....*.*\n","optimization finished, #iter = 1148\n","nu = 0.458742\n","obj = -56.010275, rho = 4.681369\n","nSV = 137, nBSV = 44\n","...*.*\n","optimization finished, #iter = 888\n","nu = 0.462410\n","obj = -52.725011, rho = 3.123800\n","nSV = 135, nBSV = 38\n","...*.*\n","optimization finished, #iter = 943\n","nu = 0.330479\n","obj = -41.715402, rho = 1.240479\n","nSV = 114, nBSV = 24\n","..*.*\n","optimization finished, #iter = 722\n","nu = 0.270889\n","obj = -31.314604, rho = 1.555216\n","nSV = 86, nBSV = 19\n","..*.*\n","optimization finished, #iter = 627\n","nu = 0.208483\n","obj = -22.225864, rho = 2.333371\n","nSV = 79, nBSV = 16\n","...*.*\n","optimization finished, #iter = 994\n","nu = 0.533370\n","obj = -75.726842, rho = -0.253453\n","nSV = 149, nBSV = 65\n","....*.*\n","optimization finished, #iter = 1016\n","nu = 0.597996\n","obj = -71.317230, rho = -1.821230\n","nSV = 149, nBSV = 63\n","...*.*\n","optimization finished, #iter = 877\n","nu = 0.327040\n","obj = -38.850683, rho = -0.633323\n","nSV = 100, nBSV = 31\n","..*.*\n","optimization finished, #iter = 732\n","nu = 0.264223\n","obj = -29.530576, rho = -0.840523\n","nSV = 89, nBSV = 23\n","Total nSV = 959\n"]}]},{"cell_type":"code","source":["!./svm-predict \\\n","  \"/content/test_svm.txt\" \\\n","  /content/model_ae_svm \\\n","  \"/content/pred.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUVgSiKppYPq","executionInfo":{"status":"ok","timestamp":1766299541023,"user_tz":-420,"elapsed":4911,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"babb94eb-cf2f-4939-ce21-988c0d791a28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 42% (84/200) (classification)\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ml-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}