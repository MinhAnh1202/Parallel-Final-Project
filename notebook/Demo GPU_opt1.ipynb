{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# GPU Optimization Version 1"],"metadata":{"id":"a24EAxJA_QUB"}},{"cell_type":"code","source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","!tar -xzvf cifar-10-binary.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umGmEDJwuu4F","executionInfo":{"status":"ok","timestamp":1766760784148,"user_tz":-420,"elapsed":17230,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"f81ed34d-9f08-48c0-82ab-eaca6c94a3b1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-26 14:52:46--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  13.9MB/s    in 13s     \n","\n","2025-12-26 14:53:01 (12.1 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n","\n","cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"markdown","source":["## 1) Huấn luyện Autoencoder:"],"metadata":{"id":"_0qiSIz__jl8"}},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mllk3VwATtDf","executionInfo":{"status":"ok","timestamp":1766760784164,"user_tz":-420,"elapsed":13,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"54b0f318-b8be-421f-9914-3b4554e4b60e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.cu\n","#include \"gpu_layers_opt1.h\"\n","\n","// --------------- Conv2D forward (optimization 1) ------------------\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // Load input tile into shared memory\n","        for (int i = ty; i < BLOCK_H; i += blockDim.y) {\n","            for (int j = tx; j < BLOCK_W; j += blockDim.x) {\n","                int h_in = row_start + i;\n","                int w_in = col_start + j;\n","\n","                if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                    size_t input_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                    smem[i][j] = input[input_idx];\n","                } else {\n","                    smem[i][j] = 0.0f;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","\n","        // Compute convolution\n","        for (int i = 0; i < K; ++i) {\n","            for (int j = 0; j < K; ++j) {\n","                size_t weight_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","                value += smem[ty + i][tx + j] * weight[weight_idx];\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    // Shared Memory chứa dY\n","    __shared__ float s_dY[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    // Tọa độ dX (output của kernel này)\n","    int h_out = blockIdx.y * TILE_H + ty;\n","    int w_out = blockIdx.x * TILE_W + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n = nc / C_in;\n","    //if (h_out >= H || w_out >= W || n >= N) return;\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // Tọa độ dY cần tải\n","        int h_global = h_out;\n","        int w_global = w_out;\n","\n","        // Tải main tile\n","        if (h_global >= 0 && h_global < H_out && w_global >= 0 && w_global < W_out) {\n","            s_dY[ty + pad][tx + pad] = dY[idx4(n, c_out, h_global, w_global, C_out, H_out, W_out)];\n","        } else {\n","            s_dY[ty + pad][tx + pad] = 0.0f;\n","        }\n","\n","        // Tải biên trên/dưới\n","        if (ty < pad) {\n","            // Biên trên\n","            int h_top = blockIdx.y * TILE_H + ty - pad;\n","            if (h_top >= 0 && h_top < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[ty][tx + pad] = dY[idx4(n, c_out, h_top, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty][tx + pad] = 0.0f;\n","            }\n","\n","            // Biên dưới\n","            int h_bottom = blockIdx.y * TILE_H + TILE_H + pad - 1 - ty;\n","            if (h_bottom >= 0 && h_bottom < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = dY[idx4(n, c_out, h_bottom, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","            }\n","        }\n","\n","        // Tải biên trái/phải\n","        if (tx < pad) {\n","            // Biên trái\n","            int w_left = blockIdx.x * TILE_W + tx - pad;\n","            if (w_left >= 0 && w_left < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][tx] = dY[idx4(n, c_out, h_global, w_left, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][tx] = 0.0f;\n","            }\n","\n","            // Biên phải\n","            int w_right = blockIdx.x * TILE_W + TILE_W + pad - 1 - tx;\n","            if (w_right >= 0 && w_right < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = dY[idx4(n, c_out, h_global, w_right, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","            }\n","        }\n","\n","        // Tải 4 góc (Thread (0,0) tải)\n","        if (tx == 0 && ty == 0) {\n","            // Góc trên trái [0][0]\n","            int h_c = blockIdx.y * TILE_H - pad;\n","            int w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[0][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc trên phải [0][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới trái [17][0]\n","            h_c = blockIdx.y * TILE_H + TILE_H + pad - 1;\n","            w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới phải [17][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","        }\n","\n","        __syncthreads();\n","\n","        // Tính convolution\n","            for (int kh = 0; kh < K; ++kh) {\n","                for (int kw = 0; kw < K; ++kw) {\n","                    int smem_y = ty + 2 * pad - kh;\n","                    int smem_x = tx + 2* pad - kw;\n","\n","                    size_t w_idx = idx4(c_out, c_in, K - 1 - kh, K - 1 - kw, C_in, K, K);\n","                    value += s_dY[smem_y][smem_x] * weight[w_idx];\n","                }\n","            }\n","        __syncthreads();\n","    }\n","\n","    if (h_out < H && w_out < W && n < N) {\n","        dX[idx4(n, c_in, h_out, w_out, C_in, H, W)] = value;\n","    }\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    float dw[K][K];\n","    for (int i = 0; i < K; ++i)\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // Load s_dY\n","                int h_out = block_h * TILE_H + ty;\n","                int w_out = block_w * TILE_W + tx;\n","\n","                if (h_out < H_out && w_out < W_out) {\n","                    s_dY[ty][tx] = dY[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)];\n","                } else {\n","                    s_dY[ty][tx] = 0.0f;\n","                }\n","\n","                // Load s_in\n","                int h_in_base = block_h * TILE_H + ty;\n","                int w_in_base = block_w * TILE_W + tx;\n","\n","                // Main tile\n","                if (h_in_base >= 0 && h_in_base < H && w_in_base >= 0 && w_in_base < W) {\n","                    s_in[ty + pad][tx + pad] = input[idx4(n, c_in, h_in_base, w_in_base, C_in, H, W)];\n","                } else {\n","                    s_in[ty + pad][tx + pad] = 0.0f;\n","                }\n","\n","                // Top/bottom borders\n","                if (ty < pad) {\n","                    int h_top = block_h * TILE_H - pad + ty;\n","                    if (h_top >= 0 && h_top < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[ty][tx + pad] = input[idx4(n, c_in, h_top, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[ty][tx + pad] = 0.0f;\n","                    }\n","\n","                    int h_bottom = block_h * TILE_H + TILE_H + pad - 1 - ty;\n","                    if (h_bottom >= 0 && h_bottom < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = input[idx4(n, c_in, h_bottom, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","                    }\n","                }\n","\n","                // Left/right borders\n","                if (tx < pad) {\n","                    int w_left = block_w * TILE_W - pad + tx;\n","                    if (w_left >= 0 && w_left < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][tx] = input[idx4(n, c_in, h_in_base, w_left, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][tx] = 0.0f;\n","                    }\n","\n","                    int w_right = block_w * TILE_W + TILE_W + pad - 1 - tx;\n","                    if (w_right >= 0 && w_right < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = input[idx4(n, c_in, h_in_base, w_right, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","                    }\n","                }\n","\n","                // Thread (0,0) loads 4 corners\n","                if (tx == 0 && ty == 0) {\n","                    int h_c = block_h * TILE_H - pad;\n","                    int w_c = block_w * TILE_W - pad;\n","                    s_in[0][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    h_c = block_h * TILE_H + TILE_H + pad - 1;\n","                    w_c = block_w * TILE_W - pad;\n","                    s_in[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","                }\n","\n","                __syncthreads();\n","\n","                // Compute dW: X * dY\n","                float val_dy = s_dY[ty][tx];\n","                for (int kh = 0; kh < K; ++kh) {\n","                    for (int kw = 0; kw < K; ++kw) {\n","                        dw[kh][kw] += s_in[ty + kh][tx + kw] * val_dy;\n","                    }\n","                }\n","\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    for (int i = 0; i < K; ++i) {\n","        for (int j = 0; j < K; ++j) {\n","            // Tính index global của dW[i][j] cho cặp filter (c_out, c_in)\n","            size_t dw_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","            // Cộng giá trị dw[i][j] của thread hiện tại vào bộ nhớ global\n","            atomicAdd(&dW[dw_idx], dw[i][j]);\n","        }\n","    }\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","    extern __shared__ float sdata[];\n","    int tid = threadIdx.x;\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","    float sum = 0.0f;\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","    sdata[tid] = sum;\n","    __syncthreads();\n","\n","    // Reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        dB[c] = sdata[0];\n","    }\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T1wg7t4T7La","executionInfo":{"status":"ok","timestamp":1766760784238,"user_tz":-420,"elapsed":73,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"08c5e9bd-3241-4624-85e2-7e5349f2fdfc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt1.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers_opt1.h\"\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0; ///\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights, biases ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhtU3GMYT-vO","executionInfo":{"status":"ok","timestamp":1766760784248,"user_tz":-420,"elapsed":8,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"606a1068-1c89-48df-e159-94460130fd6a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt1.cu\n","#include \"gpu_autoencoder_opt1.h\"\n","#include <cmath>\n","\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","// find max weight bytes\n","size_t max_w_bytes = w1_bytes;\n","if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","// find max bias bytes\n","size_t max_b_bytes = b1_bytes;\n","if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","float *h_w = (float*)malloc(max_w_bytes);\n","float *h_b = (float*)malloc(max_b_bytes);\n","\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ---------- [batch_size, channels, height, width]\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","\n","// Forward pass for ONE batch (no backward yet)\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ------------- copy input to device -------------\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ========= ENCODER =========\n","    // conv1: 3 -> 256, same 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b1, C_out * sizeof(float)));\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16, then pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b2, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT is ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","    // conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b3, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b3, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv4: 128 -> 256, 16x16, then upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b4, 256 * sizeof(float)));\n","        update_dc_bias(ae->d_b4, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv5: 256 -> 3, 32x32 (no activation, usually MSE on raw output)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b5, 3 * sizeof(float)));\n","        update_dc_bias(ae->d_b5, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","    }\n","\n","    // ------------- (optional) compute MSE loss -------------\n","    float loss_value = 0.0f;\n","        if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        // kernel giờ trả về SUM(diff^2) vào d_loss\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;  // MSE = sum / size\n","    }\n","\n","\n","    // ------------- copy output back to host -------------\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H; // 32\n","    const int W0 = ae->W; // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    //Zero all gradient buffers\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        // dU2\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW5\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_u2, ae->d_gout, ae->d_gw5, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB5\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gout, ae->d_gb5, N, C_out, H, W);\n","\n","        // SGD update W5,B5\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward: U2 <- H4 =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;   // H4 size\n","        int Hu = 32, Wu = 32; // U2 size\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int C = 256, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward: 128->256, 16x16 (input U1, output H4) =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        // dU1\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW4\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_u1, ae->d_gh4, ae->d_gw4, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB4\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh4, ae->d_gb4, N, C_out, H, W);\n","\n","        // SGD W4,B4\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward: U1 <- H3 =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;   // H3 size\n","        int Hu = 16, Wu = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int C = 128, H = 8, W = 8;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward: 128->128, 8x8 (input P2, output H3) =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        // dP2\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW3\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        CHECK_CUDA(cudaMemset(ae->d_gw3, 0, C_out * C_in * K * K * sizeof(float)));\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_p2, ae->d_gh3, ae->d_gw3, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB3\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh3, ae->d_gb3, N, C_out, H, W);\n","\n","        // SGD W3,B3\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward: P2 <- H2 =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16; // H2 size\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int C = 128, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward: 256->128, 16x16 (input P1, output H2) =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        // dP1\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW2\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        CHECK_CUDA(cudaMemset(ae->d_gw2, 0, C_out * C_in * K * K * sizeof(float)));\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_p1, ae->d_gh2, ae->d_gw2, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB2\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh2, ae->d_gb2, N, C_out, H, W);\n","\n","        // SGD W2,B2\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward: P1 <- H1 =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32; // H1 size\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int C = 256, H = 32, W = 32;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward: 3->256, 32x32 (input X0, output H1) =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        // dX0 (không dùng tiếp nhưng tính cho đủ)\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW1\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_x0, ae->d_gh1, ae->d_gw1, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB1\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh1, ae->d_gb1, N, C_out, H, W);\n","\n","        // SGD W1,B1\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // copy input [N_batch, 3, 32, 32] lên GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, rồi ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b1, 256 * sizeof(float)));\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, rồi ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b2, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","\n","         cudaError_t err = cudaGetLastError();\n","   if (err != cudaSuccess) {\n","       fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n","   }\n","   cudaDeviceSynchronize();\n","    }\n","    cudaDeviceSynchronize();\n","\n","size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNI8x13dUDV8","executionInfo":{"status":"ok","timestamp":1766760784284,"user_tz":-420,"elapsed":35,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"99031fbb-0598-4a26-84bb-6763976919fe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvY6SJ9kUFdH","executionInfo":{"status":"ok","timestamp":1766760784298,"user_tz":-420,"elapsed":14,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"59b3e47e-ab13-4604-f95f-1a9ed64cbbb4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.h\n"]}]},{"cell_type":"code","source":["%%writefile load_data.cu\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","    // for (int i = 0; i < 2; i++) {\n","    //     printf(\"Label: %d\\n\", data->test_labels[i]);\n","    //     for (int j = 0; j < IMG_SIZE; j++) {\n","    //         printf(\"%f \", data->test_images[i*IMG_SIZE + j]);\n","    //     }\n","    // }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMx-8h6pUIHS","executionInfo":{"status":"ok","timestamp":1766760784302,"user_tz":-420,"elapsed":3,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"426750c8-d028-44fb-d533-45d1a6ae3ad4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.cu\n"]}]},{"cell_type":"code","source":["%%writefile main_gpu_opt1.cu\n","// the UPDATED main() code with argc/argv\n","// main_gpu.cu (DEBUG VERSION)\n","#include <cstdio>\n","#include <ctime>\n","#include <cuda_runtime.h>\n","#include <cstdlib>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt1.h\"\n","\n","// GpuTimer dùng cudaEvent để đo time\n","struct GpuTimer {\n","    cudaEvent_t start, stop;\n","    GpuTimer() {\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","    }\n","    ~GpuTimer() {\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","    void tic() {\n","        cudaEventRecord(start, 0);\n","    }\n","    float toc() {\n","        cudaEventRecord(stop, 0);\n","        cudaEventSynchronize(stop);\n","        float ms = 0.0f;\n","        cudaEventElapsedTime(&ms, start, stop);\n","        return ms;\n","    }\n","};\n","\n","void print_gpu_memory() {\n","    size_t free_byte, total_byte;\n","    cudaError_t status = cudaMemGetInfo(&free_byte, &total_byte);\n","\n","    if (status == cudaSuccess) {\n","        double total_mb = (double)total_byte / (1024.0 * 1024.0);\n","        double free_mb = (double)free_byte / (1024.0 * 1024.0);\n","        double used_mb = total_mb - free_mb;\n","\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB / Total: %.2f MB\\n\", used_mb, total_mb);\n","    }\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","\n","    printf(\"[MAIN] Start program\\n\");\n","    fflush(stdout);\n","\n","    // ---- Check GPU device ----\n","    int deviceCount = 0;\n","    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n","    if (err != cudaSuccess) {\n","        fprintf(stderr, \"[MAIN] cudaGetDeviceCount error: %s\\n\",\n","                cudaGetErrorString(err));\n","        return 1;\n","    }\n","    printf(\"[MAIN] Num CUDA devices = %d\\n\", deviceCount);\n","    fflush(stdout);\n","\n","    // ---- Load CIFAR-10 on CPU ----\n","    Cifar10 data;\n","    load_cifar10(&data);   // câu lệnh này in: CIFAR-10 loaded successfully ...\n","    printf(\"[MAIN] After load_cifar10\\n\");\n","    fflush(stdout);\n","\n","    normalize_cifar10(&data);\n","    printf(\"[MAIN] After normalize_cifar10\\n\");\n","    fflush(stdout);\n","\n","    GpuTimer epoch_timer;\n","\n","    int batch_size = 64;\n","    int epochs     = 2;\n","    float lr       = 1e-3f;\n","    float total_time = 0.0f;\n","\n","    int num_batches = TRAIN_NUM / batch_size;\n","\n","    printf(\"[MAIN] Start training loop (epochs=%d, num_batches=%d)\\n\",\n","       epochs, num_batches);\n","    fflush(stdout);\n","\n","\n","    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float)); // dùng làm buffer tạm\n","\n","    // ---- Init GPU autoencoder ----\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","\n","\n","    for (int epoch = 0; epoch < epochs; ++epoch) {\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","\n","        epoch_timer.tic();\n","\n","        for (int b = 0; b < num_batches; ++b) {\n","            get_next_batch(&data, batch_size, b, h_batch);\n","\n","            float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n","            gpu_autoencoder_backward(&ae, lr);\n","\n","            epoch_loss += loss;\n","\n","            if ((b + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\",\n","                       epoch, b + 1, num_batches, loss);\n","                fflush(stdout);\n","            }\n","        }\n","\n","        float ms = epoch_timer.toc();\n","        total_time += ms;\n","        printf(\"==> Epoch %d done. Avg loss = %f, time = %.3f ms (%.3f s)\\n\",\n","               epoch, epoch_loss / num_batches, ms, ms / 1000.0f);\n","        fflush(stdout);\n","    }\n","\n","    printf(\"[MAIN] Training finished\\n\");\n","    printf(\"[MAIN] Total training time = %.3f s\\n\", total_time / 1000.0f);\n","    print_gpu_memory();\n","    fflush(stdout);\n","\n","    // save weights\n","    gpu_autoencoder_save_weights(&ae, \"ae_weights.bin\");\n","\n","    // ---- cleanup ----\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_output);\n","    free_cifar10(&data);\n","\n","    printf(\"[MAIN] Program finished\\n\");\n","    fflush(stdout);\n","\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TW_V3KJpUKgN","executionInfo":{"status":"ok","timestamp":1766760784305,"user_tz":-420,"elapsed":3,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"85682ad2-0306-4879-a1c2-b9848f52d1ab"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing main_gpu_opt1.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 main_gpu_opt1.cu gpu_autoencoder_opt1.cu gpu_layers_opt1.cu load_data.cu -o autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xf_iarc9u6YS","executionInfo":{"status":"ok","timestamp":1766760790557,"user_tz":-420,"elapsed":6252,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"08b14add-8555-49f6-b832-8369d5178fd1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(469)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(470)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJtcww_cu90u","outputId":"2c05a462-f471-45c2-e6ab-cd21a0eecfdc","executionInfo":{"status":"ok","timestamp":1766761134939,"user_tz":-420,"elapsed":344379,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=2, num_batches=781)\n","[TRAIN] Epoch 0, batch 100/781, loss = 0.102145\n","[TRAIN] Epoch 0, batch 200/781, loss = 0.054646\n","[TRAIN] Epoch 0, batch 300/781, loss = 0.060306\n","[TRAIN] Epoch 0, batch 400/781, loss = 0.050450\n","[TRAIN] Epoch 0, batch 500/781, loss = 0.054239\n","[TRAIN] Epoch 0, batch 600/781, loss = 0.052507\n","[TRAIN] Epoch 0, batch 700/781, loss = 0.048689\n","==> Epoch 0 done. Avg loss = 0.071280, time = 171121.391 ms (171.121 s)\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.050641\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.044396\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.045168\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.043464\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.051054\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.046129\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.041260\n","==> Epoch 1 done. Avg loss = 0.045326, time = 172036.375 ms (172.036 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 343.158 s\n","[SYSTEM] Memory Usage: 474.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n"]}]},{"cell_type":"markdown","source":["## 2) Trích xuất đặc trưng:"],"metadata":{"id":"woN0TzYz_4Ug"}},{"cell_type":"code","source":["%%writefile extract_svm_features.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cstring>\n","#include <cuda_runtime.h>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt1.h\"\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    if (argc < 2) {\n","        fprintf(stderr,\n","                \"Usage: %s <ae_weights.bin>\\n\",\n","                argv[0]);\n","        return 1;\n","    }\n","    const char* weight_file = argv[1];\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","    gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = 1000; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = 200; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}\n"],"metadata":{"id":"dm0fAEwWdqr8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761134951,"user_tz":-420,"elapsed":8,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"dff7e83e-3fd5-4af6-b7a8-d5631296f8a6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing extract_svm_features.cu\n"]}]},{"cell_type":"code","source":["!nvcc  -arch=sm_75 -O2 -o extract_svm_features \\\n","    extract_svm_features.cu gpu_autoencoder_opt1.cu gpu_layers_opt1.cu load_data.cu \\\n","    -lcudart\n"],"metadata":{"id":"zFmbvO0Ad7r7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761140752,"user_tz":-420,"elapsed":5800,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"07bbde5a-ad55-475d-e0c9-2950022ad15e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(537)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(599)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(469)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(470)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./extract_svm_features ae_weights.bin"],"metadata":{"id":"0YN5zeMtd_6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761145881,"user_tz":-420,"elapsed":5127,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"cee1189d-fb1b-4c9d-8e46-54bf7f132deb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","Loaded weights from ae_weights.bin\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/16 done\n","[SVM][TRAIN] Batch 2/16 done\n","[SVM][TRAIN] Batch 3/16 done\n","[SVM][TRAIN] Batch 4/16 done\n","[SVM][TRAIN] Batch 5/16 done\n","[SVM][TRAIN] Batch 6/16 done\n","[SVM][TRAIN] Batch 7/16 done\n","[SVM][TRAIN] Batch 8/16 done\n","[SVM][TRAIN] Batch 9/16 done\n","[SVM][TRAIN] Batch 10/16 done\n","[SVM][TRAIN] Batch 11/16 done\n","[SVM][TRAIN] Batch 12/16 done\n","[SVM][TRAIN] Batch 13/16 done\n","[SVM][TRAIN] Batch 14/16 done\n","[SVM][TRAIN] Batch 15/16 done\n","[SVM][TRAIN] Batch 16/16 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/4 done\n","[SVM][TEST] Batch 2/4 done\n","[SVM][TEST] Batch 3/4 done\n","[SVM][TEST] Batch 4/4 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}]},{"cell_type":"code","source":["!head -1 train_svm.txt"],"metadata":{"id":"BVgZ8yBYeFLE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761146077,"user_tz":-420,"elapsed":194,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"12380244-03b8-4a59-ac23-341619f4a93c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["6 1:0.0196866 2:0.0535634 3:0.0678234 4:0.0630128 5:0.0640644 6:0.0619083 7:0.0644577 8:0.0959544 9:0.028043 10:0.0344251 11:0.049183 12:0.0472093 13:0.0611862 14:0.0487541 15:0.0447294 16:0.0659535 17:0.0493801 18:0.0559602 19:0.048492 20:0.045703 21:0.0546152 22:0.0633038 23:0.0803384 24:0.0742408 25:0.0558485 26:0.0555493 27:0.0189825 28:0.0572162 29:0.0968591 30:0.0774044 31:0.13804 32:0.075676 33:0.0466125 34:0.0205181 35:0.120275 36:0.097827 37:0.103661 38:0.0697511 39:0.132606 40:0.07878 41:0.039531 42:0.0337577 43:0.0709264 44:0.0837033 45:0.0695927 46:0.0540577 47:0.0832549 48:0.0869404 49:0.0835321 50:0.054734 51:0.0539712 52:0.0706742 53:0.0695794 54:0.0436531 55:0.0442916 56:0.116751 57:0.0737354 58:0.0542295 59:0.0679187 60:0.0503443 61:0.0448665 62:0.0474078 63:0.0368743 64:0.11061 65:0.00182899 66:0 67:0 68:0 69:0 70:0 71:0 72:0 73:0.00193079 74:0 75:0 76:0 77:0 78:0 79:0 80:0 81:0.00911394 82:0 83:0 84:0 85:0 86:0 87:0 88:0 89:0.00907207 90:0 91:0 92:0 93:0 94:0 95:0 96:0 97:0.00781666 98:0 99:0 100:0 101:0 102:0 103:0 104:0 105:0.0104788 106:0 107:0 108:0 109:0 110:0 111:0 112:0 113:0.0333949 114:0 115:0 116:0 117:0 118:0 119:0 120:0 121:0.0354455 122:0 123:0 124:0 125:0 126:0 127:0 128:0 129:0.0770284 130:0.0813109 131:0.0689607 132:0.0651823 133:0.0701793 134:0.0751917 135:0.0735822 136:0.061898 137:0.0800144 138:0.0782499 139:0.0690929 140:0.0695141 141:0.091792 142:0.0906218 143:0.0663961 144:0.0639513 145:0.071015 146:0.0745917 147:0.109275 148:0.133883 149:0.123953 150:0.0960745 151:0.080999 152:0.0702033 153:0.0538747 154:0.104833 155:0.138185 156:0.135339 157:0.102197 158:0.0932469 159:0.0657349 160:0.0726739 161:0.0730083 162:0.136592 163:0.116618 164:0.0877117 165:0.0352063 166:0.083493 167:0.0202407 168:0.0971901 169:0.0817775 170:0.0967743 171:0.062039 172:0.0572731 173:0.0733967 174:0.0596233 175:0.0702154 176:0.119738 177:0.0842591 178:0.0931252 179:0.0762616 180:0.0451793 181:0.0683104 182:0.0806248 183:0.100809 184:0.0754732 185:0.102301 186:0.121628 187:0.0950143 188:0.0801087 189:0.0802375 190:0.0753424 191:0.105881 192:0.0723574 193:0 194:0 195:0.0121365 196:0.00569887 197:0 198:0.00183527 199:0.0100886 200:0.0483831 201:0.00441867 202:0.011077 203:0.0110839 204:0.00497081 205:0.0177368 206:0.00816002 207:0.026163 208:0.034999 209:0.0190543 210:0.00714141 211:0.0258513 212:0.0133349 213:0.0402525 214:0.0532459 215:0.0267937 216:0.0329752 217:0.0278369 218:0.0158519 219:0.0246708 220:0.0153295 221:0.0520609 222:0.0753793 223:0.0727181 224:0.0347182 225:0.0228032 226:0 227:0.0792779 228:0.037091 229:0.0445174 230:0.0594326 231:0.0846698 232:0.0322783 233:0 234:0 235:0.0854818 236:0.0641899 237:0.0398188 238:0.0240201 239:0.0717041 240:0.0359221 241:0.0102663 242:0.000735894 243:0.00513797 244:0.0539951 245:0.0419893 246:0.0239791 247:0.00794324 248:0.0513187 249:0.0486369 250:0.0603443 251:0.049564 252:0.0347022 253:0.0348515 254:0.0377867 255:0.0393569 256:0.100008 257:0.0135152 258:0.0175042 259:0.0164129 260:0.0125005 261:0.0165296 262:0.0374397 263:0.0187136 264:0.0129226 265:0.0035834 266:0.0145119 267:0.00473279 268:0.033674 269:0.0188157 270:0.0285321 271:0.0239591 272:0.00868773 273:0.0190551 274:0.0202291 275:0.0410086 276:0.0406907 277:0.0429583 278:0.0278665 279:0.018102 280:0.0176446 281:0.0169053 282:0.0443169 283:0.0457512 284:0.102997 285:0.0875375 286:0.100501 287:0.056165 288:0.0147557 289:0.0218535 290:0.0875613 291:0.0989878 292:0.0878946 293:0.122817 294:0.144708 295:0.054304 296:0.0246646 297:0.0311248 298:0.104399 299:0.123574 300:0.05039 301:0.0474424 302:0.0590665 303:0.062279 304:0.0734764 305:0.0355634 306:0.0365365 307:0.054486 308:0.0529257 309:0.0470308 310:0.0187349 311:0.0424983 312:0.0286408 313:0.0844119 314:0.0870888 315:0.0764998 316:0.0605086 317:0.0581319 318:0.0533585 319:0.065965 320:0.0538153 321:0 322:0 323:0 324:0 325:0 326:0 327:0 328:0 329:0 330:0 331:0 332:0 333:0 334:0 335:0 336:0 337:0 338:0 339:0 340:0 341:0 342:0 343:0 344:0 345:0 346:0 347:0 348:0 349:0 350:0 351:0 352:0 353:0 354:0 355:0 356:0 357:0 358:0 359:0 360:0 361:0 362:0 363:0 364:0 365:0 366:0 367:0 368:0 369:0 370:0 371:0 372:0 373:0 374:0 375:0 376:0 377:0 378:0 379:0 380:0 381:0 382:0 383:0 384:0 385:0.0471809 386:0.0540561 387:0.0608896 388:0.0610799 389:0.0602962 390:0.0647592 391:0.0660348 392:0.0813914 393:0.00620242 394:0.00645734 395:0.0179122 396:0.0174921 397:0.0293561 398:0.0113972 399:0.0124381 400:0.0417199 401:0.00485499 402:0.00974806 403:0.0320742 404:0.0385449 405:0.0164685 406:0.0309658 407:0.00557164 408:0.0408062 409:0.000646468 410:0.0118467 411:0.0186357 412:0.0507977 413:0.0313225 414:0.0703736 415:0.0406542 416:0.0403954 417:0.00251417 418:0.0119181 419:0.0201165 420:0 421:0 422:0.00213153 423:0.0481457 424:0.0518849 425:0 426:0 427:8.14574e-05 428:0.0334472 429:0.00571637 430:0 431:0.0180551 432:0.0632287 433:0 434:0 435:0 436:0.00132829 437:0.00959579 438:0.00981059 439:0.0124188 440:0.0407854 441:0.00597123 442:0.0169641 443:0.00507194 444:0 445:0.00341559 446:0.00179523 447:0 448:0.0435865 449:0 450:0 451:0 452:0 453:0 454:0 455:0 456:0 457:0 458:0 459:0 460:0 461:0 462:0 463:0 464:0 465:0 466:0 467:0 468:0 469:0 470:0 471:0 472:0 473:0 474:0 475:0 476:0 477:0 478:0 479:0 480:0 481:0 482:0 483:0 484:0 485:0 486:0 487:0 488:0 489:0 490:0 491:0 492:0 493:0 494:0 495:0 496:0 497:0 498:0 499:0 500:0 501:0 502:0 503:0 504:0 505:0 506:0 507:0 508:0 509:0 510:0 511:0 512:0 513:0 514:0 515:0 516:0 517:0 518:0 519:0 520:0 521:0 522:0 523:0 524:0 525:0 526:0 527:0 528:0 529:0 530:0 531:0 532:0 533:0 534:0 535:0 536:0 537:0 538:0 539:0 540:0 541:0 542:0 543:0 544:0 545:0 546:0 547:0 548:0 549:0 550:0 551:0 552:0 553:0 554:0 555:0 556:0 557:0 558:0 559:0 560:0 561:0 562:0 563:0 564:0 565:0 566:0 567:0 568:0 569:0.0559203 570:0.0329736 571:0.0275805 572:0.027614 573:0.0304087 574:0.0274637 575:0.0248328 576:0.0518796 577:0 578:0 579:0 580:0 581:0 582:0 583:0.00339149 584:0.0115567 585:0 586:0 587:0 588:0 589:0 590:0 591:0 592:0 593:0 594:0 595:0 596:0 597:0 598:0 599:0 600:0 601:0 602:0 603:0 604:0 605:0 606:0 607:0 608:0 609:0 610:0 611:0 612:0.058311 613:0.0355767 614:0 615:0 616:0 617:0 618:0.031293 619:0.00134033 620:0.024324 621:0.00176468 622:0.0421776 623:0 624:0.000515897 625:0.00220377 626:0.00128764 627:0.012732 628:0.0157882 629:0 630:0 631:0 632:0.0457041 633:0.035381 634:0.021635 635:0.00707084 636:0 637:0 638:0.000949066 639:0.00871751 640:0.0306202 641:0.0971072 642:0.133701 643:0.138118 644:0.130491 645:0.128806 646:0.14889 647:0.140669 648:0.114938 649:0.1469 650:0.146473 651:0.126686 652:0.142694 653:0.150624 654:0.142892 655:0.144816 656:0.137435 657:0.161589 658:0.147946 659:0.212027 660:0.242848 661:0.256769 662:0.211187 663:0.161786 664:0.135096 665:0.152464 666:0.223472 667:0.266684 668:0.320971 669:0.310692 670:0.31086 671:0.209263 672:0.146612 673:0.166586 674:0.242311 675:0.303288 676:0.278111 677:0.286537 678:0.321631 679:0.247601 680:0.165362 681:0.172108 682:0.218207 683:0.298132 684:0.24483 685:0.19523 686:0.179901 687:0.215066 688:0.186114 689:0.224711 690:0.18854 691:0.188546 692:0.199799 693:0.185971 694:0.178777 695:0.20042 696:0.177106 697:0.252009 698:0.216751 699:0.178249 700:0.171161 701:0.159633 702:0.150115 703:0.17518 704:0.160707 705:0.0556635 706:0.0601851 707:0.0466534 708:0.0517774 709:0.0557102 710:0.0743331 711:0.0660367 712:0.0285107 713:0.0768262 714:0.0746954 715:0.071544 716:0.0644372 717:0.0824321 718:0.0740892 719:0.0856004 720:0.0632836 721:0.0959306 722:0.0782359 723:0.0760201 724:0.148696 725:0.109964 726:0.102984 727:0.0756409 728:0.0672166 729:0.0995084 730:0.108417 731:0.14578 732:0.174956 733:0.17056 734:0.158364 735:0.0925773 736:0.0660688 737:0.0831902 738:0.16972 739:0.14459 740:0.148697 741:0.1253 742:0.146394 743:0.095839 744:0.0553816 745:0.115703 746:0.137901 747:0.137455 748:0.114167 749:0.114335 750:0.113621 751:0.0575841 752:0.0766638 753:0.105926 754:0.10041 755:0.102234 756:0.0807817 757:0.0811634 758:0.0806866 759:0.105999 760:0.102534 761:0.103691 762:0.104551 763:0.0916798 764:0.0841243 765:0.0888898 766:0.0752878 767:0.0822322 768:0.0780142 769:0 770:0 771:0 772:0 773:0 774:0 775:0 776:0 777:0 778:0 779:0 780:0 781:0 782:0 783:0 784:0 785:0 786:0 787:0 788:0 789:0 790:0 791:0 792:0 793:0 794:0 795:0 796:0 797:0 798:0 799:0 800:0 801:0 802:0 803:0 804:0 805:0 806:0 807:0 808:0 809:0 810:0 811:0 812:0 813:0 814:0 815:0 816:0 817:0 818:0 819:0 820:0 821:0 822:0 823:0 824:0 825:0 826:0 827:0 828:0 829:0 830:0 831:0 832:0 833:0.0447144 834:0.0783913 835:0.0878941 836:0.0804394 837:0.0839867 838:0.0963016 839:0.0969177 840:0.0924902 841:0.0696686 842:0.0658368 843:0.0481121 844:0.0465034 845:0.0697058 846:0.0548039 847:0.0698614 848:0.0563851 849:0.0911509 850:0.0794046 851:0.101118 852:0.145715 853:0.151435 854:0.133095 855:0.0760595 856:0.0638741 857:0.0650194 858:0.122329 859:0.160458 860:0.192827 861:0.216058 862:0.182485 863:0.167816 864:0.0541927 865:0.0753475 866:0.139805 867:0.169551 868:0.129928 869:0.117728 870:0.148068 871:0.103589 872:0.0339963 873:0.0759686 874:0.0601109 875:0.111424 876:0.121714 877:0.0810495 878:0.0368339 879:0.0618604 880:0.0499202 881:0.0569779 882:0.0643469 883:0.0618208 884:0.0783863 885:0.0883194 886:0.0686309 887:0.0920054 888:0.0611358 889:0.0598401 890:0.0422401 891:0.0479554 892:0.0564497 893:0.0556961 894:0.0515913 895:0.0337261 896:0.0466674 897:0.032755 898:0.0256098 899:0.00508717 900:0.0132217 901:0.00809829 902:0.0109777 903:0.0110733 904:0.00155818 905:0 906:0 907:0 908:0 909:0 910:0 911:0 912:0 913:0 914:0 915:0 916:0 917:0 918:0 919:0 920:0 921:0 922:0 923:0 924:0 925:0 926:0 927:0 928:0 929:0 930:0 931:0 932:0 933:0 934:0 935:0 936:0 937:0 938:0 939:0 940:0 941:0 942:0 943:0 944:0 945:0 946:0 947:0 948:0 949:0 950:0 951:0 952:0 953:0 954:0 955:0 956:0 957:0 958:0 959:0 960:0 961:0.0405579 962:0.0438762 963:0.0479637 964:0.0425252 965:0.0428143 966:0.0448263 967:0.0512478 968:0.0535631 969:0.0538951 970:0.0475414 971:0.0440747 972:0.0459809 973:0.0653503 974:0.0372558 975:0.0425285 976:0.0428366 977:0.0586443 978:0.0418271 979:0.0547348 980:0.0791436 981:0.0736804 982:0.0771248 983:0.0444147 984:0.0483227 985:0.0587861 986:0.057495 987:0.0619363 988:0.1045 989:0.08631 990:0.0859714 991:0.0699753 992:0.0497546 993:0.0506893 994:0.0568655 995:0.101314 996:0.0489186 997:0.0309833 998:0.0338614 999:0.0448831 1000:0.0574983 1001:0.0475888 1002:0.0565729 1003:0.0741852 1004:0.0592355 1005:0.0402259 1006:0.0322575 1007:0.0229764 1008:0.0717836 1009:0.0741489 1010:0.0647771 1011:0.0378359 1012:0.0392461 1013:0.0355645 1014:0.0411851 1015:0.0382319 1016:0.0882098 1017:0.0522225 1018:0.0561704 1019:0.0422284 1020:0.0276242 1021:0.0353478 1022:0.0297354 1023:0.043477 1024:0.0546748 1025:0.130163 1026:0.138608 1027:0.151338 1028:0.135374 1029:0.137314 1030:0.163313 1031:0.151445 1032:0.13111 1033:0.168462 1034:0.127815 1035:0.123171 1036:0.155123 1037:0.14404 1038:0.151782 1039:0.137781 1040:0.133689 1041:0.192839 1042:0.135046 1043:0.211462 1044:0.244231 1045:0.244559 1046:0.209641 1047:0.153859 1048:0.1452 1049:0.189313 1050:0.254288 1051:0.272117 1052:0.387827 1053:0.351532 1054:0.323138 1055:0.205777 1056:0.145033 1057:0.170087 1058:0.312786 1059:0.293496 1060:0.296466 1061:0.316421 1062:0.366861 1063:0.254039 1064:0.164104 1065:0.169771 1066:0.262914 1067:0.325751 1068:0.270176 1069:0.21742 1070:0.23388 1071:0.201513 1072:0.191906 1073:0.199726 1074:0.17089 1075:0.196637 1076:0.20969 1077:0.176461 1078:0.167977 1079:0.20659 1080:0.194599 1081:0.19747 1082:0.166426 1083:0.156486 1084:0.161827 1085:0.156923 1086:0.126169 1087:0.171044 1088:0.176716 1089:0 1090:0 1091:0 1092:0 1093:0 1094:0 1095:0 1096:0 1097:0 1098:0 1099:0 1100:0 1101:0 1102:0 1103:0 1104:0 1105:0 1106:0 1107:0 1108:0 1109:0 1110:0 1111:0 1112:0 1113:0 1114:0 1115:0 1116:0 1117:0 1118:0 1119:0 1120:0 1121:0 1122:0 1123:0 1124:0 1125:0 1126:0 1127:0 1128:0 1129:0 1130:0 1131:0 1132:0 1133:0 1134:0 1135:0 1136:0 1137:0 1138:0 1139:0 1140:0 1141:0 1142:0 1143:0 1144:0 1145:0 1146:0 1147:0 1148:0 1149:0 1150:0 1151:0 1152:0 1153:0.051798 1154:0.0761069 1155:0.0800416 1156:0.0799267 1157:0.08155 1158:0.0804083 1159:0.0985005 1160:0.0973415 1161:0.10318 1162:0.0926815 1163:0.112605 1164:0.0895961 1165:0.119976 1166:0.0984942 1167:0.113758 1168:0.112835 1169:0.11839 1170:0.123161 1171:0.0908941 1172:0.123093 1173:0.147568 1174:0.141471 1175:0.133873 1176:0.123559 1177:0.115735 1178:0.121736 1179:0.204484 1180:0.162802 1181:0.195546 1182:0.171418 1183:0.2163 1184:0.119589 1185:0.0915529 1186:0.135658 1187:0.178639 1188:0.198739 1189:0.159891 1190:0.178689 1191:0.201365 1192:0.128209 1193:0.114331 1194:0.134986 1195:0.154852 1196:0.186519 1197:0.137068 1198:0.15061 1199:0.121341 1200:0.17721 1201:0.135095 1202:0.114247 1203:0.126589 1204:0.127053 1205:0.130211 1206:0.107336 1207:0.12159 1208:0.178135 1209:0.160911 1210:0.155826 1211:0.14355 1212:0.12592 1213:0.124665 1214:0.125506 1215:0.133318 1216:0.158634 1217:0.0328421 1218:0.0021511 1219:0.00169389 1220:0 1221:0.0089141 1222:0.00678408 1223:0 1224:0 1225:0.0109413 1226:0.0144828 1227:0.0184645 1228:0.0294731 1229:0.0328383 1230:0.0115561 1231:0.0260833 1232:0.0159453 1233:0.0192539 1234:0.0111158 1235:0.0496156 1236:0.0507791 1237:0.0272265 1238:0.0356735 1239:0.0132956 1240:0.00636833 1241:0.0147627 1242:0.0599093 1243:0.0120366 1244:0 1245:0.00869212 1246:0.0286089 1247:0.0148848 1248:0.0193152 1249:0.0247175 1250:0.0570635 1251:0 1252:0.0152598 1253:0.0271982 1254:0.0203767 1255:0.0409244 1256:0.0288154 1257:0.00780997 1258:0.0253301 1259:0.0512843 1260:0.0421747 1261:0.0258031 1262:0.0436652 1263:0.0371698 1264:0.0120666 1265:0.0165127 1266:0.0185628 1267:0.0136626 1268:0.0239722 1269:0.0228749 1270:0.0237703 1271:0.0429001 1272:0 1273:0.0219015 1274:0.0355583 1275:0.0354744 1276:0.0277323 1277:0.0193721 1278:0.0185483 1279:0.0190977 1280:0.0251735 1281:0.112164 1282:0.0679649 1283:0.0553306 1284:0.0587852 1285:0.0539144 1286:0.0608449 1287:0.0411379 1288:0.0388225 1289:0.114201 1290:0.0483546 1291:0.0505951 1292:0.0866209 1293:0.0629474 1294:0.0681528 1295:0.0769066 1296:0.0466899 1297:0.114058 1298:0.0521725 1299:0.0724122 1300:0.0743555 1301:0.0607766 1302:0.0620232 1303:0.0641777 1304:0.0554609 1305:0.110566 1306:0.063877 1307:0.0990783 1308:0.0862988 1309:0.0484843 1310:0.0781099 1311:0.0227531 1312:0.0529206 1313:0.101695 1314:0.081741 1315:0.0490751 1316:0.0419906 1317:0.0687835 1318:0.116116 1319:0.0345948 1320:0.0681454 1321:0.108381 1322:0.0633793 1323:0.0962445 1324:0.0608734 1325:0.0737748 1326:0.0914548 1327:0.10117 1328:0.0639593 1329:0.135108 1330:0.0636115 1331:0.0713804 1332:0.0815113 1333:0.0737693 1334:0.0659057 1335:0.0659946 1336:0.0806919 1337:0.131289 1338:0.0521079 1339:0.0475099 1340:0.0544868 1341:0.0491213 1342:0.0386336 1343:0.051749 1344:0.0489254 1345:0 1346:0 1347:0 1348:0 1349:0 1350:0 1351:0 1352:0 1353:0.025095 1354:0 1355:0 1356:0 1357:0 1358:0 1359:0 1360:0 1361:0.0377576 1362:0 1363:0 1364:0.00967326 1365:0.00725412 1366:0 1367:0 1368:0 1369:0.0318196 1370:0 1371:0.0338167 1372:0.0563581 1373:0.0449781 1374:0.0369516 1375:0 1376:0 1377:0.0194795 1378:0.0643529 1379:0.0490887 1380:0.0202847 1381:0 1382:0.0471751 1383:0 1384:0 1385:0.0167013 1386:0.00433984 1387:0.00550647 1388:0.0104341 1389:0 1390:0 1391:0 1392:0 1393:0.048973 1394:0 1395:0 1396:0 1397:0 1398:0 1399:0 1400:0 1401:0.0722162 1402:0.00163639 1403:0 1404:0 1405:0 1406:0 1407:0 1408:0.0103719 1409:0.171022 1410:0.224851 1411:0.224879 1412:0.230232 1413:0.225513 1414:0.233828 1415:0.251999 1416:0.257872 1417:0.243066 1418:0.242413 1419:0.227811 1420:0.229497 1421:0.235651 1422:0.245622 1423:0.244172 1424:0.243241 1425:0.3067 1426:0.266416 1427:0.261938 1428:0.295871 1429:0.377739 1430:0.339039 1431:0.289188 1432:0.252565 1433:0.297542 1434:0.266307 1435:0.375117 1436:0.45162 1437:0.505906 1438:0.484507 1439:0.409963 1440:0.270317 1441:0.289296 1442:0.37139 1443:0.49521 1444:0.47419 1445:0.473 1446:0.466384 1447:0.467424 1448:0.264518 1449:0.285471 1450:0.352302 1451:0.446942 1452:0.390999 1453:0.331898 1454:0.360047 1455:0.361087 1456:0.269677 1457:0.311206 1458:0.282456 1459:0.33948 1460:0.351045 1461:0.296474 1462:0.268891 1463:0.296012 1464:0.292616 1465:0.342901 1466:0.269029 1467:0.287025 1468:0.26953 1469:0.265718 1470:0.261649 1471:0.274415 1472:0.268254 1473:0.0313154 1474:0.00887591 1475:0.00304849 1476:0 1477:0.00436192 1478:0.00449014 1479:0 1480:0 1481:0.00979603 1482:0 1483:0 1484:0.0184115 1485:0 1486:0 1487:0 1488:0 1489:0.00477735 1490:0 1491:0 1492:0 1493:0 1494:0 1495:0 1496:0 1497:0.00788595 1498:0 1499:0 1500:0 1501:0 1502:0 1503:0 1504:0 1505:0.00928714 1506:0 1507:0 1508:0 1509:0 1510:0 1511:0 1512:0 1513:0.0137851 1514:0.0114188 1515:0 1516:0 1517:0 1518:0.00245141 1519:0 1520:0 1521:0 1522:0 1523:0 1524:0 1525:0 1526:0 1527:0 1528:0 1529:0.00649576 1530:0 1531:0 1532:0 1533:0 1534:0 1535:0 1536:0.0122795 1537:0.035312 1538:0.0187136 1539:0.0121376 1540:0.00784081 1541:0.0126132 1542:0.0126224 1543:0.00656467 1544:0.00165128 1545:0.0294869 1546:0 1547:0 1548:0.0019484 1549:0.000523085 1550:0 1551:0 1552:0.00537441 1553:0 1554:0 1555:0.016473 1556:0.0329654 1557:0.0176556 1558:0.0166462 1559:0 1560:0 1561:0 1562:0.0445175 1563:0.0353118 1564:0.0110072 1565:0.00354548 1566:0 1567:0 1568:0 1569:0.00346319 1570:0.0291178 1571:0.0164552 1572:0 1573:0 1574:0.0103675 1575:0 1576:0.0312037 1577:0.0150654 1578:0.0228327 1579:0 1580:0 1581:0.0165397 1582:0.013075 1583:0.0239185 1584:0.0456715 1585:0.00297829 1586:0.00567675 1587:0 1588:0.00584815 1589:0 1590:0 1591:0.0342579 1592:0.00833696 1593:0 1594:0 1595:0 1596:0 1597:0 1598:0 1599:0.0385404 1600:0 1601:0 1602:0.0207653 1603:0.0197183 1604:0.027715 1605:0.0138208 1606:0.0368749 1607:0.0280033 1608:0.053541 1609:0.00871516 1610:0.000376722 1611:0.0210375 1612:0.0218882 1613:0.0113851 1614:0.015119 1615:0.0235801 1616:0.0391993 1617:0.0455499 1618:0.00767002 1619:0.0225819 1620:0 1621:0.00202188 1622:0.0102417 1623:0.0111292 1624:0.0537695 1625:0.0510685 1626:0.0103268 1627:0.0315197 1628:0.0383617 1629:0.0575171 1630:0.0500073 1631:0.0566069 1632:0.0555883 1633:0.0314106 1634:0.0257069 1635:0.0926985 1636:0.086925 1637:0.10432 1638:0.0817456 1639:0.106173 1640:0.0653809 1641:0.0192802 1642:0.0585904 1643:0.0908925 1644:0.0634188 1645:0.0325004 1646:0.0756036 1647:0.0718925 1648:0.0822017 1649:0.0434722 1650:0.0136402 1651:0.0400469 1652:0.0578988 1653:0.017969 1654:0.00259736 1655:0.00221376 1656:0.089388 1657:0.0696366 1658:0.0470969 1659:0.0358672 1660:0.0373068 1661:0.0418889 1662:0.045033 1663:0.0308153 1664:0.0915599 1665:0.0528708 1666:0.104688 1667:0.103585 1668:0.0992059 1669:0.0955725 1670:0.11177 1671:0.130537 1672:0.126176 1673:0.10698 1674:0.0988607 1675:0.0903611 1676:0.0807688 1677:0.11404 1678:0.10229 1679:0.104197 1680:0.118369 1681:0.136838 1682:0.128482 1683:0.100433 1684:0.14505 1685:0.197704 1686:0.175921 1687:0.152756 1688:0.138264 1689:0.128127 1690:0.120631 1691:0.203471 1692:0.253379 1693:0.317819 1694:0.297202 1695:0.28227 1696:0.138649 1697:0.116761 1698:0.180169 1699:0.301451 1700:0.298044 1701:0.321379 1702:0.278318 1703:0.303551 1704:0.13694 1705:0.115995 1706:0.200029 1707:0.251958 1708:0.228705 1709:0.159084 1710:0.211421 1711:0.169869 1712:0.163646 1713:0.145453 1714:0.132894 1715:0.178782 1716:0.19629 1717:0.150292 1718:0.125952 1719:0.121509 1720:0.204797 1721:0.151366 1722:0.151416 1723:0.142442 1724:0.11909 1725:0.11415 1726:0.123037 1727:0.116399 1728:0.182961 1729:0 1730:0.0200638 1731:0.0224263 1732:0.0252728 1733:0.0251337 1734:0.0287984 1735:0.0307223 1736:0.0255895 1737:0.0135879 1738:0.00881796 1739:0 1740:0 1741:0.0206094 1742:0 1743:0.0160518 1744:0 1745:0.0310375 1746:0.0201664 1747:0.0418168 1748:0.0389431 1749:0.0399652 1750:0.05219 1751:0.0126226 1752:0.0111321 1753:0.0257672 1754:0.0270568 1755:0.0440875 1756:0.0754261 1757:0.0801592 1758:0.0704722 1759:0.0888809 1760:0.0144015 1761:0.0197906 1762:0.0172458 1763:0.0531155 1764:0.0569421 1765:0.0583701 1766:0.0460207 1767:0.0677503 1768:0.00814496 1769:0.0346521 1770:0.0243268 1771:0.0472182 1772:0.0514854 1773:0.0281583 1774:0.0162743 1775:0.0316358 1776:0.0272141 1777:0.0741692 1778:0.0257679 1779:0.0232516 1780:0.0275001 1781:0.0367315 1782:0.0230566 1783:0.0154709 1784:0.0175992 1785:0.077174 1786:0.07098 1787:0.0518039 1788:0.0354581 1789:0.0260831 1790:0.0337576 1791:0.0125987 1792:0.00471317 1793:0.0894333 1794:0.115201 1795:0.113771 1796:0.119416 1797:0.112561 1798:0.115627 1799:0.129232 1800:0.131869 1801:0.103832 1802:0.0884538 1803:0.0929543 1804:0.0870256 1805:0.102342 1806:0.0861928 1807:0.0885157 1808:0.089083 1809:0.110382 1810:0.0946038 1811:0.0997294 1812:0.120407 1813:0.128059 1814:0.139246 1815:0.0857941 1816:0.105382 1817:0.106504 1818:0.0860282 1819:0.154728 1820:0.152473 1821:0.142496 1822:0.171993 1823:0.10129 1824:0.107124 1825:0.104712 1826:0.168145 1827:0.145621 1828:0.120746 1829:0.113937 1830:0.155995 1831:0.095994 1832:0.124934 1833:0.111335 1834:0.126276 1835:0.115156 1836:0.115806 1837:0.0947094 1838:0.103969 1839:0.0861593 1840:0.152961 1841:0.122204 1842:0.0919887 1843:0.0963256 1844:0.10181 1845:0.091969 1846:0.101233 1847:0.120646 1848:0.168682 1849:0.137743 1850:0.110182 1851:0.0873027 1852:0.0813399 1853:0.0958025 1854:0.0916474 1855:0.0977971 1856:0.118649 1857:0.0595345 1858:0.071759 1859:0.0652941 1860:0.0647469 1861:0.0703793 1862:0.0719507 1863:0.0635713 1864:0.0615282 1865:0.0160055 1866:0 1867:0 1868:0 1869:0.0207071 1870:0 1871:0 1872:0.0522471 1873:0 1874:0 1875:0.0429384 1876:0.0384762 1877:0 1878:0.00128372 1879:0 1880:0.0357901 1881:0 1882:0.0226332 1883:0 1884:0 1885:0 1886:0 1887:0 1888:0.0271367 1889:0 1890:0.0128746 1891:0 1892:0 1893:0 1894:0 1895:0 1896:0.0341514 1897:0 1898:0 1899:0 1900:0 1901:0 1902:0 1903:0.0147444 1904:0.0372969 1905:0 1906:0 1907:0 1908:0 1909:0 1910:0 1911:0.0128539 1912:0.0412047 1913:0 1914:0.0091365 1915:0 1916:0 1917:0 1918:0 1919:0 1920:0.0443069 1921:0.0945447 1922:0.113065 1923:0.115913 1924:0.11871 1925:0.115451 1926:0.138485 1927:0.131614 1928:0.138404 1929:0.132512 1930:0.135243 1931:0.136037 1932:0.126734 1933:0.155661 1934:0.152653 1935:0.165163 1936:0.128489 1937:0.153069 1938:0.150084 1939:0.149431 1940:0.186239 1941:0.188273 1942:0.218674 1943:0.156543 1944:0.162338 1945:0.166301 1946:0.148782 1947:0.208469 1948:0.303168 1949:0.327605 1950:0.302916 1951:0.223186 1952:0.158715 1953:0.157432 1954:0.270801 1955:0.272608 1956:0.302563 1957:0.301284 1958:0.318312 1959:0.260161 1960:0.147162 1961:0.134867 1962:0.240745 1963:0.259918 1964:0.246835 1965:0.180332 1966:0.230731 1967:0.192675 1968:0.160661 1969:0.137273 1970:0.171567 1971:0.226867 1972:0.223239 1973:0.17839 1974:0.152571 1975:0.155584 1976:0.22083 1977:0.232937 1978:0.225411 1979:0.189719 1980:0.181151 1981:0.180813 1982:0.170093 1983:0.184033 1984:0.195401 1985:0 1986:0 1987:0 1988:0 1989:0 1990:0 1991:0 1992:0 1993:0 1994:0 1995:0 1996:0 1997:0 1998:0 1999:0 2000:0 2001:0 2002:0 2003:0 2004:0 2005:0 2006:0 2007:0 2008:0 2009:0 2010:0 2011:0 2012:0 2013:0 2014:0 2015:0 2016:0 2017:0 2018:0 2019:0 2020:0 2021:0 2022:0 2023:0 2024:0 2025:0 2026:0 2027:0 2028:0 2029:0 2030:0 2031:0 2032:0 2033:0 2034:0 2035:0 2036:0 2037:0 2038:0 2039:0 2040:0 2041:0 2042:0 2043:0 2044:0 2045:0 2046:0 2047:0 2048:0 2049:0 2050:0 2051:0 2052:0 2053:0 2054:0 2055:0 2056:0 2057:0 2058:0 2059:0 2060:0 2061:0 2062:0 2063:0 2064:0 2065:0 2066:0 2067:0 2068:0 2069:0 2070:0 2071:0 2072:0 2073:0 2074:0 2075:0 2076:0 2077:0 2078:0 2079:0 2080:0 2081:0 2082:0 2083:0 2084:0 2085:0 2086:0 2087:0 2088:0 2089:0 2090:0 2091:0 2092:0 2093:0 2094:0 2095:0 2096:0 2097:0 2098:0 2099:0 2100:0 2101:0 2102:0 2103:0 2104:0 2105:0 2106:0 2107:0 2108:0 2109:0 2110:0 2111:0 2112:0 2113:0 2114:0 2115:0 2116:0 2117:0 2118:0 2119:0 2120:0 2121:0 2122:0 2123:0 2124:0 2125:0 2126:0 2127:0 2128:0 2129:0 2130:0 2131:0 2132:0 2133:0 2134:0 2135:0 2136:0 2137:0.00227415 2138:0 2139:0 2140:0 2141:0 2142:0 2143:0 2144:0 2145:0 2146:0 2147:0 2148:0 2149:0 2150:0 2151:0 2152:0 2153:0 2154:0 2155:0 2156:0 2157:0 2158:0 2159:0 2160:0 2161:0 2162:0 2163:0 2164:0 2165:0 2166:0 2167:0 2168:0 2169:0.00477181 2170:0 2171:0 2172:0 2173:0 2174:0 2175:0 2176:0 2177:0.0165253 2178:0.0182537 2179:0.0223294 2180:0.0314792 2181:0.0347768 2182:0.0300444 2183:0.0254081 2184:0.0366408 2185:0.0103349 2186:0.0126047 2187:0.021317 2188:0.0415621 2189:0.0375407 2190:0.0417186 2191:0.0138722 2192:0.00550386 2193:0.0145084 2194:0.0333554 2195:0.0290298 2196:0.0127908 2197:0.000608117 2198:0.0133694 2199:0.0201515 2200:0.0128771 2201:0.0161589 2202:0.0384693 2203:0.030493 2204:0.0061391 2205:0.0132397 2206:0.0216631 2207:0.00815794 2208:0.0133773 2209:0.0389214 2210:0.0235991 2211:0.02726 2212:0.0444531 2213:0.0745036 2214:0.0439848 2215:0.023866 2216:0.0261992 2217:0.030251 2218:0.0558636 2219:0.0480733 2220:0.0171611 2221:0.0373111 2222:0.0561464 2223:0.0471602 2224:0.027274 2225:0.0135897 2226:0.025033 2227:0.0406741 2228:0.0370064 2229:0.022894 2230:0.0326937 2231:0.047092 2232:0.013952 2233:0.100476 2234:0.0832861 2235:0.0550798 2236:0.0571992 2237:0.0532822 2238:0.0486888 2239:0.0565137 2240:0.0714818 2241:0.0309089 2242:0.00749622 2243:0.00817007 2244:0.00705122 2245:0.00885796 2246:0.0114913 2247:0.00525313 2248:0.0125227 2249:0.026139 2250:0 2251:0 2252:0 2253:0.00218666 2254:0.00638843 2255:0 2256:0 2257:0.0242493 2258:0 2259:0.00610336 2260:0.036579 2261:0.0267333 2262:0.00784881 2263:0 2264:0.00182789 2265:0.0158135 2266:0.0310063 2267:0.0658453 2268:0.0572512 2269:0.0412416 2270:0.0181434 2271:0.00179124 2272:0 2273:0.0135761 2274:0.0517802 2275:0.0505671 2276:0.0133396 2277:0 2278:0.0145181 2279:0 2280:0.000803573 2281:0.0172467 2282:0 2283:0 2284:0 2285:0 2286:0 2287:0 2288:0.00839501 2289:0.00632812 2290:0 2291:0 2292:0 2293:0.00105091 2294:0 2295:0 2296:0.00297028 2297:0.0223689 2298:0 2299:0 2300:0 2301:0 2302:0 2303:0.0039413 2304:0 2305:0 2306:0 2307:0 2308:0 2309:0 2310:0 2311:0 2312:0 2313:0.0143051 2314:0 2315:0 2316:0 2317:0 2318:0 2319:0 2320:0 2321:0.0251881 2322:0 2323:0 2324:0 2325:0 2326:0 2327:0 2328:0 2329:0.0268822 2330:0 2331:0 2332:0 2333:0 2334:0 2335:0 2336:0 2337:0.022302 2338:0 2339:0 2340:0 2341:0 2342:0 2343:0 2344:0 2345:0.0164188 2346:0 2347:0 2348:0 2349:0 2350:0 2351:0 2352:0 2353:0.0291527 2354:0 2355:0 2356:0 2357:0 2358:0 2359:0 2360:0 2361:0.0459265 2362:0 2363:0 2364:0 2365:0 2366:0 2367:0 2368:0 2369:0.0771654 2370:0.0727132 2371:0.0682957 2372:0.066433 2373:0.0640218 2374:0.0692643 2375:0.0625373 2376:0.0710621 2377:0.0762793 2378:0.0667122 2379:0.0720572 2380:0.0568798 2381:0.0891954 2382:0.0547411 2383:0.0659408 2384:0.0748271 2385:0.088077 2386:0.0522255 2387:0.0704263 2388:0.082039 2389:0.0709385 2390:0.0628762 2391:0.059529 2392:0.0723361 2393:0.0899821 2394:0.0685708 2395:0.0768214 2396:0.0402469 2397:0.032831 2398:0.0351755 2399:0.0344833 2400:0.0805677 2401:0.0807298 2402:0.0659652 2403:0.0345308 2404:0.0175986 2405:0.0262353 2406:0.014905 2407:0.0425153 2408:0.099739 2409:0.0901159 2410:0.0660701 2411:0.0580415 2412:0.0322914 2413:0.0648735 2414:0.0425577 2415:0.0705611 2416:0.102665 2417:0.116165 2418:0.0787403 2419:0.0547233 2420:0.0481575 2421:0.0593416 2422:0.0537777 2423:0.0716448 2424:0.07526 2425:0.0804636 2426:0.0514196 2427:0.0424656 2428:0.0421264 2429:0.0409489 2430:0.0463998 2431:0.0790779 2432:0.0243795 2433:0 2434:0 2435:0.000870375 2436:0 2437:0.000177227 2438:0 2439:0 2440:0.00308671 2441:0 2442:0 2443:0 2444:0 2445:0 2446:0 2447:0 2448:0 2449:0 2450:0 2451:0 2452:0 2453:0 2454:0 2455:0 2456:0 2457:0 2458:0 2459:0 2460:0 2461:0 2462:0 2463:0 2464:0 2465:0 2466:0 2467:0 2468:0 2469:0 2470:0 2471:0 2472:0 2473:0 2474:0 2475:0 2476:0 2477:0 2478:0 2479:0 2480:0 2481:0 2482:0 2483:0 2484:0 2485:0 2486:0 2487:0 2488:0 2489:0 2490:0.000460315 2491:0.00730837 2492:0.00107284 2493:0 2494:0.000578959 2495:0.0190219 2496:0 2497:0.0358324 2498:0.0340822 2499:0.00683111 2500:0.0114304 2501:0.0129876 2502:0.0152151 2503:0.0117011 2504:0.041935 2505:0.0483545 2506:0.0313017 2507:0.015507 2508:0.0114994 2509:0.0370394 2510:0.00294507 2511:0.0093808 2512:0.0136509 2513:0.0359218 2514:0.0111986 2515:0.00772317 2516:0.0608336 2517:0.0422433 2518:0.0261922 2519:0.0158424 2520:0.0115801 2521:0.0247699 2522:0.00154132 2523:0.0630479 2524:0.0393501 2525:0.0355151 2526:0.0147899 2527:0.0252148 2528:0.00392919 2529:0.0212617 2530:0.0254403 2531:0.035283 2532:0.0159199 2533:0 2534:0 2535:0 2536:0.00401401 2537:0.0242755 2538:0.0225372 2539:0 2540:0 2541:0.00328512 2542:0.000237122 2543:0 2544:0.0276025 2545:0.0288117 2546:0.00905475 2547:0.000661973 2548:0 2549:0 2550:0.00296997 2551:0.0160867 2552:0.019358 2553:0.0193741 2554:0.0128849 2555:0.0082174 2556:0.00923181 2557:0.00609117 2558:0.00723061 2559:0.00937412 2560:0.0120251 2561:0.0664743 2562:0.0689163 2563:0.0737651 2564:0.0815277 2565:0.0716412 2566:0.0668437 2567:0.0751534 2568:0.0882843 2569:0.0586643 2570:0.0659187 2571:0.0714896 2572:0.0787667 2573:0.0899194 2574:0.106338 2575:0.0858853 2576:0.0790323 2577:0.0820696 2578:0.0817949 2579:0.0746372 2580:0.0646482 2581:0.0798778 2582:0.0653061 2583:0.070248 2584:0.0799564 2585:0.0804314 2586:0.0781169 2587:0.0705056 2588:0.0841611 2589:0.0842676 2590:0.0691148 2591:0.0782372 2592:0.0794605 2593:0.0804899 2594:0.0712424 2595:0.0872965 2596:0.0754549 2597:0.082128 2598:0.0892275 2599:0.065995 2600:0.0807133 2601:0.0776626 2602:0.118372 2603:0.0919894 2604:0.0694981 2605:0.0563972 2606:0.0970837 2607:0.0786794 2608:0.0918256 2609:0.0675808 2610:0.0710266 2611:0.0754401 2612:0.0854975 2613:0.0707836 2614:0.0708909 2615:0.0752187 2616:0.0634632 2617:0.0803783 2618:0.110226 2619:0.108881 2620:0.108774 2621:0.106514 2622:0.0978353 2623:0.0934643 2624:0.0879324 2625:0 2626:0 2627:0 2628:0 2629:0 2630:0 2631:0 2632:0 2633:0 2634:0 2635:0 2636:0 2637:0 2638:0 2639:0 2640:0 2641:0 2642:0 2643:0 2644:0 2645:0 2646:0 2647:0 2648:0 2649:0 2650:0 2651:0 2652:0 2653:0 2654:0 2655:0 2656:0 2657:0 2658:0 2659:0 2660:0 2661:0 2662:0 2663:0 2664:0 2665:0 2666:0 2667:0 2668:0 2669:0 2670:0 2671:0 2672:0 2673:0 2674:0 2675:0 2676:0 2677:0 2678:0 2679:0 2680:0 2681:0 2682:0 2683:0 2684:0 2685:0 2686:0 2687:0 2688:0 2689:0 2690:0 2691:0 2692:0 2693:0 2694:0 2695:0 2696:0 2697:0 2698:0 2699:0 2700:0.00289499 2701:0 2702:0 2703:0 2704:0 2705:0 2706:0 2707:0 2708:0 2709:0 2710:0 2711:0 2712:0 2713:0.00579974 2714:0 2715:0 2716:0 2717:0 2718:0 2719:0 2720:0 2721:0.00293401 2722:0 2723:0 2724:0 2725:0 2726:0 2727:0 2728:0 2729:0 2730:0 2731:0 2732:0 2733:0 2734:0 2735:0 2736:0 2737:0.00308751 2738:0 2739:0 2740:0 2741:0 2742:0 2743:0 2744:0 2745:0.0554568 2746:0.00564608 2747:0 2748:0 2749:0.00194645 2750:0.00242733 2751:0.0124213 2752:0 2753:0.0012022 2754:0 2755:0 2756:0 2757:0 2758:0 2759:0 2760:0.00325382 2761:0 2762:0 2763:0 2764:0 2765:0 2766:0 2767:0 2768:0 2769:0.0171193 2770:0 2771:0 2772:0 2773:0 2774:0 2775:0 2776:0 2777:0.0226854 2778:0 2779:0 2780:0 2781:0 2782:0 2783:0 2784:0 2785:0.0224904 2786:0 2787:0 2788:0 2789:0 2790:0 2791:0 2792:0 2793:0.00931246 2794:0.00269981 2795:0 2796:0 2797:0 2798:0.0104765 2799:0 2800:0 2801:0.0128804 2802:0 2803:0 2804:0 2805:0 2806:0 2807:0 2808:0 2809:0.0892211 2810:0.0607985 2811:0.033202 2812:0.0269353 2813:0.0276437 2814:0.0201149 2815:0.0275667 2816:0.0313642 2817:0 2818:0.0128904 2819:0.00634885 2820:0.0051935 2821:0.00500006 2822:0.00629862 2823:0.00564416 2824:0.000215009 2825:0 2826:0.00238393 2827:0.00339868 2828:0 2829:0.0144405 2830:0.00645876 2831:0 2832:0.00984878 2833:0 2834:0.000353079 2835:0 2836:0 2837:0 2838:0 2839:0.00561599 2840:0.00396315 2841:0 2842:0 2843:0 2844:0 2845:0 2846:0 2847:0 2848:0.000519118 2849:0 2850:0.00636394 2851:0 2852:0 2853:0 2854:0 2855:0 2856:0 2857:0 2858:0 2859:0 2860:0 2861:0 2862:0 2863:0 2864:0 2865:0.00369577 2866:0.00382826 2867:0 2868:0 2869:0 2870:0 2871:0 2872:0 2873:0 2874:0 2875:0.00268503 2876:0 2877:0 2878:0 2879:0 2880:0 2881:0.041217 2882:0.0654593 2883:0.0689332 2884:0.0655906 2885:0.048834 2886:0.0658093 2887:0.0790284 2888:0.0728264 2889:0.0727313 2890:0.0676646 2891:0.069863 2892:0.0543186 2893:0.058836 2894:0.0600702 2895:0.0716965 2896:0.0617008 2897:0.08519 2898:0.057919 2899:0.0638577 2900:0.0521482 2901:0.0857353 2902:0.0823955 2903:0.0722202 2904:0.0707153 2905:0.0959103 2906:0.073734 2907:0.0816701 2908:0.040167 2909:0.0511659 2910:0.0629468 2911:0.0741612 2912:0.0720761 2913:0.0864704 2914:0.0110061 2915:0.0820054 2916:0.0812645 2917:0.097882 2918:0.0920097 2919:0.137971 2920:0.0782218 2921:0.0805047 2922:0.10524 2923:0.126709 2924:0.151687 2925:0.117768 2926:0.116692 2927:0.115446 2928:0.0941447 2929:0.0948331 2930:0.0690636 2931:0.0844196 2932:0.121281 2933:0.0785812 2934:0.0769674 2935:0.0615348 2936:0.112968 2937:0.0714202 2938:0.0311455 2939:0.0471495 2940:0.0660826 2941:0.0536425 2942:0.0555501 2943:0.0520987 2944:0.0931801 2945:0.0374655 2946:0.0908479 2947:0.0896725 2948:0.0951056 2949:0.0931228 2950:0.0996148 2951:0.0958428 2952:0.107839 2953:0.0614426 2954:0.0579484 2955:0.0679791 2956:0.0678916 2957:0.0767523 2958:0.0812053 2959:0.0737298 2960:0.0820653 2961:0.100961 2962:0.078832 2963:0.0702215 2964:0.058452 2965:0.064416 2966:0.065095 2967:0.0673925 2968:0.0986485 2969:0.101535 2970:0.0932242 2971:0.11826 2972:0.138422 2973:0.155322 2974:0.12604 2975:0.132423 2976:0.108026 2977:0.0827556 2978:0.147425 2979:0.182541 2980:0.203685 2981:0.203985 2982:0.208548 2983:0.155548 2984:0.119172 2985:0.095663 2986:0.193344 2987:0.18689 2988:0.139759 2989:0.128329 2990:0.17677 2991:0.139998 2992:0.12397 2993:0.111057 2994:0.102212 2995:0.126268 2996:0.140276 2997:0.0985309 2998:0.0899667 2999:0.102489 3000:0.15446 3001:0.152973 3002:0.125014 3003:0.118656 3004:0.105606 3005:0.105931 3006:0.104418 3007:0.118192 3008:0.131393 3009:0.0564502 3010:0.0633037 3011:0.0737944 3012:0.0763935 3013:0.07428 3014:0.0797089 3015:0.0901964 3016:0.0963808 3017:0.0718321 3018:0.0245525 3019:0.0199068 3020:0.0364162 3021:0.022585 3022:0.0289799 3023:0.0110558 3024:0.00773341 3025:0.0737762 3026:0.0255113 3027:0.0373901 3028:0.076702 3029:0.0596071 3030:0.0583426 3031:0.0467647 3032:0.0316849 3033:0.0674913 3034:0.0700208 3035:0.096382 3036:0.128536 3037:0.112629 3038:0.103912 3039:0.0262278 3040:0.0303233 3041:0.0570157 3042:0.140934 3043:0.0879875 3044:0.109359 3045:0.0521359 3046:0.104541 3047:0.00547798 3048:0.0430843 3049:0.0574843 3050:0.0952376 3051:0.0606949 3052:0.0427322 3053:0.0308026 3054:0.0686188 3055:0.00796338 3056:0.0719694 3057:0.0721086 3058:0.048832 3059:0.0545497 3060:0.034181 3061:0.024924 3062:0.0302246 3063:0.0527005 3064:0.0513691 3065:0.0907859 3066:0.0508849 3067:0.0299349 3068:0.0269376 3069:0.0306006 3070:0.0258435 3071:0.0691552 3072:0.0440668 3073:0.158655 3074:0.222433 3075:0.222847 3076:0.226586 3077:0.226518 3078:0.239727 3079:0.249651 3080:0.246101 3081:0.19824 3082:0.213773 3083:0.210766 3084:0.216961 3085:0.216782 3086:0.226747 3087:0.228011 3088:0.224186 3089:0.239964 3090:0.237931 3091:0.220912 3092:0.241634 3093:0.291158 3094:0.261079 3095:0.25159 3096:0.231186 3097:0.253605 3098:0.246632 3099:0.282497 3100:0.374521 3101:0.430271 3102:0.424553 3103:0.31959 3104:0.243859 3105:0.249756 3106:0.315439 3107:0.407851 3108:0.428163 3109:0.475388 3110:0.438192 3111:0.38306 3112:0.252534 3113:0.23818 3114:0.360321 3115:0.384593 3116:0.382463 3117:0.317939 3118:0.381179 3119:0.325089 3120:0.256826 3121:0.256744 3122:0.270215 3123:0.337986 3124:0.336212 3125:0.276184 3126:0.253065 3127:0.252884 3128:0.279529 3129:0.278778 3130:0.246788 3131:0.249131 3132:0.249325 3133:0.246032 3134:0.239854 3135:0.255299 3136:0.250725 3137:0.181615 3138:0.205845 3139:0.204834 3140:0.200111 3141:0.208095 3142:0.212128 3143:0.217149 3144:0.211912 3145:0.185479 3146:0.185058 3147:0.183448 3148:0.225017 3149:0.201313 3150:0.213856 3151:0.211381 3152:0.181335 3153:0.227389 3154:0.195481 3155:0.218592 3156:0.231806 3157:0.233926 3158:0.22067 3159:0.203441 3160:0.2035 3161:0.221461 3162:0.227158 3163:0.269202 3164:0.38809 3165:0.328133 3166:0.318233 3167:0.175698 3168:0.200607 3169:0.210901 3170:0.358615 3171:0.33636 3172:0.36812 3173:0.363256 3174:0.401241 3175:0.221968 3176:0.226647 3177:0.220854 3178:0.376294 3179:0.331559 3180:0.277206 3181:0.283214 3182:0.326935 3183:0.256209 3184:0.275775 3185:0.233504 3186:0.256179 3187:0.31488 3188:0.290676 3189:0.230522 3190:0.220882 3191:0.2553 3192:0.243897 3193:0.263055 3194:0.254658 3195:0.220765 3196:0.215967 3197:0.218886 3198:0.216401 3199:0.288437 3200:0.25873 3201:0 3202:0.0204277 3203:0.00608572 3204:0.00674152 3205:0.00705904 3206:0.00529876 3207:0.00978467 3208:0.0257607 3209:0.0125114 3210:0.0105394 3211:0.0137869 3212:0.0100245 3213:0.0266892 3214:0.0171635 3215:0.0174708 3216:0.0128549 3217:0.0303574 3218:0.0182669 3219:0.0126533 3220:0.0138393 3221:0.0474773 3222:0.0240211 3223:0.0217379 3224:0.0291822 3225:0.0293191 3226:0.0178929 3227:0.0564296 3228:0.0484272 3229:0.0506572 3230:0.0482155 3231:0.0477891 3232:0.0343122 3233:0.028176 3234:0.0285948 3235:0.0597771 3236:0.0823218 3237:0.0762368 3238:0.0636608 3239:0.0602074 3240:0.0478041 3241:0.0320053 3242:0.0769452 3243:0.0658858 3244:0.0370909 3245:0.0369049 3246:0.0844335 3247:0.0497475 3248:0.0374142 3249:0.0541473 3250:0.0360972 3251:0.0505035 3252:0.054706 3253:0.0257125 3254:0.0260533 3255:0.0284722 3256:0.0374094 3257:0.0322306 3258:0.0243832 3259:0.0192135 3260:0.0101683 3261:0.00956162 3262:0.0141438 3263:0.0209099 3264:0.0348396 3265:0.119368 3266:0.102916 3267:0.0824394 3268:0.0832111 3269:0.0852123 3270:0.0863143 3271:0.084547 3272:0.080936 3273:0.0840284 3274:0.0578676 3275:0.0422456 3276:0.0671642 3277:0.0571937 3278:0.0545632 3279:0.0451058 3280:0.0621183 3281:0.0576201 3282:0.044188 3283:0.0988044 3284:0.130996 3285:0.0904331 3286:0.0877389 3287:0.0552002 3288:0.0578913 3289:0.0378584 3290:0.118855 3291:0.0995653 3292:0.110925 3293:0.0682026 3294:0.0759088 3295:0.0573551 3296:0.0457567 3297:0.0593755 3298:0.108356 3299:0.0459242 3300:0 3301:0 3302:0 3303:0.00286606 3304:0.0604411 3305:0.0570489 3306:0.0219968 3307:0 3308:0.0017096 3309:0.0290177 3310:0 3311:0.0677617 3312:0.0798841 3313:0.0517996 3314:0.0251216 3315:0 3316:0 3317:0.0244594 3318:0.0412754 3319:0.0740042 3320:0.0378695 3321:0.0503601 3322:0.0372456 3323:0.0325672 3324:0.0325262 3325:0.0290213 3326:0.0217803 3327:0.053048 3328:0 3329:0.0128163 3330:0.0435929 3331:0.0341637 3332:0.032288 3333:0.0339266 3334:0.0443579 3335:0.0473352 3336:0.0387445 3337:0.0442846 3338:0.050453 3339:0.0449001 3340:0.0538303 3341:0.0677088 3342:0.0628406 3343:0.0518878 3344:0.0542464 3345:0.0453882 3346:0.0593505 3347:0.05885 3348:0.0461709 3349:0.0699156 3350:0.068174 3351:0.0733169 3352:0.0513395 3353:0.04374 3354:0.0586822 3355:0.0757796 3356:0.119298 3357:0.0935771 3358:0.102125 3359:0.0764149 3360:0.0681576 3361:0.0558398 3362:0.059412 3363:0.106545 3364:0.137079 3365:0.135693 3366:0.0869406 3367:0.0732587 3368:0.061354 3369:0.0613981 3370:0.104072 3371:0.104792 3372:0.0841153 3373:0.0798204 3374:0.114848 3375:0.0626238 3376:0.061631 3377:0.0735735 3378:0.0746132 3379:0.0885579 3380:0.0823019 3381:0.0611873 3382:0.0540612 3383:0.0548919 3384:0.0526292 3385:0.0383974 3386:0.0646565 3387:0.0768563 3388:0.0639015 3389:0.055142 3390:0.0583414 3391:0.0684202 3392:0.0672775 3393:0.0760391 3394:0.0870104 3395:0.0879557 3396:0.0894154 3397:0.092515 3398:0.0974544 3399:0.0942796 3400:0.0883188 3401:0.104737 3402:0.0796821 3403:0.0882557 3404:0.0631644 3405:0.0784974 3406:0.0763438 3407:0.0808106 3408:0.0716843 3409:0.13763 3410:0.0914776 3411:0.0788888 3412:0.128055 3413:0.156292 3414:0.144263 3415:0.109506 3416:0.0851655 3417:0.133648 3418:0.0866503 3419:0.154826 3420:0.204723 3421:0.215233 3422:0.202357 3423:0.166738 3424:0.0883479 3425:0.123413 3426:0.144708 3427:0.185536 3428:0.198872 3429:0.185251 3430:0.206491 3431:0.17633 3432:0.0847661 3433:0.115569 3434:0.147823 3435:0.154226 3436:0.143213 3437:0.108489 3438:0.130529 3439:0.0937387 3440:0.115751 3441:0.126006 3442:0.10636 3443:0.118749 3444:0.123944 3445:0.086347 3446:0.107046 3447:0.0972865 3448:0.131958 3449:0.132828 3450:0.120476 3451:0.110124 3452:0.104439 3453:0.100491 3454:0.0955182 3455:0.0744834 3456:0.105021 3457:0.0394247 3458:0.0647049 3459:0.0683901 3460:0.0490507 3461:0.0513583 3462:0.0732947 3463:0.0640161 3464:0.0518205 3465:0.0678673 3466:0.0703225 3467:0.0643313 3468:0.0866301 3469:0.0751145 3470:0.0660747 3471:0.0867603 3472:0.0595362 3473:0.092534 3474:0.0674584 3475:0.095506 3476:0.133751 3477:0.133216 3478:0.128582 3479:0.0801438 3480:0.0622991 3481:0.0944361 3482:0.0968234 3483:0.139361 3484:0.178479 3485:0.177902 3486:0.186209 3487:0.120055 3488:0.0569459 3489:0.0954524 3490:0.116276 3491:0.139432 3492:0.103139 3493:0.131136 3494:0.16677 3495:0.128401 3496:0.0652057 3497:0.0754932 3498:0.0929407 3499:0.149443 3500:0.131943 3501:0.097467 3502:0.0937793 3503:0.101839 3504:0.0738517 3505:0.115894 3506:0.0809663 3507:0.0716139 3508:0.0924015 3509:0.084475 3510:0.0788463 3511:0.0878532 3512:0.0909822 3513:0.153799 3514:0.121563 3515:0.0972688 3516:0.0905794 3517:0.0882678 3518:0.0683299 3519:0.085609 3520:0.114449 3521:0.102899 3522:0.137507 3523:0.125066 3524:0.119957 3525:0.130236 3526:0.140347 3527:0.138032 3528:0.120399 3529:0.165772 3530:0.15537 3531:0.147574 3532:0.153208 3533:0.176072 3534:0.168803 3535:0.162191 3536:0.150526 3537:0.191874 3538:0.157883 3539:0.168329 3540:0.185808 3541:0.202571 3542:0.190651 3543:0.17434 3544:0.15366 3545:0.179018 3546:0.181356 3547:0.237482 3548:0.290495 3549:0.274653 3550:0.26419 3551:0.221707 3552:0.156865 3553:0.170687 3554:0.1979 3555:0.255048 3556:0.261426 3557:0.269769 3558:0.290878 3559:0.239594 3560:0.166663 3561:0.181641 3562:0.219031 3563:0.286332 3564:0.229869 3565:0.179483 3566:0.233477 3567:0.229155 3568:0.164547 3569:0.191834 3570:0.170254 3571:0.205288 3572:0.23144 3573:0.178194 3574:0.168549 3575:0.183066 3576:0.169455 3577:0.218571 3578:0.194742 3579:0.183677 3580:0.180388 3581:0.177184 3582:0.174518 3583:0.185546 3584:0.204001 3585:0 3586:0 3587:0 3588:0 3589:0 3590:0 3591:0 3592:0 3593:0 3594:0 3595:0 3596:0 3597:0 3598:0 3599:0 3600:0 3601:0 3602:0 3603:0 3604:0 3605:0 3606:0 3607:0 3608:0 3609:0 3610:0 3611:0 3612:0 3613:0 3614:0 3615:0 3616:0 3617:0 3618:0 3619:0 3620:0 3621:0 3622:0 3623:0 3624:0 3625:0 3626:0 3627:0 3628:0 3629:0 3630:0 3631:0 3632:0 3633:0 3634:0 3635:0 3636:0 3637:0 3638:0 3639:0 3640:0 3641:0 3642:0 3643:0 3644:0 3645:0 3646:0 3647:0 3648:0 3649:0.0466495 3650:0 3651:0 3652:0 3653:0 3654:0 3655:0 3656:0 3657:0.030034 3658:0.00123411 3659:0 3660:0.0209846 3661:0.00239697 3662:0.00238882 3663:0 3664:0 3665:0 3666:0 3667:0 3668:0.0234231 3669:0 3670:0.0119271 3671:0 3672:0 3673:0 3674:0.00426544 3675:0 3676:0 3677:0 3678:0 3679:0 3680:0 3681:0.00427468 3682:0 3683:0 3684:0 3685:0 3686:0 3687:0 3688:0 3689:0.00366243 3690:0 3691:0 3692:0 3693:0 3694:0 3695:0 3696:0 3697:0 3698:0 3699:0 3700:0 3701:0 3702:0 3703:0 3704:0 3705:0.0162201 3706:0 3707:0.0167206 3708:0.0175758 3709:0.02285 3710:0.0306811 3711:0.0305348 3712:0 3713:0.0282305 3714:0.0385621 3715:0.0435644 3716:0.0401464 3717:0.0367231 3718:0.0431039 3719:0.0450972 3720:0.0463276 3721:0.0039697 3722:0 3723:0.00613201 3724:0.00445737 3725:0.0287658 3726:0.00144079 3727:0.00491103 3728:0.0370225 3729:0 3730:0.00730743 3731:0.0118403 3732:0 3733:0 3734:0.0102304 3735:0.0153417 3736:0.0383712 3737:0 3738:0.00415715 3739:0 3740:0 3741:0 3742:0.00350701 3743:0.0323776 3744:0.0446108 3745:0.00275987 3746:0 3747:0.0137721 3748:0 3749:0 3750:0 3751:0.043539 3752:0.0546369 3753:0.00561978 3754:0 3755:0 3756:0.0130382 3757:0.00390585 3758:0 3759:0 3760:0.060006 3761:0.00956558 3762:0 3763:0 3764:0 3765:0.00469377 3766:0.00452038 3767:0 3768:0.0536007 3769:0.00573435 3770:0.00940339 3771:0.0121603 3772:0.00652732 3773:0.00397034 3774:0.00583437 3775:0.0035283 3776:0.0584814 3777:0.0437906 3778:0.0481242 3779:0.0472796 3780:0.0537666 3781:0.0606865 3782:0.0655666 3783:0.0604735 3784:0.0506381 3785:0.0481128 3786:0.0624489 3787:0.0541421 3788:0.0792041 3789:0.0802449 3790:0.076675 3791:0.0703232 3792:0.0610293 3793:0.0684919 3794:0.0740911 3795:0.0801255 3796:0.13511 3797:0.123386 3798:0.101808 3799:0.0713891 3800:0.0703932 3801:0.0703941 3802:0.128577 3803:0.131969 3804:0.17945 3805:0.179112 3806:0.169674 3807:0.0756144 3808:0.0740717 3809:0.0836136 3810:0.171132 3811:0.176854 3812:0.179787 3813:0.182711 3814:0.203972 3815:0.0938664 3816:0.0984458 3817:0.0998366 3818:0.138598 3819:0.15846 3820:0.13519 3821:0.127872 3822:0.121399 3823:0.0970264 3824:0.127308 3825:0.0935908 3826:0.105872 3827:0.116685 3828:0.0907301 3829:0.0838375 3830:0.0866944 3831:0.12275 3832:0.109696 3833:0.13947 3834:0.122517 3835:0.0997101 3836:0.0884071 3837:0.0898035 3838:0.0885629 3839:0.122116 3840:0.0935879 3841:0.109157 3842:0.088883 3843:0.104922 3844:0.0988826 3845:0.0923083 3846:0.0974885 3847:0.0852702 3848:0.107521 3849:0.124935 3850:0.0813874 3851:0.0871252 3852:0.121942 3853:0.119717 3854:0.110314 3855:0.116087 3856:0.095592 3857:0.150663 3858:0.0840202 3859:0.118782 3860:0.0965724 3861:0.100526 3862:0.0885696 3863:0.094802 3864:0.0923327 3865:0.141875 3866:0.0811458 3867:0.124075 3868:0.125519 3869:0.119728 3870:0.1515 3871:0.0862149 3872:0.096566 3873:0.137167 3874:0.107354 3875:0.122226 3876:0.110472 3877:0.11528 3878:0.156412 3879:0.115468 3880:0.118186 3881:0.141457 3882:0.102711 3883:0.145089 3884:0.120529 3885:0.0921241 3886:0.105654 3887:0.158009 3888:0.125072 3889:0.173938 3890:0.09455 3891:0.0972864 3892:0.117258 3893:0.101998 3894:0.0960088 3895:0.0821574 3896:0.117361 3897:0.184585 3898:0.111923 3899:0.100058 3900:0.0982116 3901:0.0958446 3902:0.0958401 3903:0.104065 3904:0.152464 3905:0.0841579 3906:0.0961869 3907:0.0916043 3908:0.09467 3909:0.0938359 3910:0.100144 3911:0.101837 3912:0.108532 3913:0.084864 3914:0.0714279 3915:0.0636493 3916:0.0788367 3917:0.0932915 3918:0.0758322 3919:0.0751555 3920:0.0898235 3921:0.0858572 3922:0.0763527 3923:0.081745 3924:0.124409 3925:0.121972 3926:0.119002 3927:0.0830617 3928:0.0846979 3929:0.0748075 3930:0.108167 3931:0.137827 3932:0.162297 3933:0.14579 3934:0.128815 3935:0.0916645 3936:0.0891742 3937:0.0785187 3938:0.159463 3939:0.145725 3940:0.151415 3941:0.0951779 3942:0.132298 3943:0.0489475 3944:0.0842114 3945:0.0911564 3946:0.130587 3947:0.101186 3948:0.0864983 3949:0.0877136 3950:0.0822845 3951:0.0431711 3952:0.104678 3953:0.0968156 3954:0.102652 3955:0.0931764 3956:0.0595328 3957:0.0706885 3958:0.0751226 3959:0.0985199 3960:0.0903379 3961:0.0453202 3962:0.0712068 3963:0.0812265 3964:0.0679918 3965:0.0689337 3966:0.0734999 3967:0.0818201 3968:0.0748145 3969:0.0536418 3970:0 3971:0 3972:0 3973:0 3974:0 3975:0 3976:0 3977:0.000542723 3978:0 3979:0 3980:0 3981:0 3982:0 3983:0 3984:0 3985:0 3986:0 3987:0 3988:0 3989:0 3990:0 3991:0 3992:0 3993:0 3994:0 3995:0 3996:0 3997:0 3998:0 3999:0 4000:0 4001:0 4002:0 4003:0 4004:0 4005:0 4006:0 4007:0 4008:0 4009:0 4010:0 4011:0 4012:0 4013:0 4014:0 4015:0 4016:0 4017:0 4018:0 4019:0 4020:0 4021:0 4022:0 4023:0 4024:0 4025:0 4026:0 4027:0 4028:0 4029:0 4030:0 4031:0 4032:0 4033:0.114161 4034:0.154451 4035:0.159397 4036:0.159973 4037:0.158375 4038:0.171251 4039:0.178592 4040:0.184952 4041:0.153004 4042:0.150756 4043:0.157 4044:0.143992 4045:0.163544 4046:0.173636 4047:0.173964 4048:0.171566 4049:0.168141 4050:0.163911 4051:0.169856 4052:0.186218 4053:0.226374 4054:0.207233 4055:0.200379 4056:0.18293 4057:0.164501 4058:0.170286 4059:0.200825 4060:0.260206 4061:0.296354 4062:0.288374 4063:0.268231 4064:0.188295 4065:0.153623 4066:0.186346 4067:0.286771 4068:0.286111 4069:0.285525 4070:0.311317 4071:0.289524 4072:0.177423 4073:0.143028 4074:0.208545 4075:0.238973 4076:0.246811 4077:0.209185 4078:0.222932 4079:0.226234 4080:0.201843 4081:0.152869 4082:0.169891 4083:0.218187 4084:0.232171 4085:0.188918 4086:0.174342 4087:0.181988 4088:0.217102 4089:0.174536 4090:0.196239 4091:0.188373 4092:0.195325 4093:0.191829 4094:0.182407 4095:0.163819 4096:0.219352 4097:0 4098:0 4099:0 4100:0 4101:0 4102:0 4103:0 4104:0 4105:0 4106:0 4107:0 4108:0 4109:0 4110:0 4111:0 4112:0 4113:0 4114:0 4115:0 4116:0 4117:0 4118:0 4119:0 4120:0 4121:0 4122:0 4123:0 4124:0 4125:0 4126:0 4127:0 4128:0 4129:0 4130:0 4131:0 4132:0 4133:0 4134:0 4135:0 4136:0 4137:0 4138:0 4139:0 4140:0 4141:0 4142:0 4143:0 4144:0 4145:0 4146:0 4147:0 4148:0 4149:0 4150:0 4151:0 4152:0 4153:0 4154:0 4155:0 4156:0 4157:0 4158:0 4159:0 4160:0 4161:0.025397 4162:0.0768364 4163:0.0827714 4164:0.0748336 4165:0.0864293 4166:0.0920603 4167:0.100474 4168:0.0994868 4169:0.0694371 4170:0.06584 4171:0.0485305 4172:0.0577059 4173:0.0649418 4174:0.0764686 4175:0.0717834 4176:0.0586682 4177:0.112231 4178:0.0865976 4179:0.0930109 4180:0.106373 4181:0.142468 4182:0.119664 4183:0.0820651 4184:0.0621032 4185:0.104234 4186:0.125118 4187:0.163588 4188:0.242427 4189:0.303735 4190:0.311806 4191:0.170389 4192:0.0794047 4193:0.116162 4194:0.209659 4195:0.247203 4196:0.279312 4197:0.258218 4198:0.243033 4199:0.19877 4200:0.0763666 4201:0.103501 4202:0.183852 4203:0.198772 4204:0.185235 4205:0.117023 4206:0.146474 4207:0.130567 4208:0.0877585 4209:0.128552 4210:0.102866 4211:0.163277 4212:0.145206 4213:0.104564 4214:0.094136 4215:0.123281 4216:0.0774272 4217:0.1505 4218:0.104008 4219:0.0869334 4220:0.0795039 4221:0.0781498 4222:0.082312 4223:0.0960821 4224:0.0642703 4225:0 4226:0 4227:0 4228:0 4229:0 4230:0 4231:0 4232:0 4233:0 4234:0 4235:0 4236:0 4237:0 4238:0 4239:0 4240:0 4241:0 4242:0 4243:0 4244:0 4245:0 4246:0 4247:0 4248:0 4249:0 4250:0 4251:0 4252:0 4253:0 4254:0 4255:0 4256:0 4257:0 4258:0 4259:0 4260:0 4261:0 4262:0 4263:0 4264:0 4265:0 4266:0 4267:0 4268:0 4269:0 4270:0 4271:0 4272:0 4273:0 4274:0 4275:0 4276:0 4277:0 4278:0 4279:0 4280:0 4281:0 4282:0 4283:0 4284:0 4285:0 4286:0 4287:0 4288:0 4289:0.0758158 4290:0.0871343 4291:0.0829031 4292:0.0817391 4293:0.0858288 4294:0.0835984 4295:0.085973 4296:0.0909618 4297:0.0635882 4298:0.0346102 4299:0.0377486 4300:0.0335956 4301:0.0436609 4302:0.0375211 4303:0.0325802 4304:0.0428184 4305:0.0634817 4306:0.0375428 4307:0.0266477 4308:0.0665905 4309:0.0380756 4310:0.05125 4311:0.0293133 4312:0.0381682 4313:0.0553963 4314:0.0280104 4315:0.0427063 4316:0.0286016 4317:0.0324405 4318:0.0146913 4319:0.0365896 4320:0.0277557 4321:0.0571567 4322:0.0487645 4323:0.0272732 4324:0.0137485 4325:0 4326:0 4327:0.0106912 4328:0.0206076 4329:0.0561653 4330:0.0248949 4331:0 4332:0.0161366 4333:0.0197737 4334:0.0172612 4335:0 4336:0.023258 4337:0.0590945 4338:0.0169721 4339:0 4340:0 4341:0.0151618 4342:0.0256673 4343:0.0352934 4344:0.0349816 4345:0.0634137 4346:0.0283512 4347:0.0296452 4348:0.0227257 4349:0.029388 4350:0.0306709 4351:0.0270252 4352:0.00744607 4353:0.0622748 4354:0.0876714 4355:0.0985523 4356:0.0965134 4357:0.0870417 4358:0.0945919 4359:0.104141 4360:0.0980568 4361:0.0987205 4362:0.0870438 4363:0.0921833 4364:0.072833 4365:0.101799 4366:0.0789562 4367:0.0948957 4368:0.084607 4369:0.119656 4370:0.0988863 4371:0.0848529 4372:0.129242 4373:0.140162 4374:0.135838 4375:0.105034 4376:0.0974795 4377:0.118695 4378:0.100152 4379:0.165674 4380:0.162697 4381:0.209623 4382:0.188797 4383:0.195098 4384:0.103836 4385:0.0974028 4386:0.134919 4387:0.19676 4388:0.194986 4389:0.167201 4390:0.176338 4391:0.166734 4392:0.100706 4393:0.104094 4394:0.12057 4395:0.168978 4396:0.164079 4397:0.124333 4398:0.119854 4399:0.095906 4400:0.114353 4401:0.119933 4402:0.125113 4403:0.131039 4404:0.114484 4405:0.105987 4406:0.102927 4407:0.116547 4408:0.11155 4409:0.133213 4410:0.129854 4411:0.11643 4412:0.111693 4413:0.103701 4414:0.101335 4415:0.111716 4416:0.0737452 4417:0 4418:0 4419:0 4420:0 4421:0 4422:0 4423:0 4424:0 4425:0 4426:0 4427:0 4428:0 4429:0 4430:0 4431:0 4432:0 4433:0.00289534 4434:0 4435:0 4436:0.0141301 4437:0 4438:0 4439:0 4440:0 4441:0 4442:0 4443:0.00855689 4444:0.0158362 4445:0.0105339 4446:0.00888424 4447:0 4448:0 4449:0.00543438 4450:0.0324714 4451:0 4452:0.00710406 4453:0 4454:0.028182 4455:0.00497868 4456:0 4457:0.00273871 4458:0 4459:0.0194661 4460:0 4461:0 4462:0 4463:0 4464:0.0117096 4465:0.00595105 4466:0 4467:0 4468:0 4469:0 4470:0.00122442 4471:0.0029231 4472:0.0237628 4473:0 4474:0 4475:0 4476:0 4477:0 4478:0 4479:0 4480:0 4481:0.0135386 4482:0.0141858 4483:0.00364117 4484:0 4485:0.0025575 4486:0.0029766 4487:0.00685439 4488:0 4489:0.0162544 4490:0 4491:0 4492:0 4493:0 4494:0.00055108 4495:0 4496:0 4497:0 4498:0 4499:0 4500:0.0143061 4501:0 4502:0 4503:0 4504:0 4505:0 4506:0 4507:0 4508:0 4509:0 4510:0 4511:0 4512:0 4513:0 4514:0 4515:0 4516:0 4517:0 4518:0 4519:0 4520:0 4521:0 4522:0 4523:0 4524:0 4525:0 4526:0 4527:0 4528:0.00720471 4529:0.0146307 4530:0 4531:0 4532:0 4533:0 4534:0 4535:0 4536:0 4537:0 4538:0.00703071 4539:0 4540:0 4541:0 4542:0 4543:0 4544:0 4545:0.0714385 4546:0.0729033 4547:0.0536739 4548:0.061535 4549:0.0556737 4550:0.0549539 4551:0.0638865 4552:0.0505485 4553:0.105222 4554:0.0586689 4555:0.0473118 4556:0.0440419 4557:0.0490325 4558:0.0580755 4559:0.0545393 4560:0.0322135 4561:0.120201 4562:0.047806 4563:0.041675 4564:0.0575195 4565:0.0526126 4566:0.0366274 4567:0.0641844 4568:0.0443575 4569:0.11427 4570:0.0412183 4571:0.0748119 4572:0.0862217 4573:0.0509082 4574:0.0447715 4575:0.050569 4576:0.0421061 4577:0.111657 4578:0.0887149 4579:0.079707 4580:0.0844009 4581:0.0866371 4582:0.0742452 4583:0.0357965 4584:0.0442556 4585:0.106542 4586:0.098324 4587:0.0678109 4588:0.0436747 4589:0.0840106 4590:0.0825122 4591:0.057417 4592:0.0627205 4593:0.114685 4594:0.0618729 4595:0.0738641 4596:0.0520053 4597:0.0514578 4598:0.0621113 4599:0.0665236 4600:0.0701569 4601:0.179517 4602:0.130942 4603:0.103133 4604:0.0988964 4605:0.101951 4606:0.100033 4607:0.107016 4608:0.0867831 4609:0 4610:0 4611:0 4612:0 4613:0 4614:0 4615:0 4616:0 4617:0 4618:0 4619:0 4620:0 4621:0 4622:0 4623:0 4624:0 4625:0 4626:0 4627:0 4628:0 4629:0 4630:0 4631:0 4632:0 4633:0 4634:0 4635:0 4636:0 4637:0 4638:0 4639:0 4640:0 4641:0 4642:0 4643:0 4644:0 4645:0 4646:0 4647:0 4648:0 4649:0 4650:0 4651:0 4652:0 4653:0 4654:0 4655:0 4656:0 4657:0 4658:0 4659:0 4660:0 4661:0 4662:0 4663:0 4664:0 4665:0 4666:0 4667:0 4668:0 4669:0 4670:0 4671:0 4672:0 4673:0.0487463 4674:0.0333529 4675:0.025701 4676:0.0221272 4677:0.0329005 4678:0.0323067 4679:0.0240696 4680:0.0284188 4681:0.0368994 4682:0.00796411 4683:0.0106308 4684:0.0552162 4685:0.0399542 4686:0.0392457 4687:0.0235953 4688:0.016141 4689:0.0558515 4690:0.0145732 4691:0.0241097 4692:0.0334064 4693:0.0230241 4694:0.0112955 4695:0.02276 4696:0.0237339 4697:0.0552497 4698:0.0597896 4699:0.0739704 4700:0.107527 4701:0.0785076 4702:0.0843767 4703:0 4704:0.0184827 4705:0.0436434 4706:0.127613 4707:0.0507226 4708:0.0803429 4709:0.0739475 4710:0.134373 4711:0.0186606 4712:0.0414448 4713:0.0351339 4714:0.0612179 4715:0.0931914 4716:0.0511535 4717:0.0477049 4718:0.0409475 4719:0.0478784 4720:0.0549943 4721:0.0362692 4722:0.0370343 4723:0.0443129 4724:0.0516049 4725:0.0343068 4726:0.0280566 4727:0.0459191 4728:0.0605965 4729:0.0547042 4730:0.0239415 4731:0.0286051 4732:0.0287649 4733:0.0262996 4734:0.0232313 4735:0.0437186 4736:0.0692314 4737:0.042542 4738:0.0522145 4739:0.0437697 4740:0.0393944 4741:0.0239159 4742:0.0434017 4743:0.0482476 4744:0.0584422 4745:0.0725693 4746:0.0531545 4747:0.0631127 4748:0.0452684 4749:0.0688044 4750:0.0380768 4751:0.0636292 4752:0.0693038 4753:0.072271 4754:0.045934 4755:0.0718876 4756:0.0612075 4757:0.0819858 4758:0.0858098 4759:0.0695288 4760:0.0748463 4761:0.0736571 4762:0.0674018 4763:0.0867008 4764:0.0222982 4765:0.0725228 4766:0.0646171 4767:0.143598 4768:0.0793737 4769:0.0567257 4770:0 4771:0.0997171 4772:0.0989142 4773:0.0477921 4774:0.0485847 4775:0.155336 4776:0.0725002 4777:0.067545 4778:0.0395649 4779:0.0666628 4780:0.0981616 4781:0.0577313 4782:0.0602735 4783:0.0574437 4784:0.100759 4785:0.101197 4786:0.0609885 4787:0.0656467 4788:0.061706 4789:0.0451413 4790:0.0554998 4791:0.0433738 4792:0.120428 4793:0.0842029 4794:0.0642022 4795:0.0663099 4796:0.0495795 4797:0.0483567 4798:0.0458755 4799:0.0281468 4800:0.127902 4801:0.0527029 4802:0.0791482 4803:0.0773168 4804:0.078381 4805:0.0802889 4806:0.0818971 4807:0.0894088 4808:0.0760002 4809:0.0525103 4810:0.0626796 4811:0.0605791 4812:0.0791571 4813:0.0613153 4814:0.085922 4815:0.0653889 4816:0.0526394 4817:0.0646744 4818:0.0924385 4819:0.0767038 4820:0.0718486 4821:0.0812231 4822:0.0793712 4823:0.0677354 4824:0.0603111 4825:0.0680617 4826:0.0929329 4827:0.0820376 4828:0.135856 4829:0.148734 4830:0.161093 4831:0.111414 4832:0.059123 4833:0.0750124 4834:0.13003 4835:0.144379 4836:0.132274 4837:0.163586 4838:0.177729 4839:0.162325 4840:0.0904251 4841:0.0854794 4842:0.103223 4843:0.138511 4844:0.110631 4845:0.103789 4846:0.111786 4847:0.119805 4848:0.083783 4849:0.0892034 4850:0.0914235 4851:0.113889 4852:0.119235 4853:0.09689 4854:0.0800322 4855:0.114337 4856:0.0929726 4857:0.135703 4858:0.134023 4859:0.107241 4860:0.097965 4861:0.0967672 4862:0.0910631 4863:0.107534 4864:0.110556 4865:0.117438 4866:0.127523 4867:0.132845 4868:0.144076 4869:0.134787 4870:0.156473 4871:0.16523 4872:0.142308 4873:0.138518 4874:0.129483 4875:0.127145 4876:0.11795 4877:0.135843 4878:0.146757 4879:0.146077 4880:0.112735 4881:0.161076 4882:0.147669 4883:0.117997 4884:0.188592 4885:0.215018 4886:0.208798 4887:0.184306 4888:0.12995 4889:0.172431 4890:0.141192 4891:0.234619 4892:0.332696 4893:0.35571 4894:0.337194 4895:0.248682 4896:0.140902 4897:0.148248 4898:0.255083 4899:0.326999 4900:0.357347 4901:0.357609 4902:0.336972 4903:0.272286 4904:0.138818 4905:0.15892 4906:0.246968 4907:0.289857 4908:0.245158 4909:0.224938 4910:0.258969 4911:0.194038 4912:0.150809 4913:0.155553 4914:0.186859 4915:0.25129 4916:0.219851 4917:0.177187 4918:0.149095 4919:0.172913 4920:0.167162 4921:0.188683 4922:0.180312 4923:0.171063 4924:0.163695 4925:0.160576 4926:0.150732 4927:0.15353 4928:0.144262 4929:0.055473 4930:0.0669706 4931:0.0695556 4932:0.0709814 4933:0.0718732 4934:0.0649868 4935:0.071042 4936:0.0756457 4937:0.0663436 4938:0.0508657 4939:0.0601471 4940:0.0640437 4941:0.0683215 4942:0.0691167 4943:0.0643815 4944:0.0617304 4945:0.0712967 4946:0.0622656 4947:0.061733 4948:0.0738151 4949:0.0638243 4950:0.0798101 4951:0.0778074 4952:0.0654767 4953:0.0671024 4954:0.0507408 4955:0.0856145 4956:0.0765461 4957:0.0706783 4958:0.0867876 4959:0.0783971 4960:0.0681457 4961:0.0654243 4962:0.0905484 4963:0.0781555 4964:0.11053 4965:0.0805581 4966:0.0829121 4967:0.0931298 4968:0.0768403 4969:0.066339 4970:0.077544 4971:0.0831787 4972:0.0731333 4973:0.0917194 4974:0.0944463 4975:0.0810831 4976:0.0710593 4977:0.0789075 4978:0.0679197 4979:0.0814396 4980:0.0730282 4981:0.0650946 4982:0.066577 4983:0.0757148 4984:0.0865706 4985:0.102592 4986:0.0664225 4987:0.0547123 4988:0.0605209 4989:0.0630846 4990:0.0597706 4991:0.0753504 4992:0.0667647 4993:0 4994:0 4995:0 4996:0 4997:0 4998:0 4999:0 5000:0 5001:0 5002:0 5003:0 5004:0 5005:0 5006:0 5007:0 5008:0 5009:0 5010:0 5011:0 5012:0 5013:0 5014:0 5015:0 5016:0 5017:0 5018:0 5019:0 5020:0 5021:0 5022:0 5023:0 5024:0 5025:0 5026:0 5027:0 5028:0 5029:0 5030:0 5031:0 5032:0 5033:0 5034:0 5035:0 5036:0 5037:0 5038:0 5039:0 5040:0 5041:0 5042:0 5043:0 5044:0 5045:0 5046:0 5047:0 5048:0 5049:0.045472 5050:0.0217338 5051:0 5052:0.00251431 5053:0.00620399 5054:0.00236458 5055:0.000913478 5056:0 5057:0.064572 5058:0.076439 5059:0.0816937 5060:0.0872628 5061:0.093724 5062:0.0857174 5063:0.0925464 5064:0.0998179 5065:0.0762425 5066:0.0796452 5067:0.0782514 5068:0.0807587 5069:0.0790801 5070:0.094778 5071:0.0940919 5072:0.0715796 5073:0.086452 5074:0.095588 5075:0.0835995 5076:0.107251 5077:0.11481 5078:0.0997813 5079:0.116234 5080:0.086117 5081:0.0943111 5082:0.0900377 5083:0.125407 5084:0.157682 5085:0.188336 5086:0.155596 5087:0.147291 5088:0.0905484 5089:0.0864698 5090:0.150228 5091:0.168042 5092:0.204015 5093:0.178577 5094:0.208656 5095:0.149337 5096:0.0955667 5097:0.093326 5098:0.153051 5099:0.172522 5100:0.175204 5101:0.147293 5102:0.148653 5103:0.131224 5104:0.102716 5105:0.105955 5106:0.121311 5107:0.143031 5108:0.132219 5109:0.12472 5110:0.0970279 5111:0.109404 5112:0.122158 5113:0.11898 5114:0.11219 5115:0.115594 5116:0.108603 5117:0.100572 5118:0.0983258 5119:0.120818 5120:0.114201 5121:0.0711308 5122:0.0890636 5123:0.0906568 5124:0.0917722 5125:0.0960799 5126:0.0965628 5127:0.095702 5128:0.0971424 5129:0.0713782 5130:0.0838374 5131:0.0843589 5132:0.0994605 5133:0.0870393 5134:0.0935916 5135:0.100054 5136:0.0891848 5137:0.0899828 5138:0.0891546 5139:0.106899 5140:0.0715573 5141:0.105483 5142:0.0791729 5143:0.0933001 5144:0.0884687 5145:0.0884224 5146:0.110389 5147:0.115265 5148:0.143585 5149:0.135213 5150:0.130469 5151:0.0773574 5152:0.092262 5153:0.102576 5154:0.128655 5155:0.127553 5156:0.135744 5157:0.134222 5158:0.148148 5159:0.117631 5160:0.10617 5161:0.0884984 5162:0.102904 5163:0.157545 5164:0.110488 5165:0.114583 5166:0.145933 5167:0.127505 5168:0.0912361 5169:0.103439 5170:0.0848939 5171:0.118081 5172:0.119412 5173:0.0960052 5174:0.0944037 5175:0.111328 5176:0.0983677 5177:0.115948 5178:0.0862762 5179:0.0944738 5180:0.100969 5181:0.101125 5182:0.0890231 5183:0.114582 5184:0.132245 5185:0.0282496 5186:0.0440521 5187:0.045275 5188:0.0382225 5189:0.0435355 5190:0.0496894 5191:0.0490636 5192:0.04041 5193:0.0257254 5194:0.0364444 5195:0.0361972 5196:0.0377985 5197:0.0468157 5198:0.0439819 5199:0.0341957 5200:0.0380732 5201:0.0237422 5202:0.0335992 5203:0.0377815 5204:0.0482202 5205:0.0480551 5206:0.064045 5207:0.0462317 5208:0.0265751 5209:0.0145324 5210:0.0398728 5211:0.055828 5212:0.0358516 5213:0.0185773 5214:0.0168468 5215:0.0527171 5216:0.0275933 5217:0.0246219 5218:0.0630291 5219:0.0400057 5220:0.0374992 5221:0.0457029 5222:0.0272296 5223:0.0424102 5224:0.0408967 5225:0.0425491 5226:0.0694559 5227:0.0271632 5228:0.0473769 5229:0.0595191 5230:0.0578989 5231:0.0529522 5232:0.050238 5233:0.0472853 5234:0.0607095 5235:0.0483509 5236:0.0329305 5237:0.0348138 5238:0.0433938 5239:0.0515982 5240:0.0161992 5241:0.0232404 5242:0.0572599 5243:0.0559223 5244:0.038166 5245:0.0362494 5246:0.0347513 5247:0.046931 5248:0.00865822 5249:0.0540485 5250:0.039572 5251:0.0358938 5252:0.0296241 5253:0.029393 5254:0.0302242 5255:0.0261383 5256:0.0729402 5257:0.0666282 5258:0.0213971 5259:0.0177707 5260:0.039696 5261:0.0572363 5262:0.0405869 5263:0.0328405 5264:0.0663775 5265:0.0791649 5266:0.0227191 5267:0.0374053 5268:0.0473048 5269:0.0647373 5270:0.0437367 5271:0.0306351 5272:0.0328559 5273:0.0739392 5274:0.0659896 5275:0.0573279 5276:0.043975 5277:0 5278:0.0139824 5279:0.00440334 5280:0.0297047 5281:0.076228 5282:0.0489366 5283:0 5284:0 5285:0 5286:0 5287:0 5288:0.053555 5289:0.0729744 5290:0.0161451 5291:0.011424 5292:0 5293:0.0171225 5294:0.00989166 5295:0.0225493 5296:0.0465594 5297:0.0713468 5298:0.0121139 5299:0.000757145 5300:0.0124388 5301:0.0237824 5302:0.0199023 5303:0.0264637 5304:0.0378124 5305:0.0639693 5306:0 5307:0 5308:0.00785467 5309:0.00233788 5310:0.00945037 5311:0.0113078 5312:0.0353033 5313:0.122452 5314:0.118953 5315:0.109327 5316:0.108573 5317:0.111805 5318:0.114366 5319:0.108649 5320:0.112026 5321:0.112951 5322:0.113549 5323:0.121861 5324:0.15547 5325:0.169239 5326:0.125044 5327:0.11622 5328:0.136134 5329:0.117806 5330:0.113425 5331:0.170919 5332:0.166959 5333:0.15772 5334:0.146145 5335:0.117369 5336:0.133603 5337:0.117876 5338:0.154634 5339:0.193558 5340:0.176154 5341:0.141375 5342:0.155493 5343:0.115105 5344:0.125106 5345:0.138645 5346:0.197238 5347:0.156768 5348:0.144361 5349:0.118589 5350:0.152994 5351:0.104017 5352:0.159028 5353:0.130811 5354:0.139403 5355:0.155653 5356:0.141002 5357:0.131243 5358:0.133723 5359:0.155307 5360:0.157007 5361:0.135103 5362:0.125277 5363:0.117005 5364:0.119538 5365:0.127663 5366:0.120426 5367:0.136692 5368:0.154861 5369:0.151528 5370:0.162432 5371:0.129686 5372:0.13728 5373:0.135103 5374:0.125747 5375:0.128291 5376:0.139827 5377:0 5378:0 5379:0 5380:0 5381:0 5382:0 5383:0 5384:0.00168546 5385:0 5386:0 5387:0 5388:0 5389:0 5390:0 5391:0 5392:0 5393:0 5394:0 5395:0 5396:0 5397:0 5398:0 5399:0 5400:0 5401:0 5402:0 5403:2.81446e-05 5404:0 5405:0 5406:0 5407:0 5408:0 5409:0 5410:0 5411:0 5412:0 5413:0 5414:0 5415:0 5416:0 5417:0 5418:0 5419:0 5420:0 5421:0 5422:0 5423:0.0157285 5424:0 5425:0 5426:0 5427:0 5428:0 5429:0 5430:0 5431:0 5432:0 5433:0 5434:0 5435:0 5436:0 5437:0 5438:0 5439:0 5440:0.0368381 5441:0.0312307 5442:0.0531246 5443:0.0604482 5444:0.0567847 5445:0.0548313 5446:0.0678329 5447:0.0741903 5448:0.0610601 5449:0.0770814 5450:0.0592058 5451:0.0728062 5452:0.0587438 5453:0.0855963 5454:0.0651869 5455:0.0793381 5456:0.0599652 5457:0.0973585 5458:0.0794273 5459:0.0705852 5460:0.122291 5461:0.133986 5462:0.126034 5463:0.0948102 5464:0.0774266 5465:0.0976316 5466:0.0701201 5467:0.130688 5468:0.18243 5469:0.224142 5470:0.207443 5471:0.186629 5472:0.0817173 5473:0.0812761 5474:0.138519 5475:0.189059 5476:0.183154 5477:0.174988 5478:0.192555 5479:0.189615 5480:0.0851623 5481:0.0738569 5482:0.120749 5483:0.154646 5484:0.156075 5485:0.0871132 5486:0.108425 5487:0.0868249 5488:0.113273 5489:0.089089 5490:0.07298 5491:0.085899 5492:0.12558 5493:0.0852956 5494:0.0740108 5495:0.0726201 5496:0.134036 5497:0.127586 5498:0.0963531 5499:0.0956991 5500:0.0915348 5501:0.0918972 5502:0.0855947 5503:0.0868551 5504:0.126464 5505:0.0335482 5506:0.0245897 5507:0.0186056 5508:0.0182259 5509:0.0175038 5510:0.0163652 5511:0.019254 5512:0.0167818 5513:0.00631421 5514:0.0139359 5515:0.0134391 5516:0.0278649 5517:0.0348035 5518:0.0170015 5519:0.000884508 5520:0.0161526 5521:0 5522:0 5523:0.0120605 5524:0.0335061 5525:0.038419 5526:0 5527:0.0146947 5528:0.00340908 5529:0 5530:0.0155197 5531:0.0418545 5532:0.0193115 5533:0 5534:0 5535:0 5536:0.00179588 5537:0.00480034 5538:0.0314676 5539:0 5540:0 5541:0 5542:0 5543:0 5544:0.0128625 5545:0.00753299 5546:0.0440107 5547:0 5548:0 5549:0 5550:0 5551:0 5552:0.0522866 5553:0.0106278 5554:0.0199196 5555:0 5556:0 5557:0 5558:0.00784138 5559:0.013398 5560:0.00879901 5561:0.0053141 5562:0.00153736 5563:0 5564:0 5565:0 5566:0.00125117 5567:0.0176092 5568:0 5569:0 5570:0 5571:0 5572:0 5573:0 5574:0 5575:0 5576:0 5577:0 5578:0 5579:0 5580:0 5581:0 5582:0 5583:0 5584:0 5585:0 5586:0 5587:0 5588:0 5589:0 5590:0 5591:0 5592:0 5593:0 5594:0 5595:0 5596:0 5597:0 5598:0 5599:0 5600:0 5601:0 5602:0 5603:0 5604:0 5605:0 5606:0 5607:0 5608:0 5609:0 5610:0 5611:0 5612:0 5613:0 5614:0 5615:0 5616:0 5617:0 5618:0 5619:0 5620:0 5621:0 5622:0 5623:0 5624:0 5625:0 5626:0 5627:0 5628:0 5629:0 5630:0 5631:0 5632:0 5633:0 5634:0 5635:0 5636:0 5637:0 5638:0 5639:0 5640:0 5641:0 5642:0 5643:0 5644:0 5645:0 5646:0 5647:0 5648:0 5649:0 5650:0 5651:0 5652:0 5653:0 5654:0 5655:0 5656:0 5657:0 5658:0 5659:0 5660:0 5661:0 5662:0 5663:0 5664:0 5665:0 5666:0 5667:0 5668:0 5669:0 5670:0 5671:0 5672:0 5673:0 5674:0 5675:0 5676:0 5677:0 5678:0 5679:0 5680:0 5681:0 5682:0 5683:0 5684:0 5685:0 5686:0 5687:0 5688:0 5689:0 5690:0 5691:0 5692:0 5693:0 5694:0 5695:0 5696:0 5697:0 5698:0 5699:0 5700:0 5701:0 5702:0 5703:0 5704:0 5705:0 5706:0 5707:0 5708:0 5709:0 5710:0 5711:0 5712:0 5713:0 5714:0 5715:0 5716:0 5717:0 5718:0 5719:0 5720:0 5721:0 5722:0 5723:0 5724:0 5725:0 5726:0 5727:0 5728:0 5729:0 5730:0 5731:0 5732:0 5733:0 5734:0 5735:0 5736:0 5737:0 5738:0 5739:0 5740:0 5741:0 5742:0 5743:0 5744:0 5745:0 5746:0 5747:0 5748:0 5749:0 5750:0 5751:0 5752:0 5753:0 5754:0 5755:0 5756:0 5757:0 5758:0 5759:0 5760:0 5761:0.0771339 5762:0.0138981 5763:0.0223356 5764:0.017528 5765:0.0216465 5766:0.0193511 5767:0.0132992 5768:0.00250657 5769:0.0711039 5770:0.0113194 5771:0.0110646 5772:0.051653 5773:0.0302894 5774:0.0111397 5775:0.0229153 5776:0.0178851 5777:0.0595377 5778:0 5779:0.0363887 5780:0.0479473 5781:0.00898018 5782:0.0233594 5783:0.0108621 5784:0.014668 5785:0.0642495 5786:0.0799005 5787:0.0429931 5788:0.00773847 5789:0 5790:0 5791:0 5792:0 5793:0.0594454 5794:0.0452952 5795:0 5796:0 5797:0 5798:0 5799:0 5800:0.0168089 5801:0.0613903 5802:0.0132499 5803:0 5804:0 5805:0.00699372 5806:0.0235707 5807:0.0284226 5808:0.0219619 5809:0.0625346 5810:0.00138657 5811:0.00324756 5812:0.002948 5813:0.0207884 5814:0.0133922 5815:0.0168329 5816:0 5817:0.049089 5818:0.0179481 5819:0.0255033 5820:0.0238444 5821:0.0242568 5822:0.0300061 5823:0.0323578 5824:0.0394138 5825:0.0514143 5826:0.0669856 5827:0.0509046 5828:0.0569472 5829:0.0536281 5830:0.0616612 5831:0.0549147 5832:0.0784663 5833:0 5834:0 5835:0 5836:0 5837:0 5838:0 5839:0 5840:0.0677485 5841:0 5842:0 5843:0 5844:0 5845:0 5846:0 5847:0 5848:0.05318 5849:0 5850:0 5851:0 5852:0 5853:0 5854:0 5855:0 5856:0.0480272 5857:0 5858:0 5859:0 5860:0 5861:0 5862:0 5863:0 5864:0.0481734 5865:0 5866:0 5867:0 5868:0 5869:0 5870:0 5871:0 5872:0.060519 5873:0 5874:0 5875:0 5876:0 5877:0 5878:0 5879:0 5880:0.0449665 5881:0.000694539 5882:0 5883:0.00760463 5884:0.00912578 5885:0.0024383 5886:0 5887:0.000473142 5888:0.0262566 5889:0.0984012 5890:0.131747 5891:0.139039 5892:0.130854 5893:0.133866 5894:0.157964 5895:0.158443 5896:0.117803 5897:0.137991 5898:0.149089 5899:0.130657 5900:0.139122 5901:0.155632 5902:0.145631 5903:0.158975 5904:0.130573 5905:0.166231 5906:0.151278 5907:0.177079 5908:0.205013 5909:0.222937 5910:0.210956 5911:0.14914 5912:0.146238 5913:0.15254 5914:0.194892 5915:0.23019 5916:0.32811 5917:0.323113 5918:0.330348 5919:0.244663 5920:0.158099 5921:0.151083 5922:0.241105 5923:0.314479 5924:0.270361 5925:0.306137 5926:0.322675 5927:0.258044 5928:0.17116 5929:0.161463 5930:0.243462 5931:0.278015 5932:0.212575 5933:0.176931 5934:0.210231 5935:0.233938 5936:0.180681 5937:0.166753 5938:0.169902 5939:0.196729 5940:0.218475 5941:0.185746 5942:0.165097 5943:0.185556 5944:0.177652 5945:0.170231 5946:0.171178 5947:0.142272 5948:0.1397 5949:0.141959 5950:0.141063 5951:0.14764 5952:0.156897 5953:0 5954:0 5955:0 5956:0 5957:0 5958:0 5959:0 5960:0.00831985 5961:0 5962:0 5963:0 5964:0 5965:0 5966:0 5967:0 5968:0 5969:0 5970:0 5971:0 5972:0 5973:0 5974:0 5975:0 5976:0 5977:0 5978:0 5979:0 5980:0 5981:0 5982:0 5983:0 5984:0 5985:0 5986:0 5987:0 5988:0 5989:0 5990:0 5991:0 5992:0 5993:0 5994:0 5995:0 5996:0 5997:0 5998:0 5999:0 6000:0 6001:0 6002:0 6003:0 6004:0 6005:0 6006:0 6007:0 6008:0.00634111 6009:0 6010:0 6011:0 6012:0 6013:0 6014:0 6015:0 6016:0 6017:0.056179 6018:0.0733963 6019:0.0820222 6020:0.0778191 6021:0.0618823 6022:0.0750844 6023:0.0897498 6024:0.107946 6025:0.0913883 6026:0.0773863 6027:0.0746966 6028:0.0588267 6029:0.0818535 6030:0.0698526 6031:0.0719978 6032:0.105468 6033:0.107205 6034:0.0688703 6035:0.0756178 6036:0.0848262 6037:0.124434 6038:0.122147 6039:0.102231 6040:0.118324 6041:0.101908 6042:0.059116 6043:0.0889071 6044:0.094738 6045:0.122533 6046:0.149615 6047:0.125673 6048:0.118256 6049:0.083105 6050:0.0506312 6051:0.116877 6052:0.0790981 6053:0.0644398 6054:0.0859503 6055:0.165126 6056:0.120331 6057:0.0804218 6058:0.040696 6059:0.0874077 6060:0.115098 6061:0.0840341 6062:0.0613082 6063:0.100044 6064:0.13959 6065:0.108357 6066:0.0606821 6067:0.069106 6068:0.0709853 6069:0.0649002 6070:0.0660674 6071:0.0585886 6072:0.137051 6073:0.100241 6074:0.0577536 6075:0.0610443 6076:0.063312 6077:0.0681521 6078:0.0622689 6079:0.0621307 6080:0.15389 6081:0.0335793 6082:0.0556405 6083:0.0665368 6084:0.0601608 6085:0.0496304 6086:0.0716934 6087:0.0815005 6088:0.0813491 6089:0.0446912 6090:0.0449628 6091:0.0515908 6092:0.0450222 6093:0.0684916 6094:0.0678382 6095:0.0661533 6096:0.0645595 6097:0.0573112 6098:0.0592406 6099:0.0594954 6100:0.103149 6101:0.113374 6102:0.104275 6103:0.100936 6104:0.0683752 6105:0.0670507 6106:0.0801734 6107:0.13473 6108:0.205216 6109:0.20387 6110:0.174174 6111:0.136539 6112:0.0770442 6113:0.0543074 6114:0.153583 6115:0.15742 6116:0.209327 6117:0.197075 6118:0.255534 6119:0.178446 6120:0.0793084 6121:0.0674813 6122:0.152636 6123:0.179195 6124:0.175899 6125:0.145394 6126:0.158591 6127:0.127426 6128:0.093757 6129:0.0664344 6130:0.0946993 6131:0.114563 6132:0.125038 6133:0.0881399 6134:0.0829952 6135:0.0760197 6136:0.132012 6137:0.067863 6138:0.0939463 6139:0.107978 6140:0.0961426 6141:0.0965506 6142:0.0773134 6143:0.0990554 6144:0.119947 6145:0.00701834 6146:0.0347105 6147:0.0408263 6148:0.0336323 6149:0.0391986 6150:0.0453739 6151:0.043411 6152:0.0636795 6153:0.0240169 6154:0.0461966 6155:0.0458384 6156:0.0329511 6157:0.0466843 6158:0.0382598 6159:0.0570851 6160:0.0630192 6161:0.0318521 6162:0.0572909 6163:0.0656992 6164:0.0691964 6165:0.102009 6166:0.0948458 6167:0.0559049 6168:0.0835128 6169:0.0250553 6170:0.0607794 6171:0.0940935 6172:0.111891 6173:0.142014 6174:0.126127 6175:0.124965 6176:0.0858674 6177:0.027729 6178:0.0505721 6179:0.135508 6180:0.0892912 6181:0.0996672 6182:0.0731713 6183:0.122243 6184:0.0747853 6185:0.0306121 6186:0.0429609 6187:0.0590912 6188:0.095759 6189:0.0694216 6190:0.0544921 6191:0.0425271 6192:0.111406 6193:0.0424953 6194:0.0445194 6195:0.0546156 6196:0.0620685 6197:0.0522685 6198:0.0560249 6199:0.0644899 6200:0.144875 6201:0.0380771 6202:0.0804177 6203:0.0614218 6204:0.0577554 6205:0.0493005 6206:0.0539024 6207:0.0451186 6208:0.0523428 6209:0.00165173 6210:0.0197433 6211:0.0333867 6212:0.0364548 6213:0.0317108 6214:0.0360525 6215:0.042941 6216:0.0683929 6217:0.0130658 6218:0.00359599 6219:0.0135395 6220:0.0143298 6221:0.0210781 6222:0.00688921 6223:0.0252593 6224:0.0383755 6225:0.0262612 6226:0.0291454 6227:0.0287132 6228:0.0434327 6229:0.0475689 6230:0.0506607 6231:0.026166 6232:0.0555189 6233:0.0214887 6234:0.0342984 6235:0.0359577 6236:0.0691106 6237:0.124354 6238:0.111655 6239:0.129697 6240:0.0633651 6241:0.0257155 6242:0.00910744 6243:0.110705 6244:0.0704126 6245:0.112222 6246:0.100629 6247:0.126578 6248:0.0688779 6249:0 6250:0.0261916 6251:0.0962645 6252:0.0830455 6253:0.0383493 6254:0.0486713 6255:0.0927234 6256:0.0917639 6257:0.0279203 6258:0.0171642 6259:0.0252636 6260:0.0763719 6261:0.0363787 6262:0.0245454 6263:0.0355712 6264:0.0895646 6265:0.0242178 6266:0.0267569 6267:0.0145428 6268:0.0162482 6269:0.0117072 6270:0.0138515 6271:0.0249332 6272:0.083549 6273:0.0996547 6274:0.126146 6275:0.135695 6276:0.134816 6277:0.138036 6278:0.145869 6279:0.147658 6280:0.151352 6281:0.0827516 6282:0.100421 6283:0.0968579 6284:0.103384 6285:0.110444 6286:0.0945471 6287:0.0956188 6288:0.103338 6289:0.0957961 6290:0.0961714 6291:0.115015 6292:0.110585 6293:0.147465 6294:0.127422 6295:0.0990383 6296:0.113851 6297:0.101651 6298:0.14422 6299:0.116762 6300:0.136542 6301:0.134568 6302:0.152883 6303:0.10668 6304:0.117589 6305:0.118099 6306:0.149768 6307:0.112975 6308:0.113094 6309:0.125943 6310:0.146206 6311:0.128393 6312:0.120471 6313:0.115424 6314:0.104089 6315:0.128851 6316:0.118891 6317:0.104932 6318:0.11293 6319:0.137306 6320:0.131192 6321:0.117491 6322:0.0988324 6323:0.114917 6324:0.113565 6325:0.112385 6326:0.11095 6327:0.126414 6328:0.094335 6329:0.147049 6330:0.125112 6331:0.111155 6332:0.11155 6333:0.112099 6334:0.105573 6335:0.121429 6336:0.120001 6337:0.0940345 6338:0.120619 6339:0.114392 6340:0.115584 6341:0.129236 6342:0.154155 6343:0.144801 6344:0.128344 6345:0.127821 6346:0.124271 6347:0.12276 6348:0.133684 6349:0.136492 6350:0.133235 6351:0.143757 6352:0.129787 6353:0.148737 6354:0.144033 6355:0.148176 6356:0.239996 6357:0.223144 6358:0.215546 6359:0.163573 6360:0.144954 6361:0.150216 6362:0.157677 6363:0.254643 6364:0.294908 6365:0.316819 6366:0.297519 6367:0.216734 6368:0.159423 6369:0.14754 6370:0.25633 6371:0.266835 6372:0.288819 6373:0.27437 6374:0.290176 6375:0.201382 6376:0.158295 6377:0.156303 6378:0.245505 6379:0.25547 6380:0.257426 6381:0.192758 6382:0.202864 6383:0.142783 6384:0.164062 6385:0.195766 6386:0.169579 6387:0.19596 6388:0.179097 6389:0.165889 6390:0.144958 6391:0.160003 6392:0.163167 6393:0.209662 6394:0.228694 6395:0.214556 6396:0.192838 6397:0.177925 6398:0.171534 6399:0.185627 6400:0.184746 6401:0.00230812 6402:0 6403:0 6404:0 6405:0 6406:0 6407:0 6408:0 6409:0 6410:0 6411:0 6412:0 6413:0 6414:0 6415:0 6416:0 6417:0 6418:0 6419:0 6420:0 6421:0 6422:0 6423:0 6424:0 6425:0 6426:0 6427:0.000294942 6428:0 6429:0 6430:0 6431:0 6432:0 6433:0 6434:0.0104338 6435:0 6436:0 6437:0 6438:0 6439:0 6440:0 6441:0 6442:0 6443:0 6444:0 6445:0 6446:0 6447:0 6448:0 6449:0 6450:0 6451:0 6452:0 6453:0 6454:0 6455:0 6456:0 6457:0 6458:0 6459:0 6460:0 6461:0 6462:0 6463:0 6464:0 6465:0 6466:0 6467:0 6468:0 6469:0 6470:0 6471:0 6472:0 6473:0 6474:0 6475:0 6476:0 6477:0 6478:0 6479:0 6480:0 6481:0 6482:0 6483:0 6484:0 6485:0 6486:0 6487:0 6488:0 6489:0 6490:0 6491:0 6492:0 6493:0 6494:0 6495:0 6496:0 6497:0 6498:0 6499:0 6500:0 6501:0 6502:0 6503:0 6504:0 6505:0 6506:0 6507:0 6508:0 6509:0 6510:0 6511:0 6512:0 6513:0 6514:0 6515:0 6516:0 6517:0 6518:0 6519:0 6520:0 6521:0 6522:0 6523:0 6524:0 6525:0 6526:0 6527:0 6528:0.0031584 6529:0 6530:0 6531:0 6532:0 6533:0 6534:0 6535:0 6536:0 6537:0 6538:0 6539:0 6540:0 6541:0 6542:0 6543:0 6544:0 6545:0 6546:0 6547:0 6548:0 6549:0 6550:0 6551:0 6552:0 6553:0 6554:0 6555:0 6556:0 6557:0 6558:0 6559:0 6560:0 6561:0 6562:0 6563:0 6564:0 6565:0 6566:0 6567:0 6568:0 6569:0 6570:0 6571:0 6572:0 6573:0 6574:0 6575:0 6576:0 6577:0 6578:0 6579:0 6580:0 6581:0 6582:0 6583:0 6584:0 6585:0 6586:0 6587:0 6588:0 6589:0 6590:0 6591:0 6592:0 6593:0.0179017 6594:0.0598769 6595:0.0622916 6596:0.0568859 6597:0.0663426 6598:0.0611585 6599:0.0650716 6600:0.0692934 6601:0.0263439 6602:0.0426362 6603:0.037561 6604:0.0607425 6605:0.0617051 6606:0.0705664 6607:0.0454331 6608:0.0460486 6609:0.0373702 6610:0.0438628 6611:0.0625304 6612:0.0590017 6613:0.0639009 6614:0.0521589 6615:0.0792891 6616:0.0462198 6617:0.0391334 6618:0.0910084 6619:0.0704667 6620:0.131515 6621:0.0832479 6622:0.10237 6623:0.0581089 6624:0.0502797 6625:0.0637439 6626:0.0877367 6627:0.104625 6628:0.135755 6629:0.175314 6630:0.152239 6631:0.0939927 6632:0.0791361 6633:0.0601247 6634:0.126397 6635:0.13492 6636:0.120357 6637:0.112083 6638:0.134142 6639:0.112089 6640:0.0585855 6641:0.0558955 6642:0.0775714 6643:0.110234 6644:0.107022 6645:0.0760859 6646:0.0586061 6647:0.0526668 6648:0.0403294 6649:0.0606967 6650:0.0670852 6651:0.0747692 6652:0.0612677 6653:0.0580785 6654:0.0619408 6655:0.0810356 6656:0.0598706 6657:0.0102141 6658:0.00806309 6659:0.0170056 6660:0.0176346 6661:0.0143508 6662:0.011984 6663:0.0167585 6664:0.048262 6665:0 6666:0 6667:0 6668:0 6669:0.00664637 6670:0.00702163 6671:0 6672:0.0212688 6673:0 6674:0 6675:0 6676:0 6677:0 6678:0.00229121 6679:0 6680:0.0164045 6681:0 6682:0 6683:0.0131467 6684:0 6685:0 6686:0 6687:0.0150019 6688:0.0168827 6689:0 6690:0.00516024 6691:0.0219107 6692:0.00339254 6693:0 6694:0 6695:0.00218757 6696:0.019007 6697:0.00433069 6698:0 6699:0.0155549 6700:0 6701:0.00298096 6702:0 6703:0 6704:0.036733 6705:0 6706:0 6707:2.56984e-05 6708:0 6709:0 6710:0 6711:0 6712:0.0370817 6713:0.0195454 6714:0.0151316 6715:0.0202013 6716:0.0144393 6717:0.0164108 6718:0.0109639 6719:0.019616 6720:0.0318556 6721:0 6722:0 6723:0 6724:0 6725:0 6726:0 6727:0 6728:0 6729:0 6730:0 6731:0 6732:0 6733:0 6734:0 6735:0 6736:0 6737:0 6738:0 6739:0 6740:0 6741:0 6742:0 6743:0 6744:0 6745:0 6746:0 6747:0 6748:0 6749:0 6750:0 6751:0 6752:0 6753:0 6754:0 6755:0 6756:0 6757:0 6758:0 6759:0 6760:0 6761:0 6762:0 6763:0 6764:0 6765:0 6766:0 6767:0 6768:0 6769:0 6770:0 6771:0 6772:0 6773:0 6774:0 6775:0 6776:0 6777:0 6778:0 6779:0 6780:0 6781:0 6782:0 6783:0 6784:0 6785:0 6786:0 6787:0 6788:0 6789:0 6790:0 6791:0 6792:0 6793:0 6794:0 6795:0 6796:0 6797:0 6798:0 6799:0 6800:0 6801:0 6802:0 6803:0 6804:0 6805:0 6806:0 6807:0 6808:0 6809:0 6810:0 6811:0 6812:0 6813:0 6814:0 6815:0 6816:0 6817:0 6818:0 6819:0 6820:0 6821:0 6822:0 6823:0 6824:0 6825:0 6826:0 6827:0 6828:0 6829:0 6830:0 6831:0 6832:0 6833:0 6834:0 6835:0 6836:0 6837:0 6838:0 6839:0 6840:0 6841:0.00797804 6842:0 6843:0 6844:0 6845:0 6846:0 6847:0 6848:0 6849:0.0811304 6850:0.141311 6851:0.156222 6852:0.157437 6853:0.16247 6854:0.185676 6855:0.170695 6856:0.159337 6857:0.144515 6858:0.167677 6859:0.154781 6860:0.148947 6861:0.138047 6862:0.164601 6863:0.185045 6864:0.16179 6865:0.190464 6866:0.204024 6867:0.191974 6868:0.240871 6869:0.266249 6870:0.265481 6871:0.191977 6872:0.183014 6873:0.186658 6874:0.190399 6875:0.263453 6876:0.356325 6877:0.429865 6878:0.396379 6879:0.360349 6880:0.200228 6881:0.169021 6882:0.279771 6883:0.355577 6884:0.352213 6885:0.360274 6886:0.345103 6887:0.360038 6888:0.184704 6889:0.178504 6890:0.223109 6891:0.343541 6892:0.299627 6893:0.242906 6894:0.20401 6895:0.260426 6896:0.181886 6897:0.18463 6898:0.200523 6899:0.231672 6900:0.267578 6901:0.238522 6902:0.200028 6903:0.210351 6904:0.216963 6905:0.238276 6906:0.243645 6907:0.226119 6908:0.213364 6909:0.213667 6910:0.201066 6911:0.185183 6912:0.189653 6913:0 6914:0 6915:0 6916:0 6917:0 6918:0 6919:0 6920:0.0115702 6921:0 6922:0 6923:0 6924:0 6925:0 6926:0 6927:0 6928:0.0153197 6929:0 6930:0 6931:0 6932:0 6933:0 6934:0 6935:0 6936:0.0161258 6937:0 6938:0 6939:0 6940:0 6941:0 6942:0 6943:0 6944:0.0158918 6945:0 6946:0 6947:0 6948:0 6949:0 6950:0 6951:0 6952:0.0273136 6953:0 6954:0 6955:0 6956:0 6957:0 6958:0 6959:0 6960:0.0199908 6961:0 6962:0 6963:0 6964:0 6965:0 6966:0 6967:0 6968:0.0428305 6969:0 6970:0 6971:0 6972:0 6973:0 6974:0 6975:0 6976:0.0519985 6977:0.0693933 6978:0.09474 6979:0.0963282 6980:0.0954801 6981:0.0815442 6982:0.093908 6983:0.105322 6984:0.100891 6985:0.103002 6986:0.109745 6987:0.118227 6988:0.0902773 6989:0.138948 6990:0.10653 6991:0.116326 6992:0.114512 6993:0.108709 6994:0.108221 6995:0.121717 6996:0.139473 6997:0.18149 6998:0.180954 6999:0.143961 7000:0.10248 7001:0.114729 7002:0.113631 7003:0.206861 7004:0.158123 7005:0.185317 7006:0.170689 7007:0.19684 7008:0.100893 7009:0.0974941 7010:0.114787 7011:0.195766 7012:0.184033 7013:0.13215 7014:0.137937 7015:0.185648 7016:0.0987799 7017:0.117198 7018:0.12549 7019:0.15536 7020:0.169669 7021:0.128617 7022:0.128746 7023:0.100328 7024:0.12407 7025:0.128675 7026:0.120911 7027:0.120394 7028:0.115975 7029:0.117279 7030:0.112173 7031:0.117404 7032:0.132465 7033:0.130457 7034:0.12003 7035:0.104451 7036:0.10467 7037:0.106626 7038:0.109573 7039:0.10331 7040:0.140424 7041:0.0160611 7042:0.078397 7043:0.0850637 7044:0.0795269 7045:0.0786596 7046:0.0933748 7047:0.108597 7048:0.101557 7049:0.0694178 7050:0.0726639 7051:0.073369 7052:0.0579904 7053:0.0899153 7054:0.0870901 7055:0.0850175 7056:0.064795 7057:0.129778 7058:0.107549 7059:0.0719427 7060:0.116602 7061:0.136702 7062:0.145 7063:0.125661 7064:0.0864338 7065:0.132213 7066:0.103379 7067:0.155956 7068:0.23223 7069:0.296858 7070:0.243058 7071:0.254755 7072:0.100463 7073:0.0987778 7074:0.135397 7075:0.268908 7076:0.282974 7077:0.289038 7078:0.280858 7079:0.296554 7080:0.105713 7081:0.0956339 7082:0.191492 7083:0.270443 7084:0.228051 7085:0.160744 7086:0.193709 7087:0.207213 7088:0.134084 7089:0.124458 7090:0.116762 7091:0.167713 7092:0.190723 7093:0.125576 7094:0.0993761 7095:0.0937092 7096:0.138016 7097:0.132238 7098:0.108037 7099:0.118337 7100:0.101941 7101:0.100128 7102:0.0989973 7103:0.106237 7104:0.143561 7105:0.0464097 7106:0.0659007 7107:0.0610279 7108:0.0645385 7109:0.06736 7110:0.069172 7111:0.0667804 7112:0.0764548 7113:0.0753558 7114:0.0550456 7115:0.053583 7116:0.0502537 7117:0.0826478 7118:0.0541823 7119:0.0490674 7120:0.0676477 7121:0.0725569 7122:0.042345 7123:0.0614134 7124:0.0772822 7125:0.0894181 7126:0.087976 7127:0.0670566 7128:0.0760217 7129:0.0710037 7130:0.0640184 7131:0.115444 7132:0.0786265 7133:0.0766807 7134:0.0660631 7135:0.0911296 7136:0.082601 7137:0.0623597 7138:0.0522135 7139:0.125113 7140:0.0775489 7141:0.0454711 7142:0.0344015 7143:0.0843911 7144:0.088937 7145:0.0794746 7146:0.0640233 7147:0.0545229 7148:0.100255 7149:0.0558433 7150:0.0665193 7151:0.0475374 7152:0.105084 7153:0.117864 7154:0.0743523 7155:0.0695989 7156:0.0405271 7157:0.0415407 7158:0.0561413 7159:0.0655903 7160:0.10495 7161:0.0840774 7162:0.0762159 7163:0.0754795 7164:0.0488861 7165:0.0426051 7166:0.0571666 7167:0.0644903 7168:0.131495 7169:0 7170:0 7171:0 7172:0 7173:0 7174:0 7175:0 7176:0.0558456 7177:0 7178:0 7179:0 7180:0 7181:0 7182:0 7183:0 7184:0.0599758 7185:0 7186:0 7187:0 7188:0 7189:0 7190:0 7191:0 7192:0.0534399 7193:0 7194:0 7195:0 7196:0 7197:0 7198:0 7199:0 7200:0.0622882 7201:0 7202:0 7203:0 7204:0 7205:0 7206:0 7207:0 7208:0.0518499 7209:0 7210:0 7211:0 7212:0 7213:0 7214:0 7215:0 7216:0.039621 7217:0 7218:0 7219:0 7220:0 7221:0 7222:0 7223:0 7224:0.0362169 7225:0 7226:0 7227:0 7228:0 7229:0 7230:0 7231:0 7232:0.0416937 7233:0.0283004 7234:0 7235:0 7236:0 7237:0 7238:0 7239:0 7240:0 7241:0.026052 7242:0 7243:0 7244:0 7245:0 7246:0 7247:0 7248:0.00301085 7249:0.0282075 7250:0 7251:0 7252:0 7253:0 7254:0 7255:0 7256:0 7257:0.0241407 7258:0 7259:0 7260:0 7261:0 7262:0 7263:0 7264:0 7265:0.0229144 7266:0 7267:0 7268:0 7269:0 7270:0 7271:0 7272:0 7273:0.0302594 7274:0 7275:0 7276:0 7277:0 7278:0 7279:0 7280:0 7281:0.0245501 7282:0 7283:0 7284:0 7285:0 7286:0 7287:0 7288:0 7289:0.0350975 7290:0 7291:0 7292:0 7293:0 7294:0 7295:0 7296:0 7297:0 7298:0 7299:0 7300:0 7301:0 7302:0 7303:0 7304:0 7305:0 7306:0 7307:0 7308:0 7309:0 7310:0 7311:0 7312:0 7313:0 7314:0 7315:0 7316:0 7317:0 7318:0 7319:0 7320:0 7321:0 7322:0 7323:0 7324:0 7325:0 7326:0 7327:0 7328:0 7329:0 7330:0 7331:0 7332:0 7333:0 7334:0 7335:0 7336:0 7337:0 7338:0 7339:0 7340:0 7341:0 7342:0 7343:0 7344:0 7345:0 7346:0 7347:0 7348:0 7349:0 7350:0 7351:0 7352:0 7353:0 7354:0 7355:0 7356:0 7357:0 7358:0 7359:0 7360:0 7361:0.00677324 7362:0 7363:0 7364:0 7365:0 7366:0 7367:0 7368:0 7369:0 7370:0 7371:0 7372:0.00301895 7373:0 7374:0 7375:0 7376:0 7377:0 7378:0 7379:0 7380:0 7381:0 7382:0 7383:0 7384:0 7385:0 7386:0 7387:0 7388:0 7389:0 7390:0 7391:0 7392:0 7393:0 7394:0 7395:0 7396:0 7397:0 7398:0 7399:0 7400:0 7401:0 7402:0 7403:0 7404:0 7405:0 7406:0 7407:0 7408:0 7409:0 7410:0 7411:0 7412:0 7413:0 7414:0 7415:0 7416:0 7417:0 7418:0 7419:0 7420:0 7421:0 7422:0 7423:0 7424:0 7425:0.0396988 7426:0.0400478 7427:0.0297091 7428:0.03902 7429:0.0266985 7430:0.0226872 7431:0.044039 7432:0.0358987 7433:0.0266722 7434:0.0137654 7435:0.0237179 7436:0.00896789 7437:0.0188221 7438:0.0230456 7439:0.00516256 7440:0.0435405 7441:0.0397816 7442:0.012388 7443:0 7444:0.0064554 7445:0.0106042 7446:0.0371969 7447:0.0101582 7448:0.0451233 7449:0.0519564 7450:0.00792438 7451:0.0155218 7452:0.0124564 7453:0 7454:0 7455:0 7456:0.0444232 7457:0.0528234 7458:0 7459:0.0508122 7460:0.0356752 7461:0.0533984 7462:0.0488025 7463:0.0183185 7464:0.0560337 7465:0.0507998 7466:0.0602863 7467:0.0519607 7468:0.0537644 7469:0.0471828 7470:0.0563792 7471:0.0286716 7472:0.0605013 7473:0.05025 7474:0.0221565 7475:0.037507 7476:0.034118 7477:0.0205053 7478:0.0120584 7479:0.02352 7480:0.0628899 7481:0.0722205 7482:0.0242696 7483:0.0276554 7484:0.0303601 7485:0.0206413 7486:0.0212759 7487:0.0606385 7488:0.0632129 7489:0.0837301 7490:0.0866892 7491:0.0917475 7492:0.0963639 7493:0.0907331 7494:0.103508 7495:0.104179 7496:0.106336 7497:0.0894749 7498:0.0950746 7499:0.0972833 7500:0.102202 7501:0.111187 7502:0.0999086 7503:0.104079 7504:0.112051 7505:0.103628 7506:0.0982782 7507:0.107194 7508:0.106001 7509:0.11845 7510:0.120761 7511:0.107378 7512:0.110475 7513:0.112781 7514:0.110988 7515:0.122582 7516:0.145745 7517:0.154138 7518:0.151685 7519:0.154257 7520:0.117244 7521:0.0959833 7522:0.11831 7523:0.167318 7524:0.181534 7525:0.16736 7526:0.187761 7527:0.173471 7528:0.132347 7529:0.0974477 7530:0.149419 7531:0.139196 7532:0.174539 7533:0.140685 7534:0.143063 7535:0.132611 7536:0.140005 7537:0.109367 7538:0.119625 7539:0.134609 7540:0.13586 7541:0.116206 7542:0.104612 7543:0.0992506 7544:0.140832 7545:0.12016 7546:0.124126 7547:0.12586 7548:0.124274 7549:0.118645 7550:0.114594 7551:0.133631 7552:0.139788 7553:0 7554:0 7555:0 7556:0 7557:0 7558:0 7559:0 7560:0 7561:0 7562:0 7563:0 7564:0 7565:0 7566:0 7567:0 7568:0 7569:0 7570:0 7571:0 7572:0 7573:0 7574:0 7575:0 7576:0 7577:0 7578:0 7579:0 7580:0 7581:0 7582:0 7583:0 7584:0 7585:0 7586:0 7587:0 7588:0 7589:0 7590:0 7591:0 7592:0 7593:0 7594:0 7595:0 7596:0 7597:0 7598:0 7599:0 7600:0 7601:0 7602:0 7603:0 7604:0 7605:0 7606:0 7607:0 7608:0 7609:0 7610:0 7611:0 7612:0 7613:0 7614:0 7615:0 7616:0 7617:0.067771 7618:0.0834427 7619:0.0788761 7620:0.0878606 7621:0.0745495 7622:0.0719408 7623:0.0993743 7624:0.0886216 7625:0.0991537 7626:0.0942871 7627:0.0959897 7628:0.0784222 7629:0.0946672 7630:0.0973549 7631:0.0934002 7632:0.0967679 7633:0.114405 7634:0.0943767 7635:0.0917062 7636:0.0906235 7637:0.119086 7638:0.106693 7639:0.1116 7640:0.0981425 7641:0.127062 7642:0.0920014 7643:0.134715 7644:0.14686 7645:0.135475 7646:0.130674 7647:0.130977 7648:0.110285 7649:0.104977 7650:0.116632 7651:0.149613 7652:0.179023 7653:0.169361 7654:0.165221 7655:0.167847 7656:0.105221 7657:0.121975 7658:0.150117 7659:0.148847 7660:0.163919 7661:0.1546 7662:0.162575 7663:0.147191 7664:0.123929 7665:0.131843 7666:0.136064 7667:0.125408 7668:0.129268 7669:0.102559 7670:0.110226 7671:0.121476 7672:0.139316 7673:0.120278 7674:0.112483 7675:0.117205 7676:0.113654 7677:0.105493 7678:0.101387 7679:0.0996333 7680:0.100523 7681:0.0530065 7682:0.0557446 7683:0.0542748 7684:0.0526789 7685:0.0496941 7686:0.0596363 7687:0.0662278 7688:0.0763619 7689:0.0900993 7690:0.060416 7691:0.0591866 7692:0.0283808 7693:0.0927249 7694:0.0498823 7695:0.063383 7696:0.0671667 7697:0.112088 7698:0.0729569 7699:0.0663211 7700:0.128611 7701:0.125234 7702:0.147839 7703:0.0863221 7704:0.0712963 7705:0.108268 7706:0.0510398 7707:0.137033 7708:0.19621 7709:0.224171 7710:0.220709 7711:0.171792 7712:0.0763524 7713:0.0776018 7714:0.184826 7715:0.188959 7716:0.172106 7717:0.147033 7718:0.180169 7719:0.16009 7720:0.0742804 7721:0.0957955 7722:0.0974175 7723:0.137146 7724:0.148762 7725:0.0931266 7726:0.0854273 7727:0.0691424 7728:0.116424 7729:0.115859 7730:0.0733806 7731:0.0949103 7732:0.106717 7733:0.0849916 7734:0.0777816 7735:0.0907277 7736:0.134478 7737:0.131241 7738:0.105798 7739:0.0860202 7740:0.0789247 7741:0.0769854 7742:0.0660727 7743:0.0683562 7744:0.0707707 7745:0.0660406 7746:0.0660993 7747:0.0679016 7748:0.0697288 7749:0.0788585 7750:0.0648808 7751:0.0654316 7752:0.108591 7753:0.0551689 7754:0.0546906 7755:0.0499551 7756:0.0719483 7757:0.0549915 7758:0.0624691 7759:0.0612602 7760:0.0797539 7761:0.0536214 7762:0.0534265 7763:0.0828085 7764:0.0421614 7765:0.0346058 7766:0.0222235 7767:0.062166 7768:0.0646613 7769:0.0597181 7770:0.0624167 7771:0.0370342 7772:0 7773:0 7774:0 7775:0.0535462 7776:0.0678111 7777:0.0705013 7778:0.0270369 7779:0.0295776 7780:0.0425562 7781:0.0557908 7782:0.0137884 7783:0.0794954 7784:0.0641944 7785:0.0666762 7786:0.0413585 7787:0.0297447 7788:0.0554664 7789:0.0568348 7790:0.0760831 7791:0.0723937 7792:0.0555908 7793:0.0636129 7794:0.0416542 7795:0.0542297 7796:0.0619768 7797:0.0597952 7798:0.0660925 7799:0.059043 7800:0.104369 7801:0.0869141 7802:0.051929 7803:0.0563421 7804:0.0576004 7805:0.0534602 7806:0.0557587 7807:0.0577238 7808:0.0875115 7809:0.0343453 7810:0.0126447 7811:0.011289 7812:0.00195304 7813:0.00436167 7814:0.00491541 7815:0.00601084 7816:0.00460711 7817:0 7818:0 7819:0 7820:0 7821:0 7822:0 7823:0 7824:0 7825:0 7826:0 7827:0 7828:0 7829:0 7830:0 7831:0 7832:0 7833:0 7834:0 7835:0 7836:0 7837:0 7838:0 7839:0 7840:0 7841:0 7842:0 7843:0 7844:0 7845:0 7846:0 7847:0 7848:0 7849:0 7850:0 7851:0 7852:0 7853:0 7854:0 7855:0 7856:0 7857:0 7858:0 7859:0 7860:0 7861:0 7862:0 7863:0 7864:0 7865:0.032216 7866:0.0401205 7867:0.0478266 7868:0.0346087 7869:0.0263053 7870:0.0164681 7871:0.0224284 7872:0.0140857 7873:0 7874:0 7875:0 7876:0 7877:0 7878:0 7879:0 7880:0 7881:0 7882:0 7883:0 7884:0 7885:0 7886:0 7887:0 7888:0 7889:0 7890:0 7891:0 7892:0.0134274 7893:0.0117399 7894:0.0164739 7895:0.0131977 7896:0 7897:0 7898:0.0302213 7899:0.0247054 7900:0.0422548 7901:0.0269641 7902:0.02481 7903:0.0156415 7904:0 7905:0 7906:0.0317808 7907:0.0255399 7908:0.00403062 7909:0 7910:0.0162826 7911:0 7912:0 7913:0 7914:0.0156234 7915:0 7916:0 7917:0 7918:0.00298708 7919:0 7920:0 7921:0 7922:0 7923:0 7924:0 7925:0 7926:0 7927:0 7928:0 7929:0 7930:0 7931:0 7932:0 7933:0 7934:0 7935:0.0229589 7936:0 7937:0.0132834 7938:0.0152828 7939:0.0184441 7940:0.0229815 7941:0.0174858 7942:0.0255969 7943:0.0280772 7944:0.0484571 7945:0.0184399 7946:0.01647 7947:0.0227932 7948:0.0272475 7949:0.0376309 7950:0.0366197 7951:0.0162835 7952:0.0486727 7953:0.0282832 7954:0.0262556 7955:0.0315863 7956:0.0417761 7957:0.0495813 7958:0.0541919 7959:0.048347 7960:0.0449928 7961:0.016646 7962:0.0215546 7963:0.0580822 7964:0.0491614 7965:0.0534128 7966:0.0649407 7967:0.0405135 7968:0.0406114 7969:0.0173524 7970:0.0474949 7971:0.0787216 7972:0.0465753 7973:0.00787444 7974:0.0343872 7975:0.0209978 7976:0.0376969 7977:0.0133024 7978:0.0218697 7979:0.0503585 7980:0.0342707 7981:0.028889 7982:0.0105968 7983:0.0405045 7984:0.0599842 7985:0.0385527 7986:0.0176699 7987:0.0102645 7988:0.00113211 7989:0.0123642 7990:0.0210996 7991:0.0194793 7992:0.0665101 7993:0.038315 7994:0.0349611 7995:0.0351484 7996:0.0372748 7997:0.0382683 7998:0.0181986 7999:0.0334278 8000:0.0412817 8001:0.0340864 8002:0.0266747 8003:0.0316371 8004:0.030496 8005:0.0300739 8006:0.0309525 8007:0.0285374 8008:0.0308915 8009:0.0090953 8010:0 8011:0 8012:0 8013:0 8014:0 8015:0 8016:0.0121254 8017:0 8018:0 8019:0.011851 8020:0 8021:0.0158033 8022:0 8023:0 8024:0.00156583 8025:0 8026:0 8027:0 8028:0 8029:0 8030:0 8031:0 8032:0.00904585 8033:0.000761916 8034:0 8035:0 8036:0 8037:0 8038:0 8039:0 8040:0.0117162 8041:0.00827837 8042:0 8043:0 8044:0 8045:0 8046:0 8047:0 8048:0.00319373 8049:0.00781113 8050:0 8051:0 8052:0 8053:0 8054:0 8055:0 8056:0 8057:0 8058:0 8059:0 8060:0 8061:0 8062:0 8063:0 8064:0.0232145 8065:0 8066:0 8067:0 8068:0 8069:0 8070:0 8071:0 8072:0 8073:0 8074:0 8075:0 8076:0 8077:0 8078:0 8079:0 8080:0 8081:0 8082:0 8083:0 8084:0 8085:0 8086:0 8087:0 8088:0 8089:0 8090:0 8091:0 8092:0 8093:0 8094:0 8095:0 8096:0 8097:0 8098:0 8099:0 8100:0 8101:0 8102:0 8103:0 8104:0 8105:0 8106:0 8107:0 8108:0 8109:0 8110:0 8111:0 8112:0 8113:0 8114:0 8115:0 8116:0 8117:0 8118:0 8119:0 8120:0 8121:0 8122:0 8123:0 8124:0 8125:0 8126:0 8127:0 8128:0 8129:0 8130:0 8131:0 8132:0 8133:0 8134:0 8135:0 8136:0 8137:0.0134307 8138:0 8139:0 8140:0 8141:0.0100819 8142:0 8143:0 8144:0 8145:0.00295802 8146:0 8147:0.0199513 8148:0.027722 8149:0.0302621 8150:0.0161272 8151:0.013753 8152:0 8153:0 8154:0.024593 8155:0.0448775 8156:0.0305151 8157:0.018543 8158:0.0328562 8159:0.0381248 8160:0 8161:0.00178052 8162:0.0130347 8163:0.0418108 8164:0.0375746 8165:0.0248849 8166:0.0130313 8167:0.0317875 8168:0 8169:0.0118563 8170:0.00460889 8171:0.0144695 8172:0.0315069 8173:0.00889437 8174:0.0194691 8175:0.0052118 8176:0 8177:0.0439078 8178:0.0271971 8179:0.00562176 8180:0 8181:0 8182:0 8183:0.0167223 8184:0 8185:0.026896 8186:0.0354167 8187:0.0401342 8188:0.0108336 8189:0.011361 8190:0.0057945 8191:0.0214326 8192:0.0231091\n"]}]},{"cell_type":"markdown","source":["## 3) Huấn luyện SVM:"],"metadata":{"id":"9-uQ7S76uAH5"}},{"cell_type":"code","source":["!git clone https://github.com/cjlin1/libsvm.git\n","%cd libsvm\n","!make"],"metadata":{"id":"qK3xYbwEt-IQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761151458,"user_tz":-420,"elapsed":5380,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"69862d21-e8e1-4c64-e366-47ebed061821"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'libsvm'...\n","remote: Enumerating objects: 4201, done.\u001b[K\n","remote: Counting objects: 100% (232/232), done.\u001b[K\n","remote: Compressing objects: 100% (113/113), done.\u001b[K\n","remote: Total 4201 (delta 142), reused 119 (delta 119), pack-reused 3969 (from 3)\u001b[K\n","Receiving objects: 100% (4201/4201), 9.91 MiB | 18.52 MiB/s, done.\n","Resolving deltas: 100% (2318/2318), done.\n","/content/libsvm\n","g++ -Wall -Wconversion -O3 -fPIC -c svm.cpp\n","g++ -Wall -Wconversion -O3 -fPIC svm-train.c svm.o -o svm-train -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-predict.c svm.o -o svm-predict -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-scale.c -o svm-scale\n"]}]},{"cell_type":"code","source":["!./svm-train -s 0 -t 0 -c 1.0 \\\n","  \"/content/train_svm.txt\" \\\n","  /content/model_ae_svm"],"metadata":{"id":"RlNT7iL2uHps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761172984,"user_tz":-420,"elapsed":21519,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"50cf66c0-18e2-49df-dd52-eb32a556afed"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":[".....*..*\n","optimization finished, #iter = 1492\n","nu = 0.245260\n","obj = -29.943886, rho = -3.493772\n","nSV = 105, nBSV = 16\n","......*..*\n","optimization finished, #iter = 1704\n","nu = 0.603519\n","obj = -82.971158, rho = 3.318647\n","nSV = 166, nBSV = 73\n","......*..*\n","optimization finished, #iter = 1809\n","nu = 0.262696\n","obj = -31.621355, rho = -3.819079\n","nSV = 121, nBSV = 12\n",".....*..*\n","optimization finished, #iter = 1567\n","nu = 0.459547\n","obj = -60.393632, rho = 2.756493\n","nSV = 143, nBSV = 53\n",".....*...*\n","optimization finished, #iter = 1718\n","nu = 0.379453\n","obj = -47.064683, rho = -2.355419\n","nSV = 143, nBSV = 36\n","..*..*\n","optimization finished, #iter = 875\n","nu = 0.153576\n","obj = -19.137181, rho = -0.889922\n","nSV = 73, nBSV = 10\n",".....*...*\n","optimization finished, #iter = 1594\n","nu = 0.512450\n","obj = -65.580952, rho = 1.063530\n","nSV = 140, nBSV = 59\n",".....*...*\n","optimization finished, #iter = 1648\n","nu = 0.450036\n","obj = -53.494736, rho = -0.827437\n","nSV = 135, nBSV = 38\n","....*.*\n","optimization finished, #iter = 1163\n","nu = 0.200532\n","obj = -23.916680, rho = 0.145894\n","nSV = 77, nBSV = 13\n",".....*..*\n","optimization finished, #iter = 1519\n","nu = 0.256065\n","obj = -30.835728, rho = 4.536750\n","nSV = 102, nBSV = 20\n",".......*....*\n","optimization finished, #iter = 2463\n","nu = 0.470865\n","obj = -61.240333, rho = 0.940483\n","nSV = 165, nBSV = 43\n","...*..*\n","optimization finished, #iter = 1129\n","nu = 0.224288\n","obj = -26.240384, rho = 3.830631\n","nSV = 94, nBSV = 17\n",".....*..*\n","optimization finished, #iter = 1636\n","nu = 0.320504\n","obj = -38.609421, rho = 0.529659\n","nSV = 129, nBSV = 28\n",".....*..*\n","optimization finished, #iter = 1532\n","nu = 0.305695\n","obj = -37.445657, rho = 3.437594\n","nSV = 117, nBSV = 24\n","......*...*\n","optimization finished, #iter = 1870\n","nu = 0.311890\n","obj = -36.411059, rho = 4.151047\n","nSV = 115, nBSV = 19\n","....*...*\n","optimization finished, #iter = 1334\n","nu = 0.233198\n","obj = -25.476330, rho = 2.638268\n","nSV = 96, nBSV = 12\n",".....*..*\n","optimization finished, #iter = 1609\n","nu = 0.303692\n","obj = -38.666243, rho = 2.021792\n","nSV = 115, nBSV = 25\n","....*.*\n","optimization finished, #iter = 1130\n","nu = 0.256768\n","obj = -29.533740, rho = -5.496937\n","nSV = 108, nBSV = 16\n",".....*..*\n","optimization finished, #iter = 1560\n","nu = 0.631595\n","obj = -91.201979, rho = -0.105970\n","nSV = 162, nBSV = 84\n",".....*..*\n","optimization finished, #iter = 1419\n","nu = 0.467542\n","obj = -58.439863, rho = -5.857262\n","nSV = 140, nBSV = 49\n","..*.*\n","optimization finished, #iter = 705\n","nu = 0.229640\n","obj = -28.050489, rho = -2.118941\n","nSV = 75, nBSV = 22\n","......*.....*\n","optimization finished, #iter = 2144\n","nu = 0.482447\n","obj = -60.002348, rho = -2.487119\n","nSV = 131, nBSV = 51\n",".....*..*\n","optimization finished, #iter = 1432\n","nu = 0.475880\n","obj = -57.923177, rho = -2.925857\n","nSV = 126, nBSV = 52\n","....*..*\n","optimization finished, #iter = 1375\n","nu = 0.281835\n","obj = -36.726555, rho = -1.350009\n","nSV = 98, nBSV = 24\n",".....*..*\n","optimization finished, #iter = 1609\n","nu = 0.237308\n","obj = -27.085143, rho = 4.603863\n","nSV = 104, nBSV = 11\n",".....*...*\n","optimization finished, #iter = 1905\n","nu = 0.304250\n","obj = -36.124055, rho = 1.919529\n","nSV = 130, nBSV = 16\n",".....*..*\n","optimization finished, #iter = 1587\n","nu = 0.283425\n","obj = -34.671854, rho = 3.386397\n","nSV = 111, nBSV = 16\n",".......*...*\n","optimization finished, #iter = 2239\n","nu = 0.329940\n","obj = -37.508662, rho = 4.594996\n","nSV = 126, nBSV = 18\n",".....*...*\n","optimization finished, #iter = 1704\n","nu = 0.271312\n","obj = -29.076931, rho = 2.728454\n","nSV = 117, nBSV = 10\n",".....*...*\n","optimization finished, #iter = 1762\n","nu = 0.281928\n","obj = -35.082206, rho = 2.688797\n","nSV = 120, nBSV = 22\n",".......*...*\n","optimization finished, #iter = 2174\n","nu = 0.393319\n","obj = -46.827901, rho = -4.846348\n","nSV = 139, nBSV = 33\n","...*..*\n","optimization finished, #iter = 994\n","nu = 0.253669\n","obj = -30.792156, rho = -2.271807\n","nSV = 87, nBSV = 23\n",".......*..*\n","optimization finished, #iter = 1825\n","nu = 0.462009\n","obj = -55.892364, rho = -2.618835\n","nSV = 125, nBSV = 48\n","......*...*\n","optimization finished, #iter = 1685\n","nu = 0.473419\n","obj = -56.828904, rho = -2.937602\n","nSV = 132, nBSV = 48\n",".....*..*\n","optimization finished, #iter = 1578\n","nu = 0.350796\n","obj = -46.338294, rho = -1.260071\n","nSV = 117, nBSV = 36\n","....*..*\n","optimization finished, #iter = 1368\n","nu = 0.203155\n","obj = -22.372250, rho = 1.699951\n","nSV = 87, nBSV = 11\n",".......*....*\n","optimization finished, #iter = 2146\n","nu = 0.432772\n","obj = -50.358561, rho = 2.787120\n","nSV = 135, nBSV = 32\n","......*...*\n","optimization finished, #iter = 1847\n","nu = 0.420537\n","obj = -46.569907, rho = 2.295563\n","nSV = 133, nBSV = 32\n","......*..*\n","optimization finished, #iter = 1715\n","nu = 0.308527\n","obj = -37.809864, rho = 2.082997\n","nSV = 112, nBSV = 20\n","....*..*\n","optimization finished, #iter = 1251\n","nu = 0.249327\n","obj = -28.337956, rho = 0.035025\n","nSV = 87, nBSV = 22\n","....*..*\n","optimization finished, #iter = 1130\n","nu = 0.186499\n","obj = -19.410524, rho = 1.419503\n","nSV = 81, nBSV = 12\n","......*...*\n","optimization finished, #iter = 1927\n","nu = 0.519964\n","obj = -73.235849, rho = -0.113391\n","nSV = 148, nBSV = 64\n",".......*...*\n","optimization finished, #iter = 1820\n","nu = 0.556842\n","obj = -64.882704, rho = -0.774245\n","nSV = 145, nBSV = 56\n","......*..*\n","optimization finished, #iter = 1690\n","nu = 0.302178\n","obj = -35.692840, rho = 0.392961\n","nSV = 106, nBSV = 23\n",".....*.*\n","optimization finished, #iter = 1307\n","nu = 0.243180\n","obj = -26.733936, rho = -0.353926\n","nSV = 86, nBSV = 20\n","Total nSV = 958\n"]}]},{"cell_type":"code","source":["!./svm-predict \\\n","  \"/content/test_svm.txt\" \\\n","  /content/model_ae_svm \\\n","  \"/content/pred.txt\""],"metadata":{"id":"yUmv0Z_ZuKBk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766761178661,"user_tz":-420,"elapsed":5674,"user":{"displayName":"Kim Ngân Trần Hoàng","userId":"18250031841149821427"}},"outputId":"758fdc3d-bc4f-48b4-a0a5-a19e94c82bb4"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 38.5% (77/200) (classification)\n"]}]}]}