{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faEDE33-lrk9","outputId":"8c634d71-c81b-489c-ad70-476319af6250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%%writefile load_data.h\n","#pragma once\n","#include <stdint.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#define TRAIN_NUM    50000\n","#define TEST_NUM     10000\n","#define IMG_SIZE     (32*32*3)     // 3072\n","\n","typedef struct {\n","    float* train_images;   // [50000 * 3072]\n","    float* test_images;    // [10000 * 3072]\n","    uint8_t* train_labels; // [50000]\n","    uint8_t* test_labels;  // [10000]\n","    int* train_indices;\n","} Cifar10;\n","\n","void load_cifar10(Cifar10* data);\n","void normalize_cifar10(Cifar10* data);\n","void shuffle_cifar10(Cifar10* data);\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n","void print_cifar10(Cifar10* data);\n","void free_cifar10(Cifar10* data);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzxBOS-GZFZJ","outputId":"e32b4369-8363-41cd-d80f-ace0f0d5873c","executionInfo":{"status":"ok","timestamp":1765986334042,"user_tz":-420,"elapsed":10,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.h\n"]}]},{"cell_type":"code","source":["%%writefile load_data.cu\n","#include \"load_data.h\"\n","\n","static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n","    FILE* f = fopen(filename, \"rb\");\n","    if (!f) {\n","        perror(filename);\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    uint8_t buffer[3073];\n","    for (int i = 0; i < 10000; i++) {\n","        if (fread(buffer, 1, 3073, f) != 3073) {\n","            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n","            fclose(f);\n","            exit(EXIT_FAILURE);\n","        }\n","        labels[i] = buffer[0];\n","        for (int j = 0; j < 3072; j++) {\n","            images_start[i * 3072 + j] = (float)buffer[1 + j];  //Covert unit8 to float\n","        }\n","    }\n","    fclose(f);\n","}\n","\n","void load_cifar10(Cifar10* data) {\n","    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n","    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n","    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n","    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n","\n","    if (!data->train_images || !data->test_images ||\n","        !data->train_labels  || !data->test_labels) {\n","        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n","    for (int i = 0; i < TRAIN_NUM; i++) {\n","        data->train_indices[i] = i;\n","    }\n","\n","    //Load training data\n","    for (int i = 1; i <= 5; i++) {\n","        char filename[100];\n","        snprintf(filename, sizeof(filename), \"cifar-10-batches-bin/data_batch_%d.bin\", i);\n","        read_batch(filename,\n","                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n","                   data->train_labels + (i-1) * 10000);\n","    }\n","\n","    //Load test data\n","    read_batch(\"cifar-10-batches-bin/test_batch.bin\",\n","               data->test_images, data->test_labels);\n","\n","    printf(\"CIFAR-10 loaded successfully\\n\");\n","}\n","\n","void normalize_cifar10(Cifar10* data) {\n","    for (size_t i = 0; i < TRAIN_NUM * IMG_SIZE; i++) {\n","        data->train_images[i] /= 255.0f;\n","    }\n","    for (size_t i = 0; i < TEST_NUM * IMG_SIZE; i++) {\n","        data->test_images[i] /= 255.0f;\n","    }\n","}\n","\n","// Shuffle indices\n","void shuffle_cifar10(Cifar10* data) {\n","    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n","        int j = rand() % (i + 1);\n","        int temp = data->train_indices[i];\n","        data->train_indices[i] = data->train_indices[j];\n","        data->train_indices[j] = temp;\n","    }\n","}\n","\n","void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n","    size_t start = batch_id * batch_size;\n","    for (size_t i = 0; i < batch_size; i++) {\n","        int idx = data->train_indices[start + i];\n","\n","        memcpy(batch_images + i * IMG_SIZE,\n","               data->train_images + idx * IMG_SIZE,\n","               IMG_SIZE * sizeof(float));\n","    }\n","}\n","\n","void print_cifar10(Cifar10* data){\n","    for (int i = 0; i < 2; i++) {\n","        printf(\"Label: %d\\n\", data->train_labels[i]);\n","        for (int j = 0; j < IMG_SIZE; j++) {\n","            printf(\"%f \", data->train_images[i*IMG_SIZE + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","    // for (int i = 0; i < 2; i++) {\n","    //     printf(\"Label: %d\\n\", data->test_labels[i]);\n","    //     for (int j = 0; j < IMG_SIZE; j++) {\n","    //         printf(\"%f \", data->test_images[i*IMG_SIZE + j]);\n","    //     }\n","    // }\n","}\n","\n","void free_cifar10(Cifar10* data) {\n","    free(data->train_images);\n","    free(data->test_images);\n","    free(data->train_labels);\n","    free(data->test_labels);\n","    free(data->train_indices);\n","\n","    data->train_images = data->test_images = NULL;\n","    data->train_labels = data->test_labels = NULL;\n","    data->train_indices = NULL;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AKnqlO4oZF9E","outputId":"971f40db-8885-4ef7-bdc8-61f13ab435ac","executionInfo":{"status":"ok","timestamp":1765986336303,"user_tz":-420,"elapsed":12,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing load_data.cu\n"]}]},{"cell_type":"code","source":["%%writefile cpu_layers.h\n","#pragma once\n","#include <stdio.h>\n","#include <float.h>\n","\n","void Relu(float* input, int N, float* output);\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void MaxPool2D_Forward(float* input, int input_width, int input_height,\n","    int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","    int scale_factor, int filter_count,\n","    float* output, int output_height, int output_width);\n","float MSE(float* input, float* output, int size);\n","void Relu_Backward(float* d_output, float* input,int N);\n","void MSE_Gradient(float* input, float* output, int size, float* d_output);\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width);\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights);\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height, int filter_count, float* d_biases);\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zp_xwdfSYe77","outputId":"fcc3cfb1-0941-4cd9-8436-703b69853964","executionInfo":{"status":"ok","timestamp":1765986337140,"user_tz":-420,"elapsed":10,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_layers.h\n"]}]},{"cell_type":"code","source":["%%writefile cpu_layers.c\n","#include \"cpu_layers.h\"\n","\n","void Relu(float* input, int N, float* output) {\n","    for (int i = 0; i < N; i++) {\n","        output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n","    }\n","}\n","\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width)\n","{\n","    // // Tính toán kích thước output\n","    // int H_out = (input_height + 2 * padding - kernel_height) / stride + 1;\n","    // int W_out = (input_width + 2 * padding - kernel_width) / stride + 1;\n","    // int output_size = filter_count * H_out * W_out;\n","    // output = (float*)malloc(output_size * sizeof(float));\n","    // if (output == NULL) {\n","    //     fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","    //     return;\n","    // }\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua chiều cao output\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            // Lặp qua chiều rộng output\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float sum = 0.0f;\n","                // Lặp qua kênh đầu vào (c_in)\n","                for (int c_in = 0; c_in < input_channels; c_in++) {\n","                    // Lặp qua kernel height\n","                    for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                        // Lặp qua kernel width\n","                        for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                            // Vị trí input tương ứng\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float val = 0.0f;\n","                            // Kiểm tra zero padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int channel_size = input_width * input_height;\n","                                val = input[c_in * channel_size + h_in * input_width + w_in];\n","                            }\n","\n","                            int weight_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            k_h * kernel_width + k_w;\n","\n","                            sum += val * kernel[weight_idx];\n","                        }\n","                    }\n","                }\n","                sum += biases[c_out];  // Thêm bias\n","                int output_idx = h_out * output_width + w_out + c_out * output_width * output_height;\n","                output[output_idx] = sum;\n","            }\n","        }\n","    }\n","}\n","\n","\n","void MaxPool2D_Forward(float* input, int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width) {\n","    // int H_out = (input_height - filter_height) / stride + 1;\n","    // int W_out = (input_width - filter_width) / stride + 1;\n","\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float max_val = -FLT_MAX;\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = c * plane_size_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                        }\n","                    }\n","                }\n","                int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                output[output_idx] = max_val;\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","int scale_factor, int filter_count, float* output, int output_height, int output_width) {\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float val = input[c * plane_size_in + h_in * input_width + w_in];\n","                for (int sh = 0; sh < scale_factor; sh++) { // Gấp đôi hàng\n","                    for (int sw = 0; sw < scale_factor; sw++) { // Gấp đôi cột\n","                        int h_out = h_in * scale_factor + sh;\n","                        int w_out = w_in * scale_factor + sw;\n","                        int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                        output[output_idx] = val;\n","                    }\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","float MSE(float* input, float* output, int size) {\n","    float sum = 0.0f;\n","    for (int i = 0; i < size; i++) {\n","        sum += (output[i] - input[i]) * (output[i] - input[i]);\n","    }\n","    return sum / size;\n","}\n","\n","void Relu_Backward(float* d_output, float* input,int N) {\n","    for (int i = 0; i < N; i++) {\n","        d_output[i] = input[i] > 0.0f ? d_output[i] : 0.0f;\n","    }\n","}\n","\n","void MSE_Gradient(float* input, float* output, int size, float* d_output) {\n","    float sum = 0.0f;\n","    float factor = 2.0f / size;\n","    for (int i = 0; i < size; i++) {\n","        d_output[i] = factor * (output[i] - input[i]);\n","    }\n","}\n","\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* d_input)\n","{\n","    // Chỉ gán vị trí giá trị max của input ban đầu là gradient của lớp tiếp theo (d_output), còn lại là 0\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) { // Khởi tạo gradient của input ban đầu là 0\n","        d_input[i] = 0.0f;\n","    }\n","\n","    for (int c = 0; c < filter_count; c++) {\n","\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","\n","                float max_val = -FLT_MAX;\n","                int max_input_idx = -1;\n","\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = channel_offset_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                            max_input_idx = input_idx;\n","                        }\n","                    }\n","                }\n","                //Lấy gradient từ output\n","                if (max_input_idx != -1) {\n","                    int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                    d_input[max_input_idx] += d_output[output_idx];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width) {\n","\n","    int plane_size_in = d_input_height * d_input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) {\n","        d_input[i] = 0.0f;\n","    }\n","    for (int c = 0; c < filter_count; c++) {\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < d_input_height; h_in++) {\n","            for (int w_in = 0; w_in < d_input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                int h_start_out = h_in * scale_factor;\n","                int w_start_out = w_in * scale_factor;\n","                for (int sh = 0; sh < scale_factor; sh++) {\n","                    for (int sw = 0; sw < scale_factor; sw++) {\n","                        int h_out = h_start_out + sh;\n","                        int w_out = w_start_out + sw;\n","                        if (h_out < d_output_height && w_out < d_output_width) {\n","                            int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                            sum_gradient += d_output[output_idx];\n","                        }\n","                    }\n","                }\n","                int input_idx = channel_offset_in + h_in * d_input_width + w_in;\n","                d_input[input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input) {\n","    // Thực hiện tích chập giữa dE/dO và kernel (xoay 180 độ) để tính dE/dI\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh input (kênh output gradient)\n","    for (int c_in = 0; c_in < input_channels; c_in++) {\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                // Lặp qua kênh output (số lượng filter)\n","                for (int c_out = 0; c_out < filter_count; c_out++) {\n","                    // Lặp qua kernel (xoay 180 độ)\n","                    for (int kh = 0; kh < kernel_height; kh++) {\n","                        for (int kw = 0; kw < kernel_width; kw++) {\n","                            int h_out = h_in - kh + padding;\n","                            int w_out = w_in - kw + padding;\n","                            float d_output_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_out >= 0 && h_out < d_output_height && w_out >= 0 && w_out < d_output_width) {\n","                                int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                                d_output_val = d_output[d_output_idx];\n","                            }\n","                            // Tính chỉ số kernel (xoay 180 độ)\n","                            int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            (kernel_height - 1 - kh) * kernel_width + (kernel_width - 1 - kw);\n","\n","                            sum_gradient += d_output_val * kernel[kernel_idx];\n","                        }\n","                    }\n","                }\n","                int d_input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                d_input[d_input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights) {\n","    // Thực hiện tích chập giữa dE/dO và input để tính dE/dW\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua kênh đầu vào\n","        for (int c_in = 0; c_in < input_channels; c_in++) {\n","            // Lặp qua kích thước kernel\n","            for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                    float sum_gradient = 0.0f;\n","                    // Lặp qua output grid (d_output) để tích lũy\n","                    for (int h_out = 0; h_out < d_output_height; h_out++) {\n","                        for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float input_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                                input_val = input[input_idx];\n","                            }\n","                            int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                            float d_output_val = d_output[d_output_idx];\n","                            // Tính gradient\n","                            sum_gradient += input_val * d_output_val;\n","                        }\n","                    }\n","                    int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                    k_h * kernel_width + k_w;\n","                    d_weights[kernel_idx] += sum_gradient;\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height,\n","    int filter_count, float* d_biases) {\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua từng filter\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        float sum_gradient = 0.0f;\n","        // Lặp qua từng vị trí trong output\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                // Tính chỉ số trong mảng d_output\n","                int output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                // Cộng dồn gradient từ d_output\n","                sum_gradient += d_output[output_idx];\n","            }\n","        }\n","        d_biases[c_out] += sum_gradient;\n","    }\n","}\n","\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params) {\n","    for (int i = 0; i < N_params; i++) {\n","        weights[i] -= (learning_rate * d_weights[i]);\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBGd4bJAYgvt","outputId":"90785c56-d15f-4f21-c294-b4aef6f866b2","executionInfo":{"status":"ok","timestamp":1765986337934,"user_tz":-420,"elapsed":17,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_layers.c\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt2.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward_opt2(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight_opt2(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);\n"],"metadata":{"id":"ViSZDlCj2IL0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1efe872f-1342-4480-e779-a4313347a490","executionInfo":{"status":"ok","timestamp":1765986338672,"user_tz":-420,"elapsed":10,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt2.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt2.cu\n","// ============================================================================\n","// OPTIMIZED CONV2D - FIX CRITICAL ISSUES\n","// Target: 12.83ms → 7-8ms (forward), similar for backward\n","// ============================================================================\n","\n","#include <cuda_runtime.h>\n","#include \"gpu_layers_opt2.h\"\n","\n","\n","// Optimal configurations\n","#define TILE_W 16\n","#define TILE_H 16\n","#define K 3\n","\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","// ============================================================================\n","// FORWARD PASS - FIXED\n","// Key fix: Efficient cooperative loading\n","// ============================================================================\n","__global__ void conv2d_forward_opt2(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","    int tid = ty * blockDim.x + tx;\n","    int num_threads = blockDim.x * blockDim.y;  // 256\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    // Base position for loading (với padding)\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    // Loop over input channels\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // ✅ OPTIMIZED: Cooperative loading\n","        // Total elements = 18x18 = 324\n","        // 256 threads → mỗi thread load 1-2 elements\n","        int total_elements = BLOCK_H * BLOCK_W;\n","\n","        for (int idx = tid; idx < total_elements; idx += num_threads) {\n","            int i = idx / BLOCK_W;  // row\n","            int j = idx % BLOCK_W;  // col\n","\n","            int h_in = row_start + i;\n","            int w_in = col_start + j;\n","\n","            // Load with bounds checking\n","            if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                smem[i][j] = input[idx4(n, c_in, h_in, w_in, C_in, H, W)];\n","            } else {\n","                smem[i][j] = 0.0f;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // ✅ OPTIMIZED: Compute convolution with unrolling\n","        if (h_out < H_out && w_out < W_out && n < N) {\n","            int smem_row = ty * stride;\n","            int smem_col = tx * stride;\n","\n","            #pragma unroll\n","            for (int i = 0; i < K; ++i) {\n","                #pragma unroll\n","                for (int j = 0; j < K; ++j) {\n","                    float in_val = smem[smem_row + i][smem_col + j];\n","                    float w_val = weight[idx4(c_out, c_in, i, j, C_in, K, K)];\n","                    value += in_val * w_val;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    // Write output with bias from constant memory\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","// ============================================================================\n","// BACKWARD INPUT - FIXED\n","// Key fix: Parallel halo loading\n","// ============================================================================\n","\n","__global__ void conv2d_backward_input_opt2(\n","    float* __restrict__ dY,     // [N, C_out, H, W]\n","    float* __restrict__ weight, // [C_out, C_in, K, K]\n","    float* __restrict__ dX,     // [N, C_in, H, W]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    // Only support K=3, pad=1, stride=1\n","    if (stride != 1 || pad != 1 || K != 3) return;\n","\n","    const int H_out = H;\n","    const int W_out = W;\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int base_h = blockIdx.y * TILE_H;\n","    int base_w = blockIdx.x * TILE_W;\n","\n","    int h_in = base_h + ty;\n","    int w_in = base_w + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n    = nc / C_in;\n","\n","    if (n >= N || c_in >= C_in) return;\n","\n","    // Shared memory for tile + halo\n","    __shared__ float s_dY[TILE_H + 2][TILE_W + 2];\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // ---- Load dY tile (n, c_out, :, :) into shared memory ----\n","        for (int sy = ty; sy < TILE_H + 2; sy += blockDim.y) {\n","            for (int sx = tx; sx < TILE_W + 2; sx += blockDim.x) {\n","                int ho = base_h + sy - 1;  // -pad\n","                int wo = base_w + sx - 1;  // -pad\n","\n","                float v = 0.0f;\n","                if (ho >= 0 && ho < H_out && wo >= 0 && wo < W_out) {\n","                    v = dY[idx4(n, c_out, ho, wo, C_out, H_out, W_out)];\n","                }\n","                s_dY[sy][sx] = v;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // ---- Compute gradient for (n, c_in, h_in, w_in) ----\n","        if (h_in < H && w_in < W) {\n","            #pragma unroll\n","            for (int kh = 0; kh < K; ++kh) {\n","                #pragma unroll\n","                for (int kw = 0; kw < K; ++kw) {\n","                    // Shared memory indices\n","                    int sy = ty + 2 - kh;\n","                    int sx = tx + 2 - kw;\n","\n","                    float dy = s_dY[sy][sx];\n","\n","                    // CRITICAL FIX: Flip the kernel indices\n","                    int kh_flip = K - 1 - kh;  // 2, 1, 0\n","                    int kw_flip = K - 1 - kw;  // 2, 1, 0\n","\n","                    // Weight layout [C_out, C_in, K, K]\n","                    float w = weight[idx4(c_out, c_in, kh_flip, kw_flip, C_in, K, K)];\n","                    value += dy * w;\n","                }\n","            }\n","        }\n","\n","        __syncthreads();\n","    }\n","\n","    if (h_in < H && w_in < W) {\n","        dX[idx4(n, c_in, h_in, w_in, C_in, H, W)] = value;\n","    }\n","}\n","// ============================================================================\n","// BACKWARD WEIGHT - FIXED\n","// Key fix: Block-level reduction BEFORE atomic\n","// ============================================================================\n","__global__ void conv2d_backward_weight_opt2(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","    __shared__ float s_dw[K * K][256];  // Reduction buffer\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","    int tid = ty * blockDim.x + tx;\n","    int num_threads = blockDim.x * blockDim.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    // Local accumulator\n","    float dw[K][K];\n","    #pragma unroll\n","    for (int i = 0; i < K; ++i)\n","        #pragma unroll\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // ✅ FIX: Cooperative loading cho dY\n","                int h_out = block_h * TILE_H;\n","                int w_out = block_w * TILE_W;\n","\n","                for (int idx = tid; idx < TILE_H * TILE_W; idx += num_threads) {\n","                    int i = idx / TILE_W;\n","                    int j = idx % TILE_W;\n","                    int h = h_out + i;\n","                    int w = w_out + j;\n","\n","                    if (h < H_out && w < W_out) {\n","                        s_dY[i][j] = dY[idx4(n, c_out, h, w, C_out, H_out, W_out)];\n","                    } else {\n","                        s_dY[i][j] = 0.0f;\n","                    }\n","                }\n","\n","                // ✅ FIX: Cooperative loading cho input với halo\n","                int h_in_base = block_h * TILE_H - pad;\n","                int w_in_base = block_w * TILE_W - pad;\n","\n","                for (int idx = tid; idx < BLOCK_H * BLOCK_W; idx += num_threads) {\n","                    int i = idx / BLOCK_W;\n","                    int j = idx % BLOCK_W;\n","                    int h = h_in_base + i;\n","                    int w = w_in_base + j;\n","\n","                    if (h >= 0 && h < H && w >= 0 && w < W) {\n","                        s_in[i][j] = input[idx4(n, c_in, h, w, C_in, H, W)];\n","                    } else {\n","                        s_in[i][j] = 0.0f;\n","                    }\n","                }\n","                __syncthreads();\n","\n","                // Compute local dW\n","                if (ty < TILE_H && tx < TILE_W) {\n","                    float val_dy = s_dY[ty][tx];\n","                    #pragma unroll\n","                    for (int kh = 0; kh < K; ++kh) {\n","                        #pragma unroll\n","                        for (int kw = 0; kw < K; ++kw) {\n","                            dw[kh][kw] += s_in[ty * stride + kh][tx * stride + kw] * val_dy;\n","                        }\n","                    }\n","                }\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    // ✅ FIX: Block-level reduction TRƯỚC atomic\n","    // Mỗi kernel element có reduction riêng\n","    #pragma unroll\n","    for (int kh = 0; kh < K; ++kh) {\n","        #pragma unroll\n","        for (int kw = 0; kw < K; ++kw) {\n","            int k_idx = kh * K + kw;\n","\n","            // Store to shared memory\n","            s_dw[k_idx][tid] = dw[kh][kw];\n","            __syncthreads();\n","\n","            // Tree reduction\n","            for (int s = num_threads / 2; s > 0; s >>= 1) {\n","                if (tid < s) {\n","                    s_dw[k_idx][tid] += s_dw[k_idx][tid + s];\n","                }\n","                __syncthreads();\n","            }\n","\n","            // ONLY thread 0 does atomic\n","            if (tid == 0) {\n","                size_t dw_idx = idx4(c_out, c_in, kh, kw, C_in, K, K);\n","                atomicAdd(&dW[dw_idx], s_dw[k_idx][0]);\n","            }\n","        }\n","    }\n","}\n","\n","// ============================================================================\n","// BACKWARD BIAS - OPTIMIZED (warp-level reduction)\n","// ============================================================================\n","__global__ void conv2d_backward_bias_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","\n","    int tid = threadIdx.x;\n","    int lane = tid % 32;\n","\n","    float sum = 0.0f;\n","\n","    // Grid-stride loop\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","\n","    // Warp-level reduction\n","    #pragma unroll\n","    for (int offset = 16; offset > 0; offset >>= 1) {\n","        sum += __shfl_down_sync(0xffffffff, sum, offset);\n","    }\n","\n","    // First thread in each warp writes to shared memory\n","    __shared__ float warp_sums[32];\n","    int warp_id = tid / 32;\n","\n","    if (lane == 0) {\n","        warp_sums[warp_id] = sum;\n","    }\n","    __syncthreads();\n","\n","    // Final reduction by first warp\n","    if (warp_id == 0) {\n","        sum = (lane < (blockDim.x / 32)) ? warp_sums[lane] : 0.0f;\n","\n","        #pragma unroll\n","        for (int offset = 16; offset > 0; offset >>= 1) {\n","            sum += __shfl_down_sync(0xffffffff, sum, offset);\n","        }\n","\n","        if (lane == 0) {\n","            dB[c] = sum;\n","        }\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}\n"],"metadata":{"id":"7Dakajm12LYu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"944fb293-2af1-4a5e-c620-345035f157e5","executionInfo":{"status":"ok","timestamp":1765986339453,"user_tz":-420,"elapsed":23,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt2.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt2.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers_opt2.h\"\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0; ///\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights, biases ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);"],"metadata":{"id":"zDclPxoc2NOW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc811d71-3719-4efc-ba80-9e82d12060d7","executionInfo":{"status":"ok","timestamp":1765986340165,"user_tz":-420,"elapsed":20,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt2.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt2.cu\n","#include \"gpu_autoencoder_opt2.h\"\n","#include <cmath>\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","// find max weight bytes\n","size_t max_w_bytes = w1_bytes;\n","if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","// find max bias bytes\n","size_t max_b_bytes = b1_bytes;\n","if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","float *h_w = (float*)malloc(max_w_bytes);\n","float *h_b = (float*)malloc(max_b_bytes);\n","\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ---------- [batch_size, channels, height, width]\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Copy input to device\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);  // 256 threads per block\n","\n","    // ========= ENCODER =========\n","\n","    // Layer 1: conv1: 3 -> 256, 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        // ✅ Grid configuration cho optimized kernel\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,  // Ceiling division\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        // Update constant memory bias\n","        update_dc_bias(ae->d_b1, C_out);\n","\n","        // ✅ Launch optimized kernel\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // Layer 2: conv2: 256 -> 128, 16x16, pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // Pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT: ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","\n","    // Layer 3: conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b3, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // Upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // Layer 4: conv4: 128 -> 256, 16x16, upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b4, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // Upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // Layer 5: conv5: 256 -> 3, 32x32 (output layer)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b5, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","    }\n","\n","    // ========= COMPUTE LOSS (optional) =========\n","    float loss_value = 0.0f;\n","    if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;\n","    }\n","\n","    // Copy output back to host\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H;\n","    const int W0 = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ✅ OPTIMIZATION: Zero gradients asynchronously in streams\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        // dU2 - ✅ OPTIMIZED KERNEL\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,  // Use TILE_W instead of block2d.x\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW5 - ✅ OPTIMIZED KERNEL\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_u2, ae->d_gout, ae->d_gw5,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB5 - ✅ OPTIMIZED KERNEL\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gout, ae->d_gb5, N, C_out, H, W);\n","\n","        // SGD update\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;\n","        int Hu = 32, Wu = 32;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4, N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int C = 256, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward: 128->256, 16x16 =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_u1, ae->d_gh4, ae->d_gw4,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh4, ae->d_gb4, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;\n","        int Hu = 16, Wu = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3, N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int C = 128, H = 8, W = 8;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward: 128->128, 8x8 =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED (note: cudaMemset removed, already done at start)\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_p2, ae->d_gh3, ae->d_gw3,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh3, ae->d_gb3, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16;\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2, N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int C = 128, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward: 256->128, 16x16 =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_p1, ae->d_gh2, ae->d_gw2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh2, ae->d_gb2, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32;\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1, N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int C = 256, H = 32, W = 32;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward: 3->256, 32x32 =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_x0, ae->d_gh1, ae->d_gw1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ✅ OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh1, ae->d_gb1, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // copy input [N_batch, 3, 32, 32] lên GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, rồi ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b1, 256 * sizeof(float)));\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, rồi ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        // copy bias to constant memory\n","        //CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, ae->d_b2, 128 * sizeof(float)));\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","\n","         cudaError_t err = cudaGetLastError();\n","   if (err != cudaSuccess) {\n","       fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n","   }\n","   cudaDeviceSynchronize();\n","    }\n","    cudaDeviceSynchronize();\n","\n","size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","\n","}"],"metadata":{"id":"WHLTztyo2PGz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16bd8565-823e-424e-f834-49aae3eefe95","executionInfo":{"status":"ok","timestamp":1765986340911,"user_tz":-420,"elapsed":45,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt2.cu\n"]}]},{"cell_type":"code","source":["%%writefile main_gpu.cu\n","// the UPDATED main() code with argc/argv\n","// main_gpu.cu (DEBUG VERSION)\n","#include <cstdio>\n","#include <ctime>\n","#include <cuda_runtime.h>\n","#include <cstdlib>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt2.h\"\n","\n","// GpuTimer dùng cudaEvent để đo time (đúng spec Phase 2.5)\n","struct GpuTimer {\n","    cudaEvent_t start, stop;\n","    GpuTimer() {\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","    }\n","    ~GpuTimer() {\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","    void tic() {\n","        cudaEventRecord(start, 0);\n","    }\n","    float toc() {\n","        cudaEventRecord(stop, 0);\n","        cudaEventSynchronize(stop);\n","        float ms = 0.0f;\n","        cudaEventElapsedTime(&ms, start, stop);\n","        return ms;\n","    }\n","};\n","\n","void print_gpu_memory() {\n","    size_t free_byte, total_byte;\n","    cudaError_t status = cudaMemGetInfo(&free_byte, &total_byte);\n","\n","    if (status == cudaSuccess) {\n","        double total_mb = (double)total_byte / (1024.0 * 1024.0);\n","        double free_mb = (double)free_byte / (1024.0 * 1024.0);\n","        double used_mb = total_mb - free_mb;\n","\n","        printf(\"[SYSTEM] Memory Usage: %.2f MB / Total: %.2f MB\\n\", used_mb, total_mb);\n","    }\n","}\n","\n","int main(int argc, char** argv) {\n","    srand((unsigned int)time(NULL));\n","\n","\n","    printf(\"[MAIN] Start program\\n\");\n","    fflush(stdout);\n","\n","    // ---- Check GPU device ----\n","    int deviceCount = 0;\n","    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n","    if (err != cudaSuccess) {\n","        fprintf(stderr, \"[MAIN] cudaGetDeviceCount error: %s\\n\",\n","                cudaGetErrorString(err));\n","        return 1;\n","    }\n","    printf(\"[MAIN] Num CUDA devices = %d\\n\", deviceCount);\n","    fflush(stdout);\n","\n","    // ---- Load CIFAR-10 on CPU ----\n","    Cifar10 data;\n","    load_cifar10(&data);   // câu lệnh này in: CIFAR-10 loaded successfully ...\n","    printf(\"[MAIN] After load_cifar10\\n\");\n","    fflush(stdout);\n","\n","    normalize_cifar10(&data);\n","    printf(\"[MAIN] After normalize_cifar10\\n\");\n","    fflush(stdout);\n","\n","    GpuTimer epoch_timer;\n","\n","    int batch_size = 64;\n","    int epochs     = 20;\n","    float lr       = 0.001f;\n","    float total_time = 0.0f;\n","\n","    int num_batches = TRAIN_NUM / batch_size;\n","\n","    printf(\"[MAIN] Start training loop (epochs=%d, num_batches=%d)\\n\",\n","       epochs, num_batches);\n","    fflush(stdout);\n","\n","\n","    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float)); // dùng làm buffer tạm\n","\n","    // ---- Init GPU autoencoder ----\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","\n","\n","    for (int epoch = 0; epoch < epochs; ++epoch) {\n","        shuffle_cifar10(&data);\n","        double epoch_loss = 0.0;\n","\n","        epoch_timer.tic();\n","\n","        for (int b = 0; b < num_batches; ++b) {\n","            get_next_batch(&data, batch_size, b, h_batch);\n","\n","            float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n","            gpu_autoencoder_backward(&ae, lr);\n","\n","            epoch_loss += loss;\n","\n","            if ((b + 1) % 100 == 0) {\n","                printf(\"[TRAIN] Epoch %d, batch %d/%d, loss = %f\\n\",\n","                       epoch, b + 1, num_batches, loss);\n","                fflush(stdout);\n","            }\n","        }\n","\n","        float ms = epoch_timer.toc();\n","        total_time += ms;\n","        printf(\"==> Epoch %d done. Avg loss = %f, time = %.3f ms (%.3f s)\\n\",\n","               epoch, epoch_loss / num_batches, ms, ms / 1000.0f);\n","        fflush(stdout);\n","    }\n","\n","    printf(\"[MAIN] Training finished\\n\");\n","    printf(\"[MAIN] Total training time = %.3f s\\n\", total_time / 1000.0f);\n","    print_gpu_memory();\n","    fflush(stdout);\n","\n","    // save weights\n","    gpu_autoencoder_save_weights(&ae, \"ae_weights.bin\");\n","\n","    // ---- cleanup ----\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_output);\n","    free_cifar10(&data);\n","\n","    printf(\"[MAIN] Program finished\\n\");\n","    fflush(stdout);\n","\n","    return 0;\n","}\n"],"metadata":{"id":"c119_Cw02VEt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4aeecd8d-92c5-4d24-c981-72dde4a36c13","executionInfo":{"status":"ok","timestamp":1765986420806,"user_tz":-420,"elapsed":12,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main_gpu.cu\n"]}]},{"cell_type":"code","source":["!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","!tar -xzvf cifar-10-binary.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SijGautWY2T4","outputId":"4d9f205c-a515-4c61-a415-281044f6a946","executionInfo":{"status":"ok","timestamp":1765986404043,"user_tz":-420,"elapsed":21598,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-17 15:46:22--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz.1’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  9.47MB/s    in 19s     \n","\n","2025-12-17 15:46:42 (8.76 MB/s) - ‘cifar-10-binary.tar.gz.1’ saved [170052171/170052171]\n","\n","cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 main_gpu.cu gpu_autoencoder_opt2.cu gpu_layers_opt2.cu load_data.cu -o autoencoder_gpu"],"metadata":{"id":"7tohxlV42Ti4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"580e3fe9-35d9-4e09-f37a-1c92e4e9f16c","executionInfo":{"status":"ok","timestamp":1765986430569,"user_tz":-420,"elapsed":5215,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(462)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./autoencoder_gpu"],"metadata":{"id":"sSkPZfdr2mxT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"20547ebf-43d4-4411-a61a-f0f112e776ee","executionInfo":{"status":"ok","timestamp":1765989384457,"user_tz":-420,"elapsed":2951018,"user":{"displayName":"Vanessaaa","userId":"15084574806463207345"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=20, num_batches=781)\n","[TRAIN] Epoch 0, batch 100/781, loss = 0.129834\n","[TRAIN] Epoch 0, batch 200/781, loss = 0.061157\n","[TRAIN] Epoch 0, batch 300/781, loss = 0.056731\n","[TRAIN] Epoch 0, batch 400/781, loss = 0.061700\n","[TRAIN] Epoch 0, batch 500/781, loss = 0.054644\n","[TRAIN] Epoch 0, batch 600/781, loss = 0.060234\n","[TRAIN] Epoch 0, batch 700/781, loss = 0.054485\n","==> Epoch 0 done. Avg loss = 0.081602, time = 145155.141 ms (145.155 s)\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.051491\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.051245\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.048358\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.047856\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.048961\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.046305\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.045921\n","==> Epoch 1 done. Avg loss = 0.049587, time = 147569.281 ms (147.569 s)\n","[TRAIN] Epoch 2, batch 100/781, loss = 0.046787\n","[TRAIN] Epoch 2, batch 200/781, loss = 0.041337\n","[TRAIN] Epoch 2, batch 300/781, loss = 0.047461\n","[TRAIN] Epoch 2, batch 400/781, loss = 0.039576\n","[TRAIN] Epoch 2, batch 500/781, loss = 0.040609\n","[TRAIN] Epoch 2, batch 600/781, loss = 0.042643\n","[TRAIN] Epoch 2, batch 700/781, loss = 0.040945\n","==> Epoch 2 done. Avg loss = 0.042854, time = 147638.406 ms (147.638 s)\n","[TRAIN] Epoch 3, batch 100/781, loss = 0.036783\n","[TRAIN] Epoch 3, batch 200/781, loss = 0.035743\n","[TRAIN] Epoch 3, batch 300/781, loss = 0.040743\n","[TRAIN] Epoch 3, batch 400/781, loss = 0.034905\n","[TRAIN] Epoch 3, batch 500/781, loss = 0.035805\n","[TRAIN] Epoch 3, batch 600/781, loss = 0.031370\n","[TRAIN] Epoch 3, batch 700/781, loss = 0.036162\n","==> Epoch 3 done. Avg loss = 0.036624, time = 147554.297 ms (147.554 s)\n","[TRAIN] Epoch 4, batch 100/781, loss = 0.034814\n","[TRAIN] Epoch 4, batch 200/781, loss = 0.030880\n","[TRAIN] Epoch 4, batch 300/781, loss = 0.031073\n","[TRAIN] Epoch 4, batch 400/781, loss = 0.028651\n","[TRAIN] Epoch 4, batch 500/781, loss = 0.033524\n","[TRAIN] Epoch 4, batch 600/781, loss = 0.026998\n","[TRAIN] Epoch 4, batch 700/781, loss = 0.029543\n","==> Epoch 4 done. Avg loss = 0.031299, time = 147727.109 ms (147.727 s)\n","[TRAIN] Epoch 5, batch 100/781, loss = 0.028201\n","[TRAIN] Epoch 5, batch 200/781, loss = 0.027004\n","[TRAIN] Epoch 5, batch 300/781, loss = 0.029716\n","[TRAIN] Epoch 5, batch 400/781, loss = 0.027092\n","[TRAIN] Epoch 5, batch 500/781, loss = 0.028990\n","[TRAIN] Epoch 5, batch 600/781, loss = 0.028117\n","[TRAIN] Epoch 5, batch 700/781, loss = 0.024513\n","==> Epoch 5 done. Avg loss = 0.027418, time = 147738.844 ms (147.739 s)\n","[TRAIN] Epoch 6, batch 100/781, loss = 0.027367\n","[TRAIN] Epoch 6, batch 200/781, loss = 0.025362\n","[TRAIN] Epoch 6, batch 300/781, loss = 0.023704\n","[TRAIN] Epoch 6, batch 400/781, loss = 0.024439\n","[TRAIN] Epoch 6, batch 500/781, loss = 0.025704\n","[TRAIN] Epoch 6, batch 600/781, loss = 0.023207\n","[TRAIN] Epoch 6, batch 700/781, loss = 0.023240\n","==> Epoch 6 done. Avg loss = 0.024940, time = 147687.562 ms (147.688 s)\n","[TRAIN] Epoch 7, batch 100/781, loss = 0.024479\n","[TRAIN] Epoch 7, batch 200/781, loss = 0.023389\n","[TRAIN] Epoch 7, batch 300/781, loss = 0.022893\n","[TRAIN] Epoch 7, batch 400/781, loss = 0.022957\n","[TRAIN] Epoch 7, batch 500/781, loss = 0.024881\n","[TRAIN] Epoch 7, batch 600/781, loss = 0.022433\n","[TRAIN] Epoch 7, batch 700/781, loss = 0.023971\n","==> Epoch 7 done. Avg loss = 0.023395, time = 147583.688 ms (147.584 s)\n","[TRAIN] Epoch 8, batch 100/781, loss = 0.021447\n","[TRAIN] Epoch 8, batch 200/781, loss = 0.021385\n","[TRAIN] Epoch 8, batch 300/781, loss = 0.022289\n","[TRAIN] Epoch 8, batch 400/781, loss = 0.022186\n","[TRAIN] Epoch 8, batch 500/781, loss = 0.022641\n","[TRAIN] Epoch 8, batch 600/781, loss = 0.023258\n","[TRAIN] Epoch 8, batch 700/781, loss = 0.025899\n","==> Epoch 8 done. Avg loss = 0.022312, time = 147588.719 ms (147.589 s)\n","[TRAIN] Epoch 9, batch 100/781, loss = 0.019226\n","[TRAIN] Epoch 9, batch 200/781, loss = 0.021006\n","[TRAIN] Epoch 9, batch 300/781, loss = 0.020938\n","[TRAIN] Epoch 9, batch 400/781, loss = 0.022154\n","[TRAIN] Epoch 9, batch 500/781, loss = 0.022350\n","[TRAIN] Epoch 9, batch 600/781, loss = 0.023177\n","[TRAIN] Epoch 9, batch 700/781, loss = 0.021243\n","==> Epoch 9 done. Avg loss = 0.021427, time = 147555.891 ms (147.556 s)\n","[TRAIN] Epoch 10, batch 100/781, loss = 0.021249\n","[TRAIN] Epoch 10, batch 200/781, loss = 0.022014\n","[TRAIN] Epoch 10, batch 300/781, loss = 0.022388\n","[TRAIN] Epoch 10, batch 400/781, loss = 0.020743\n","[TRAIN] Epoch 10, batch 500/781, loss = 0.021607\n","[TRAIN] Epoch 10, batch 600/781, loss = 0.021502\n","[TRAIN] Epoch 10, batch 700/781, loss = 0.019019\n","==> Epoch 10 done. Avg loss = 0.020663, time = 147672.734 ms (147.673 s)\n","[TRAIN] Epoch 11, batch 100/781, loss = 0.020120\n","[TRAIN] Epoch 11, batch 200/781, loss = 0.019202\n","[TRAIN] Epoch 11, batch 300/781, loss = 0.020672\n","[TRAIN] Epoch 11, batch 400/781, loss = 0.021393\n","[TRAIN] Epoch 11, batch 500/781, loss = 0.018510\n","[TRAIN] Epoch 11, batch 600/781, loss = 0.021117\n","[TRAIN] Epoch 11, batch 700/781, loss = 0.019198\n","==> Epoch 11 done. Avg loss = 0.019978, time = 147580.156 ms (147.580 s)\n","[TRAIN] Epoch 12, batch 100/781, loss = 0.019771\n","[TRAIN] Epoch 12, batch 200/781, loss = 0.018670\n","[TRAIN] Epoch 12, batch 300/781, loss = 0.019292\n","[TRAIN] Epoch 12, batch 400/781, loss = 0.019373\n","[TRAIN] Epoch 12, batch 500/781, loss = 0.020153\n","[TRAIN] Epoch 12, batch 600/781, loss = 0.019169\n","[TRAIN] Epoch 12, batch 700/781, loss = 0.018813\n","==> Epoch 12 done. Avg loss = 0.019355, time = 147522.266 ms (147.522 s)\n","[TRAIN] Epoch 13, batch 100/781, loss = 0.018119\n","[TRAIN] Epoch 13, batch 200/781, loss = 0.019863\n","[TRAIN] Epoch 13, batch 300/781, loss = 0.019770\n","[TRAIN] Epoch 13, batch 400/781, loss = 0.021536\n","[TRAIN] Epoch 13, batch 500/781, loss = 0.019195\n","[TRAIN] Epoch 13, batch 600/781, loss = 0.018575\n","[TRAIN] Epoch 13, batch 700/781, loss = 0.017316\n","==> Epoch 13 done. Avg loss = 0.018790, time = 147509.703 ms (147.510 s)\n","[TRAIN] Epoch 14, batch 100/781, loss = 0.019328\n","[TRAIN] Epoch 14, batch 200/781, loss = 0.017406\n","[TRAIN] Epoch 14, batch 300/781, loss = 0.019259\n","[TRAIN] Epoch 14, batch 400/781, loss = 0.018902\n","[TRAIN] Epoch 14, batch 500/781, loss = 0.018276\n","[TRAIN] Epoch 14, batch 600/781, loss = 0.018008\n","[TRAIN] Epoch 14, batch 700/781, loss = 0.019381\n","==> Epoch 14 done. Avg loss = 0.018307, time = 147609.484 ms (147.609 s)\n","[TRAIN] Epoch 15, batch 100/781, loss = 0.018973\n","[TRAIN] Epoch 15, batch 200/781, loss = 0.017118\n","[TRAIN] Epoch 15, batch 300/781, loss = 0.018021\n","[TRAIN] Epoch 15, batch 400/781, loss = 0.015621\n","[TRAIN] Epoch 15, batch 500/781, loss = 0.016785\n","[TRAIN] Epoch 15, batch 600/781, loss = 0.018369\n","[TRAIN] Epoch 15, batch 700/781, loss = 0.018676\n","==> Epoch 15 done. Avg loss = 0.017899, time = 147638.469 ms (147.638 s)\n","[TRAIN] Epoch 16, batch 100/781, loss = 0.018581\n","[TRAIN] Epoch 16, batch 200/781, loss = 0.018972\n","[TRAIN] Epoch 16, batch 300/781, loss = 0.018030\n","[TRAIN] Epoch 16, batch 400/781, loss = 0.016468\n","[TRAIN] Epoch 16, batch 500/781, loss = 0.017151\n","[TRAIN] Epoch 16, batch 600/781, loss = 0.018035\n","[TRAIN] Epoch 16, batch 700/781, loss = 0.016077\n","==> Epoch 16 done. Avg loss = 0.017554, time = 147663.094 ms (147.663 s)\n","[TRAIN] Epoch 17, batch 100/781, loss = 0.018426\n","[TRAIN] Epoch 17, batch 200/781, loss = 0.018304\n","[TRAIN] Epoch 17, batch 300/781, loss = 0.016590\n","[TRAIN] Epoch 17, batch 400/781, loss = 0.016998\n","[TRAIN] Epoch 17, batch 500/781, loss = 0.019018\n","[TRAIN] Epoch 17, batch 600/781, loss = 0.015243\n","[TRAIN] Epoch 17, batch 700/781, loss = 0.017306\n","==> Epoch 17 done. Avg loss = 0.017258, time = 147701.344 ms (147.701 s)\n","[TRAIN] Epoch 18, batch 100/781, loss = 0.016967\n","[TRAIN] Epoch 18, batch 200/781, loss = 0.017542\n","[TRAIN] Epoch 18, batch 300/781, loss = 0.016990\n","[TRAIN] Epoch 18, batch 400/781, loss = 0.015383\n","[TRAIN] Epoch 18, batch 500/781, loss = 0.017138\n","[TRAIN] Epoch 18, batch 600/781, loss = 0.015467\n","[TRAIN] Epoch 18, batch 700/781, loss = 0.015939\n","==> Epoch 18 done. Avg loss = 0.017008, time = 147706.453 ms (147.706 s)\n","[TRAIN] Epoch 19, batch 100/781, loss = 0.017235\n","[TRAIN] Epoch 19, batch 200/781, loss = 0.017116\n","[TRAIN] Epoch 19, batch 300/781, loss = 0.017616\n","[TRAIN] Epoch 19, batch 400/781, loss = 0.015730\n","[TRAIN] Epoch 19, batch 500/781, loss = 0.016363\n","[TRAIN] Epoch 19, batch 600/781, loss = 0.016179\n","[TRAIN] Epoch 19, batch 700/781, loss = 0.017932\n","==> Epoch 19 done. Avg loss = 0.016789, time = 147665.078 ms (147.665 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 2950.067 s\n","[SYSTEM] Memory Usage: 474.94 MB / Total: 15095.06 MB\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n"]}]},{"cell_type":"code","source":["!nvprof ./autoencoder_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AOypZZb8WCV","outputId":"a46f9dac-a12e-42a1-c25b-5b99c07bb7d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[MAIN] Start program\n","==10430== NVPROF is profiling process 10430, command: ./autoencoder_gpu\n","[MAIN] Num CUDA devices = 1\n","CIFAR-10 loaded successfully\n","[MAIN] After load_cifar10\n","[MAIN] After normalize_cifar10\n","[MAIN] Start training loop (epochs=20, num_batches=781)\n","[TRAIN] Epoch 0, batch 100/781, loss = 0.067678\n","[TRAIN] Epoch 0, batch 200/781, loss = 0.055294\n","[TRAIN] Epoch 0, batch 300/781, loss = 0.049324\n","[TRAIN] Epoch 0, batch 400/781, loss = 0.060609\n","[TRAIN] Epoch 0, batch 500/781, loss = 0.053078\n","[TRAIN] Epoch 0, batch 600/781, loss = 0.055392\n","[TRAIN] Epoch 0, batch 700/781, loss = 0.055826\n","==> Epoch 0 done. Avg loss = 0.067840, time = 146808.703 ms (146.809 s)\n","[TRAIN] Epoch 1, batch 100/781, loss = 0.053637\n","[TRAIN] Epoch 1, batch 200/781, loss = 0.056488\n","[TRAIN] Epoch 1, batch 300/781, loss = 0.051624\n","[TRAIN] Epoch 1, batch 400/781, loss = 0.051598\n","[TRAIN] Epoch 1, batch 500/781, loss = 0.047673\n","[TRAIN] Epoch 1, batch 600/781, loss = 0.050283\n","[TRAIN] Epoch 1, batch 700/781, loss = 0.054144\n","==> Epoch 1 done. Avg loss = 0.051902, time = 147020.031 ms (147.020 s)\n","[TRAIN] Epoch 2, batch 100/781, loss = 0.051328\n","[TRAIN] Epoch 2, batch 200/781, loss = 0.044272\n","[TRAIN] Epoch 2, batch 300/781, loss = 0.048972\n","[TRAIN] Epoch 2, batch 400/781, loss = 0.045104\n","[TRAIN] Epoch 2, batch 500/781, loss = 0.044283\n","[TRAIN] Epoch 2, batch 600/781, loss = 0.045033\n","[TRAIN] Epoch 2, batch 700/781, loss = 0.049710\n","==> Epoch 2 done. Avg loss = 0.047106, time = 147072.453 ms (147.072 s)\n","[TRAIN] Epoch 3, batch 100/781, loss = 0.043900\n","[TRAIN] Epoch 3, batch 200/781, loss = 0.045202\n","[TRAIN] Epoch 3, batch 300/781, loss = 0.043795\n","[TRAIN] Epoch 3, batch 400/781, loss = 0.045641\n","[TRAIN] Epoch 3, batch 500/781, loss = 0.042270\n","[TRAIN] Epoch 3, batch 600/781, loss = 0.037141\n","[TRAIN] Epoch 3, batch 700/781, loss = 0.041372\n","==> Epoch 3 done. Avg loss = 0.042683, time = 147138.219 ms (147.138 s)\n","[TRAIN] Epoch 4, batch 100/781, loss = 0.039324\n","[TRAIN] Epoch 4, batch 200/781, loss = 0.040215\n","[TRAIN] Epoch 4, batch 300/781, loss = 0.040909\n","[TRAIN] Epoch 4, batch 400/781, loss = 0.042441\n","[TRAIN] Epoch 4, batch 500/781, loss = 0.035077\n","[TRAIN] Epoch 4, batch 600/781, loss = 0.039162\n","[TRAIN] Epoch 4, batch 700/781, loss = 0.035222\n","==> Epoch 4 done. Avg loss = 0.038512, time = 147146.688 ms (147.147 s)\n","[TRAIN] Epoch 5, batch 100/781, loss = 0.034025\n","[TRAIN] Epoch 5, batch 200/781, loss = 0.035837\n","[TRAIN] Epoch 5, batch 300/781, loss = 0.036389\n","[TRAIN] Epoch 5, batch 400/781, loss = 0.032869\n","[TRAIN] Epoch 5, batch 500/781, loss = 0.033067\n","[TRAIN] Epoch 5, batch 600/781, loss = 0.034386\n","[TRAIN] Epoch 5, batch 700/781, loss = 0.032543\n","==> Epoch 5 done. Avg loss = 0.034858, time = 147226.141 ms (147.226 s)\n","[TRAIN] Epoch 6, batch 100/781, loss = 0.031270\n","[TRAIN] Epoch 6, batch 200/781, loss = 0.036262\n","[TRAIN] Epoch 6, batch 300/781, loss = 0.032232\n","[TRAIN] Epoch 6, batch 400/781, loss = 0.033037\n","[TRAIN] Epoch 6, batch 500/781, loss = 0.027413\n","[TRAIN] Epoch 6, batch 600/781, loss = 0.030215\n","[TRAIN] Epoch 6, batch 700/781, loss = 0.031461\n","==> Epoch 6 done. Avg loss = 0.031833, time = 147568.047 ms (147.568 s)\n","[TRAIN] Epoch 7, batch 100/781, loss = 0.027847\n","[TRAIN] Epoch 7, batch 200/781, loss = 0.032811\n","[TRAIN] Epoch 7, batch 300/781, loss = 0.031745\n","[TRAIN] Epoch 7, batch 400/781, loss = 0.026259\n","[TRAIN] Epoch 7, batch 500/781, loss = 0.030304\n","[TRAIN] Epoch 7, batch 600/781, loss = 0.028149\n","[TRAIN] Epoch 7, batch 700/781, loss = 0.027938\n","==> Epoch 7 done. Avg loss = 0.029433, time = 147369.406 ms (147.369 s)\n","[TRAIN] Epoch 8, batch 100/781, loss = 0.028682\n","[TRAIN] Epoch 8, batch 200/781, loss = 0.027146\n","[TRAIN] Epoch 8, batch 300/781, loss = 0.027902\n","[TRAIN] Epoch 8, batch 400/781, loss = 0.029431\n","[TRAIN] Epoch 8, batch 500/781, loss = 0.028311\n","[TRAIN] Epoch 8, batch 600/781, loss = 0.028479\n","[TRAIN] Epoch 8, batch 700/781, loss = 0.025064\n","==> Epoch 8 done. Avg loss = 0.027570, time = 147394.594 ms (147.395 s)\n","[TRAIN] Epoch 9, batch 100/781, loss = 0.028879\n","[TRAIN] Epoch 9, batch 200/781, loss = 0.025523\n","[TRAIN] Epoch 9, batch 300/781, loss = 0.022812\n","[TRAIN] Epoch 9, batch 400/781, loss = 0.028192\n","[TRAIN] Epoch 9, batch 500/781, loss = 0.028509\n","[TRAIN] Epoch 9, batch 600/781, loss = 0.027775\n","[TRAIN] Epoch 9, batch 700/781, loss = 0.025819\n","==> Epoch 9 done. Avg loss = 0.026117, time = 147154.562 ms (147.155 s)\n","[TRAIN] Epoch 10, batch 100/781, loss = 0.027878\n","[TRAIN] Epoch 10, batch 200/781, loss = 0.023683\n","[TRAIN] Epoch 10, batch 300/781, loss = 0.024622\n","[TRAIN] Epoch 10, batch 400/781, loss = 0.023520\n","[TRAIN] Epoch 10, batch 500/781, loss = 0.021658\n","[TRAIN] Epoch 10, batch 600/781, loss = 0.025310\n","[TRAIN] Epoch 10, batch 700/781, loss = 0.026336\n","==> Epoch 10 done. Avg loss = 0.024962, time = 147244.938 ms (147.245 s)\n","[TRAIN] Epoch 11, batch 100/781, loss = 0.025512\n","[TRAIN] Epoch 11, batch 200/781, loss = 0.025388\n","[TRAIN] Epoch 11, batch 300/781, loss = 0.025661\n","[TRAIN] Epoch 11, batch 400/781, loss = 0.025580\n","[TRAIN] Epoch 11, batch 500/781, loss = 0.022808\n","[TRAIN] Epoch 11, batch 600/781, loss = 0.024387\n","[TRAIN] Epoch 11, batch 700/781, loss = 0.023743\n","==> Epoch 11 done. Avg loss = 0.024048, time = 147260.719 ms (147.261 s)\n","[TRAIN] Epoch 12, batch 100/781, loss = 0.024880\n","[TRAIN] Epoch 12, batch 200/781, loss = 0.021640\n","[TRAIN] Epoch 12, batch 300/781, loss = 0.022479\n","[TRAIN] Epoch 12, batch 400/781, loss = 0.026058\n","[TRAIN] Epoch 12, batch 500/781, loss = 0.023822\n","[TRAIN] Epoch 12, batch 600/781, loss = 0.022294\n","[TRAIN] Epoch 12, batch 700/781, loss = 0.020934\n","==> Epoch 12 done. Avg loss = 0.023305, time = 147233.797 ms (147.234 s)\n","[TRAIN] Epoch 13, batch 100/781, loss = 0.022608\n","[TRAIN] Epoch 13, batch 200/781, loss = 0.021435\n","[TRAIN] Epoch 13, batch 300/781, loss = 0.021661\n","[TRAIN] Epoch 13, batch 400/781, loss = 0.025270\n","[TRAIN] Epoch 13, batch 500/781, loss = 0.023094\n","[TRAIN] Epoch 13, batch 600/781, loss = 0.022300\n","[TRAIN] Epoch 13, batch 700/781, loss = 0.020977\n","==> Epoch 13 done. Avg loss = 0.022690, time = 147295.766 ms (147.296 s)\n","[TRAIN] Epoch 14, batch 100/781, loss = 0.021055\n","[TRAIN] Epoch 14, batch 200/781, loss = 0.023008\n","[TRAIN] Epoch 14, batch 300/781, loss = 0.023452\n","[TRAIN] Epoch 14, batch 400/781, loss = 0.026164\n","[TRAIN] Epoch 14, batch 500/781, loss = 0.022329\n","[TRAIN] Epoch 14, batch 600/781, loss = 0.019192\n","[TRAIN] Epoch 14, batch 700/781, loss = 0.022751\n","==> Epoch 14 done. Avg loss = 0.022161, time = 147414.891 ms (147.415 s)\n","[TRAIN] Epoch 15, batch 100/781, loss = 0.024580\n","[TRAIN] Epoch 15, batch 200/781, loss = 0.020482\n","[TRAIN] Epoch 15, batch 300/781, loss = 0.020300\n","[TRAIN] Epoch 15, batch 400/781, loss = 0.022177\n","[TRAIN] Epoch 15, batch 500/781, loss = 0.022579\n","[TRAIN] Epoch 15, batch 600/781, loss = 0.022215\n","[TRAIN] Epoch 15, batch 700/781, loss = 0.021108\n","==> Epoch 15 done. Avg loss = 0.021693, time = 147301.422 ms (147.301 s)\n","[TRAIN] Epoch 16, batch 100/781, loss = 0.019561\n","[TRAIN] Epoch 16, batch 200/781, loss = 0.021654\n","[TRAIN] Epoch 16, batch 300/781, loss = 0.023478\n","[TRAIN] Epoch 16, batch 400/781, loss = 0.022017\n","[TRAIN] Epoch 16, batch 500/781, loss = 0.020172\n","[TRAIN] Epoch 16, batch 600/781, loss = 0.021650\n","[TRAIN] Epoch 16, batch 700/781, loss = 0.019612\n","==> Epoch 16 done. Avg loss = 0.021266, time = 147197.422 ms (147.197 s)\n","[TRAIN] Epoch 17, batch 100/781, loss = 0.021751\n","[TRAIN] Epoch 17, batch 200/781, loss = 0.020616\n","[TRAIN] Epoch 17, batch 300/781, loss = 0.021884\n","[TRAIN] Epoch 17, batch 400/781, loss = 0.021512\n","[TRAIN] Epoch 17, batch 500/781, loss = 0.019645\n","[TRAIN] Epoch 17, batch 600/781, loss = 0.021960\n","[TRAIN] Epoch 17, batch 700/781, loss = 0.020566\n","==> Epoch 17 done. Avg loss = 0.020866, time = 147259.203 ms (147.259 s)\n","[TRAIN] Epoch 18, batch 100/781, loss = 0.021149\n","[TRAIN] Epoch 18, batch 200/781, loss = 0.021708\n","[TRAIN] Epoch 18, batch 300/781, loss = 0.022193\n","[TRAIN] Epoch 18, batch 400/781, loss = 0.019017\n","[TRAIN] Epoch 18, batch 500/781, loss = 0.017971\n","[TRAIN] Epoch 18, batch 600/781, loss = 0.020994\n","[TRAIN] Epoch 18, batch 700/781, loss = 0.021048\n","==> Epoch 18 done. Avg loss = 0.020494, time = 147290.422 ms (147.290 s)\n","[TRAIN] Epoch 19, batch 100/781, loss = 0.019392\n","[TRAIN] Epoch 19, batch 200/781, loss = 0.019768\n","[TRAIN] Epoch 19, batch 300/781, loss = 0.019785\n","[TRAIN] Epoch 19, batch 400/781, loss = 0.020185\n","[TRAIN] Epoch 19, batch 500/781, loss = 0.020172\n","[TRAIN] Epoch 19, batch 600/781, loss = 0.019759\n","[TRAIN] Epoch 19, batch 700/781, loss = 0.019159\n","==> Epoch 19 done. Avg loss = 0.020148, time = 147281.328 ms (147.281 s)\n","[MAIN] Training finished\n","[MAIN] Total training time = 2944.679 s\n","Saved weights to ae_weights.bin\n","[MAIN] Program finished\n","==10430== Profiling application: ./autoencoder_gpu\n","==10430== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   37.82%  1.1e+03s     78100  14.210ms  2.2779ms  33.909ms  conv2d_backward_weight_opt2(float*, float*, float*, int, int, int, int, int, int, int)\n","                   29.84%  875.799s     78100  11.214ms  2.1468ms  47.462ms  conv2d_forward_opt2(float*, float*, float*, int, int, int, int, int, int, int)\n","                   29.70%  871.512s     78100  11.159ms  2.1445ms  31.482ms  conv2d_backward_input_opt2(float*, float*, float*, int, int, int, int, int, int, int)\n","                    0.53%  15.5853s     62480  249.44us  20.639us  734.41us  relu_backward(float*, float*, float*, int)\n","                    0.45%  13.2436s     31240  423.93us  96.445us  760.81us  maxpool2x2_backward(float*, float*, float*, int, int, int, int)\n","                    0.40%  11.6633s     62480  186.67us  10.016us  656.72us  relu_forward(float*, int)\n","                    0.31%  9.07955s     78100  116.26us  12.127us  351.32us  conv2d_backward_bias_opt2(float*, float*, int, int, int, int)\n","                    0.27%  8.02157s     31240  256.77us  53.662us  827.66us  upsample2x2_forward(float*, float*, int, int, int, int)\n","                    0.20%  5.77949s     31240  185.00us  50.558us  318.74us  maxpool2x2_forward(float*, float*, int, int, int, int)\n","                    0.19%  5.59030s     31240  178.95us  46.975us  309.27us  upsample2x2_backward(float*, float*, int, int, int, int)\n","                    0.18%  5.18864s    203060  25.552us     415ns  290.39us  [CUDA memset]\n","                    0.04%  1.07796s     31250  34.494us  1.0240us  109.08us  [CUDA memcpy DtoH]\n","                    0.04%  1.04751s     15630  67.019us     704ns  103.68us  [CUDA memcpy HtoD]\n","                    0.02%  655.44ms    156200  4.1960us  1.4710us  11.488us  sgd_update(float*, float*, int, float)\n","                    0.01%  200.81ms     15620  12.855us  11.808us  24.576us  mse_loss_forward(float*, float*, float*, int)\n","                    0.01%  163.56ms     78100  2.0940us  1.4080us  3.4880us  [CUDA memcpy DtoD]\n","                    0.01%  154.21ms     15620  9.8720us  9.5040us  12.224us  mse_loss_backward(float*, float*, float*, int)\n","      API calls:   99.65%  2.9e+03s     46880  62.521ms  10.686us  167.92ms  cudaMemcpy\n","                    0.16%  4.63867s    749760  6.1860us  3.0150us  2.4479ms  cudaLaunchKernel\n","                    0.09%  2.59743s        20  129.87ms  128.90ms  131.83ms  cudaEventSynchronize\n","                    0.06%  1.68888s    203060  8.3170us  2.0840us  2.3027ms  cudaMemset\n","                    0.04%  1.31226s     78100  16.802us  5.0470us  2.7460ms  cudaMemcpyToSymbol\n","                    0.00%  94.880ms         2  47.440ms  4.1820us  94.876ms  cudaEventCreate\n","                    0.00%  12.310ms        41  300.25us  2.5670us  1.1163ms  cudaFree\n","                    0.00%  1.9650ms        41  47.926us  2.0270us  159.24us  cudaMalloc\n","                    0.00%  570.81us        40  14.270us  7.4310us  31.121us  cudaEventRecord\n","                    0.00%  163.94us        20  8.1970us  5.7700us  10.097us  cudaEventElapsedTime\n","                    0.00%  161.15us       114  1.4130us     109ns  62.112us  cuDeviceGetAttribute\n","                    0.00%  10.888us         1  10.888us  10.888us  10.888us  cuDeviceGetName\n","                    0.00%  10.600us         2  5.3000us     850ns  9.7500us  cudaEventDestroy\n","                    0.00%  9.4270us         1  9.4270us  9.4270us  9.4270us  cuDeviceGetPCIBusId\n","                    0.00%  1.6780us         3     559ns     145ns  1.1130us  cuDeviceGetCount\n","                    0.00%     984ns         2     492ns     135ns     849ns  cuDeviceGet\n","                    0.00%     715ns         1     715ns     715ns     715ns  cudaGetDeviceCount\n","                    0.00%     503ns         1     503ns     503ns     503ns  cuDeviceTotalMem\n","                    0.00%     477ns         1     477ns     477ns     477ns  cuModuleGetLoadingMode\n","                    0.00%     388ns         1     388ns     388ns     388ns  cuDeviceGetUuid\n"]}]},{"cell_type":"code","source":["%%writefile extract_svm_features.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cstring>\n","#include <cuda_runtime.h>\n","\n","#include \"load_data.h\"\n","#include \"gpu_autoencoder_opt2.h\"\n","\n","// ghi 1 dòng theo format LIBSVM: label index:val ...\n","void write_svm_line(FILE* f, int label,\n","                    const float* feat, int dim)\n","{\n","    fprintf(f, \"%d\", label);\n","\n","    // In TOÀN BỘ feature, không bỏ qua zero\n","    for (int j = 0; j < dim; ++j) {\n","        float v = feat[j];\n","        fprintf(f, \" %d:%g\", j + 1, v);\n","    }\n","    fprintf(f, \"\\n\");\n","}\n","\n","\n","int main(int argc, char** argv)\n","{\n","    if (argc < 2) {\n","        fprintf(stderr,\n","                \"Usage: %s <ae_weights.bin>\\n\",\n","                argv[0]);\n","        return 1;\n","    }\n","    const char* weight_file = argv[1];\n","\n","    printf(\"[SVM] Loading CIFAR-10...\\n\");\n","    Cifar10 data;\n","    load_cifar10(&data);\n","    normalize_cifar10(&data);\n","\n","    // batch_size cho encoder khi extract feature\n","    int batch_size = 64;\n","    GPUAutoencoder ae;\n","    gpu_autoencoder_init(&ae, batch_size);\n","    gpu_autoencoder_load_weights(&ae, weight_file);\n","\n","\n","    float* h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n","    float* h_latent = (float*)malloc(batch_size * AE_LATENT_DIM * sizeof(float));\n","    if (!h_batch || !h_latent) {\n","        fprintf(stderr, \"Host malloc failed\\n\");\n","        return 1;\n","    }\n","\n","    // ====== TRAIN: 50k ảnh -> train_svm.txt ======\n","    FILE* f_train = fopen(\"train_svm.txt\", \"w\");\n","    if (!f_train) {\n","        perror(\"train_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_train           = TRAIN_NUM; // 50000\n","    int num_batches_train = (N_train + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting train features...\\n\");\n","    for (int b = 0; b < num_batches_train; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_train) {\n","            cur_bs = N_train - start;\n","        }\n","\n","        // copy ảnh [start, start+cur_bs) vào h_batch\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.train_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // encoder-only\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        // ghi ra file theo format LIBSVM\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.train_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TRAIN] Batch %d/%d done\\n\",\n","               b + 1, num_batches_train);\n","        fflush(stdout);\n","    }\n","    fclose(f_train);\n","    printf(\"[SVM] Saved train_svm.txt\\n\");\n","\n","    // ====== TEST: 10k ảnh -> test_svm.txt ======\n","    FILE* f_test = fopen(\"test_svm.txt\", \"w\");\n","    if (!f_test) {\n","        perror(\"test_svm.txt\");\n","        return 1;\n","    }\n","\n","    int N_test           = TEST_NUM; // 10000\n","    int num_batches_test = (N_test + batch_size - 1) / batch_size;\n","\n","    printf(\"[SVM] Extracting test features...\\n\");\n","    for (int b = 0; b < num_batches_test; ++b) {\n","        int start = b * batch_size;\n","        int cur_bs = batch_size;\n","        if (start + cur_bs > N_test) {\n","            cur_bs = N_test - start;\n","        }\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            memcpy(h_batch + i * IMG_SIZE,\n","                   data.test_images + idx * IMG_SIZE,\n","                   IMG_SIZE * sizeof(float));\n","        }\n","\n","        // **Không còn debug cudaMemcpy w1/b1, không in input nữa**\n","\n","        gpu_autoencoder_encode_batch(&ae, h_batch, h_latent, cur_bs);\n","\n","        for (int i = 0; i < cur_bs; ++i) {\n","            int idx = start + i;\n","            int label = data.test_labels[idx];\n","            const float* feat = h_latent + i * AE_LATENT_DIM;\n","            write_svm_line(f_test, label, feat, AE_LATENT_DIM);\n","        }\n","\n","        printf(\"[SVM][TEST] Batch %d/%d done\\n\",\n","               b + 1, num_batches_test);\n","        fflush(stdout);\n","    }\n","    fclose(f_test);\n","    printf(\"[SVM] Saved test_svm.txt\\n\");\n","\n","    // cleanup\n","    gpu_autoencoder_free(&ae);\n","    free(h_batch);\n","    free(h_latent);\n","    free_cifar10(&data);\n","\n","    printf(\"[SVM] Done.\\n\");\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKoe0leX-hP3","outputId":"7bf8bce8-d7c9-4694-a798-ccbecb3e40ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing extract_svm_features.cu\n"]}]},{"cell_type":"code","source":["!nvcc  -arch=sm_75 -O2 -o extract_svm_features \\\n","    extract_svm_features.cu gpu_autoencoder_opt2.cu gpu_layers_opt2.cu load_data.cu \\\n","    -lcudart"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VI_miWOV-q-e","outputId":"ec8930da-2589-47b2-ab4c-4c3a417f1b1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(15)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(531)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(593)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(462)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt2.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n"]}]},{"cell_type":"code","source":["!./extract_svm_features ae_weights.bin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VGRXLDx-ugn","outputId":"ebac9dba-e5ee-4282-f64e-0a99b1d307b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[SVM] Loading CIFAR-10...\n","CIFAR-10 loaded successfully\n","Loaded weights from ae_weights.bin\n","[SVM] Extracting train features...\n","[SVM][TRAIN] Batch 1/782 done\n","[SVM][TRAIN] Batch 2/782 done\n","[SVM][TRAIN] Batch 3/782 done\n","[SVM][TRAIN] Batch 4/782 done\n","[SVM][TRAIN] Batch 5/782 done\n","[SVM][TRAIN] Batch 6/782 done\n","[SVM][TRAIN] Batch 7/782 done\n","[SVM][TRAIN] Batch 8/782 done\n","[SVM][TRAIN] Batch 9/782 done\n","[SVM][TRAIN] Batch 10/782 done\n","[SVM][TRAIN] Batch 11/782 done\n","[SVM][TRAIN] Batch 12/782 done\n","[SVM][TRAIN] Batch 13/782 done\n","[SVM][TRAIN] Batch 14/782 done\n","[SVM][TRAIN] Batch 15/782 done\n","[SVM][TRAIN] Batch 16/782 done\n","[SVM][TRAIN] Batch 17/782 done\n","[SVM][TRAIN] Batch 18/782 done\n","[SVM][TRAIN] Batch 19/782 done\n","[SVM][TRAIN] Batch 20/782 done\n","[SVM][TRAIN] Batch 21/782 done\n","[SVM][TRAIN] Batch 22/782 done\n","[SVM][TRAIN] Batch 23/782 done\n","[SVM][TRAIN] Batch 24/782 done\n","[SVM][TRAIN] Batch 25/782 done\n","[SVM][TRAIN] Batch 26/782 done\n","[SVM][TRAIN] Batch 27/782 done\n","[SVM][TRAIN] Batch 28/782 done\n","[SVM][TRAIN] Batch 29/782 done\n","[SVM][TRAIN] Batch 30/782 done\n","[SVM][TRAIN] Batch 31/782 done\n","[SVM][TRAIN] Batch 32/782 done\n","[SVM][TRAIN] Batch 33/782 done\n","[SVM][TRAIN] Batch 34/782 done\n","[SVM][TRAIN] Batch 35/782 done\n","[SVM][TRAIN] Batch 36/782 done\n","[SVM][TRAIN] Batch 37/782 done\n","[SVM][TRAIN] Batch 38/782 done\n","[SVM][TRAIN] Batch 39/782 done\n","[SVM][TRAIN] Batch 40/782 done\n","[SVM][TRAIN] Batch 41/782 done\n","[SVM][TRAIN] Batch 42/782 done\n","[SVM][TRAIN] Batch 43/782 done\n","[SVM][TRAIN] Batch 44/782 done\n","[SVM][TRAIN] Batch 45/782 done\n","[SVM][TRAIN] Batch 46/782 done\n","[SVM][TRAIN] Batch 47/782 done\n","[SVM][TRAIN] Batch 48/782 done\n","[SVM][TRAIN] Batch 49/782 done\n","[SVM][TRAIN] Batch 50/782 done\n","[SVM][TRAIN] Batch 51/782 done\n","[SVM][TRAIN] Batch 52/782 done\n","[SVM][TRAIN] Batch 53/782 done\n","[SVM][TRAIN] Batch 54/782 done\n","[SVM][TRAIN] Batch 55/782 done\n","[SVM][TRAIN] Batch 56/782 done\n","[SVM][TRAIN] Batch 57/782 done\n","[SVM][TRAIN] Batch 58/782 done\n","[SVM][TRAIN] Batch 59/782 done\n","[SVM][TRAIN] Batch 60/782 done\n","[SVM][TRAIN] Batch 61/782 done\n","[SVM][TRAIN] Batch 62/782 done\n","[SVM][TRAIN] Batch 63/782 done\n","[SVM][TRAIN] Batch 64/782 done\n","[SVM][TRAIN] Batch 65/782 done\n","[SVM][TRAIN] Batch 66/782 done\n","[SVM][TRAIN] Batch 67/782 done\n","[SVM][TRAIN] Batch 68/782 done\n","[SVM][TRAIN] Batch 69/782 done\n","[SVM][TRAIN] Batch 70/782 done\n","[SVM][TRAIN] Batch 71/782 done\n","[SVM][TRAIN] Batch 72/782 done\n","[SVM][TRAIN] Batch 73/782 done\n","[SVM][TRAIN] Batch 74/782 done\n","[SVM][TRAIN] Batch 75/782 done\n","[SVM][TRAIN] Batch 76/782 done\n","[SVM][TRAIN] Batch 77/782 done\n","[SVM][TRAIN] Batch 78/782 done\n","[SVM][TRAIN] Batch 79/782 done\n","[SVM][TRAIN] Batch 80/782 done\n","[SVM][TRAIN] Batch 81/782 done\n","[SVM][TRAIN] Batch 82/782 done\n","[SVM][TRAIN] Batch 83/782 done\n","[SVM][TRAIN] Batch 84/782 done\n","[SVM][TRAIN] Batch 85/782 done\n","[SVM][TRAIN] Batch 86/782 done\n","[SVM][TRAIN] Batch 87/782 done\n","[SVM][TRAIN] Batch 88/782 done\n","[SVM][TRAIN] Batch 89/782 done\n","[SVM][TRAIN] Batch 90/782 done\n","[SVM][TRAIN] Batch 91/782 done\n","[SVM][TRAIN] Batch 92/782 done\n","[SVM][TRAIN] Batch 93/782 done\n","[SVM][TRAIN] Batch 94/782 done\n","[SVM][TRAIN] Batch 95/782 done\n","[SVM][TRAIN] Batch 96/782 done\n","[SVM][TRAIN] Batch 97/782 done\n","[SVM][TRAIN] Batch 98/782 done\n","[SVM][TRAIN] Batch 99/782 done\n","[SVM][TRAIN] Batch 100/782 done\n","[SVM][TRAIN] Batch 101/782 done\n","[SVM][TRAIN] Batch 102/782 done\n","[SVM][TRAIN] Batch 103/782 done\n","[SVM][TRAIN] Batch 104/782 done\n","[SVM][TRAIN] Batch 105/782 done\n","[SVM][TRAIN] Batch 106/782 done\n","[SVM][TRAIN] Batch 107/782 done\n","[SVM][TRAIN] Batch 108/782 done\n","[SVM][TRAIN] Batch 109/782 done\n","[SVM][TRAIN] Batch 110/782 done\n","[SVM][TRAIN] Batch 111/782 done\n","[SVM][TRAIN] Batch 112/782 done\n","[SVM][TRAIN] Batch 113/782 done\n","[SVM][TRAIN] Batch 114/782 done\n","[SVM][TRAIN] Batch 115/782 done\n","[SVM][TRAIN] Batch 116/782 done\n","[SVM][TRAIN] Batch 117/782 done\n","[SVM][TRAIN] Batch 118/782 done\n","[SVM][TRAIN] Batch 119/782 done\n","[SVM][TRAIN] Batch 120/782 done\n","[SVM][TRAIN] Batch 121/782 done\n","[SVM][TRAIN] Batch 122/782 done\n","[SVM][TRAIN] Batch 123/782 done\n","[SVM][TRAIN] Batch 124/782 done\n","[SVM][TRAIN] Batch 125/782 done\n","[SVM][TRAIN] Batch 126/782 done\n","[SVM][TRAIN] Batch 127/782 done\n","[SVM][TRAIN] Batch 128/782 done\n","[SVM][TRAIN] Batch 129/782 done\n","[SVM][TRAIN] Batch 130/782 done\n","[SVM][TRAIN] Batch 131/782 done\n","[SVM][TRAIN] Batch 132/782 done\n","[SVM][TRAIN] Batch 133/782 done\n","[SVM][TRAIN] Batch 134/782 done\n","[SVM][TRAIN] Batch 135/782 done\n","[SVM][TRAIN] Batch 136/782 done\n","[SVM][TRAIN] Batch 137/782 done\n","[SVM][TRAIN] Batch 138/782 done\n","[SVM][TRAIN] Batch 139/782 done\n","[SVM][TRAIN] Batch 140/782 done\n","[SVM][TRAIN] Batch 141/782 done\n","[SVM][TRAIN] Batch 142/782 done\n","[SVM][TRAIN] Batch 143/782 done\n","[SVM][TRAIN] Batch 144/782 done\n","[SVM][TRAIN] Batch 145/782 done\n","[SVM][TRAIN] Batch 146/782 done\n","[SVM][TRAIN] Batch 147/782 done\n","[SVM][TRAIN] Batch 148/782 done\n","[SVM][TRAIN] Batch 149/782 done\n","[SVM][TRAIN] Batch 150/782 done\n","[SVM][TRAIN] Batch 151/782 done\n","[SVM][TRAIN] Batch 152/782 done\n","[SVM][TRAIN] Batch 153/782 done\n","[SVM][TRAIN] Batch 154/782 done\n","[SVM][TRAIN] Batch 155/782 done\n","[SVM][TRAIN] Batch 156/782 done\n","[SVM][TRAIN] Batch 157/782 done\n","[SVM][TRAIN] Batch 158/782 done\n","[SVM][TRAIN] Batch 159/782 done\n","[SVM][TRAIN] Batch 160/782 done\n","[SVM][TRAIN] Batch 161/782 done\n","[SVM][TRAIN] Batch 162/782 done\n","[SVM][TRAIN] Batch 163/782 done\n","[SVM][TRAIN] Batch 164/782 done\n","[SVM][TRAIN] Batch 165/782 done\n","[SVM][TRAIN] Batch 166/782 done\n","[SVM][TRAIN] Batch 167/782 done\n","[SVM][TRAIN] Batch 168/782 done\n","[SVM][TRAIN] Batch 169/782 done\n","[SVM][TRAIN] Batch 170/782 done\n","[SVM][TRAIN] Batch 171/782 done\n","[SVM][TRAIN] Batch 172/782 done\n","[SVM][TRAIN] Batch 173/782 done\n","[SVM][TRAIN] Batch 174/782 done\n","[SVM][TRAIN] Batch 175/782 done\n","[SVM][TRAIN] Batch 176/782 done\n","[SVM][TRAIN] Batch 177/782 done\n","[SVM][TRAIN] Batch 178/782 done\n","[SVM][TRAIN] Batch 179/782 done\n","[SVM][TRAIN] Batch 180/782 done\n","[SVM][TRAIN] Batch 181/782 done\n","[SVM][TRAIN] Batch 182/782 done\n","[SVM][TRAIN] Batch 183/782 done\n","[SVM][TRAIN] Batch 184/782 done\n","[SVM][TRAIN] Batch 185/782 done\n","[SVM][TRAIN] Batch 186/782 done\n","[SVM][TRAIN] Batch 187/782 done\n","[SVM][TRAIN] Batch 188/782 done\n","[SVM][TRAIN] Batch 189/782 done\n","[SVM][TRAIN] Batch 190/782 done\n","[SVM][TRAIN] Batch 191/782 done\n","[SVM][TRAIN] Batch 192/782 done\n","[SVM][TRAIN] Batch 193/782 done\n","[SVM][TRAIN] Batch 194/782 done\n","[SVM][TRAIN] Batch 195/782 done\n","[SVM][TRAIN] Batch 196/782 done\n","[SVM][TRAIN] Batch 197/782 done\n","[SVM][TRAIN] Batch 198/782 done\n","[SVM][TRAIN] Batch 199/782 done\n","[SVM][TRAIN] Batch 200/782 done\n","[SVM][TRAIN] Batch 201/782 done\n","[SVM][TRAIN] Batch 202/782 done\n","[SVM][TRAIN] Batch 203/782 done\n","[SVM][TRAIN] Batch 204/782 done\n","[SVM][TRAIN] Batch 205/782 done\n","[SVM][TRAIN] Batch 206/782 done\n","[SVM][TRAIN] Batch 207/782 done\n","[SVM][TRAIN] Batch 208/782 done\n","[SVM][TRAIN] Batch 209/782 done\n","[SVM][TRAIN] Batch 210/782 done\n","[SVM][TRAIN] Batch 211/782 done\n","[SVM][TRAIN] Batch 212/782 done\n","[SVM][TRAIN] Batch 213/782 done\n","[SVM][TRAIN] Batch 214/782 done\n","[SVM][TRAIN] Batch 215/782 done\n","[SVM][TRAIN] Batch 216/782 done\n","[SVM][TRAIN] Batch 217/782 done\n","[SVM][TRAIN] Batch 218/782 done\n","[SVM][TRAIN] Batch 219/782 done\n","[SVM][TRAIN] Batch 220/782 done\n","[SVM][TRAIN] Batch 221/782 done\n","[SVM][TRAIN] Batch 222/782 done\n","[SVM][TRAIN] Batch 223/782 done\n","[SVM][TRAIN] Batch 224/782 done\n","[SVM][TRAIN] Batch 225/782 done\n","[SVM][TRAIN] Batch 226/782 done\n","[SVM][TRAIN] Batch 227/782 done\n","[SVM][TRAIN] Batch 228/782 done\n","[SVM][TRAIN] Batch 229/782 done\n","[SVM][TRAIN] Batch 230/782 done\n","[SVM][TRAIN] Batch 231/782 done\n","[SVM][TRAIN] Batch 232/782 done\n","[SVM][TRAIN] Batch 233/782 done\n","[SVM][TRAIN] Batch 234/782 done\n","[SVM][TRAIN] Batch 235/782 done\n","[SVM][TRAIN] Batch 236/782 done\n","[SVM][TRAIN] Batch 237/782 done\n","[SVM][TRAIN] Batch 238/782 done\n","[SVM][TRAIN] Batch 239/782 done\n","[SVM][TRAIN] Batch 240/782 done\n","[SVM][TRAIN] Batch 241/782 done\n","[SVM][TRAIN] Batch 242/782 done\n","[SVM][TRAIN] Batch 243/782 done\n","[SVM][TRAIN] Batch 244/782 done\n","[SVM][TRAIN] Batch 245/782 done\n","[SVM][TRAIN] Batch 246/782 done\n","[SVM][TRAIN] Batch 247/782 done\n","[SVM][TRAIN] Batch 248/782 done\n","[SVM][TRAIN] Batch 249/782 done\n","[SVM][TRAIN] Batch 250/782 done\n","[SVM][TRAIN] Batch 251/782 done\n","[SVM][TRAIN] Batch 252/782 done\n","[SVM][TRAIN] Batch 253/782 done\n","[SVM][TRAIN] Batch 254/782 done\n","[SVM][TRAIN] Batch 255/782 done\n","[SVM][TRAIN] Batch 256/782 done\n","[SVM][TRAIN] Batch 257/782 done\n","[SVM][TRAIN] Batch 258/782 done\n","[SVM][TRAIN] Batch 259/782 done\n","[SVM][TRAIN] Batch 260/782 done\n","[SVM][TRAIN] Batch 261/782 done\n","[SVM][TRAIN] Batch 262/782 done\n","[SVM][TRAIN] Batch 263/782 done\n","[SVM][TRAIN] Batch 264/782 done\n","[SVM][TRAIN] Batch 265/782 done\n","[SVM][TRAIN] Batch 266/782 done\n","[SVM][TRAIN] Batch 267/782 done\n","[SVM][TRAIN] Batch 268/782 done\n","[SVM][TRAIN] Batch 269/782 done\n","[SVM][TRAIN] Batch 270/782 done\n","[SVM][TRAIN] Batch 271/782 done\n","[SVM][TRAIN] Batch 272/782 done\n","[SVM][TRAIN] Batch 273/782 done\n","[SVM][TRAIN] Batch 274/782 done\n","[SVM][TRAIN] Batch 275/782 done\n","[SVM][TRAIN] Batch 276/782 done\n","[SVM][TRAIN] Batch 277/782 done\n","[SVM][TRAIN] Batch 278/782 done\n","[SVM][TRAIN] Batch 279/782 done\n","[SVM][TRAIN] Batch 280/782 done\n","[SVM][TRAIN] Batch 281/782 done\n","[SVM][TRAIN] Batch 282/782 done\n","[SVM][TRAIN] Batch 283/782 done\n","[SVM][TRAIN] Batch 284/782 done\n","[SVM][TRAIN] Batch 285/782 done\n","[SVM][TRAIN] Batch 286/782 done\n","[SVM][TRAIN] Batch 287/782 done\n","[SVM][TRAIN] Batch 288/782 done\n","[SVM][TRAIN] Batch 289/782 done\n","[SVM][TRAIN] Batch 290/782 done\n","[SVM][TRAIN] Batch 291/782 done\n","[SVM][TRAIN] Batch 292/782 done\n","[SVM][TRAIN] Batch 293/782 done\n","[SVM][TRAIN] Batch 294/782 done\n","[SVM][TRAIN] Batch 295/782 done\n","[SVM][TRAIN] Batch 296/782 done\n","[SVM][TRAIN] Batch 297/782 done\n","[SVM][TRAIN] Batch 298/782 done\n","[SVM][TRAIN] Batch 299/782 done\n","[SVM][TRAIN] Batch 300/782 done\n","[SVM][TRAIN] Batch 301/782 done\n","[SVM][TRAIN] Batch 302/782 done\n","[SVM][TRAIN] Batch 303/782 done\n","[SVM][TRAIN] Batch 304/782 done\n","[SVM][TRAIN] Batch 305/782 done\n","[SVM][TRAIN] Batch 306/782 done\n","[SVM][TRAIN] Batch 307/782 done\n","[SVM][TRAIN] Batch 308/782 done\n","[SVM][TRAIN] Batch 309/782 done\n","[SVM][TRAIN] Batch 310/782 done\n","[SVM][TRAIN] Batch 311/782 done\n","[SVM][TRAIN] Batch 312/782 done\n","[SVM][TRAIN] Batch 313/782 done\n","[SVM][TRAIN] Batch 314/782 done\n","[SVM][TRAIN] Batch 315/782 done\n","[SVM][TRAIN] Batch 316/782 done\n","[SVM][TRAIN] Batch 317/782 done\n","[SVM][TRAIN] Batch 318/782 done\n","[SVM][TRAIN] Batch 319/782 done\n","[SVM][TRAIN] Batch 320/782 done\n","[SVM][TRAIN] Batch 321/782 done\n","[SVM][TRAIN] Batch 322/782 done\n","[SVM][TRAIN] Batch 323/782 done\n","[SVM][TRAIN] Batch 324/782 done\n","[SVM][TRAIN] Batch 325/782 done\n","[SVM][TRAIN] Batch 326/782 done\n","[SVM][TRAIN] Batch 327/782 done\n","[SVM][TRAIN] Batch 328/782 done\n","[SVM][TRAIN] Batch 329/782 done\n","[SVM][TRAIN] Batch 330/782 done\n","[SVM][TRAIN] Batch 331/782 done\n","[SVM][TRAIN] Batch 332/782 done\n","[SVM][TRAIN] Batch 333/782 done\n","[SVM][TRAIN] Batch 334/782 done\n","[SVM][TRAIN] Batch 335/782 done\n","[SVM][TRAIN] Batch 336/782 done\n","[SVM][TRAIN] Batch 337/782 done\n","[SVM][TRAIN] Batch 338/782 done\n","[SVM][TRAIN] Batch 339/782 done\n","[SVM][TRAIN] Batch 340/782 done\n","[SVM][TRAIN] Batch 341/782 done\n","[SVM][TRAIN] Batch 342/782 done\n","[SVM][TRAIN] Batch 343/782 done\n","[SVM][TRAIN] Batch 344/782 done\n","[SVM][TRAIN] Batch 345/782 done\n","[SVM][TRAIN] Batch 346/782 done\n","[SVM][TRAIN] Batch 347/782 done\n","[SVM][TRAIN] Batch 348/782 done\n","[SVM][TRAIN] Batch 349/782 done\n","[SVM][TRAIN] Batch 350/782 done\n","[SVM][TRAIN] Batch 351/782 done\n","[SVM][TRAIN] Batch 352/782 done\n","[SVM][TRAIN] Batch 353/782 done\n","[SVM][TRAIN] Batch 354/782 done\n","[SVM][TRAIN] Batch 355/782 done\n","[SVM][TRAIN] Batch 356/782 done\n","[SVM][TRAIN] Batch 357/782 done\n","[SVM][TRAIN] Batch 358/782 done\n","[SVM][TRAIN] Batch 359/782 done\n","[SVM][TRAIN] Batch 360/782 done\n","[SVM][TRAIN] Batch 361/782 done\n","[SVM][TRAIN] Batch 362/782 done\n","[SVM][TRAIN] Batch 363/782 done\n","[SVM][TRAIN] Batch 364/782 done\n","[SVM][TRAIN] Batch 365/782 done\n","[SVM][TRAIN] Batch 366/782 done\n","[SVM][TRAIN] Batch 367/782 done\n","[SVM][TRAIN] Batch 368/782 done\n","[SVM][TRAIN] Batch 369/782 done\n","[SVM][TRAIN] Batch 370/782 done\n","[SVM][TRAIN] Batch 371/782 done\n","[SVM][TRAIN] Batch 372/782 done\n","[SVM][TRAIN] Batch 373/782 done\n","[SVM][TRAIN] Batch 374/782 done\n","[SVM][TRAIN] Batch 375/782 done\n","[SVM][TRAIN] Batch 376/782 done\n","[SVM][TRAIN] Batch 377/782 done\n","[SVM][TRAIN] Batch 378/782 done\n","[SVM][TRAIN] Batch 379/782 done\n","[SVM][TRAIN] Batch 380/782 done\n","[SVM][TRAIN] Batch 381/782 done\n","[SVM][TRAIN] Batch 382/782 done\n","[SVM][TRAIN] Batch 383/782 done\n","[SVM][TRAIN] Batch 384/782 done\n","[SVM][TRAIN] Batch 385/782 done\n","[SVM][TRAIN] Batch 386/782 done\n","[SVM][TRAIN] Batch 387/782 done\n","[SVM][TRAIN] Batch 388/782 done\n","[SVM][TRAIN] Batch 389/782 done\n","[SVM][TRAIN] Batch 390/782 done\n","[SVM][TRAIN] Batch 391/782 done\n","[SVM][TRAIN] Batch 392/782 done\n","[SVM][TRAIN] Batch 393/782 done\n","[SVM][TRAIN] Batch 394/782 done\n","[SVM][TRAIN] Batch 395/782 done\n","[SVM][TRAIN] Batch 396/782 done\n","[SVM][TRAIN] Batch 397/782 done\n","[SVM][TRAIN] Batch 398/782 done\n","[SVM][TRAIN] Batch 399/782 done\n","[SVM][TRAIN] Batch 400/782 done\n","[SVM][TRAIN] Batch 401/782 done\n","[SVM][TRAIN] Batch 402/782 done\n","[SVM][TRAIN] Batch 403/782 done\n","[SVM][TRAIN] Batch 404/782 done\n","[SVM][TRAIN] Batch 405/782 done\n","[SVM][TRAIN] Batch 406/782 done\n","[SVM][TRAIN] Batch 407/782 done\n","[SVM][TRAIN] Batch 408/782 done\n","[SVM][TRAIN] Batch 409/782 done\n","[SVM][TRAIN] Batch 410/782 done\n","[SVM][TRAIN] Batch 411/782 done\n","[SVM][TRAIN] Batch 412/782 done\n","[SVM][TRAIN] Batch 413/782 done\n","[SVM][TRAIN] Batch 414/782 done\n","[SVM][TRAIN] Batch 415/782 done\n","[SVM][TRAIN] Batch 416/782 done\n","[SVM][TRAIN] Batch 417/782 done\n","[SVM][TRAIN] Batch 418/782 done\n","[SVM][TRAIN] Batch 419/782 done\n","[SVM][TRAIN] Batch 420/782 done\n","[SVM][TRAIN] Batch 421/782 done\n","[SVM][TRAIN] Batch 422/782 done\n","[SVM][TRAIN] Batch 423/782 done\n","[SVM][TRAIN] Batch 424/782 done\n","[SVM][TRAIN] Batch 425/782 done\n","[SVM][TRAIN] Batch 426/782 done\n","[SVM][TRAIN] Batch 427/782 done\n","[SVM][TRAIN] Batch 428/782 done\n","[SVM][TRAIN] Batch 429/782 done\n","[SVM][TRAIN] Batch 430/782 done\n","[SVM][TRAIN] Batch 431/782 done\n","[SVM][TRAIN] Batch 432/782 done\n","[SVM][TRAIN] Batch 433/782 done\n","[SVM][TRAIN] Batch 434/782 done\n","[SVM][TRAIN] Batch 435/782 done\n","[SVM][TRAIN] Batch 436/782 done\n","[SVM][TRAIN] Batch 437/782 done\n","[SVM][TRAIN] Batch 438/782 done\n","[SVM][TRAIN] Batch 439/782 done\n","[SVM][TRAIN] Batch 440/782 done\n","[SVM][TRAIN] Batch 441/782 done\n","[SVM][TRAIN] Batch 442/782 done\n","[SVM][TRAIN] Batch 443/782 done\n","[SVM][TRAIN] Batch 444/782 done\n","[SVM][TRAIN] Batch 445/782 done\n","[SVM][TRAIN] Batch 446/782 done\n","[SVM][TRAIN] Batch 447/782 done\n","[SVM][TRAIN] Batch 448/782 done\n","[SVM][TRAIN] Batch 449/782 done\n","[SVM][TRAIN] Batch 450/782 done\n","[SVM][TRAIN] Batch 451/782 done\n","[SVM][TRAIN] Batch 452/782 done\n","[SVM][TRAIN] Batch 453/782 done\n","[SVM][TRAIN] Batch 454/782 done\n","[SVM][TRAIN] Batch 455/782 done\n","[SVM][TRAIN] Batch 456/782 done\n","[SVM][TRAIN] Batch 457/782 done\n","[SVM][TRAIN] Batch 458/782 done\n","[SVM][TRAIN] Batch 459/782 done\n","[SVM][TRAIN] Batch 460/782 done\n","[SVM][TRAIN] Batch 461/782 done\n","[SVM][TRAIN] Batch 462/782 done\n","[SVM][TRAIN] Batch 463/782 done\n","[SVM][TRAIN] Batch 464/782 done\n","[SVM][TRAIN] Batch 465/782 done\n","[SVM][TRAIN] Batch 466/782 done\n","[SVM][TRAIN] Batch 467/782 done\n","[SVM][TRAIN] Batch 468/782 done\n","[SVM][TRAIN] Batch 469/782 done\n","[SVM][TRAIN] Batch 470/782 done\n","[SVM][TRAIN] Batch 471/782 done\n","[SVM][TRAIN] Batch 472/782 done\n","[SVM][TRAIN] Batch 473/782 done\n","[SVM][TRAIN] Batch 474/782 done\n","[SVM][TRAIN] Batch 475/782 done\n","[SVM][TRAIN] Batch 476/782 done\n","[SVM][TRAIN] Batch 477/782 done\n","[SVM][TRAIN] Batch 478/782 done\n","[SVM][TRAIN] Batch 479/782 done\n","[SVM][TRAIN] Batch 480/782 done\n","[SVM][TRAIN] Batch 481/782 done\n","[SVM][TRAIN] Batch 482/782 done\n","[SVM][TRAIN] Batch 483/782 done\n","[SVM][TRAIN] Batch 484/782 done\n","[SVM][TRAIN] Batch 485/782 done\n","[SVM][TRAIN] Batch 486/782 done\n","[SVM][TRAIN] Batch 487/782 done\n","[SVM][TRAIN] Batch 488/782 done\n","[SVM][TRAIN] Batch 489/782 done\n","[SVM][TRAIN] Batch 490/782 done\n","[SVM][TRAIN] Batch 491/782 done\n","[SVM][TRAIN] Batch 492/782 done\n","[SVM][TRAIN] Batch 493/782 done\n","[SVM][TRAIN] Batch 494/782 done\n","[SVM][TRAIN] Batch 495/782 done\n","[SVM][TRAIN] Batch 496/782 done\n","[SVM][TRAIN] Batch 497/782 done\n","[SVM][TRAIN] Batch 498/782 done\n","[SVM][TRAIN] Batch 499/782 done\n","[SVM][TRAIN] Batch 500/782 done\n","[SVM][TRAIN] Batch 501/782 done\n","[SVM][TRAIN] Batch 502/782 done\n","[SVM][TRAIN] Batch 503/782 done\n","[SVM][TRAIN] Batch 504/782 done\n","[SVM][TRAIN] Batch 505/782 done\n","[SVM][TRAIN] Batch 506/782 done\n","[SVM][TRAIN] Batch 507/782 done\n","[SVM][TRAIN] Batch 508/782 done\n","[SVM][TRAIN] Batch 509/782 done\n","[SVM][TRAIN] Batch 510/782 done\n","[SVM][TRAIN] Batch 511/782 done\n","[SVM][TRAIN] Batch 512/782 done\n","[SVM][TRAIN] Batch 513/782 done\n","[SVM][TRAIN] Batch 514/782 done\n","[SVM][TRAIN] Batch 515/782 done\n","[SVM][TRAIN] Batch 516/782 done\n","[SVM][TRAIN] Batch 517/782 done\n","[SVM][TRAIN] Batch 518/782 done\n","[SVM][TRAIN] Batch 519/782 done\n","[SVM][TRAIN] Batch 520/782 done\n","[SVM][TRAIN] Batch 521/782 done\n","[SVM][TRAIN] Batch 522/782 done\n","[SVM][TRAIN] Batch 523/782 done\n","[SVM][TRAIN] Batch 524/782 done\n","[SVM][TRAIN] Batch 525/782 done\n","[SVM][TRAIN] Batch 526/782 done\n","[SVM][TRAIN] Batch 527/782 done\n","[SVM][TRAIN] Batch 528/782 done\n","[SVM][TRAIN] Batch 529/782 done\n","[SVM][TRAIN] Batch 530/782 done\n","[SVM][TRAIN] Batch 531/782 done\n","[SVM][TRAIN] Batch 532/782 done\n","[SVM][TRAIN] Batch 533/782 done\n","[SVM][TRAIN] Batch 534/782 done\n","[SVM][TRAIN] Batch 535/782 done\n","[SVM][TRAIN] Batch 536/782 done\n","[SVM][TRAIN] Batch 537/782 done\n","[SVM][TRAIN] Batch 538/782 done\n","[SVM][TRAIN] Batch 539/782 done\n","[SVM][TRAIN] Batch 540/782 done\n","[SVM][TRAIN] Batch 541/782 done\n","[SVM][TRAIN] Batch 542/782 done\n","[SVM][TRAIN] Batch 543/782 done\n","[SVM][TRAIN] Batch 544/782 done\n","[SVM][TRAIN] Batch 545/782 done\n","[SVM][TRAIN] Batch 546/782 done\n","[SVM][TRAIN] Batch 547/782 done\n","[SVM][TRAIN] Batch 548/782 done\n","[SVM][TRAIN] Batch 549/782 done\n","[SVM][TRAIN] Batch 550/782 done\n","[SVM][TRAIN] Batch 551/782 done\n","[SVM][TRAIN] Batch 552/782 done\n","[SVM][TRAIN] Batch 553/782 done\n","[SVM][TRAIN] Batch 554/782 done\n","[SVM][TRAIN] Batch 555/782 done\n","[SVM][TRAIN] Batch 556/782 done\n","[SVM][TRAIN] Batch 557/782 done\n","[SVM][TRAIN] Batch 558/782 done\n","[SVM][TRAIN] Batch 559/782 done\n","[SVM][TRAIN] Batch 560/782 done\n","[SVM][TRAIN] Batch 561/782 done\n","[SVM][TRAIN] Batch 562/782 done\n","[SVM][TRAIN] Batch 563/782 done\n","[SVM][TRAIN] Batch 564/782 done\n","[SVM][TRAIN] Batch 565/782 done\n","[SVM][TRAIN] Batch 566/782 done\n","[SVM][TRAIN] Batch 567/782 done\n","[SVM][TRAIN] Batch 568/782 done\n","[SVM][TRAIN] Batch 569/782 done\n","[SVM][TRAIN] Batch 570/782 done\n","[SVM][TRAIN] Batch 571/782 done\n","[SVM][TRAIN] Batch 572/782 done\n","[SVM][TRAIN] Batch 573/782 done\n","[SVM][TRAIN] Batch 574/782 done\n","[SVM][TRAIN] Batch 575/782 done\n","[SVM][TRAIN] Batch 576/782 done\n","[SVM][TRAIN] Batch 577/782 done\n","[SVM][TRAIN] Batch 578/782 done\n","[SVM][TRAIN] Batch 579/782 done\n","[SVM][TRAIN] Batch 580/782 done\n","[SVM][TRAIN] Batch 581/782 done\n","[SVM][TRAIN] Batch 582/782 done\n","[SVM][TRAIN] Batch 583/782 done\n","[SVM][TRAIN] Batch 584/782 done\n","[SVM][TRAIN] Batch 585/782 done\n","[SVM][TRAIN] Batch 586/782 done\n","[SVM][TRAIN] Batch 587/782 done\n","[SVM][TRAIN] Batch 588/782 done\n","[SVM][TRAIN] Batch 589/782 done\n","[SVM][TRAIN] Batch 590/782 done\n","[SVM][TRAIN] Batch 591/782 done\n","[SVM][TRAIN] Batch 592/782 done\n","[SVM][TRAIN] Batch 593/782 done\n","[SVM][TRAIN] Batch 594/782 done\n","[SVM][TRAIN] Batch 595/782 done\n","[SVM][TRAIN] Batch 596/782 done\n","[SVM][TRAIN] Batch 597/782 done\n","[SVM][TRAIN] Batch 598/782 done\n","[SVM][TRAIN] Batch 599/782 done\n","[SVM][TRAIN] Batch 600/782 done\n","[SVM][TRAIN] Batch 601/782 done\n","[SVM][TRAIN] Batch 602/782 done\n","[SVM][TRAIN] Batch 603/782 done\n","[SVM][TRAIN] Batch 604/782 done\n","[SVM][TRAIN] Batch 605/782 done\n","[SVM][TRAIN] Batch 606/782 done\n","[SVM][TRAIN] Batch 607/782 done\n","[SVM][TRAIN] Batch 608/782 done\n","[SVM][TRAIN] Batch 609/782 done\n","[SVM][TRAIN] Batch 610/782 done\n","[SVM][TRAIN] Batch 611/782 done\n","[SVM][TRAIN] Batch 612/782 done\n","[SVM][TRAIN] Batch 613/782 done\n","[SVM][TRAIN] Batch 614/782 done\n","[SVM][TRAIN] Batch 615/782 done\n","[SVM][TRAIN] Batch 616/782 done\n","[SVM][TRAIN] Batch 617/782 done\n","[SVM][TRAIN] Batch 618/782 done\n","[SVM][TRAIN] Batch 619/782 done\n","[SVM][TRAIN] Batch 620/782 done\n","[SVM][TRAIN] Batch 621/782 done\n","[SVM][TRAIN] Batch 622/782 done\n","[SVM][TRAIN] Batch 623/782 done\n","[SVM][TRAIN] Batch 624/782 done\n","[SVM][TRAIN] Batch 625/782 done\n","[SVM][TRAIN] Batch 626/782 done\n","[SVM][TRAIN] Batch 627/782 done\n","[SVM][TRAIN] Batch 628/782 done\n","[SVM][TRAIN] Batch 629/782 done\n","[SVM][TRAIN] Batch 630/782 done\n","[SVM][TRAIN] Batch 631/782 done\n","[SVM][TRAIN] Batch 632/782 done\n","[SVM][TRAIN] Batch 633/782 done\n","[SVM][TRAIN] Batch 634/782 done\n","[SVM][TRAIN] Batch 635/782 done\n","[SVM][TRAIN] Batch 636/782 done\n","[SVM][TRAIN] Batch 637/782 done\n","[SVM][TRAIN] Batch 638/782 done\n","[SVM][TRAIN] Batch 639/782 done\n","[SVM][TRAIN] Batch 640/782 done\n","[SVM][TRAIN] Batch 641/782 done\n","[SVM][TRAIN] Batch 642/782 done\n","[SVM][TRAIN] Batch 643/782 done\n","[SVM][TRAIN] Batch 644/782 done\n","[SVM][TRAIN] Batch 645/782 done\n","[SVM][TRAIN] Batch 646/782 done\n","[SVM][TRAIN] Batch 647/782 done\n","[SVM][TRAIN] Batch 648/782 done\n","[SVM][TRAIN] Batch 649/782 done\n","[SVM][TRAIN] Batch 650/782 done\n","[SVM][TRAIN] Batch 651/782 done\n","[SVM][TRAIN] Batch 652/782 done\n","[SVM][TRAIN] Batch 653/782 done\n","[SVM][TRAIN] Batch 654/782 done\n","[SVM][TRAIN] Batch 655/782 done\n","[SVM][TRAIN] Batch 656/782 done\n","[SVM][TRAIN] Batch 657/782 done\n","[SVM][TRAIN] Batch 658/782 done\n","[SVM][TRAIN] Batch 659/782 done\n","[SVM][TRAIN] Batch 660/782 done\n","[SVM][TRAIN] Batch 661/782 done\n","[SVM][TRAIN] Batch 662/782 done\n","[SVM][TRAIN] Batch 663/782 done\n","[SVM][TRAIN] Batch 664/782 done\n","[SVM][TRAIN] Batch 665/782 done\n","[SVM][TRAIN] Batch 666/782 done\n","[SVM][TRAIN] Batch 667/782 done\n","[SVM][TRAIN] Batch 668/782 done\n","[SVM][TRAIN] Batch 669/782 done\n","[SVM][TRAIN] Batch 670/782 done\n","[SVM][TRAIN] Batch 671/782 done\n","[SVM][TRAIN] Batch 672/782 done\n","[SVM][TRAIN] Batch 673/782 done\n","[SVM][TRAIN] Batch 674/782 done\n","[SVM][TRAIN] Batch 675/782 done\n","[SVM][TRAIN] Batch 676/782 done\n","[SVM][TRAIN] Batch 677/782 done\n","[SVM][TRAIN] Batch 678/782 done\n","[SVM][TRAIN] Batch 679/782 done\n","[SVM][TRAIN] Batch 680/782 done\n","[SVM][TRAIN] Batch 681/782 done\n","[SVM][TRAIN] Batch 682/782 done\n","[SVM][TRAIN] Batch 683/782 done\n","[SVM][TRAIN] Batch 684/782 done\n","[SVM][TRAIN] Batch 685/782 done\n","[SVM][TRAIN] Batch 686/782 done\n","[SVM][TRAIN] Batch 687/782 done\n","[SVM][TRAIN] Batch 688/782 done\n","[SVM][TRAIN] Batch 689/782 done\n","[SVM][TRAIN] Batch 690/782 done\n","[SVM][TRAIN] Batch 691/782 done\n","[SVM][TRAIN] Batch 692/782 done\n","[SVM][TRAIN] Batch 693/782 done\n","[SVM][TRAIN] Batch 694/782 done\n","[SVM][TRAIN] Batch 695/782 done\n","[SVM][TRAIN] Batch 696/782 done\n","[SVM][TRAIN] Batch 697/782 done\n","[SVM][TRAIN] Batch 698/782 done\n","[SVM][TRAIN] Batch 699/782 done\n","[SVM][TRAIN] Batch 700/782 done\n","[SVM][TRAIN] Batch 701/782 done\n","[SVM][TRAIN] Batch 702/782 done\n","[SVM][TRAIN] Batch 703/782 done\n","[SVM][TRAIN] Batch 704/782 done\n","[SVM][TRAIN] Batch 705/782 done\n","[SVM][TRAIN] Batch 706/782 done\n","[SVM][TRAIN] Batch 707/782 done\n","[SVM][TRAIN] Batch 708/782 done\n","[SVM][TRAIN] Batch 709/782 done\n","[SVM][TRAIN] Batch 710/782 done\n","[SVM][TRAIN] Batch 711/782 done\n","[SVM][TRAIN] Batch 712/782 done\n","[SVM][TRAIN] Batch 713/782 done\n","[SVM][TRAIN] Batch 714/782 done\n","[SVM][TRAIN] Batch 715/782 done\n","[SVM][TRAIN] Batch 716/782 done\n","[SVM][TRAIN] Batch 717/782 done\n","[SVM][TRAIN] Batch 718/782 done\n","[SVM][TRAIN] Batch 719/782 done\n","[SVM][TRAIN] Batch 720/782 done\n","[SVM][TRAIN] Batch 721/782 done\n","[SVM][TRAIN] Batch 722/782 done\n","[SVM][TRAIN] Batch 723/782 done\n","[SVM][TRAIN] Batch 724/782 done\n","[SVM][TRAIN] Batch 725/782 done\n","[SVM][TRAIN] Batch 726/782 done\n","[SVM][TRAIN] Batch 727/782 done\n","[SVM][TRAIN] Batch 728/782 done\n","[SVM][TRAIN] Batch 729/782 done\n","[SVM][TRAIN] Batch 730/782 done\n","[SVM][TRAIN] Batch 731/782 done\n","[SVM][TRAIN] Batch 732/782 done\n","[SVM][TRAIN] Batch 733/782 done\n","[SVM][TRAIN] Batch 734/782 done\n","[SVM][TRAIN] Batch 735/782 done\n","[SVM][TRAIN] Batch 736/782 done\n","[SVM][TRAIN] Batch 737/782 done\n","[SVM][TRAIN] Batch 738/782 done\n","[SVM][TRAIN] Batch 739/782 done\n","[SVM][TRAIN] Batch 740/782 done\n","[SVM][TRAIN] Batch 741/782 done\n","[SVM][TRAIN] Batch 742/782 done\n","[SVM][TRAIN] Batch 743/782 done\n","[SVM][TRAIN] Batch 744/782 done\n","[SVM][TRAIN] Batch 745/782 done\n","[SVM][TRAIN] Batch 746/782 done\n","[SVM][TRAIN] Batch 747/782 done\n","[SVM][TRAIN] Batch 748/782 done\n","[SVM][TRAIN] Batch 749/782 done\n","[SVM][TRAIN] Batch 750/782 done\n","[SVM][TRAIN] Batch 751/782 done\n","[SVM][TRAIN] Batch 752/782 done\n","[SVM][TRAIN] Batch 753/782 done\n","[SVM][TRAIN] Batch 754/782 done\n","[SVM][TRAIN] Batch 755/782 done\n","[SVM][TRAIN] Batch 756/782 done\n","[SVM][TRAIN] Batch 757/782 done\n","[SVM][TRAIN] Batch 758/782 done\n","[SVM][TRAIN] Batch 759/782 done\n","[SVM][TRAIN] Batch 760/782 done\n","[SVM][TRAIN] Batch 761/782 done\n","[SVM][TRAIN] Batch 762/782 done\n","[SVM][TRAIN] Batch 763/782 done\n","[SVM][TRAIN] Batch 764/782 done\n","[SVM][TRAIN] Batch 765/782 done\n","[SVM][TRAIN] Batch 766/782 done\n","[SVM][TRAIN] Batch 767/782 done\n","[SVM][TRAIN] Batch 768/782 done\n","[SVM][TRAIN] Batch 769/782 done\n","[SVM][TRAIN] Batch 770/782 done\n","[SVM][TRAIN] Batch 771/782 done\n","[SVM][TRAIN] Batch 772/782 done\n","[SVM][TRAIN] Batch 773/782 done\n","[SVM][TRAIN] Batch 774/782 done\n","[SVM][TRAIN] Batch 775/782 done\n","[SVM][TRAIN] Batch 776/782 done\n","[SVM][TRAIN] Batch 777/782 done\n","[SVM][TRAIN] Batch 778/782 done\n","[SVM][TRAIN] Batch 779/782 done\n","[SVM][TRAIN] Batch 780/782 done\n","[SVM][TRAIN] Batch 781/782 done\n","[SVM][TRAIN] Batch 782/782 done\n","[SVM] Saved train_svm.txt\n","[SVM] Extracting test features...\n","[SVM][TEST] Batch 1/157 done\n","[SVM][TEST] Batch 2/157 done\n","[SVM][TEST] Batch 3/157 done\n","[SVM][TEST] Batch 4/157 done\n","[SVM][TEST] Batch 5/157 done\n","[SVM][TEST] Batch 6/157 done\n","[SVM][TEST] Batch 7/157 done\n","[SVM][TEST] Batch 8/157 done\n","[SVM][TEST] Batch 9/157 done\n","[SVM][TEST] Batch 10/157 done\n","[SVM][TEST] Batch 11/157 done\n","[SVM][TEST] Batch 12/157 done\n","[SVM][TEST] Batch 13/157 done\n","[SVM][TEST] Batch 14/157 done\n","[SVM][TEST] Batch 15/157 done\n","[SVM][TEST] Batch 16/157 done\n","[SVM][TEST] Batch 17/157 done\n","[SVM][TEST] Batch 18/157 done\n","[SVM][TEST] Batch 19/157 done\n","[SVM][TEST] Batch 20/157 done\n","[SVM][TEST] Batch 21/157 done\n","[SVM][TEST] Batch 22/157 done\n","[SVM][TEST] Batch 23/157 done\n","[SVM][TEST] Batch 24/157 done\n","[SVM][TEST] Batch 25/157 done\n","[SVM][TEST] Batch 26/157 done\n","[SVM][TEST] Batch 27/157 done\n","[SVM][TEST] Batch 28/157 done\n","[SVM][TEST] Batch 29/157 done\n","[SVM][TEST] Batch 30/157 done\n","[SVM][TEST] Batch 31/157 done\n","[SVM][TEST] Batch 32/157 done\n","[SVM][TEST] Batch 33/157 done\n","[SVM][TEST] Batch 34/157 done\n","[SVM][TEST] Batch 35/157 done\n","[SVM][TEST] Batch 36/157 done\n","[SVM][TEST] Batch 37/157 done\n","[SVM][TEST] Batch 38/157 done\n","[SVM][TEST] Batch 39/157 done\n","[SVM][TEST] Batch 40/157 done\n","[SVM][TEST] Batch 41/157 done\n","[SVM][TEST] Batch 42/157 done\n","[SVM][TEST] Batch 43/157 done\n","[SVM][TEST] Batch 44/157 done\n","[SVM][TEST] Batch 45/157 done\n","[SVM][TEST] Batch 46/157 done\n","[SVM][TEST] Batch 47/157 done\n","[SVM][TEST] Batch 48/157 done\n","[SVM][TEST] Batch 49/157 done\n","[SVM][TEST] Batch 50/157 done\n","[SVM][TEST] Batch 51/157 done\n","[SVM][TEST] Batch 52/157 done\n","[SVM][TEST] Batch 53/157 done\n","[SVM][TEST] Batch 54/157 done\n","[SVM][TEST] Batch 55/157 done\n","[SVM][TEST] Batch 56/157 done\n","[SVM][TEST] Batch 57/157 done\n","[SVM][TEST] Batch 58/157 done\n","[SVM][TEST] Batch 59/157 done\n","[SVM][TEST] Batch 60/157 done\n","[SVM][TEST] Batch 61/157 done\n","[SVM][TEST] Batch 62/157 done\n","[SVM][TEST] Batch 63/157 done\n","[SVM][TEST] Batch 64/157 done\n","[SVM][TEST] Batch 65/157 done\n","[SVM][TEST] Batch 66/157 done\n","[SVM][TEST] Batch 67/157 done\n","[SVM][TEST] Batch 68/157 done\n","[SVM][TEST] Batch 69/157 done\n","[SVM][TEST] Batch 70/157 done\n","[SVM][TEST] Batch 71/157 done\n","[SVM][TEST] Batch 72/157 done\n","[SVM][TEST] Batch 73/157 done\n","[SVM][TEST] Batch 74/157 done\n","[SVM][TEST] Batch 75/157 done\n","[SVM][TEST] Batch 76/157 done\n","[SVM][TEST] Batch 77/157 done\n","[SVM][TEST] Batch 78/157 done\n","[SVM][TEST] Batch 79/157 done\n","[SVM][TEST] Batch 80/157 done\n","[SVM][TEST] Batch 81/157 done\n","[SVM][TEST] Batch 82/157 done\n","[SVM][TEST] Batch 83/157 done\n","[SVM][TEST] Batch 84/157 done\n","[SVM][TEST] Batch 85/157 done\n","[SVM][TEST] Batch 86/157 done\n","[SVM][TEST] Batch 87/157 done\n","[SVM][TEST] Batch 88/157 done\n","[SVM][TEST] Batch 89/157 done\n","[SVM][TEST] Batch 90/157 done\n","[SVM][TEST] Batch 91/157 done\n","[SVM][TEST] Batch 92/157 done\n","[SVM][TEST] Batch 93/157 done\n","[SVM][TEST] Batch 94/157 done\n","[SVM][TEST] Batch 95/157 done\n","[SVM][TEST] Batch 96/157 done\n","[SVM][TEST] Batch 97/157 done\n","[SVM][TEST] Batch 98/157 done\n","[SVM][TEST] Batch 99/157 done\n","[SVM][TEST] Batch 100/157 done\n","[SVM][TEST] Batch 101/157 done\n","[SVM][TEST] Batch 102/157 done\n","[SVM][TEST] Batch 103/157 done\n","[SVM][TEST] Batch 104/157 done\n","[SVM][TEST] Batch 105/157 done\n","[SVM][TEST] Batch 106/157 done\n","[SVM][TEST] Batch 107/157 done\n","[SVM][TEST] Batch 108/157 done\n","[SVM][TEST] Batch 109/157 done\n","[SVM][TEST] Batch 110/157 done\n","[SVM][TEST] Batch 111/157 done\n","[SVM][TEST] Batch 112/157 done\n","[SVM][TEST] Batch 113/157 done\n","[SVM][TEST] Batch 114/157 done\n","[SVM][TEST] Batch 115/157 done\n","[SVM][TEST] Batch 116/157 done\n","[SVM][TEST] Batch 117/157 done\n","[SVM][TEST] Batch 118/157 done\n","[SVM][TEST] Batch 119/157 done\n","[SVM][TEST] Batch 120/157 done\n","[SVM][TEST] Batch 121/157 done\n","[SVM][TEST] Batch 122/157 done\n","[SVM][TEST] Batch 123/157 done\n","[SVM][TEST] Batch 124/157 done\n","[SVM][TEST] Batch 125/157 done\n","[SVM][TEST] Batch 126/157 done\n","[SVM][TEST] Batch 127/157 done\n","[SVM][TEST] Batch 128/157 done\n","[SVM][TEST] Batch 129/157 done\n","[SVM][TEST] Batch 130/157 done\n","[SVM][TEST] Batch 131/157 done\n","[SVM][TEST] Batch 132/157 done\n","[SVM][TEST] Batch 133/157 done\n","[SVM][TEST] Batch 134/157 done\n","[SVM][TEST] Batch 135/157 done\n","[SVM][TEST] Batch 136/157 done\n","[SVM][TEST] Batch 137/157 done\n","[SVM][TEST] Batch 138/157 done\n","[SVM][TEST] Batch 139/157 done\n","[SVM][TEST] Batch 140/157 done\n","[SVM][TEST] Batch 141/157 done\n","[SVM][TEST] Batch 142/157 done\n","[SVM][TEST] Batch 143/157 done\n","[SVM][TEST] Batch 144/157 done\n","[SVM][TEST] Batch 145/157 done\n","[SVM][TEST] Batch 146/157 done\n","[SVM][TEST] Batch 147/157 done\n","[SVM][TEST] Batch 148/157 done\n","[SVM][TEST] Batch 149/157 done\n","[SVM][TEST] Batch 150/157 done\n","[SVM][TEST] Batch 151/157 done\n","[SVM][TEST] Batch 152/157 done\n","[SVM][TEST] Batch 153/157 done\n","[SVM][TEST] Batch 154/157 done\n","[SVM][TEST] Batch 155/157 done\n","[SVM][TEST] Batch 156/157 done\n","[SVM][TEST] Batch 157/157 done\n","[SVM] Saved test_svm.txt\n","[SVM] Done.\n"]}]},{"cell_type":"code","source":["!cp train_svm.txt \"/content/drive/MyDrive/Parallel - Final Project/train_svm.txt\"\n","!cp test_svm.txt \"/content/drive/MyDrive/Parallel - Final Project/test_svm.txt\""],"metadata":{"id":"IBBGuTGGALyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tạo train_svm_small với 10000 dòng đầu\n","!head -n 4000 \"/content/drive/MyDrive/Parallel - Final Project/train_svm.txt\" > /content/train_svm_small.txt\n","\n","# Tạo test_svm_small với 2000 dòng đầu\n","!head -n 800 \"/content/drive/MyDrive/Parallel - Final Project/test_svm.txt\" > /content/test_svm_small.txt"],"metadata":{"id":"6CaU6aboAYRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/train_svm_small.txt \"/content/drive/MyDrive/Parallel - Final Project/train_svm_small.txt\"\n","!cp /content/test_svm_small.txt  \"/content/drive/MyDrive/Parallel - Final Project/test_svm_small.txt\""],"metadata":{"id":"-3FmZfOlAZJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/cjlin1/libsvm.git\n","%cd libsvm\n","!make"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bb7hK38IGTKY","outputId":"28981ccf-5d42-41cc-fc7c-6be67f2a554a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'libsvm'...\n","remote: Enumerating objects: 4201, done.\u001b[K\n","remote: Counting objects: 100% (230/230), done.\u001b[K\n","remote: Compressing objects: 100% (112/112), done.\u001b[K\n","remote: Total 4201 (delta 140), reused 118 (delta 118), pack-reused 3971 (from 3)\u001b[K\n","Receiving objects: 100% (4201/4201), 9.92 MiB | 16.33 MiB/s, done.\n","Resolving deltas: 100% (2317/2317), done.\n","/content/libsvm\n","g++ -Wall -Wconversion -O3 -fPIC -c svm.cpp\n","g++ -Wall -Wconversion -O3 -fPIC svm-train.c svm.o -o svm-train -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-predict.c svm.o -o svm-predict -lm\n","g++ -Wall -Wconversion -O3 -fPIC svm-scale.c -o svm-scale\n"]}]},{"cell_type":"code","source":["%cd /content/libsvm\n","\n","!./svm-train -s 0 -t 0 -c 1.0 \\\n","  \"/content/drive/MyDrive/Parallel - Final Project/train_svm_small.txt\" \\\n","  /content/model_ae_svm\n","\n","!./svm-predict \\\n","  \"/content/drive/MyDrive/Parallel - Final Project/test_svm_small.txt\" \\\n","  /content/model_ae_svm \\\n","  /content/pred.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xw-5ARBGXIK","outputId":"f3dcba01-6a4f-4b63-bf7e-67b6e1bdb58f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/libsvm\n",".........*.....*\n","optimization finished, #iter = 3142\n","nu = 0.184038\n","obj = -20.387511, rho = -3.810686\n","nSV = 105, nBSV = 7\n",".............*......*\n","optimization finished, #iter = 4118\n","nu = 0.516252\n","obj = -67.628452, rho = 2.802563\n","nSV = 164, nBSV = 58\n","..........*.....*\n","optimization finished, #iter = 3318\n","nu = 0.175541\n","obj = -20.691013, rho = -4.340881\n","nSV = 110, nBSV = 5\n","...........*......*\n","optimization finished, #iter = 3509\n","nu = 0.384025\n","obj = -46.995882, rho = 2.521878\n","nSV = 136, nBSV = 34\n",".............*......*\n","optimization finished, #iter = 3973\n","nu = 0.285219\n","obj = -32.777594, rho = -2.729153\n","nSV = 128, nBSV = 12\n",".....*...*\n","optimization finished, #iter = 1650\n","nu = 0.113230\n","obj = -13.589840, rho = -0.916518\n","nSV = 67, nBSV = 8\n","...............*......*\n","optimization finished, #iter = 4245\n","nu = 0.439588\n","obj = -51.525350, rho = 1.630970\n","nSV = 140, nBSV = 35\n","...............*.....*\n","optimization finished, #iter = 3924\n","nu = 0.353999\n","obj = -39.543064, rho = -0.136117\n","nSV = 134, nBSV = 27\n","......*...*\n","optimization finished, #iter = 1998\n","nu = 0.129870\n","obj = -14.437656, rho = 0.248231\n","nSV = 70, nBSV = 6\n","............*.....*\n","optimization finished, #iter = 3511\n","nu = 0.188983\n","obj = -21.008748, rho = 4.481999\n","nSV = 105, nBSV = 7\n","..................*......*\n","optimization finished, #iter = 5369\n","nu = 0.384066\n","obj = -45.767887, rho = 0.980868\n","nSV = 165, nBSV = 21\n","........*.....*\n","optimization finished, #iter = 2644\n","nu = 0.159987\n","obj = -16.937018, rho = 3.524208\n","nSV = 89, nBSV = 6\n","...........*.....*\n","optimization finished, #iter = 3422\n","nu = 0.233689\n","obj = -26.310792, rho = 0.604710\n","nSV = 121, nBSV = 12\n","..........*......*\n","optimization finished, #iter = 3345\n","nu = 0.235225\n","obj = -26.117108, rho = 2.885812\n","nSV = 111, nBSV = 9\n","...............*........*\n","optimization finished, #iter = 4570\n","nu = 0.230814\n","obj = -24.178983, rho = 5.237632\n","nSV = 119, nBSV = 9\n","..........*....*\n","optimization finished, #iter = 2742\n","nu = 0.157197\n","obj = -15.778496, rho = 2.951273\n","nSV = 91, nBSV = 3\n","............*.....*\n","optimization finished, #iter = 3686\n","nu = 0.233264\n","obj = -27.397460, rho = 2.425223\n","nSV = 112, nBSV = 12\n",".........*....*\n","optimization finished, #iter = 2762\n","nu = 0.184945\n","obj = -20.093690, rho = -5.528165\n","nSV = 102, nBSV = 4\n",".............*......*\n","optimization finished, #iter = 3836\n","nu = 0.559508\n","obj = -72.614825, rho = -0.076279\n","nSV = 156, nBSV = 59\n","..............*......*\n","optimization finished, #iter = 4144\n","nu = 0.380975\n","obj = -43.730817, rho = -5.401438\n","nSV = 137, nBSV = 25\n",".......*...*\n","optimization finished, #iter = 2033\n","nu = 0.167786\n","obj = -18.086209, rho = -1.858433\n","nSV = 79, nBSV = 9\n","................*.......*\n","optimization finished, #iter = 4538\n","nu = 0.383953\n","obj = -44.300378, rho = -0.705133\n","nSV = 125, nBSV = 28\n",".............*.....*\n","optimization finished, #iter = 3431\n","nu = 0.379252\n","obj = -41.899430, rho = -2.471049\n","nSV = 122, nBSV = 30\n","...........*.....*\n","optimization finished, #iter = 3232\n","nu = 0.221232\n","obj = -26.503645, rho = -1.232669\n","nSV = 100, nBSV = 15\n","...........*....*\n","optimization finished, #iter = 3168\n","nu = 0.155027\n","obj = -17.125585, rho = 5.235152\n","nSV = 99, nBSV = 5\n","............*.....*\n","optimization finished, #iter = 3814\n","nu = 0.208955\n","obj = -22.919154, rho = 2.365786\n","nSV = 116, nBSV = 5\n","..........*......*\n","optimization finished, #iter = 3445\n","nu = 0.205055\n","obj = -24.197676, rho = 3.350805\n","nSV = 112, nBSV = 10\n","...............*.......*\n","optimization finished, #iter = 4624\n","nu = 0.232782\n","obj = -25.340566, rho = 5.630586\n","nSV = 124, nBSV = 5\n","...........*.....*\n","optimization finished, #iter = 3278\n","nu = 0.172911\n","obj = -17.398189, rho = 3.581335\n","nSV = 104, nBSV = 1\n","..........*......*\n","optimization finished, #iter = 3526\n","nu = 0.218952\n","obj = -25.586358, rho = 3.008874\n","nSV = 112, nBSV = 11\n","...............*.....*\n","optimization finished, #iter = 4188\n","nu = 0.290736\n","obj = -32.326467, rho = -4.991865\n","nSV = 126, nBSV = 17\n","........*....*\n","optimization finished, #iter = 2377\n","nu = 0.184573\n","obj = -21.041069, rho = -2.821143\n","nSV = 87, nBSV = 11\n","...............*.......*\n","optimization finished, #iter = 4233\n","nu = 0.357490\n","obj = -39.161019, rho = -1.220094\n","nSV = 120, nBSV = 25\n","..............*.......*\n","optimization finished, #iter = 3945\n","nu = 0.387482\n","obj = -41.411758, rho = -2.012443\n","nSV = 122, nBSV = 28\n","............*......*\n","optimization finished, #iter = 3717\n","nu = 0.270017\n","obj = -31.912014, rho = -0.534063\n","nSV = 113, nBSV = 18\n","........*...*\n","optimization finished, #iter = 2271\n","nu = 0.127790\n","obj = -13.742523, rho = 1.041068\n","nSV = 78, nBSV = 3\n",".................*........*\n","optimization finished, #iter = 4869\n","nu = 0.342285\n","obj = -37.330453, rho = 3.363464\n","nSV = 130, nBSV = 18\n",".............*.......*\n","optimization finished, #iter = 3837\n","nu = 0.301320\n","obj = -31.130501, rho = 2.496579\n","nSV = 120, nBSV = 11\n","...........*......*\n","optimization finished, #iter = 3515\n","nu = 0.238735\n","obj = -27.766459, rho = 2.119388\n","nSV = 111, nBSV = 13\n","..........*....*\n","optimization finished, #iter = 2725\n","nu = 0.170247\n","obj = -18.217040, rho = 0.347807\n","nSV = 84, nBSV = 7\n","......*..*\n","optimization finished, #iter = 1529\n","nu = 0.118678\n","obj = -11.205868, rho = 2.150865\n","nSV = 63, nBSV = 2\n","..............*..........*\n","optimization finished, #iter = 5004\n","nu = 0.438587\n","obj = -58.100331, rho = -0.259138\n","nSV = 140, nBSV = 39\n","...................*.....*\n","optimization finished, #iter = 4412\n","nu = 0.452654\n","obj = -48.889692, rho = -0.727931\n","nSV = 138, nBSV = 29\n","............*....*\n","optimization finished, #iter = 3202\n","nu = 0.221162\n","obj = -24.659628, rho = 0.220471\n","nSV = 87, nBSV = 14\n",".........*....*\n","optimization finished, #iter = 2494\n","nu = 0.160565\n","obj = -16.511133, rho = -0.036934\n","nSV = 80, nBSV = 4\n","Total nSV = 966\n","Accuracy = 39% (78/200) (classification)\n"]}]},{"cell_type":"code","source":["%cd /content/libsvm\n","\n","!./svm-train -s 0 -t 0 -c 1.0 \\\n"," /content/train_svm_small.txt \\\n","  /content/model_ae_svm\n","\n","!./svm-predict \\\n","  /content/test_svm_small.txt \\\n","  /content/model_ae_svm \\\n","  /content/pred.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJjY2AztHUSV","outputId":"76f18b35-f8e4-4dd8-fc10-3948080b587d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/libsvm\n",".......*....*\n","optimization finished, #iter = 9418\n","nu = 0.167658\n","obj = -81.269062, rho = -4.558449\n","nSV = 278, nBSV = 44\n","...........*.....*\n","optimization finished, #iter = 13632\n","nu = 0.444621\n","obj = -253.480395, rho = 4.592529\n","nSV = 509, nBSV = 231\n","........*...*\n","optimization finished, #iter = 9038\n","nu = 0.162415\n","obj = -72.858169, rho = -4.949104\n","nSV = 275, nBSV = 40\n","..........*.....*\n","optimization finished, #iter = 12595\n","nu = 0.408758\n","obj = -236.268783, rho = 4.439975\n","nSV = 479, nBSV = 217\n","...........*.....*\n","optimization finished, #iter = 12924\n","nu = 0.298063\n","obj = -146.327587, rho = -1.487168\n","nSV = 393, nBSV = 118\n",".....*...*\n","optimization finished, #iter = 7330\n","nu = 0.128470\n","obj = -68.458602, rho = 0.833275\n","nSV = 211, nBSV = 49\n","..............*......*\n","optimization finished, #iter = 16198\n","nu = 0.449197\n","obj = -242.518445, rho = 3.388061\n","nSV = 516, nBSV = 219\n","..........*.....*\n","optimization finished, #iter = 12357\n","nu = 0.348301\n","obj = -186.378443, rho = 0.792858\n","nSV = 429, nBSV = 158\n",".......*..*\n","optimization finished, #iter = 8061\n","nu = 0.151375\n","obj = -74.451730, rho = 0.800941\n","nSV = 226, nBSV = 45\n",".........*...*\n","optimization finished, #iter = 10814\n","nu = 0.174113\n","obj = -85.868881, rho = 6.109249\n","nSV = 272, nBSV = 58\n","................*.......*\n","optimization finished, #iter = 18542\n","nu = 0.456199\n","obj = -226.262176, rho = 1.549540\n","nSV = 542, nBSV = 188\n","..........*...*\n","optimization finished, #iter = 11633\n","nu = 0.179767\n","obj = -91.059058, rho = 7.402825\n","nSV = 292, nBSV = 63\n","...........*.....*\n","optimization finished, #iter = 12907\n","nu = 0.236776\n","obj = -113.291177, rho = 2.606713\n","nSV = 356, nBSV = 76\n","..........*.....*\n","optimization finished, #iter = 12679\n","nu = 0.225838\n","obj = -112.320694, rho = 7.585588\n","nSV = 329, nBSV = 76\n","............*....*\n","optimization finished, #iter = 13488\n","nu = 0.217857\n","obj = -103.363178, rho = 7.214371\n","nSV = 325, nBSV = 62\n",".........*.....*\n","optimization finished, #iter = 12070\n","nu = 0.176030\n","obj = -84.780378, rho = 4.387791\n","nSV = 297, nBSV = 54\n","..........*.....*\n","optimization finished, #iter = 12264\n","nu = 0.247883\n","obj = -125.832570, rho = 6.582150\n","nSV = 349, nBSV = 95\n","........*....*\n","optimization finished, #iter = 9918\n","nu = 0.161456\n","obj = -70.575715, rho = -7.180252\n","nSV = 258, nBSV = 36\n",".............*....*\n","optimization finished, #iter = 15108\n","nu = 0.597871\n","obj = -365.923787, rho = 1.317633\n","nSV = 629, nBSV = 370\n","............*.....*\n","optimization finished, #iter = 14226\n","nu = 0.409788\n","obj = -223.902712, rho = -4.777018\n","nSV = 467, nBSV = 198\n","........*....*\n","optimization finished, #iter = 10033\n","nu = 0.206096\n","obj = -112.291865, rho = 0.891610\n","nSV = 278, nBSV = 92\n","..............*....*\n","optimization finished, #iter = 15179\n","nu = 0.437343\n","obj = -246.038456, rho = -1.227963\n","nSV = 476, nBSV = 225\n",".............*....*\n","optimization finished, #iter = 14026\n","nu = 0.377934\n","obj = -209.575645, rho = -2.778579\n","nSV = 445, nBSV = 189\n","..........*....*\n","optimization finished, #iter = 11576\n","nu = 0.266443\n","obj = -147.607083, rho = -1.026888\n","nSV = 323, nBSV = 124\n",".........*......*\n","optimization finished, #iter = 12022\n","nu = 0.178469\n","obj = -78.685991, rho = 8.076393\n","nSV = 285, nBSV = 43\n","..........*.....*\n","optimization finished, #iter = 11618\n","nu = 0.201133\n","obj = -83.852519, rho = 3.049548\n","nSV = 314, nBSV = 53\n","...........*...*\n","optimization finished, #iter = 11286\n","nu = 0.224466\n","obj = -108.724579, rho = 6.870011\n","nSV = 309, nBSV = 78\n","...........*......*\n","optimization finished, #iter = 12831\n","nu = 0.195616\n","obj = -83.043153, rho = 7.244740\n","nSV = 309, nBSV = 45\n",".........*.....*\n","optimization finished, #iter = 10910\n","nu = 0.171668\n","obj = -73.159087, rho = 5.101625\n","nSV = 297, nBSV = 44\n","..........*......*\n","optimization finished, #iter = 12674\n","nu = 0.233418\n","obj = -106.145489, rho = 5.745061\n","nSV = 329, nBSV = 71\n","..............*......*\n","optimization finished, #iter = 16452\n","nu = 0.366167\n","obj = -190.120424, rho = -4.760985\n","nSV = 454, nBSV = 155\n",".........*....*\n","optimization finished, #iter = 10870\n","nu = 0.248966\n","obj = -136.388005, rho = -1.606893\n","nSV = 323, nBSV = 109\n","..............*......*\n","optimization finished, #iter = 16630\n","nu = 0.431703\n","obj = -243.828267, rho = -2.390641\n","nSV = 487, nBSV = 215\n","............*.....*\n","optimization finished, #iter = 14513\n","nu = 0.419069\n","obj = -236.906398, rho = -5.657305\n","nSV = 473, nBSV = 205\n","...........*......*\n","optimization finished, #iter = 14279\n","nu = 0.338192\n","obj = -190.409060, rho = -0.790277\n","nSV = 404, nBSV = 174\n","........*....*\n","optimization finished, #iter = 10028\n","nu = 0.171198\n","obj = -80.299672, rho = 2.993990\n","nSV = 251, nBSV = 56\n","...............*.......*\n","optimization finished, #iter = 17340\n","nu = 0.390517\n","obj = -191.937520, rho = 3.552392\n","nSV = 445, nBSV = 161\n","................*......*\n","optimization finished, #iter = 17119\n","nu = 0.393336\n","obj = -198.998549, rho = 2.298602\n","nSV = 455, nBSV = 168\n","...........*....*\n","optimization finished, #iter = 12495\n","nu = 0.259030\n","obj = -125.940488, rho = 2.595624\n","nSV = 343, nBSV = 97\n",".........*...*\n","optimization finished, #iter = 10138\n","nu = 0.197962\n","obj = -95.996486, rho = -0.004022\n","nSV = 281, nBSV = 77\n","........*..*\n","optimization finished, #iter = 8419\n","nu = 0.173163\n","obj = -84.133432, rho = -1.288166\n","nSV = 260, nBSV = 67\n","..............*.....*\n","optimization finished, #iter = 15570\n","nu = 0.504790\n","obj = -281.353542, rho = 0.150210\n","nSV = 536, nBSV = 276\n","...................*.......*\n","optimization finished, #iter = 20720\n","nu = 0.602133\n","obj = -330.941278, rho = -3.371373\n","nSV = 619, nBSV = 304\n","............*.....*\n","optimization finished, #iter = 13265\n","nu = 0.235573\n","obj = -113.456811, rho = 0.306519\n","nSV = 310, nBSV = 94\n","...........*....*\n","optimization finished, #iter = 12141\n","nu = 0.206351\n","obj = -101.359391, rho = 0.488523\n","nSV = 298, nBSV = 77\n","Total nSV = 3648\n","Accuracy = 47.25% (378/800) (classification)\n"]}]}]}