{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3U1srqX1Xbm2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765305494454,"user_tz":-420,"elapsed":1761,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"7917f253-1aab-4008-fde7-37ec99ed70e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%%writefile gpu_layers.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","\n","__global__ void conv2d_forward_naive(\n","    const float* __restrict__ input,    // [N, C_in, H, W]\n","    const float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    const float* __restrict__ bias,     // [C_out]\n","    float* __restrict__ output,         // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    const float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    const float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    const float* __restrict__ x,       // forward output/input to ReLU\n","    const float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    const float* __restrict__ input,\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_backward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input_naive(\n","    const float* __restrict__ dY,\n","    const float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias_naive(\n","    const float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    const float* __restrict__ grad,\n","    int size,\n","    float lr);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGe_w8qBXxLP","executionInfo":{"status":"ok","timestamp":1765305494458,"user_tz":-420,"elapsed":2,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"cd3148ab-2914-4d6f-ee71-32f32394f0fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting gpu_layers.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers.cu\n","#include \"gpu_layers.h\"\n","\n","// --------------- Conv2D forward (không đổi) ------------------\n","__global__ void conv2d_forward_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ weight,\n","    const float* __restrict__ bias,\n","    float* __restrict__ output,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n      = nc / C_out;\n","    int c_out  = nc % C_out;\n","    if (n >= N) return;\n","\n","    float sum = bias ? bias[c_out] : 0.0f;\n","\n","    for (int c_in = 0; c_in < C_in; ++c_in) {\n","        for (int kh = 0; kh < K; ++kh) {\n","            for (int kw = 0; kw < K; ++kw) {\n","                int h_in = h_out * stride + kh - pad;\n","                int w_in = w_out * stride + kw - pad;\n","                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n","                    continue;\n","\n","                int in_idx = ((n * C_in + c_in) * H + h_in) * W + w_in;\n","                int w_idx = (((c_out * C_in + c_in) * K) + kh) * K + kw;\n","                sum += weight[w_idx] * input[in_idx];\n","            }\n","        }\n","    }\n","    int out_idx = ((n * C_out + c_out) * H_out + h_out) * W_out + w_out;\n","    output[out_idx] = sum;\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = v > 0.0f ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 ------------------\n","__global__ void maxpool2x2_forward(\n","    const float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 ------------------\n","__global__ void upsample2x2_forward(\n","    const float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in  = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // reduce trong block\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    const float* __restrict__ x,\n","    const float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        grad_x[i] = (v > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward - FIX ------------------\n","// BUG FIX: Chỉ ghi vào vị trí max, không ghi đè 4 vị trí\n","__global__ void maxpool2x2_backward(\n","    const float* __restrict__ input,\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    // Tìm max\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    // FIX: Chỉ ghi vào vị trí max (grad_in đã được zero trước đó)\n","    // Mỗi pooling window độc lập, không có conflict\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc   = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input_naive(\n","    const float* __restrict__ dY,\n","    const float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int w = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w >= W || h >= H) return;\n","\n","    int n = nc / C_in;\n","    int c_in = nc % C_in;\n","    if (n >= N) return;\n","\n","    float sum = 0.0f;\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        for (int kh = 0; kh < K; ++kh) {\n","            for (int kw = 0; kw < K; ++kw) {\n","                int h_out = h + pad - kh;\n","                int w_out = w + pad - kw;\n","\n","                if (h_out % stride != 0 || w_out % stride != 0) continue;\n","\n","                h_out /= stride;\n","                w_out /= stride;\n","\n","                if (h_out < 0 || h_out >= H_out ||\n","                    w_out < 0 || w_out >= W_out)\n","                    continue;\n","\n","                int dy_idx = idx4(n, c_out, h_out, w_out,\n","                                  C_out, H_out, W_out);\n","\n","                int kh_flip = K - 1 - kh;\n","                int kw_flip = K - 1 - kw;\n","                int w_idx = (((c_out * C_in + c_in) * K) + kh_flip) * K + kw_flip;\n","                sum += dY[dy_idx] * weight[w_idx];\n","            }\n","        }\n","    }\n","\n","    int dx_idx = idx4(n, c_in, h, w, C_in, H, W);\n","    dX[dx_idx] = sum;\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","// Mỗi thread tính toàn bộ gradient cho 1 weight element\n","// Không có conflict vì mỗi thread ghi vào vị trí riêng biệt\n","__global__ void conv2d_backward_weight_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    int total = C_out * C_in * K * K;\n","    if (idx >= total) return;\n","\n","    int kw = idx % K;\n","    int tmp = idx / K;\n","    int kh = tmp % K;\n","    tmp /= K;\n","    int c_in = tmp % C_in;\n","    int c_out = tmp / C_in;\n","\n","    float sum = 0.0f;\n","    for (int n = 0; n < N; ++n) {\n","        for (int h_out = 0; h_out < H_out; ++h_out) {\n","            for (int w_out = 0; w_out < W_out; ++w_out) {\n","                int h_in = h_out * stride + kh - pad;\n","                int w_in = w_out * stride + kw - pad;\n","                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n","                    continue;\n","\n","                int in_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                int dy_idx = idx4(n, c_out, h_out, w_out,\n","                                  C_out, H_out, W_out);\n","                sum += dY[dy_idx] * input[in_idx];\n","            }\n","        }\n","    }\n","\n","    // Mỗi thread ghi vào vị trí riêng, không conflict\n","    dW[idx] += sum;\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","// Mỗi thread tính gradient cho 1 bias element\n","__global__ void conv2d_backward_bias_naive(\n","    const float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int c_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (c_out >= C_out) return;\n","\n","    float sum = 0.0f;\n","    for (int n = 0; n < N; ++n) {\n","        for (int h = 0; h < H_out; ++h) {\n","            for (int w = 0; w < W_out; ++w) {\n","                int idx = idx4(n, c_out, h, w, C_out, H_out, W_out);\n","                sum += dY[idx];\n","            }\n","        }\n","    }\n","\n","    // Mỗi thread ghi vào vị trí riêng, không conflict\n","    dB[c_out] += sum;\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    const float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0s2nwQRAXyqf","executionInfo":{"status":"ok","timestamp":1765305494460,"user_tz":-420,"elapsed":1,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"ce865f33-5406-4e5c-b949-a025411d68b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting gpu_layers.cu\n"]}]},{"cell_type":"code","source":["%%writefile cpu_layers.h\n","#pragma once\n","#include <stdio.h>\n","#include <float.h>\n","\n","void Relu(float* input, int N, float* output);\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void MaxPool2D_Forward(float* input, int input_width, int input_height,\n","    int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","    int scale_factor, int filter_count,\n","    float* output, int output_height, int output_width);\n","float MSE(float* input, float* output, int size);\n","void Relu_Backward(float* d_output, float* input,int N);\n","void MSE_Gradient(float* input, float* output, int size, float* d_output);\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width);\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights);\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height, int filter_count, float* d_biases);\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSB9Cuf_X9B-","executionInfo":{"status":"ok","timestamp":1765305494462,"user_tz":-420,"elapsed":1,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"87592462-e6da-4429-a9bc-0e6e68ec6ee8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting cpu_layers.h\n"]}]},{"cell_type":"code","source":["%%writefile cpu_layers.c\n","#include \"cpu_layers.h\"\n","\n","void Relu(float* input, int N, float* output) {\n","    for (int i = 0; i < N; i++) {\n","        output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n","    }\n","}\n","\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width)\n","{\n","    // // Tính toán kích thước output\n","    // int H_out = (input_height + 2 * padding - kernel_height) / stride + 1;\n","    // int W_out = (input_width + 2 * padding - kernel_width) / stride + 1;\n","    // int output_size = filter_count * H_out * W_out;\n","    // output = (float*)malloc(output_size * sizeof(float));\n","    // if (output == NULL) {\n","    //     fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","    //     return;\n","    // }\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua chiều cao output\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            // Lặp qua chiều rộng output\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float sum = 0.0f;\n","                // Lặp qua kênh đầu vào (c_in)\n","                for (int c_in = 0; c_in < input_channels; c_in++) {\n","                    // Lặp qua kernel height\n","                    for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                        // Lặp qua kernel width\n","                        for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                            // Vị trí input tương ứng\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float val = 0.0f;\n","                            // Kiểm tra zero padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int channel_size = input_width * input_height;\n","                                val = input[c_in * channel_size + h_in * input_width + w_in];\n","                            }\n","\n","                            int weight_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            k_h * kernel_width + k_w;\n","\n","                            sum += val * kernel[weight_idx];\n","                        }\n","                    }\n","                }\n","                sum += biases[c_out];  // Thêm bias\n","                int output_idx = h_out * output_width + w_out + c_out * output_width * output_height;\n","                output[output_idx] = sum;\n","            }\n","        }\n","    }\n","}\n","\n","\n","void MaxPool2D_Forward(float* input, int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width) {\n","    // int H_out = (input_height - filter_height) / stride + 1;\n","    // int W_out = (input_width - filter_width) / stride + 1;\n","\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float max_val = -FLT_MAX;\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = c * plane_size_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                        }\n","                    }\n","                }\n","                int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                output[output_idx] = max_val;\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","int scale_factor, int filter_count, float* output, int output_height, int output_width) {\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float val = input[c * plane_size_in + h_in * input_width + w_in];\n","                for (int sh = 0; sh < scale_factor; sh++) { // Gấp đôi hàng\n","                    for (int sw = 0; sw < scale_factor; sw++) { // Gấp đôi cột\n","                        int h_out = h_in * scale_factor + sh;\n","                        int w_out = w_in * scale_factor + sw;\n","                        int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                        output[output_idx] = val;\n","                    }\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","float MSE(float* input, float* output, int size) {\n","    float sum = 0.0f;\n","    for (int i = 0; i < size; i++) {\n","        sum += (output[i] - input[i]) * (output[i] - input[i]);\n","    }\n","    return sum / size;\n","}\n","\n","void Relu_Backward(float* d_output, float* input,int N) {\n","    for (int i = 0; i < N; i++) {\n","        d_output[i] = input[i] > 0.0f ? d_output[i] : 0.0f;\n","    }\n","}\n","\n","void MSE_Gradient(float* input, float* output, int size, float* d_output) {\n","    float sum = 0.0f;\n","    float factor = 2.0f / size;\n","    for (int i = 0; i < size; i++) {\n","        d_output[i] = factor * (output[i] - input[i]);\n","    }\n","}\n","\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* d_input)\n","{\n","    // Chỉ gán vị trí giá trị max của input ban đầu là gradient của lớp tiếp theo (d_output), còn lại là 0\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) { // Khởi tạo gradient của input ban đầu là 0\n","        d_input[i] = 0.0f;\n","    }\n","\n","    for (int c = 0; c < filter_count; c++) {\n","\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","\n","                float max_val = -FLT_MAX;\n","                int max_input_idx = -1;\n","\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = channel_offset_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                            max_input_idx = input_idx;\n","                        }\n","                    }\n","                }\n","                //Lấy gradient từ output\n","                if (max_input_idx != -1) {\n","                    int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                    d_input[max_input_idx] += d_output[output_idx];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width) {\n","\n","    int plane_size_in = d_input_height * d_input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) {\n","        d_input[i] = 0.0f;\n","    }\n","    for (int c = 0; c < filter_count; c++) {\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < d_input_height; h_in++) {\n","            for (int w_in = 0; w_in < d_input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                int h_start_out = h_in * scale_factor;\n","                int w_start_out = w_in * scale_factor;\n","                for (int sh = 0; sh < scale_factor; sh++) {\n","                    for (int sw = 0; sw < scale_factor; sw++) {\n","                        int h_out = h_start_out + sh;\n","                        int w_out = w_start_out + sw;\n","                        if (h_out < d_output_height && w_out < d_output_width) {\n","                            int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                            sum_gradient += d_output[output_idx];\n","                        }\n","                    }\n","                }\n","                int input_idx = channel_offset_in + h_in * d_input_width + w_in;\n","                d_input[input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input) {\n","    // Thực hiện tích chập giữa dE/dO và kernel (xoay 180 độ) để tính dE/dI\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh input (kênh output gradient)\n","    for (int c_in = 0; c_in < input_channels; c_in++) {\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                // Lặp qua kênh output (số lượng filter)\n","                for (int c_out = 0; c_out < filter_count; c_out++) {\n","                    // Lặp qua kernel (xoay 180 độ)\n","                    for (int kh = 0; kh < kernel_height; kh++) {\n","                        for (int kw = 0; kw < kernel_width; kw++) {\n","                            int h_out = h_in - kh + padding;\n","                            int w_out = w_in - kw + padding;\n","                            float d_output_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_out >= 0 && h_out < d_output_height && w_out >= 0 && w_out < d_output_width) {\n","                                int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                                d_output_val = d_output[d_output_idx];\n","                            }\n","                            // Tính chỉ số kernel (xoay 180 độ)\n","                            int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            (kernel_height - 1 - kh) * kernel_width + (kernel_width - 1 - kw);\n","\n","                            sum_gradient += d_output_val * kernel[kernel_idx];\n","                        }\n","                    }\n","                }\n","                int d_input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                d_input[d_input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights) {\n","    // Thực hiện tích chập giữa dE/dO và input để tính dE/dW\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua kênh đầu vào\n","        for (int c_in = 0; c_in < input_channels; c_in++) {\n","            // Lặp qua kích thước kernel\n","            for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                    float sum_gradient = 0.0f;\n","                    // Lặp qua output grid (d_output) để tích lũy\n","                    for (int h_out = 0; h_out < d_output_height; h_out++) {\n","                        for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float input_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                                input_val = input[input_idx];\n","                            }\n","                            int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                            float d_output_val = d_output[d_output_idx];\n","                            // Tính gradient\n","                            sum_gradient += input_val * d_output_val;\n","                        }\n","                    }\n","                    int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                    k_h * kernel_width + k_w;\n","                    d_weights[kernel_idx] += sum_gradient;\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height,\n","    int filter_count, float* d_biases) {\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua từng filter\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        float sum_gradient = 0.0f;\n","        // Lặp qua từng vị trí trong output\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                // Tính chỉ số trong mảng d_output\n","                int output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                // Cộng dồn gradient từ d_output\n","                sum_gradient += d_output[output_idx];\n","            }\n","        }\n","        d_biases[c_out] += sum_gradient;\n","    }\n","}\n","\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params) {\n","    for (int i = 0; i < N_params; i++) {\n","        weights[i] -= (learning_rate * d_weights[i]);\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIJIDwOEX3zF","executionInfo":{"status":"ok","timestamp":1765305494496,"user_tz":-420,"elapsed":34,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"74a254b9-43ac-4d06-d114-471cff59696e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting cpu_layers.c\n"]}]},{"cell_type":"code","source":["%%writefile verify_gpu_cpu_layers.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","extern \"C\" {\n","    #include \"cpu_layers.h\"   // đã có sẵn\n","}\n","\n","#include \"gpu_layers.h\"       // chứa các kernel GPU + CHECK_CUDA\n","\n","// Hàm tiện ích so sánh 2 mảng\n","void compare_arrays(const char* name,\n","                    const float* a,\n","                    const float* b,\n","                    int n)\n","{\n","    double max_abs = 0.0;\n","    double sum_abs = 0.0;\n","    double sum_sq  = 0.0;\n","\n","    for (int i = 0; i < n; ++i) {\n","        double diff = (double)a[i] - (double)b[i];\n","        double ad   = fabs(diff);\n","\n","        if (ad > max_abs) max_abs = ad;\n","        sum_abs += ad;\n","        sum_sq  += diff * diff;\n","    }\n","\n","    double mean_abs = sum_abs / n;\n","    double rmse     = std::sqrt(sum_sq / n);\n","\n","    printf(\"[%s] max|diff| = %.6g, mean|diff| = %.6g, RMSE = %.6g\\n\",\n","           name, max_abs, mean_abs, rmse);\n","}\n","\n","/*-------------------------------------------------------------\n","  TEST CONV2D (forward + backward) – cái này bạn đang chạy OK,\n","  mình để ví dụ đơn giản N=1 cho gọn.\n","-------------------------------------------------------------*/\n","void test_conv2d()\n","{\n","    printf(\"==================== test_conv2d ====================\\n\");\n","\n","    int N      = 1;\n","    int C_in   = 3;\n","    int C_out  = 4;\n","    int H      = 8;\n","    int W      = 8;\n","    int K      = 3;\n","    int pad    = 1;\n","    int stride = 1;\n","\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int in_size   = N * C_in * H * W;\n","    int w_size    = C_out * C_in * K * K;\n","    int b_size    = C_out;\n","    int out_size  = N * C_out * H_out * W_out;\n","\n","    // Cấp phát host\n","    float *h_x     = (float*)malloc(in_size  * sizeof(float));\n","    float *h_w     = (float*)malloc(w_size   * sizeof(float));\n","    float *h_b     = (float*)malloc(b_size   * sizeof(float));\n","    float *h_y_cpu = (float*)malloc(out_size * sizeof(float));\n","    float *h_y_gpu = (float*)malloc(out_size * sizeof(float));\n","\n","    float *h_dy    = (float*)malloc(out_size * sizeof(float));\n","    float *h_dx_cpu= (float*)malloc(in_size  * sizeof(float));\n","    float *h_dx_gpu= (float*)malloc(in_size  * sizeof(float));\n","    float *h_dw_cpu= (float*)malloc(w_size   * sizeof(float));\n","    float *h_dw_gpu= (float*)malloc(w_size   * sizeof(float));\n","    float *h_db_cpu= (float*)malloc(b_size   * sizeof(float));\n","    float *h_db_gpu= (float*)malloc(b_size   * sizeof(float));\n","\n","    // Init dữ liệu determinisitc\n","    for (int i = 0; i < in_size; ++i)  h_x[i] = (float)((i % 17) - 8) / 10.0f;\n","    for (int i = 0; i < w_size; ++i)   h_w[i] = (float)((i % 13) - 6) / 7.0f;\n","    for (int i = 0; i < b_size; ++i)   h_b[i] = (float)((i % 5) - 2) / 3.0f;\n","    for (int i = 0; i < out_size; ++i) h_dy[i]= (float)((i % 11) - 5) / 9.0f;\n","\n","    // ===== CPU FORWARD =====\n","    Conv2D_Forward(\n","        h_x, W, H, C_in,\n","        h_w, K, K,\n","        h_b,\n","        pad, stride,\n","        C_out,\n","        h_y_cpu,\n","        H_out, W_out);\n","\n","    // ===== GPU FORWARD =====\n","    float *d_x=nullptr, *d_w=nullptr, *d_b=nullptr, *d_y=nullptr;\n","    CHECK_CUDA(cudaMalloc(&d_x, in_size  * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_w, w_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_b, b_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_y, out_size * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_x, h_x, in_size * sizeof(float), cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(d_w, h_w, w_size * sizeof(float), cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(d_b, h_b, b_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16,16);\n","    dim3 gridConv(\n","        (W_out + block2d.x - 1)/block2d.x,\n","        (H_out + block2d.y - 1)/block2d.y,\n","        N * C_out);\n","\n","    conv2d_forward_naive<<<gridConv, block2d>>>(\n","        d_x, d_w, d_b, d_y,\n","        N, C_in, H, W,\n","        C_out, K, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_y_gpu, d_y, out_size * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"Conv2D_Forward\", h_y_cpu, h_y_gpu, out_size);\n","\n","    // ===== CPU BACKWARD =====\n","    Conv2D_Backward_Input(\n","        h_dy, W_out, H_out,\n","        h_w, K, K,\n","        W, H, C_in,\n","        pad, stride, C_out,\n","        h_dx_cpu);\n","\n","    Conv2D_Backward_Kernel(\n","        h_dy, W_out, H_out,\n","        h_x, W, H, C_in,\n","        K, K,\n","        pad, stride, C_out,\n","        h_dw_cpu);\n","\n","    Conv2D_Backward_Biases(\n","        h_dy, W_out, H_out,\n","        C_out,\n","        h_db_cpu);\n","\n","    // ===== GPU BACKWARD =====\n","    float *d_dy=nullptr, *d_dx=nullptr, *d_dw=nullptr, *d_db=nullptr;\n","    CHECK_CUDA(cudaMalloc(&d_dy, out_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_dx, in_size  * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_dw, w_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_db, b_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_dy, h_dy, out_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    // dX\n","    dim3 gridIn(\n","        (W + block2d.x - 1)/block2d.x,\n","        (H + block2d.y - 1)/block2d.y,\n","        N * C_in);\n","    conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","        d_dy, d_w, d_dx,\n","        N, C_in, H, W,\n","        C_out, K, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // dW\n","    int num_w = w_size;\n","    int t = 256;\n","    int b = (num_w + t - 1)/t;\n","    conv2d_backward_weight_naive<<<b, t>>>(\n","        d_x, d_dy, d_dw,\n","        N, C_in, H, W,\n","        C_out, K, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // dB\n","    int tb = 256;\n","    int bb = (C_out + tb - 1)/tb;\n","    conv2d_backward_bias_naive<<<bb, tb>>>(\n","        d_dy, d_db,\n","        N, C_out, H_out, W_out);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_dx_gpu, d_dx, in_size  * sizeof(float), cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_dw_gpu, d_dw, w_size   * sizeof(float), cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_db_gpu, d_db, b_size   * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"Conv2D_Backward_Input (dX)\", h_dx_cpu, h_dx_gpu, in_size);\n","    compare_arrays(\"Conv2D_Backward_Kernel (dW)\", h_dw_cpu, h_dw_gpu, w_size);\n","    compare_arrays(\"Conv2D_Backward_Biases (dB)\", h_db_cpu, h_db_gpu, b_size);\n","\n","    // cleanup\n","    free(h_x); free(h_w); free(h_b); free(h_y_cpu); free(h_y_gpu);\n","    free(h_dy); free(h_dx_cpu); free(h_dx_gpu);\n","    free(h_dw_cpu); free(h_dw_gpu);\n","    free(h_db_cpu); free(h_db_gpu);\n","\n","    cudaFree(d_x); cudaFree(d_w); cudaFree(d_b); cudaFree(d_y);\n","    cudaFree(d_dy); cudaFree(d_dx); cudaFree(d_dw); cudaFree(d_db);\n","}\n","\n","/*-------------------------------------------------------------\n","  TEST MaxPool + UpSample (forward + backward)\n","  Quan trọng: truyền đúng H,W (H_input/H_pool), không dùng H_out nhầm.\n","-------------------------------------------------------------*/\n","void test_pool_upsample()\n","{\n","    printf(\"\\n==================== test_pool_upsample ====================\\n\");\n","\n","    int N = 1;\n","    int C = 3;\n","    int H = 8;\n","    int W = 8;\n","\n","    int pool_k     = 2;\n","    int pool_stride= 2;\n","    int H_pool     = H / pool_k;  // 4\n","    int W_pool     = W / pool_k;  // 4\n","\n","    int scale      = 2;\n","    int H_up       = H_pool * scale; // 8\n","    int W_up       = W_pool * scale; // 8\n","\n","    int in_size    = N * C * H      * W;\n","    int pool_size  = N * C * H_pool * W_pool;\n","    int up_size    = N * C * H_up   * W_up;\n","\n","    // Host buffers\n","    float *h_in              = (float*)malloc(in_size   * sizeof(float));\n","    float *h_pool_cpu        = (float*)malloc(pool_size * sizeof(float));\n","    float *h_pool_gpu        = (float*)malloc(pool_size * sizeof(float));\n","    float *h_up_cpu          = (float*)malloc(up_size   * sizeof(float));\n","    float *h_up_gpu          = (float*)malloc(up_size   * sizeof(float));\n","    float *h_pool_grad       = (float*)malloc(pool_size * sizeof(float));\n","    float *h_in_grad_cpu     = (float*)malloc(in_size   * sizeof(float));\n","    float *h_in_grad_gpu     = (float*)malloc(in_size   * sizeof(float));\n","    float *h_up_grad         = (float*)malloc(up_size   * sizeof(float));\n","    float *h_pool_grad2_cpu  = (float*)malloc(pool_size * sizeof(float));\n","    float *h_pool_grad2_gpu  = (float*)malloc(pool_size * sizeof(float));\n","\n","    // Init input\n","    for (int i = 0; i < in_size; ++i)\n","        h_in[i] = (float)((i % 13) - 6) / 7.0f;\n","\n","    // ===== CPU MaxPool Forward =====\n","    MaxPool2D_Forward(\n","        h_in,\n","        W, H,\n","        pool_k, pool_k,\n","        pool_stride,\n","        C,\n","        h_pool_cpu,\n","        H_pool, W_pool);\n","\n","    // ===== GPU MaxPool Forward =====\n","    float *d_in=nullptr, *d_pool=nullptr, *d_up=nullptr;\n","    float *d_pool_grad=nullptr, *d_in_grad=nullptr;\n","    float *d_up_grad=nullptr, *d_pool_grad2=nullptr;\n","\n","    CHECK_CUDA(cudaMalloc(&d_in,   in_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_pool, pool_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_up,   up_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_in, h_in, in_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16,16);\n","    dim3 gridPool(\n","        (W_pool + block2d.x - 1) / block2d.x,\n","        (H_pool + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    // GPU pool fwd: H,W là kích thước input (8x8)\n","    maxpool2x2_forward<<<gridPool, block2d>>>(\n","        d_in, d_pool,\n","        N, C, H, W);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_pool_gpu, d_pool, pool_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"MaxPool2D_Forward\", h_pool_cpu, h_pool_gpu, pool_size);\n","\n","    // ===== CPU MaxPool Backward =====\n","    for (int i = 0; i < pool_size; ++i)\n","        h_pool_grad[i] = (float)((i % 7) - 3) / 5.0f;\n","\n","    MaxPool2D_Backward(\n","        h_pool_grad, W_pool, H_pool,\n","        h_in,\n","        W, H,\n","        pool_k, pool_k, pool_stride,\n","        C,\n","        h_in_grad_cpu);\n","\n","    // ===== GPU MaxPool Backward =====\n","    CHECK_CUDA(cudaMalloc(&d_pool_grad, pool_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_in_grad,   in_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_pool_grad, h_pool_grad, pool_size * sizeof(float),\n","                          cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemset(d_in_grad, 0, in_size * sizeof(float)));\n","\n","    dim3 gridPoolB(\n","        (W_pool + block2d.x - 1) / block2d.x,\n","        (H_pool + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    maxpool2x2_backward<<<gridPoolB, block2d>>>(\n","        d_in, d_pool_grad, d_in_grad,\n","        N, C, H, W);   // H,W = kích thước INPUT (8x8)\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_in_grad_gpu, d_in_grad, in_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"MaxPool2D_Backward\", h_in_grad_cpu, h_in_grad_gpu, in_size);\n","\n","    // ===== CPU UpSample Forward (4x4 -> 8x8) =====\n","    UpSample2D_Forward(\n","        h_pool_cpu,\n","        W_pool, H_pool,\n","        scale,\n","        C,\n","        h_up_cpu,\n","        H_up, W_up);\n","\n","    // ===== GPU UpSample Forward =====\n","    dim3 gridUp(\n","        (W_up + block2d.x - 1) / block2d.x,\n","        (H_up + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    upsample2x2_forward<<<gridUp, block2d>>>(\n","        d_pool, d_up,\n","        N, C,\n","        H_pool, W_pool);   // H,W = kích thước input (4x4)\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_up_gpu, d_up, up_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"UpSample2D_Forward\", h_up_cpu, h_up_gpu, up_size);\n","\n","    // ===== CPU UpSample Backward =====\n","    for (int i = 0; i < up_size; ++i)\n","        h_up_grad[i] = (float)((i % 11) - 5) / 9.0f;\n","\n","    UpSample2D_Backward(\n","        h_up_grad,\n","        W_up, H_up,\n","        scale,\n","        C,\n","        h_pool_grad2_cpu,\n","        H_pool, W_pool);\n","\n","    // ===== GPU UpSample Backward =====\n","    CHECK_CUDA(cudaMalloc(&d_up_grad,    up_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_pool_grad2, pool_size * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_up_grad, h_up_grad, up_size * sizeof(float),\n","                          cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemset(d_pool_grad2, 0, pool_size * sizeof(float)));\n","\n","    dim3 gridUpB(\n","        (W_pool + block2d.x - 1) / block2d.x,\n","        (H_pool + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    upsample2x2_backward<<<gridUpB, block2d>>>(\n","        d_up_grad, d_pool_grad2,\n","        N, C,\n","        H_pool, W_pool);   // CHÚ Ý: H,W = kích thước INPUT (4x4), KHÔNG phải 8x8\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_pool_grad2_gpu, d_pool_grad2, pool_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"UpSample2D_Backward\", h_pool_grad2_cpu, h_pool_grad2_gpu, pool_size);\n","\n","    // cleanup\n","    free(h_in);\n","    free(h_pool_cpu); free(h_pool_gpu);\n","    free(h_up_cpu);   free(h_up_gpu);\n","    free(h_pool_grad);\n","    free(h_in_grad_cpu); free(h_in_grad_gpu);\n","    free(h_up_grad);\n","    free(h_pool_grad2_cpu); free(h_pool_grad2_gpu);\n","\n","    cudaFree(d_in);\n","    cudaFree(d_pool);\n","    cudaFree(d_up);\n","    cudaFree(d_pool_grad);\n","    cudaFree(d_in_grad);\n","    cudaFree(d_up_grad);\n","    cudaFree(d_pool_grad2);\n","}\n","\n","int main()\n","{\n","    test_conv2d();\n","    test_pool_upsample();\n","\n","    CHECK_CUDA(cudaDeviceReset());\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTDlgbMGYENP","executionInfo":{"status":"ok","timestamp":1765305494523,"user_tz":-420,"elapsed":27,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"c41cc7d2-ae18-48f1-c067-f42c1e43ed99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting verify_gpu_cpu_layers.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 \\\n","  verify_gpu_cpu_layers.cu gpu_layers.cu cpu_layers.c \\\n","  -o verify_kernels"],"metadata":{"id":"kpN1i48MYvqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./verify_kernels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMR3pgdUYxr0","executionInfo":{"status":"ok","timestamp":1765305497895,"user_tz":-420,"elapsed":292,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"b7f874f9-3ade-4511-e6fd-b9c563ab9b83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================== test_conv2d ====================\n","[Conv2D_Forward] max|diff| = 5.96046e-07, mean|diff| = 1.04599e-07, RMSE = 1.44526e-07\n","[Conv2D_Backward_Input (dX)] max|diff| = 2.38419e-07, mean|diff| = 5.21541e-08, RMSE = 8.97555e-08\n","[Conv2D_Backward_Kernel (dW)] max|diff| = 4.76837e-07, mean|diff| = 6.1864e-08, RMSE = 1.04859e-07\n","[Conv2D_Backward_Biases (dB)] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","\n","==================== test_pool_upsample ====================\n","[MaxPool2D_Forward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","[MaxPool2D_Backward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","[UpSample2D_Forward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","[UpSample2D_Backward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n"]}]},{"cell_type":"markdown","source":["## GPU Optimization Version 1:"],"metadata":{"id":"1JOZZ8czQLI5"}},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bK1dejSCQQUB","executionInfo":{"status":"ok","timestamp":1765305497895,"user_tz":-420,"elapsed":4,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"22888594-b13f-4a79-92b3-909044c37615"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting gpu_layers_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.cu\n","#include \"gpu_layers_opt1.h\"\n","\n","// --------------- Conv2D forward (optimization 1) ------------------\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // Load input tile into shared memory\n","        for (int i = ty; i < BLOCK_H; i += blockDim.y) {\n","            for (int j = tx; j < BLOCK_W; j += blockDim.x) {\n","                int h_in = row_start + i;\n","                int w_in = col_start + j;\n","\n","                if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                    size_t input_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                    smem[i][j] = input[input_idx];\n","                } else {\n","                    smem[i][j] = 0.0f;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","\n","        // Compute convolution\n","        for (int i = 0; i < K; ++i) {\n","            for (int j = 0; j < K; ++j) {\n","                size_t weight_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","                value += smem[ty + i][tx + j] * weight[weight_idx];\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    // Shared Memory chứa dY\n","    __shared__ float s_dY[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    // Tọa độ dX (output của kernel này)\n","    int h_out = blockIdx.y * TILE_H + ty;\n","    int w_out = blockIdx.x * TILE_W + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n = nc / C_in;\n","    //if (h_out >= H || w_out >= W || n >= N) return;\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // Tọa độ dY cần tải\n","        int h_global = h_out;\n","        int w_global = w_out;\n","\n","        // Tải main tile\n","        if (h_global >= 0 && h_global < H_out && w_global >= 0 && w_global < W_out) {\n","            s_dY[ty + pad][tx + pad] = dY[idx4(n, c_out, h_global, w_global, C_out, H_out, W_out)];\n","        } else {\n","            s_dY[ty + pad][tx + pad] = 0.0f;\n","        }\n","\n","        // Tải biên trên/dưới\n","        if (ty < pad) {\n","            // Biên trên\n","            int h_top = blockIdx.y * TILE_H + ty - pad;\n","            if (h_top >= 0 && h_top < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[ty][tx + pad] = dY[idx4(n, c_out, h_top, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty][tx + pad] = 0.0f;\n","            }\n","\n","            // Biên dưới\n","            int h_bottom = blockIdx.y * TILE_H + TILE_H + pad - 1 - ty;\n","            if (h_bottom >= 0 && h_bottom < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = dY[idx4(n, c_out, h_bottom, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","            }\n","        }\n","\n","        // Tải biên trái/phải\n","        if (tx < pad) {\n","            // Biên trái\n","            int w_left = blockIdx.x * TILE_W + tx - pad;\n","            if (w_left >= 0 && w_left < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][tx] = dY[idx4(n, c_out, h_global, w_left, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][tx] = 0.0f;\n","            }\n","\n","            // Biên phải\n","            int w_right = blockIdx.x * TILE_W + TILE_W + pad - 1 - tx;\n","            if (w_right >= 0 && w_right < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = dY[idx4(n, c_out, h_global, w_right, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","            }\n","        }\n","\n","        // Tải 4 góc (Thread (0,0) tải)\n","        if (tx == 0 && ty == 0) {\n","            // Góc trên trái [0][0]\n","            int h_c = blockIdx.y * TILE_H - pad;\n","            int w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[0][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc trên phải [0][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới trái [17][0]\n","            h_c = blockIdx.y * TILE_H + TILE_H + pad - 1;\n","            w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới phải [17][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","        }\n","\n","        __syncthreads();\n","\n","        // Tính convolution\n","            for (int kh = 0; kh < K; ++kh) {\n","                for (int kw = 0; kw < K; ++kw) {\n","                    int smem_y = ty + 2 * pad - kh;\n","                    int smem_x = tx + 2* pad - kw;\n","\n","                    size_t w_idx = idx4(c_out, c_in, K - 1 - kh, K - 1 - kw, C_in, K, K);\n","                    value += s_dY[smem_y][smem_x] * weight[w_idx];\n","                }\n","            }\n","        __syncthreads();\n","    }\n","\n","    if (h_out < H && w_out < W && n < N) {\n","        dX[idx4(n, c_in, h_out, w_out, C_in, H, W)] = value;\n","    }\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    float dw[K][K];\n","    for (int i = 0; i < K; ++i)\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // Load s_dY\n","                int h_out = block_h * TILE_H + ty;\n","                int w_out = block_w * TILE_W + tx;\n","\n","                if (h_out < H_out && w_out < W_out) {\n","                    s_dY[ty][tx] = dY[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)];\n","                } else {\n","                    s_dY[ty][tx] = 0.0f;\n","                }\n","\n","                // Load s_in\n","                int h_in_base = block_h * TILE_H + ty;\n","                int w_in_base = block_w * TILE_W + tx;\n","\n","                // Main tile\n","                if (h_in_base >= 0 && h_in_base < H && w_in_base >= 0 && w_in_base < W) {\n","                    s_in[ty + pad][tx + pad] = input[idx4(n, c_in, h_in_base, w_in_base, C_in, H, W)];\n","                } else {\n","                    s_in[ty + pad][tx + pad] = 0.0f;\n","                }\n","\n","                // Top/bottom borders\n","                if (ty < pad) {\n","                    int h_top = block_h * TILE_H - pad + ty;\n","                    if (h_top >= 0 && h_top < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[ty][tx + pad] = input[idx4(n, c_in, h_top, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[ty][tx + pad] = 0.0f;\n","                    }\n","\n","                    int h_bottom = block_h * TILE_H + TILE_H + pad - 1 - ty;\n","                    if (h_bottom >= 0 && h_bottom < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = input[idx4(n, c_in, h_bottom, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","                    }\n","                }\n","\n","                // Left/right borders\n","                if (tx < pad) {\n","                    int w_left = block_w * TILE_W - pad + tx;\n","                    if (w_left >= 0 && w_left < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][tx] = input[idx4(n, c_in, h_in_base, w_left, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][tx] = 0.0f;\n","                    }\n","\n","                    int w_right = block_w * TILE_W + TILE_W + pad - 1 - tx;\n","                    if (w_right >= 0 && w_right < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = input[idx4(n, c_in, h_in_base, w_right, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","                    }\n","                }\n","\n","                // Thread (0,0) loads 4 corners\n","                if (tx == 0 && ty == 0) {\n","                    int h_c = block_h * TILE_H - pad;\n","                    int w_c = block_w * TILE_W - pad;\n","                    s_in[0][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    h_c = block_h * TILE_H + TILE_H + pad - 1;\n","                    w_c = block_w * TILE_W - pad;\n","                    s_in[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","                }\n","\n","                __syncthreads();\n","\n","                // Compute dW: X * dY\n","                float val_dy = s_dY[ty][tx];\n","                for (int kh = 0; kh < K; ++kh) {\n","                    for (int kw = 0; kw < K; ++kw) {\n","                        dw[kh][kw] += s_in[ty + kh][tx + kw] * val_dy;\n","                    }\n","                }\n","\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    // Write accumulated gradients\n","    for (int i = 0; i < K; ++i) {\n","        for (int j = 0; j < K; ++j) {\n","            // Tính index global của dW[i][j] cho cặp filter (c_out, c_in)\n","            size_t dw_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","            // Cộng giá trị dw[i][j] của thread hiện tại vào bộ nhớ global\n","            atomicAdd(&dW[dw_idx], dw[i][j]);\n","        }\n","    }\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","    extern __shared__ float sdata[];\n","    int tid = threadIdx.x;\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","    float sum = 0.0f;\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","    sdata[tid] = sum;\n","    __syncthreads();\n","\n","    // Reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        dB[c] = sdata[0];\n","    }\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5HCVMv_QYQ0","executionInfo":{"status":"ok","timestamp":1765305497904,"user_tz":-420,"elapsed":10,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"b0bfd384-4b15-4a58-cb97-b1c727a71b53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting gpu_layers_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile verify_gpuOpt1_cpu_layers.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","extern \"C\" {\n","    #include \"cpu_layers.h\"   // đã có sẵn\n","}\n","\n","#include \"gpu_layers_opt1.h\"       // chứa các kernel GPU + CHECK_CUDA\n","\n","// Hàm tiện ích so sánh 2 mảng\n","void compare_arrays(const char* name,\n","                    const float* a,\n","                    const float* b,\n","                    int n)\n","{\n","    double max_abs = 0.0;\n","    double sum_abs = 0.0;\n","    double sum_sq  = 0.0;\n","\n","    for (int i = 0; i < n; ++i) {\n","        double diff = (double)a[i] - (double)b[i];\n","        double ad   = fabs(diff);\n","\n","        if (ad > max_abs) max_abs = ad;\n","        sum_abs += ad;\n","        sum_sq  += diff * diff;\n","    }\n","\n","    double mean_abs = sum_abs / n;\n","    double rmse     = std::sqrt(sum_sq / n);\n","\n","    printf(\"[%s] max|diff| = %.6g, mean|diff| = %.6g, RMSE = %.6g\\n\",\n","           name, max_abs, mean_abs, rmse);\n","}\n","\n","/*-------------------------------------------------------------\n","  TEST CONV2D (forward + backward)\n","-------------------------------------------------------------*/\n","void test_conv2d()\n","{\n","    printf(\"==================== test_conv2d ====================\\n\");\n","\n","    int N      = 1;\n","    int C_in   = 3;\n","    int C_out  = 4;\n","    int H      = 8;\n","    int W      = 8;\n","    //int K      = 3;\n","    int pad    = 1;\n","    int stride = 1;\n","\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int in_size   = N * C_in * H * W;\n","    int w_size    = C_out * C_in * K * K;\n","    int b_size    = C_out;\n","    int out_size  = N * C_out * H_out * W_out;\n","\n","    // Cấp phát host\n","    float *h_x     = (float*)malloc(in_size  * sizeof(float));\n","    float *h_w     = (float*)malloc(w_size   * sizeof(float));\n","    float *h_b     = (float*)malloc(b_size   * sizeof(float));\n","    float *h_y_cpu = (float*)malloc(out_size * sizeof(float));\n","    float *h_y_gpu = (float*)malloc(out_size * sizeof(float));\n","\n","    float *h_dy    = (float*)malloc(out_size * sizeof(float));\n","    float *h_dx_cpu= (float*)malloc(in_size  * sizeof(float));\n","    float *h_dx_gpu= (float*)malloc(in_size  * sizeof(float));\n","    float *h_dw_cpu= (float*)malloc(w_size   * sizeof(float));\n","    float *h_dw_gpu= (float*)malloc(w_size   * sizeof(float));\n","    float *h_db_cpu= (float*)malloc(b_size   * sizeof(float));\n","    float *h_db_gpu= (float*)malloc(b_size   * sizeof(float));\n","\n","    // Init dữ liệu determinisitc\n","    for (int i = 0; i < in_size; ++i)  h_x[i] = (float)((i % 17) - 8) / 10.0f;\n","    for (int i = 0; i < w_size; ++i)   h_w[i] = (float)((i % 13) - 6) / 7.0f;\n","    for (int i = 0; i < b_size; ++i)   h_b[i] = (float)((i % 5) - 2) / 3.0f;\n","    for (int i = 0; i < out_size; ++i) h_dy[i]= (float)((i % 11) - 5) / 9.0f;\n","\n","    // ===== CPU FORWARD =====\n","    Conv2D_Forward(\n","        h_x, W, H, C_in,\n","        h_w, K, K,\n","        h_b,\n","        pad, stride,\n","        C_out,\n","        h_y_cpu,\n","        H_out, W_out);\n","\n","    // ===== GPU FORWARD =====\n","    float *d_x=nullptr, *d_w=nullptr, *d_b=nullptr, *d_y=nullptr;\n","    CHECK_CUDA(cudaMalloc(&d_x, in_size  * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_w, w_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_b, b_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_y, out_size * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_x, h_x, in_size * sizeof(float), cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(d_w, h_w, w_size * sizeof(float), cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(d_b, h_b, b_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16,16);\n","    dim3 gridConv(\n","        (W_out + block2d.x - 1)/block2d.x,\n","        (H_out + block2d.y - 1)/block2d.y,\n","        N * C_out);\n","    update_dc_bias(d_b, C_out);\n","\n","    conv2d_forward<<<gridConv, block2d>>>(\n","        d_x, d_w, d_y,\n","        N, C_in, H, W,\n","        C_out, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_y_gpu, d_y, out_size * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"Conv2D_Forward\", h_y_cpu, h_y_gpu, out_size);\n","\n","    // ===== CPU BACKWARD =====\n","    Conv2D_Backward_Input(\n","        h_dy, W_out, H_out,\n","        h_w, K, K,\n","        W, H, C_in,\n","        pad, stride, C_out,\n","        h_dx_cpu);\n","\n","    Conv2D_Backward_Kernel(\n","        h_dy, W_out, H_out,\n","        h_x, W, H, C_in,\n","        K, K,\n","        pad, stride, C_out,\n","        h_dw_cpu);\n","\n","    Conv2D_Backward_Biases(\n","        h_dy, W_out, H_out,\n","        C_out,\n","        h_db_cpu);\n","\n","    // ===== GPU BACKWARD =====\n","    float *d_dy=nullptr, *d_dx=nullptr, *d_dw=nullptr, *d_db=nullptr;\n","    CHECK_CUDA(cudaMalloc(&d_dy, out_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_dx, in_size  * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_dw, w_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_db, b_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_dy, h_dy, out_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    // dX\n","    dim3 gridIn(\n","        (W + block2d.x - 1)/block2d.x,\n","        (H + block2d.y - 1)/block2d.y,\n","        N * C_in);\n","    conv2d_backward_input<<<gridIn, block2d>>>(\n","        d_dy, d_w, d_dx,\n","        N, C_in, H, W,\n","        C_out, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // dW\n","    dim3 blockW(16, 16);\n","    dim3 gridW(1, 1, C_out * C_in);\n","    conv2d_backward_weight<<<gridW, blockW>>>(\n","        d_x, d_dy, d_dw,\n","        N, C_in, H, W,\n","        C_out, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // dB\n","    dim3 blockB(256);\n","    dim3 gridB(C_out);\n","    size_t shmem_size = 256 * sizeof(float);\n","    conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(\n","        d_dy, d_db,\n","        N, C_out, H_out, W_out);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_dx_gpu, d_dx, in_size  * sizeof(float), cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_dw_gpu, d_dw, w_size   * sizeof(float), cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_db_gpu, d_db, b_size   * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"Conv2D_Backward_Input (dX)\", h_dx_cpu, h_dx_gpu, in_size);\n","    compare_arrays(\"Conv2D_Backward_Kernel (dW)\", h_dw_cpu, h_dw_gpu, w_size);\n","    compare_arrays(\"Conv2D_Backward_Biases (dB)\", h_db_cpu, h_db_gpu, b_size);\n","\n","    // cleanup\n","    free(h_x); free(h_w); free(h_b); free(h_y_cpu); free(h_y_gpu);\n","    free(h_dy); free(h_dx_cpu); free(h_dx_gpu);\n","    free(h_dw_cpu); free(h_dw_gpu);\n","    free(h_db_cpu); free(h_db_gpu);\n","\n","    cudaFree(d_x); cudaFree(d_w); cudaFree(d_b); cudaFree(d_y);\n","    cudaFree(d_dy); cudaFree(d_dx); cudaFree(d_dw); cudaFree(d_db);\n","}\n","\n","/*-------------------------------------------------------------\n","  TEST MaxPool + UpSample (forward + backward)\n","  Quan trọng: truyền đúng H,W (H_input/H_pool), không dùng H_out nhầm.\n","-------------------------------------------------------------*/\n","void test_pool_upsample()\n","{\n","    printf(\"\\n==================== test_pool_upsample ====================\\n\");\n","\n","    int N = 1;\n","    int C = 3;\n","    int H = 8;\n","    int W = 8;\n","\n","    int pool_k     = 2;\n","    int pool_stride= 2;\n","    int H_pool     = H / pool_k;  // 4\n","    int W_pool     = W / pool_k;  // 4\n","\n","    int scale      = 2;\n","    int H_up       = H_pool * scale; // 8\n","    int W_up       = W_pool * scale; // 8\n","\n","    int in_size    = N * C * H      * W;\n","    int pool_size  = N * C * H_pool * W_pool;\n","    int up_size    = N * C * H_up   * W_up;\n","\n","    // Host buffers\n","    float *h_in              = (float*)malloc(in_size   * sizeof(float));\n","    float *h_pool_cpu        = (float*)malloc(pool_size * sizeof(float));\n","    float *h_pool_gpu        = (float*)malloc(pool_size * sizeof(float));\n","    float *h_up_cpu          = (float*)malloc(up_size   * sizeof(float));\n","    float *h_up_gpu          = (float*)malloc(up_size   * sizeof(float));\n","    float *h_pool_grad       = (float*)malloc(pool_size * sizeof(float));\n","    float *h_in_grad_cpu     = (float*)malloc(in_size   * sizeof(float));\n","    float *h_in_grad_gpu     = (float*)malloc(in_size   * sizeof(float));\n","    float *h_up_grad         = (float*)malloc(up_size   * sizeof(float));\n","    float *h_pool_grad2_cpu  = (float*)malloc(pool_size * sizeof(float));\n","    float *h_pool_grad2_gpu  = (float*)malloc(pool_size * sizeof(float));\n","\n","    // Init input\n","    for (int i = 0; i < in_size; ++i)\n","        h_in[i] = (float)((i % 13) - 6) / 7.0f;\n","\n","    // ===== CPU MaxPool Forward =====\n","    MaxPool2D_Forward(\n","        h_in,\n","        W, H,\n","        pool_k, pool_k,\n","        pool_stride,\n","        C,\n","        h_pool_cpu,\n","        H_pool, W_pool);\n","\n","    // ===== GPU MaxPool Forward =====\n","    float *d_in=nullptr, *d_pool=nullptr, *d_up=nullptr;\n","    float *d_pool_grad=nullptr, *d_in_grad=nullptr;\n","    float *d_up_grad=nullptr, *d_pool_grad2=nullptr;\n","\n","    CHECK_CUDA(cudaMalloc(&d_in,   in_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_pool, pool_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_up,   up_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_in, h_in, in_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16,16);\n","    dim3 gridPool(\n","        (W_pool + block2d.x - 1) / block2d.x,\n","        (H_pool + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    // GPU pool fwd: H,W là kích thước input (8x8)\n","    maxpool2x2_forward<<<gridPool, block2d>>>(\n","        d_in, d_pool,\n","        N, C, H, W);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_pool_gpu, d_pool, pool_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"MaxPool2D_Forward\", h_pool_cpu, h_pool_gpu, pool_size);\n","\n","    // ===== CPU MaxPool Backward =====\n","    for (int i = 0; i < pool_size; ++i)\n","        h_pool_grad[i] = (float)((i % 7) - 3) / 5.0f;\n","\n","    MaxPool2D_Backward(\n","        h_pool_grad, W_pool, H_pool,\n","        h_in,\n","        W, H,\n","        pool_k, pool_k, pool_stride,\n","        C,\n","        h_in_grad_cpu);\n","\n","    // ===== GPU MaxPool Backward =====\n","    CHECK_CUDA(cudaMalloc(&d_pool_grad, pool_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_in_grad,   in_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_pool_grad, h_pool_grad, pool_size * sizeof(float),\n","                          cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemset(d_in_grad, 0, in_size * sizeof(float)));\n","\n","    dim3 gridPoolB(\n","        (W_pool + block2d.x - 1) / block2d.x,\n","        (H_pool + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    maxpool2x2_backward<<<gridPoolB, block2d>>>(\n","        d_in, d_pool_grad, d_in_grad,\n","        N, C, H, W);   // H,W = kích thước INPUT (8x8)\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_in_grad_gpu, d_in_grad, in_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"MaxPool2D_Backward\", h_in_grad_cpu, h_in_grad_gpu, in_size);\n","\n","    // ===== CPU UpSample Forward (4x4 -> 8x8) =====\n","    UpSample2D_Forward(\n","        h_pool_cpu,\n","        W_pool, H_pool,\n","        scale,\n","        C,\n","        h_up_cpu,\n","        H_up, W_up);\n","\n","    // ===== GPU UpSample Forward =====\n","    dim3 gridUp(\n","        (W_up + block2d.x - 1) / block2d.x,\n","        (H_up + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    upsample2x2_forward<<<gridUp, block2d>>>(\n","        d_pool, d_up,\n","        N, C,\n","        H_pool, W_pool);   // H,W = kích thước input (4x4)\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_up_gpu, d_up, up_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"UpSample2D_Forward\", h_up_cpu, h_up_gpu, up_size);\n","\n","    // ===== CPU UpSample Backward =====\n","    for (int i = 0; i < up_size; ++i)\n","        h_up_grad[i] = (float)((i % 11) - 5) / 9.0f;\n","\n","    UpSample2D_Backward(\n","        h_up_grad,\n","        W_up, H_up,\n","        scale,\n","        C,\n","        h_pool_grad2_cpu,\n","        H_pool, W_pool);\n","\n","    // ===== GPU UpSample Backward =====\n","    CHECK_CUDA(cudaMalloc(&d_up_grad,    up_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_pool_grad2, pool_size * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_up_grad, h_up_grad, up_size * sizeof(float),\n","                          cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemset(d_pool_grad2, 0, pool_size * sizeof(float)));\n","\n","    dim3 gridUpB(\n","        (W_pool + block2d.x - 1) / block2d.x,\n","        (H_pool + block2d.y - 1) / block2d.y,\n","        N * C);\n","\n","    upsample2x2_backward<<<gridUpB, block2d>>>(\n","        d_up_grad, d_pool_grad2,\n","        N, C,\n","        H_pool, W_pool);   // CHÚ Ý: H,W = kích thước INPUT (4x4), KHÔNG phải 8x8\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    CHECK_CUDA(cudaMemcpy(h_pool_grad2_gpu, d_pool_grad2, pool_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","\n","    compare_arrays(\"UpSample2D_Backward\", h_pool_grad2_cpu, h_pool_grad2_gpu, pool_size);\n","\n","    // cleanup\n","    free(h_in);\n","    free(h_pool_cpu); free(h_pool_gpu);\n","    free(h_up_cpu);   free(h_up_gpu);\n","    free(h_pool_grad);\n","    free(h_in_grad_cpu); free(h_in_grad_gpu);\n","    free(h_up_grad);\n","    free(h_pool_grad2_cpu); free(h_pool_grad2_gpu);\n","\n","    cudaFree(d_in);\n","    cudaFree(d_pool);\n","    cudaFree(d_up);\n","    cudaFree(d_pool_grad);\n","    cudaFree(d_in_grad);\n","    cudaFree(d_up_grad);\n","    cudaFree(d_pool_grad2);\n","}\n","\n","int main()\n","{\n","    test_conv2d();\n","    test_pool_upsample();\n","\n","    CHECK_CUDA(cudaDeviceReset());\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOuztEAEKJbm","executionInfo":{"status":"ok","timestamp":1765306083253,"user_tz":-420,"elapsed":5,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"421eddcc-f553-4f44-a242-96d059ef41c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting verify_gpuOpt1_cpu_layers.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 \\\n","  verify_gpuOpt1_cpu_layers.cu gpu_layers_opt1.cu cpu_layers.c \\\n","  -o verify_kernels"],"metadata":{"id":"q_qecbPSQxX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./verify_kernels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGKPpt9cQ_28","executionInfo":{"status":"ok","timestamp":1765306087115,"user_tz":-420,"elapsed":376,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"9129bf73-32d5-4ead-875d-878ee8b56f3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================== test_conv2d ====================\n","[Conv2D_Forward] max|diff| = 2.38419e-07, mean|diff| = 6.0659e-08, RMSE = 1.02412e-07\n","[Conv2D_Backward_Input (dX)] max|diff| = 2.38419e-07, mean|diff| = 5.21541e-08, RMSE = 8.97555e-08\n","[Conv2D_Backward_Kernel (dW)] max|diff| = 5.96046e-07, mean|diff| = 1.45562e-07, RMSE = 1.94332e-07\n","[Conv2D_Backward_Biases (dB)] max|diff| = 1.19209e-07, mean|diff| = 4.09782e-08, RMSE = 6.18892e-08\n","\n","==================== test_pool_upsample ====================\n","[MaxPool2D_Forward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","[MaxPool2D_Backward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","[UpSample2D_Forward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n","[UpSample2D_Backward] max|diff| = 0, mean|diff| = 0, RMSE = 0\n"]}]},{"cell_type":"code","source":["%%writefile verify_gpuOpt1_cpu_layers.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cmath>\n","#include <cstring>\n","#include <cuda_runtime.h>\n","\n","extern \"C\" {\n","    #include \"cpu_layers.h\"\n","}\n","\n","#include \"gpu_layers_opt1.h\"\n","\n","// Helper: In mảng\n","void print_array(const char* name, const float* arr, int n) {\n","    printf(\"%s: \", name);\n","    for (int i = 0; i < n; ++i) {\n","        printf(\"%.4f \", arr[i]);\n","    }\n","    printf(\"\\n\");\n","}\n","\n","// Helper: So sánh\n","void compare_arrays(const char* name, const float* a, const float* b, int n) {\n","    double max_abs = 0.0;\n","    double sum_sq  = 0.0;\n","    for (int i = 0; i < n; ++i) {\n","        double diff = (double)a[i] - (double)b[i];\n","        double ad   = fabs(diff);\n","        if (ad > max_abs) max_abs = ad;\n","        sum_sq  += diff * diff;\n","    }\n","    printf(\"[%s] max|diff| = %.6g, RMSE = %.6g\\n\", name, max_abs, std::sqrt(sum_sq / n));\n","}\n","\n","void test_conv2d()\n","{\n","    printf(\"==================== test_conv2d (PYTORCH DATA) ====================\\n\");\n","\n","    // Config khớp với dữ liệu bạn đưa:\n","    // Input: [1, 3, 2, 2] -> N=1, C_in=3, H=2, W=2\n","    // Weight: [1, 3, 3, 3] -> C_out=1, K=3\n","    int N = 1;\n","    int C_in = 3;\n","    int C_out = 1;\n","    int H = 2;\n","    int W = 2;\n","    int pad = 1;\n","    int stride = 1;\n","\n","    int H_out = (H + 2 * pad - K) / stride + 1; // (2+2-3)/1 + 1 = 2\n","    int W_out = (W + 2 * pad - K) / stride + 1; // 2\n","\n","    int in_size   = N * C_in * H * W;\n","    int w_size    = C_out * C_in * K * K;\n","    int b_size    = C_out;\n","    int out_size  = N * C_out * H_out * W_out;\n","\n","    // --- Alloc ---\n","    float *h_x     = (float*)malloc(in_size  * sizeof(float));\n","    float *h_w     = (float*)malloc(w_size   * sizeof(float));\n","    float *h_b     = (float*)malloc(b_size   * sizeof(float));\n","    float *h_dy    = (float*)malloc(out_size * sizeof(float));\n","\n","    float *h_y_cpu = (float*)malloc(out_size * sizeof(float));\n","    float *h_y_gpu = (float*)malloc(out_size * sizeof(float));\n","    float *h_dx_cpu= (float*)malloc(in_size  * sizeof(float));\n","    float *h_dx_gpu= (float*)malloc(in_size  * sizeof(float));\n","    float *h_dw_cpu= (float*)malloc(w_size   * sizeof(float));\n","    float *h_dw_gpu= (float*)malloc(w_size   * sizeof(float));\n","    float *h_db_cpu= (float*)malloc(b_size   * sizeof(float));\n","    float *h_db_gpu= (float*)malloc(b_size   * sizeof(float));\n","\n","    // --- Init Data (SỬA LỖI CÚ PHÁP Ở ĐÂY) ---\n","    printf(\"\\n--- INPUT DATA ---\\n\");\n","\n","    // Input Tensor [1, 3, 2, 2]\n","    float raw_x[] = {\n","         0.5716f,  0.5095f, -0.4530f, -0.6242f, // Channel 0\n","         0.6942f, -1.3731f, -0.5511f, -1.0356f, // Channel 1\n","         0.4376f, -0.2553f, -0.3711f,  1.1056f  // Channel 2\n","    };\n","    memcpy(h_x, raw_x, sizeof(raw_x));\n","    print_array(\"h_x (Input)\", h_x, in_size);\n","\n","    // Weight Tensor [1, 3, 3, 3]\n","    float raw_w[] = {\n","        // Channel 0 (3x3)\n","        -1.9601f,  1.7431f,  1.9581f,\n","         0.8421f,  1.3120f, -0.4370f,\n","        -0.0117f,  0.5837f,  0.3514f,\n","        // Channel 1 (3x3)\n","        -0.8800f,  0.2600f, -0.7063f,\n","        -0.3977f,  1.5940f, -1.0829f,\n","        -0.0248f, -1.3013f,  1.1109f,\n","        // Channel 2 (3x3)\n","         0.7878f, -0.0066f, -0.5166f,\n","        -1.7315f,  1.4600f, -0.3321f,\n","        -1.0723f,  0.5769f,  1.2464f\n","    };\n","    memcpy(h_w, raw_w, sizeof(raw_w));\n","    print_array(\"h_w (Weights)\", h_w, w_size);\n","\n","    // Bias [1]\n","    float raw_b[] = {-1.9176f};\n","    memcpy(h_b, raw_b, sizeof(raw_b));\n","    print_array(\"h_b (Bias)\", h_b, b_size);\n","\n","    // dY (Grad Output) - Giả lập\n","    for (int i = 0; i < out_size; ++i) h_dy[i]= (float)((i % 11) - 5) / 9.0f;\n","    print_array(\"h_dy (Grad Output)\", h_dy, out_size);\n","\n","    // ================= CPU FORWARD =================\n","    Conv2D_Forward(h_x, W, H, C_in, h_w, K, K, h_b, pad, stride, C_out, h_y_cpu, H_out, W_out);\n","\n","    // ================= GPU FORWARD =================\n","    float *d_x, *d_w, *d_b, *d_y;\n","    CHECK_CUDA(cudaMalloc(&d_x, in_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_w, w_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_b, b_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_y, out_size * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_x, h_x, in_size * sizeof(float), cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(d_w, h_w, w_size * sizeof(float), cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(d_b, h_b, b_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16,16);\n","    dim3 gridConv(\n","        (W_out + block2d.x - 1)/block2d.x,\n","        (H_out + block2d.y - 1)/block2d.y,\n","        N * C_out);\n","\n","    // FIX: Dùng hàm helper update_dc_bias đã khai báo\n","    update_dc_bias(d_b, C_out);\n","\n","    conv2d_forward<<<gridConv, block2d>>>(d_x, d_w, d_y, N, C_in, H, W, C_out, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","    CHECK_CUDA(cudaMemcpy(h_y_gpu, d_y, out_size * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","    printf(\"\\n--- FORWARD RESULTS ---\\n\");\n","    print_array(\"h_y_cpu\", h_y_cpu, out_size);\n","    print_array(\"h_y_gpu\", h_y_gpu, out_size);\n","    compare_arrays(\"Conv2D_Forward\", h_y_cpu, h_y_gpu, out_size);\n","\n","    // ================= CPU BACKWARD =================\n","    Conv2D_Backward_Input(h_dy, W_out, H_out, h_w, K, K, W, H, C_in, pad, stride, C_out, h_dx_cpu);\n","\n","    memset(h_dw_cpu, 0, w_size * sizeof(float));\n","    Conv2D_Backward_Kernel(h_dy, W_out, H_out, h_x, W, H, C_in, K, K, pad, stride, C_out, h_dw_cpu);\n","\n","    memset(h_db_cpu, 0, b_size * sizeof(float));\n","    Conv2D_Backward_Biases(h_dy, W_out, H_out, C_out, h_db_cpu);\n","\n","    // ================= GPU BACKWARD =================\n","    float *d_dy, *d_dx, *d_dw, *d_db;\n","    CHECK_CUDA(cudaMalloc(&d_dy, out_size * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_dx, in_size  * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_dw, w_size   * sizeof(float)));\n","    CHECK_CUDA(cudaMalloc(&d_db, b_size   * sizeof(float)));\n","\n","    CHECK_CUDA(cudaMemcpy(d_dy, h_dy, out_size * sizeof(float), cudaMemcpyHostToDevice));\n","\n","    // FIX: Memset GPU buffers về 0\n","    CHECK_CUDA(cudaMemset(d_dx, 0, in_size * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(d_dw, 0, w_size * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(d_db, 0, b_size * sizeof(float)));\n","\n","    // dX\n","    dim3 gridIn((W + block2d.x - 1)/block2d.x, (H + block2d.y - 1)/block2d.y, N * C_in);\n","    conv2d_backward_input<<<gridIn, block2d>>>(d_dy, d_w, d_dx, N, C_in, H, W, C_out, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // dW\n","    dim3 blockW(16, 16);\n","    dim3 gridW(1, 1, C_out * C_in);\n","    conv2d_backward_weight<<<gridW, blockW>>>(d_x, d_dy, d_dw, N, C_in, H, W, C_out, pad, stride);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // dB\n","    dim3 blockB(256);\n","    dim3 gridB(C_out);\n","    conv2d_backward_bias<<<gridB, blockB, 256*sizeof(float)>>>(d_dy, d_db, N, C_out, H_out, W_out);\n","    CHECK_CUDA(cudaDeviceSynchronize());\n","\n","    // Readback\n","    CHECK_CUDA(cudaMemcpy(h_dx_gpu, d_dx, in_size  * sizeof(float), cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_dw_gpu, d_dw, w_size   * sizeof(float), cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_db_gpu, d_db, b_size   * sizeof(float), cudaMemcpyDeviceToHost));\n","\n","    printf(\"\\n--- BACKWARD RESULTS (dX) ---\\n\");\n","    print_array(\"h_dx_cpu\", h_dx_cpu, in_size);\n","    print_array(\"h_dx_gpu\", h_dx_gpu, in_size);\n","    compare_arrays(\"Conv2D_Backward_Input (dX)\", h_dx_cpu, h_dx_gpu, in_size);\n","\n","    printf(\"\\n--- BACKWARD RESULTS (dW) ---\\n\");\n","    print_array(\"h_dw_cpu\", h_dw_cpu, w_size);\n","    print_array(\"h_dw_gpu\", h_dw_gpu, w_size);\n","    compare_arrays(\"Conv2D_Backward_Kernel (dW)\", h_dw_cpu, h_dw_gpu, w_size);\n","\n","    printf(\"\\n--- BACKWARD RESULTS (dB) ---\\n\");\n","    print_array(\"h_db_cpu\", h_db_cpu, b_size);\n","    print_array(\"h_db_gpu\", h_db_gpu, b_size);\n","    compare_arrays(\"Conv2D_Backward_Biases (dB)\", h_db_cpu, h_db_gpu, b_size);\n","\n","    // Free\n","    free(h_x); free(h_w); free(h_b); free(h_y_cpu); free(h_y_gpu);\n","    free(h_dy); free(h_dx_cpu); free(h_dx_gpu); free(h_dw_cpu); free(h_dw_gpu); free(h_db_cpu); free(h_db_gpu);\n","    cudaFree(d_x); cudaFree(d_w); cudaFree(d_b); cudaFree(d_y); cudaFree(d_dy); cudaFree(d_dx); cudaFree(d_dw); cudaFree(d_db);\n","}\n","\n","void test_pool_upsample() {\n","    printf(\"[SKIP] test_pool_upsample (Already Passed)\\n\");\n","}\n","\n","int main() {\n","    test_conv2d();\n","    // test_pool_upsample();\n","    CHECK_CUDA(cudaDeviceReset());\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CaYA3B2TUDSU","executionInfo":{"status":"ok","timestamp":1765305497911,"user_tz":-420,"elapsed":2,"user":{"displayName":"Phú Nguyễn Ngọc (PhúCombank)","userId":"16011295277664454023"}},"outputId":"4db07e7d-1192-4b75-f41a-adc3c3d2d2a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting verify_gpuOpt1_cpu_layers.cu\n"]}]}]}