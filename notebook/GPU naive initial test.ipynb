{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytty6f1vtGFZ",
        "outputId": "86e2aae8-faf2-4161-ab65-c5d56fbf3c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile load_data.h\n",
        "#pragma once\n",
        "#include <stdint.h>\n",
        "#include <stddef.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#define TRAIN_NUM    50000\n",
        "#define TEST_NUM     10000\n",
        "#define IMG_SIZE     (32*32*3)     // 3072\n",
        "\n",
        "typedef struct {\n",
        "    float*   train_images;   // [50000 * 3072]\n",
        "    float*   test_images;    // [10000 * 3072]\n",
        "    uint8_t* train_labels;   // [50000]\n",
        "    uint8_t* test_labels;    // [10000]\n",
        "    int*     train_indices;\n",
        "} Cifar10;\n",
        "\n",
        "#ifdef __cplusplus\n",
        "extern \"C\" {\n",
        "#endif\n",
        "\n",
        "void load_cifar10(Cifar10* data, const char* data_dir);\n",
        "void normalize_cifar10(Cifar10* data);\n",
        "void shuffle_cifar10(Cifar10* data);\n",
        "void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images);\n",
        "void free_cifar10(Cifar10* data);\n",
        "\n",
        "#ifdef __cplusplus\n",
        "}\n",
        "#endif\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXyfPUrjy2Ba",
        "outputId": "8694a386-5a8f-4b38-a69e-290ee3afdc81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing load_data.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile load_data.cu\n",
        "#include \"load_data.h\"\n",
        "\n",
        "static void read_batch(const char* filename, float* images_start, uint8_t* labels) {\n",
        "    FILE* f = fopen(filename, \"rb\");\n",
        "    if (!f) {\n",
        "        perror(filename);\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    uint8_t buffer[3073];\n",
        "    for (int i = 0; i < 10000; i++) {\n",
        "        if (fread(buffer, 1, 3073, f) != 3073) {\n",
        "            fprintf(stderr, \"Error: incomplete read in %s at image %d\\n\", filename, i);\n",
        "            fclose(f);\n",
        "            exit(EXIT_FAILURE);\n",
        "        }\n",
        "        labels[i] = buffer[0];\n",
        "        for (int j = 0; j < 3072; j++) {\n",
        "            images_start[i * 3072 + j] = (float)buffer[1 + j];  // uint8 -> float\n",
        "        }\n",
        "    }\n",
        "    fclose(f);\n",
        "}\n",
        "\n",
        "// must match the prototype in load_data.h\n",
        "void load_cifar10(Cifar10* data, const char* data_dir) {\n",
        "    data->train_images = (float*)malloc(TRAIN_NUM * IMG_SIZE * sizeof(float));\n",
        "    data->test_images  = (float*)malloc(TEST_NUM  * IMG_SIZE * sizeof(float));\n",
        "    data->train_labels = (uint8_t*)malloc(TRAIN_NUM * sizeof(uint8_t));\n",
        "    data->test_labels  = (uint8_t*)malloc(TEST_NUM  * sizeof(uint8_t));\n",
        "\n",
        "    if (!data->train_images || !data->test_images ||\n",
        "        !data->train_labels  || !data->test_labels) {\n",
        "        fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    data->train_indices = (int*)malloc(TRAIN_NUM * sizeof(int));\n",
        "    for (int i = 0; i < TRAIN_NUM; i++) {\n",
        "        data->train_indices[i] = i;\n",
        "    }\n",
        "\n",
        "    // Load training data: data_batch_1.bin ... data_batch_5.bin\n",
        "    for (int i = 1; i <= 5; i++) {\n",
        "        char filename[512];\n",
        "        snprintf(filename, sizeof(filename), \"%s/data_batch_%d.bin\", data_dir, i);\n",
        "        read_batch(filename,\n",
        "                   data->train_images + (i-1) * 10000 * IMG_SIZE,\n",
        "                   data->train_labels + (i-1) * 10000);\n",
        "    }\n",
        "\n",
        "    // Load test data: test_batch.bin\n",
        "    char test_file[512];\n",
        "    snprintf(test_file, sizeof(test_file), \"%s/test_batch.bin\", data_dir);\n",
        "    read_batch(test_file,\n",
        "               data->test_images, data->test_labels);\n",
        "\n",
        "    printf(\"CIFAR-10 loaded successfully from %s\\n\", data_dir);\n",
        "}\n",
        "\n",
        "void normalize_cifar10(Cifar10* data) {\n",
        "    for (size_t i = 0; i < (size_t)TRAIN_NUM * IMG_SIZE; i++) {\n",
        "        data->train_images[i] /= 255.0f;\n",
        "    }\n",
        "    for (size_t i = 0; i < (size_t)TEST_NUM * IMG_SIZE; i++) {\n",
        "        data->test_images[i] /= 255.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "void shuffle_cifar10(Cifar10* data) {\n",
        "    for (int i = TRAIN_NUM - 1; i > 0; i--) {\n",
        "        int j = rand() % (i + 1);\n",
        "        int temp = data->train_indices[i];\n",
        "        data->train_indices[i] = data->train_indices[j];\n",
        "        data->train_indices[j] = temp;\n",
        "    }\n",
        "}\n",
        "\n",
        "void get_next_batch(Cifar10* data, size_t batch_size, size_t batch_id, float* batch_images) {\n",
        "    size_t start = batch_id * batch_size;\n",
        "    for (size_t i = 0; i < batch_size; i++) {\n",
        "        int idx = data->train_indices[start + i];\n",
        "        memcpy(batch_images + i * IMG_SIZE,\n",
        "               data->train_images + idx * IMG_SIZE,\n",
        "               IMG_SIZE * sizeof(float));\n",
        "    }\n",
        "}\n",
        "\n",
        "void free_cifar10(Cifar10* data) {\n",
        "    free(data->train_images);\n",
        "    free(data->test_images);\n",
        "    free(data->train_labels);\n",
        "    free(data->test_labels);\n",
        "    free(data->train_indices);\n",
        "\n",
        "    data->train_images = data->test_images = NULL;\n",
        "    data->train_labels = data->test_labels = NULL;\n",
        "    data->train_indices = NULL;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2MX1EKbwQeB",
        "outputId": "53c5004c-3df8-439d-ac49-56802848f620"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing load_data.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_layers.h\n",
        "#pragma once\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define CHECK_CUDA(call)                                                \\\n",
        "    do {                                                                \\\n",
        "        cudaError_t err = call;                                         \\\n",
        "        if (err != cudaSuccess) {                                       \\\n",
        "            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n",
        "                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n",
        "            exit(EXIT_FAILURE);                                         \\\n",
        "        }                                                               \\\n",
        "    } while (0)\n",
        "\n",
        "// NCHW layout: [N, C, H, W]\n",
        "__device__ __host__ inline int idx4(int n, int c, int h, int w,\n",
        "                                    int C, int H, int W) {\n",
        "    return ((n * C + c) * H + h) * W + w;\n",
        "}\n",
        "\n",
        "// ==== KERNEL DECLARATIONS ====\n",
        "\n",
        "__global__ void conv2d_forward_naive(\n",
        "    const float* __restrict__ input,    // [N, C_in, H, W]\n",
        "    const float* __restrict__ weight,   // [C_out, C_in, K, K]\n",
        "    const float* __restrict__ bias,     // [C_out]\n",
        "    float* __restrict__ output,         // [N, C_out, H_out, W_out]\n",
        "    int N, int C_in, int H, int W,\n",
        "    int C_out, int K, int pad, int stride);\n",
        "\n",
        "__global__ void relu_forward(float* x, int size);\n",
        "\n",
        "__global__ void maxpool2x2_forward(\n",
        "    const float* __restrict__ input,  // [N, C, H, W]\n",
        "    float* __restrict__ output,       // [N, C, H/2, W/2]\n",
        "    int N, int C, int H, int W);\n",
        "\n",
        "__global__ void upsample2x2_forward(\n",
        "    const float* __restrict__ input,  // [N, C, H, W]\n",
        "    float* __restrict__ output,       // [N, C, 2H, 2W]\n",
        "    int N, int C, int H, int W);\n",
        "\n",
        "__global__ void mse_loss_forward(\n",
        "    const float* __restrict__ output,\n",
        "    const float* __restrict__ target,\n",
        "    float* __restrict__ loss,   // single float on device\n",
        "    int size);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2PQ5CXKyIAb",
        "outputId": "3700fe6b-3ab9-46c6-f7d7-6e4ede6244b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu_layers.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_layers.cu\n",
        "// your kernels\n",
        "#include \"gpu_layers.h\"\n",
        "\n",
        "// --------------- Conv2D forward (naive) ------------------\n",
        "// each thread computes ONE output pixel (n, c_out, h_out, w_out)\n",
        "__global__ void conv2d_forward_naive(\n",
        "    const float* __restrict__ input,\n",
        "    const float* __restrict__ weight,\n",
        "    const float* __restrict__ bias,\n",
        "    float* __restrict__ output,\n",
        "    int N, int C_in, int H, int W,\n",
        "    int C_out, int K, int pad, int stride)\n",
        "{\n",
        "    int H_out = (H + 2 * pad - K) / stride + 1;\n",
        "    int W_out = (W + 2 * pad - K) / stride + 1;\n",
        "\n",
        "    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int nc    = blockIdx.z;  // pack (n, c_out) into z-dimension\n",
        "\n",
        "    if (w_out >= W_out || h_out >= H_out) return;\n",
        "\n",
        "    int n      = nc / C_out;\n",
        "    int c_out  = nc % C_out;\n",
        "    if (n >= N) return;\n",
        "\n",
        "    float sum = bias ? bias[c_out] : 0.0f;\n",
        "\n",
        "    for (int c_in = 0; c_in < C_in; ++c_in) {\n",
        "        for (int kh = 0; kh < K; ++kh) {\n",
        "            for (int kw = 0; kw < K; ++kw) {\n",
        "                int h_in = h_out * stride + kh - pad;\n",
        "                int w_in = w_out * stride + kw - pad;\n",
        "                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n",
        "                    continue;\n",
        "\n",
        "                int in_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n",
        "\n",
        "                int w_idx = (((c_out * C_in + c_in) * K) + kh) * K + kw;\n",
        "                sum += weight[w_idx] * input[in_idx];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int out_idx = idx4(n, c_out, h_out, w_out, C_out, H_out, W_out);\n",
        "    output[out_idx] = sum;\n",
        "}\n",
        "\n",
        "// --------------- ReLU ------------------\n",
        "__global__ void relu_forward(float* x, int size)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        float v = x[i];\n",
        "        x[i] = v > 0.0f ? v : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// --------------- MaxPool 2x2 (stride 2) ------------------\n",
        "__global__ void maxpool2x2_forward(\n",
        "    const float* __restrict__ input,\n",
        "    float* __restrict__ output,\n",
        "    int N, int C, int H, int W)\n",
        "{\n",
        "    int H_out = H / 2;\n",
        "    int W_out = W / 2;\n",
        "\n",
        "    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int nc    = blockIdx.z;\n",
        "\n",
        "    if (w_out >= W_out || h_out >= H_out) return;\n",
        "\n",
        "    int n = nc / C;\n",
        "    int c = nc % C;\n",
        "    if (n >= N) return;\n",
        "\n",
        "    int h_in0 = h_out * 2;\n",
        "    int w_in0 = w_out * 2;\n",
        "\n",
        "    float m = -1e30f;\n",
        "    for (int dh = 0; dh < 2; ++dh) {\n",
        "        for (int dw = 0; dw < 2; ++dw) {\n",
        "            int h_in = h_in0 + dh;\n",
        "            int w_in = w_in0 + dw;\n",
        "            int idx = idx4(n, c, h_in, w_in, C, H, W);\n",
        "            float v = input[idx];\n",
        "            if (v > m) m = v;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n",
        "    output[out_idx] = m;\n",
        "}\n",
        "\n",
        "// --------------- UpSample 2x2 (nearest) ------------------\n",
        "__global__ void upsample2x2_forward(\n",
        "    const float* __restrict__ input,\n",
        "    float* __restrict__ output,\n",
        "    int N, int C, int H, int W)\n",
        "{\n",
        "    int H_out = H * 2;\n",
        "    int W_out = W * 2;\n",
        "\n",
        "    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int nc    = blockIdx.z;\n",
        "\n",
        "    if (w_out >= W_out || h_out >= H_out) return;\n",
        "\n",
        "    int n = nc / C;\n",
        "    int c = nc % C;\n",
        "    if (n >= N) return;\n",
        "\n",
        "    int h_in = h_out / 2;\n",
        "    int w_in = w_out / 2;\n",
        "\n",
        "    int idx_in  = idx4(n, c, h_in, w_in, C, H, W);\n",
        "    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n",
        "    output[idx_out] = input[idx_in];\n",
        "}\n",
        "\n",
        "// --------------- MSE loss (naive) ------------------\n",
        "__global__ void mse_loss_forward(\n",
        "    const float* __restrict__ output,\n",
        "    const float* __restrict__ target,\n",
        "    float* __restrict__ loss,\n",
        "    int size)\n",
        "{\n",
        "    // many threads atomically add into one float â€“ naive but simple\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i >= size) return;\n",
        "\n",
        "    float diff = output[i] - target[i];\n",
        "    float val = diff * diff;\n",
        "\n",
        "    atomicAdd(loss, val / size);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fURnvwpmwRIt",
        "outputId": "ecde3b8e-6902-4d75-92f1-c580f44adc27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu_layers.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_autoencoder.h\n",
        "// header for GPUAutoencoder (the struct + declarations)\n",
        "#pragma once\n",
        "#include \"gpu_layers.h\"\n",
        "\n",
        "// This autoencoder matches the project architecture exactly.\n",
        "// Layout: NCHW [batch, channels, height, width]\n",
        "struct GPUAutoencoder {\n",
        "    int N;   // batch size\n",
        "    int H;   // 32\n",
        "    int W;   // 32;\n",
        "\n",
        "    // --- Conv layer parameters ---\n",
        "    // conv1: 3 -> 256 (3x3)\n",
        "    float *d_w1, *d_b1;\n",
        "    // conv2: 256 -> 128\n",
        "    float *d_w2, *d_b2;\n",
        "    // conv3: 128 -> 128\n",
        "    float *d_w3, *d_b3;\n",
        "    // conv4: 128 -> 256\n",
        "    float *d_w4, *d_b4;\n",
        "    // conv5: 256 -> 3\n",
        "    float *d_w5, *d_b5;\n",
        "\n",
        "    // --- Activations ---\n",
        "    // Input batch\n",
        "    float *d_x0;   // [N, 3, 32, 32]\n",
        "\n",
        "    // Encoder\n",
        "    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n",
        "    float *d_p1;   // pool1   : [N, 256, 16, 16]\n",
        "    float *d_h2;   // conv2   : [N, 128, 16, 16]\n",
        "    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n",
        "\n",
        "    // Decoder\n",
        "    float *d_h3;   // conv3   : [N, 128,  8,  8]\n",
        "    float *d_u1;   // up1     : [N, 128, 16, 16]\n",
        "    float *d_h4;   // conv4   : [N, 256, 16, 16]\n",
        "    float *d_u2;   // up2     : [N, 256, 32, 32]\n",
        "    float *d_out;  // conv5   : [N,   3, 32, 32]\n",
        "\n",
        "    // Loss buffer\n",
        "    float *d_loss; // single float on device\n",
        "};\n",
        "\n",
        "// API\n",
        "void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n",
        "void gpu_autoencoder_free(GPUAutoencoder *ae);\n",
        "\n",
        "// Forward on GPU:\n",
        "//   h_input  : host pointer [N * 3 * 32 * 32]\n",
        "//   h_output : host pointer [N * 3 * 32 * 32]\n",
        "//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n",
        "//   otherwise returns 0.0f.\n",
        "float gpu_autoencoder_forward(\n",
        "    GPUAutoencoder *ae,\n",
        "    const float *h_input,\n",
        "    float *h_output,\n",
        "    bool compute_loss = true);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP5N__skwYMY",
        "outputId": "7b5f4783-a611-4bb4-8468-f38604f3ed09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu_autoencoder.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gpu_autoencoder.cu\n",
        "// your GPUAutoencoder implementation\n",
        "#include <cstdlib>\n",
        "#include <cstdio>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"gpu_layers.h\"\n",
        "#include \"gpu_autoencoder.h\"\n",
        "\n",
        "static inline float rand_uniform(float scale = 0.01f) {\n",
        "    return scale * ( (float)rand() / (float)RAND_MAX - 0.5f );\n",
        "}\n",
        "\n",
        "void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n",
        "    ae->N = batch_size;\n",
        "    ae->H = 32;\n",
        "    ae->W = 32;\n",
        "\n",
        "    const int N = ae->N;\n",
        "    const int H = ae->H;\n",
        "    const int W = ae->W;\n",
        "\n",
        "    // ---------- allocate weights ----------\n",
        "    const int K = 3;\n",
        "\n",
        "    int C_in1 = 3,   C_out1 = 256;\n",
        "    int C_in2 = 256, C_out2 = 128;\n",
        "    int C_in3 = 128, C_out3 = 128;\n",
        "    int C_in4 = 128, C_out4 = 256;\n",
        "    int C_in5 = 256, C_out5 = 3;\n",
        "\n",
        "    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n",
        "    size_t b1_bytes = C_out1 * sizeof(float);\n",
        "    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n",
        "    size_t b2_bytes = C_out2 * sizeof(float);\n",
        "    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n",
        "    size_t b3_bytes = C_out3 * sizeof(float);\n",
        "    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n",
        "    size_t b4_bytes = C_out4 * sizeof(float);\n",
        "    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n",
        "    size_t b5_bytes = C_out5 * sizeof(float);\n",
        "\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n",
        "\n",
        "    // init weights on host\n",
        "// find max weight bytes\n",
        "size_t max_w_bytes = w1_bytes;\n",
        "if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n",
        "if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n",
        "if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n",
        "if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n",
        "\n",
        "// find max bias bytes\n",
        "size_t max_b_bytes = b1_bytes;\n",
        "if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n",
        "if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n",
        "if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n",
        "if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n",
        "\n",
        "float *h_w = (float*)malloc(max_w_bytes);\n",
        "float *h_b = (float*)malloc(max_b_bytes);\n",
        "\n",
        "\n",
        "    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n",
        "        size_t w_cnt = w_bytes / sizeof(float);\n",
        "        size_t b_cnt = b_bytes / sizeof(float);\n",
        "        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(0.01f);\n",
        "        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = 0.0f;\n",
        "        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n",
        "        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n",
        "    };\n",
        "\n",
        "    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n",
        "    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n",
        "    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n",
        "    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n",
        "    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n",
        "\n",
        "    free(h_w);\n",
        "    free(h_b);\n",
        "\n",
        "    // ---------- allocate activations ----------\n",
        "    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n",
        "    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n",
        "    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n",
        "    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n",
        "    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n",
        "    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n",
        "    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n",
        "    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n",
        "    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n",
        "    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n",
        "\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n",
        "\n",
        "    // loss buffer\n",
        "    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n",
        "}\n",
        "\n",
        "void gpu_autoencoder_free(GPUAutoencoder *ae) {\n",
        "    // weights\n",
        "    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n",
        "    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n",
        "    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n",
        "    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n",
        "    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n",
        "\n",
        "    // activations\n",
        "    cudaFree(ae->d_x0);\n",
        "    cudaFree(ae->d_h1);\n",
        "    cudaFree(ae->d_p1);\n",
        "    cudaFree(ae->d_h2);\n",
        "    cudaFree(ae->d_p2);\n",
        "    cudaFree(ae->d_h3);\n",
        "    cudaFree(ae->d_u1);\n",
        "    cudaFree(ae->d_h4);\n",
        "    cudaFree(ae->d_u2);\n",
        "    cudaFree(ae->d_out);\n",
        "\n",
        "    cudaFree(ae->d_loss);\n",
        "}\n",
        "\n",
        "// Forward pass for ONE batch (no backward yet)\n",
        "float gpu_autoencoder_forward(\n",
        "    GPUAutoencoder *ae,\n",
        "    const float *h_input,\n",
        "    float *h_output,\n",
        "    bool compute_loss)\n",
        "{\n",
        "    const int N = ae->N;\n",
        "    const int H = ae->H;\n",
        "    const int W = ae->W;\n",
        "    const int K = 3;\n",
        "    const int pad = 1;\n",
        "    const int stride = 1;\n",
        "\n",
        "    // ------------- copy input to device -------------\n",
        "    size_t in_bytes = N * 3 * H * W * sizeof(float);\n",
        "    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    dim3 block2d(16, 16);\n",
        "\n",
        "    // ========= ENCODER =========\n",
        "    // conv1: 3 -> 256, same 32x32\n",
        "    {\n",
        "        int C_in = 3, C_out = 256;\n",
        "        int H_out = 32, W_out = 32;\n",
        "        dim3 gridConv(\n",
        "            (W_out + block2d.x - 1) / block2d.x,\n",
        "            (H_out + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        conv2d_forward_naive<<<gridConv, block2d>>>(\n",
        "            ae->d_x0, ae->d_w1, ae->d_b1, ae->d_h1,\n",
        "            N, C_in, H, W, C_out, K, pad, stride);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        // ReLU\n",
        "        int size = N * C_out * H_out * W_out;\n",
        "        int t = 256;\n",
        "        int b = (size + t - 1) / t;\n",
        "        relu_forward<<<b, t>>>(ae->d_h1, size);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        // MaxPool 2x2 -> 16x16\n",
        "        int Hp = 16, Wp = 16;\n",
        "        dim3 gridPool(\n",
        "            (Wp + block2d.x - 1) / block2d.x,\n",
        "            (Hp + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        maxpool2x2_forward<<<gridPool, block2d>>>(\n",
        "            ae->d_h1, ae->d_p1,\n",
        "            N, C_out, H_out, W_out);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "    }\n",
        "\n",
        "    // conv2: 256 -> 128, 16x16, then pool -> 8x8\n",
        "    {\n",
        "        int C_in = 256, C_out = 128;\n",
        "        int H_in = 16, W_in = 16;\n",
        "        int H_out = 16, W_out = 16;\n",
        "        dim3 gridConv(\n",
        "            (W_out + block2d.x - 1) / block2d.x,\n",
        "            (H_out + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        conv2d_forward_naive<<<gridConv, block2d>>>(\n",
        "            ae->d_p1, ae->d_w2, ae->d_b2, ae->d_h2,\n",
        "            N, C_in, H_in, W_in, C_out, K, pad, stride);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        int size = N * C_out * H_out * W_out;\n",
        "        int t = 256;\n",
        "        int b = (size + t - 1) / t;\n",
        "        relu_forward<<<b, t>>>(ae->d_h2, size);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        // pool -> 8x8\n",
        "        int Hp = 8, Wp = 8;\n",
        "        dim3 gridPool(\n",
        "            (Wp + block2d.x - 1) / block2d.x,\n",
        "            (Hp + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        maxpool2x2_forward<<<gridPool, block2d>>>(\n",
        "            ae->d_h2, ae->d_p2,\n",
        "            N, C_out, H_out, W_out);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "    }\n",
        "\n",
        "    // LATENT is ae->d_p2: [N, 128, 8, 8]\n",
        "\n",
        "    // ========= DECODER =========\n",
        "    // conv3: 128 -> 128, 8x8\n",
        "    {\n",
        "        int C_in = 128, C_out = 128;\n",
        "        int H_in = 8, W_in = 8;\n",
        "        int H_out = 8, W_out = 8;\n",
        "\n",
        "        dim3 gridConv(\n",
        "            (W_out + block2d.x - 1) / block2d.x,\n",
        "            (H_out + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        conv2d_forward_naive<<<gridConv, block2d>>>(\n",
        "            ae->d_p2, ae->d_w3, ae->d_b3, ae->d_h3,\n",
        "            N, C_in, H_in, W_in, C_out, K, pad, stride);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        int size = N * C_out * H_out * W_out;\n",
        "        int t = 256;\n",
        "        int b = (size + t - 1) / t;\n",
        "        relu_forward<<<b, t>>>(ae->d_h3, size);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        // upsample 8x8 -> 16x16\n",
        "        int Hu = 16, Wu = 16;\n",
        "        dim3 gridUp(\n",
        "            (Wu + block2d.x - 1) / block2d.x,\n",
        "            (Hu + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        upsample2x2_forward<<<gridUp, block2d>>>(\n",
        "            ae->d_h3, ae->d_u1,\n",
        "            N, C_out, H_in, W_in);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "    }\n",
        "\n",
        "    // conv4: 128 -> 256, 16x16, then upsample 16->32\n",
        "    {\n",
        "        int C_in = 128, C_out = 256;\n",
        "        int H_in = 16, W_in = 16;\n",
        "        int H_out = 16, W_out = 16;\n",
        "\n",
        "        dim3 gridConv(\n",
        "            (W_out + block2d.x - 1) / block2d.x,\n",
        "            (H_out + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        conv2d_forward_naive<<<gridConv, block2d>>>(\n",
        "            ae->d_u1, ae->d_w4, ae->d_b4, ae->d_h4,\n",
        "            N, C_in, H_in, W_in, C_out, K, pad, stride);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        int size = N * C_out * H_out * W_out;\n",
        "        int t = 256;\n",
        "        int b = (size + t - 1) / t;\n",
        "        relu_forward<<<b, t>>>(ae->d_h4, size);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "        // upsample 16x16 -> 32x32\n",
        "        int Hu = 32, Wu = 32;\n",
        "        dim3 gridUp(\n",
        "            (Wu + block2d.x - 1) / block2d.x,\n",
        "            (Hu + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        upsample2x2_forward<<<gridUp, block2d>>>(\n",
        "            ae->d_h4, ae->d_u2,\n",
        "            N, C_out, H_in, W_in);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "    }\n",
        "\n",
        "    // conv5: 256 -> 3, 32x32 (no activation, usually MSE on raw output)\n",
        "    {\n",
        "        int C_in = 256, C_out = 3;\n",
        "        int H_in = 32, W_in = 32;\n",
        "        int H_out = 32, W_out = 32;\n",
        "\n",
        "        dim3 gridConv(\n",
        "            (W_out + block2d.x - 1) / block2d.x,\n",
        "            (H_out + block2d.y - 1) / block2d.y,\n",
        "            N * C_out);\n",
        "\n",
        "        conv2d_forward_naive<<<gridConv, block2d>>>(\n",
        "            ae->d_u2, ae->d_w5, ae->d_b5, ae->d_out,\n",
        "            N, C_in, H_in, W_in, C_out, K, pad, stride);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "    }\n",
        "\n",
        "    // ------------- (optional) compute MSE loss -------------\n",
        "    float loss_value = 0.0f;\n",
        "    if (compute_loss) {\n",
        "        int size = N * 3 * 32 * 32;\n",
        "        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n",
        "        int t = 256;\n",
        "        int b = (size + t - 1) / t;\n",
        "        mse_loss_forward<<<b, t>>>(\n",
        "            ae->d_out, ae->d_x0, ae->d_loss, size);\n",
        "        CHECK_CUDA(cudaDeviceSynchronize());\n",
        "        CHECK_CUDA(cudaMemcpy(&loss_value, ae->d_loss,\n",
        "                              sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost));\n",
        "    }\n",
        "\n",
        "    // ------------- copy output back to host -------------\n",
        "    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n",
        "    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n",
        "                          out_bytes,\n",
        "                          cudaMemcpyDeviceToHost));\n",
        "\n",
        "    return loss_value;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ReKy-XwVsT",
        "outputId": "74e95901-d81c-4fca-b3aa-ccbf741583c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu_autoencoder.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main_gpu.cu\n",
        "// the UPDATED main() code with argc/argv\n",
        "#include <cstdio>\n",
        "#include <ctime>\n",
        "#include \"load_data.h\"\n",
        "#include \"gpu_autoencoder.h\"   // your header\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    srand((unsigned int)time(NULL));\n",
        "\n",
        "    if (argc < 2) {\n",
        "        fprintf(stderr, \"Usage: %s <path_to_cifar-10-batches-bin>\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "    const char* data_dir = argv[1];\n",
        "\n",
        "    // ---- Load CIFAR-10 on CPU ----\n",
        "    Cifar10 data;\n",
        "    load_cifar10(&data, data_dir);\n",
        "    normalize_cifar10(&data);\n",
        "\n",
        "    int batch_size = 64;           // GPU phase suggests 64\n",
        "    int num_batches = TRAIN_NUM / batch_size;\n",
        "\n",
        "    float *h_batch  = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n",
        "    float *h_output = (float*)malloc(batch_size * IMG_SIZE * sizeof(float));\n",
        "\n",
        "    // ---- Init GPU autoencoder ----\n",
        "    GPUAutoencoder ae;\n",
        "    gpu_autoencoder_init(&ae, batch_size);\n",
        "\n",
        "    // Take one batch, just to test forward\n",
        "    shuffle_cifar10(&data);\n",
        "    get_next_batch(&data, batch_size, 0, h_batch);\n",
        "\n",
        "    float loss = gpu_autoencoder_forward(&ae, h_batch, h_output, true);\n",
        "    printf(\"Single GPU forward done. MSE loss = %f\\n\", loss);\n",
        "\n",
        "    // inspect first few output pixels\n",
        "    for (int i = 0; i < 10; ++i) {\n",
        "        printf(\"%f \", h_output[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // ---- cleanup ----\n",
        "    gpu_autoencoder_free(&ae);\n",
        "    free(h_batch);\n",
        "    free(h_output);\n",
        "    free_cifar10(&data);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUae-5X1wZ5e",
        "outputId": "7cf506d5-953b-4689-b366-349260f75686"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -O2 main_gpu.cu gpu_autoencoder.cu gpu_layers.cu load_data.cu -o autoencoder_gpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0s-erZ0wcJ-",
        "outputId": "99b2d934-d7ed-41d0-f5c4-8a65e3b173e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mmain_gpu.cu(22)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"num_batches\"\u001b[0m was declared but never referenced\n",
            "      int num_batches = 50000 / batch_size;\n",
            "          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(19)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n",
            "      const int H = ae->H;\n",
            "                ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(20)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n",
            "      const int W = ae->W;\n",
            "                ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./autoencoder_gpu \"/content/drive/MyDrive/Parallel - Final Project/Data/cifar-10-batches-bin\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp412P3fweDc",
        "outputId": "c8cee62e-38f1-433f-d838-ccb04a86376d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 loaded successfully from /content/drive/MyDrive/Parallel - Final Project/Data/cifar-10-batches-bin\n",
            "Single GPU forward done. MSE loss = 0.269242\n",
            "0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 \n"
          ]
        }
      ]
    }
  ]
}