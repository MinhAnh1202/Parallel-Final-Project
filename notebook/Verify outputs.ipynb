{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSTbu+vQqNlcZGBs2Rbhnx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Verify outputs between CPU and GPU versions"],"metadata":{"id":"BCkyjxtJLYvn"}},{"cell_type":"markdown","source":["## 1) Verify outputs between CPU and GPU naive"],"metadata":{"id":"afgqr-RQLhyi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qejWcOD6b0jM","executionInfo":{"status":"ok","timestamp":1765960825496,"user_tz":-420,"elapsed":19,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"83c8a835-cb40-4d60-980d-e5a062fa321d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_layers.h\n"]}],"source":["%%writefile cpu_layers.h\n","#pragma once\n","#include <stdio.h>\n","#include <float.h>\n","\n","void Relu(float* input, int N, float* output);\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void MaxPool2D_Forward(float* input, int input_width, int input_height,\n","    int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width);\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","    int scale_factor, int filter_count,\n","    float* output, int output_height, int output_width);\n","float MSE(float* input, float* output, int size);\n","void Relu_Backward(float* d_output, float* input,int N);\n","void MSE_Gradient(float* input, float* output, int size, float* d_output);\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width);\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input);\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights);\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height, int filter_count, float* d_biases);\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params);\n"]},{"cell_type":"code","source":["%%writefile cpu_layers.c\n","#include \"cpu_layers.h\"\n","\n","void Relu(float* input, int N, float* output) {\n","    for (int i = 0; i < N; i++) {\n","        output[i] = input[i] > 0.0f ? input[i] : 0.0f;\n","    }\n","}\n","\n","void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n","    float* kernel, int kernel_width, int kernel_height,\n","    float* biases, int padding, int stride, int filter_count,\n","    float* output, int output_height, int output_width)\n","{\n","    // // Tính toán kích thước output\n","    // int H_out = (input_height + 2 * padding - kernel_height) / stride + 1;\n","    // int W_out = (input_width + 2 * padding - kernel_width) / stride + 1;\n","    // int output_size = filter_count * H_out * W_out;\n","    // output = (float*)malloc(output_size * sizeof(float));\n","    // if (output == NULL) {\n","    //     fprintf(stderr, \"ERROR: Memory allocation failed!\\n\");\n","    //     return;\n","    // }\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua chiều cao output\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            // Lặp qua chiều rộng output\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float sum = 0.0f;\n","                // Lặp qua kênh đầu vào (c_in)\n","                for (int c_in = 0; c_in < input_channels; c_in++) {\n","                    // Lặp qua kernel height\n","                    for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                        // Lặp qua kernel width\n","                        for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                            // Vị trí input tương ứng\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float val = 0.0f;\n","                            // Kiểm tra zero padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int channel_size = input_width * input_height;\n","                                val = input[c_in * channel_size + h_in * input_width + w_in];\n","                            }\n","\n","                            int weight_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            k_h * kernel_width + k_w;\n","\n","                            sum += val * kernel[weight_idx];\n","                        }\n","                    }\n","                }\n","                sum += biases[c_out];  // Thêm bias\n","                int output_idx = h_out * output_width + w_out + c_out * output_width * output_height;\n","                output[output_idx] = sum;\n","            }\n","        }\n","    }\n","}\n","\n","\n","void MaxPool2D_Forward(float* input, int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* output, int output_height, int output_width) {\n","    // int H_out = (input_height - filter_height) / stride + 1;\n","    // int W_out = (input_width - filter_width) / stride + 1;\n","\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_out = 0; h_out < output_height; h_out++) {\n","            for (int w_out = 0; w_out < output_width; w_out++) {\n","                float max_val = -FLT_MAX;\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = c * plane_size_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                        }\n","                    }\n","                }\n","                int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                output[output_idx] = max_val;\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Forward(float* input, int input_width, int input_height,\n","int scale_factor, int filter_count, float* output, int output_height, int output_width) {\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = output_height * output_width;\n","    for (int c = 0; c < filter_count; c++) {\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float val = input[c * plane_size_in + h_in * input_width + w_in];\n","                for (int sh = 0; sh < scale_factor; sh++) { // Gấp đôi hàng\n","                    for (int sw = 0; sw < scale_factor; sw++) { // Gấp đôi cột\n","                        int h_out = h_in * scale_factor + sh;\n","                        int w_out = w_in * scale_factor + sw;\n","                        int output_idx = c * plane_size_out + h_out * output_width + w_out;\n","                        output[output_idx] = val;\n","                    }\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","float MSE(float* input, float* output, int size) {\n","    float sum = 0.0f;\n","    for (int i = 0; i < size; i++) {\n","        sum += (output[i] - input[i]) * (output[i] - input[i]);\n","    }\n","    return sum / size;\n","}\n","\n","void Relu_Backward(float* d_output, float* input,int N) {\n","    for (int i = 0; i < N; i++) {\n","        d_output[i] = input[i] > 0.0f ? d_output[i] : 0.0f;\n","    }\n","}\n","\n","void MSE_Gradient(float* input, float* output, int size, float* d_output) {\n","    float sum = 0.0f;\n","    float factor = 2.0f / size;\n","    for (int i = 0; i < size; i++) {\n","        d_output[i] = factor * (output[i] - input[i]);\n","    }\n","}\n","\n","void MaxPool2D_Backward(float* d_output, int d_output_width, int d_output_height, float* input,\n","    int input_width, int input_height, int filter_width, int filter_height, int stride, int filter_count,\n","    float* d_input)\n","{\n","    // Chỉ gán vị trí giá trị max của input ban đầu là gradient của lớp tiếp theo (d_output), còn lại là 0\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) { // Khởi tạo gradient của input ban đầu là 0\n","        d_input[i] = 0.0f;\n","    }\n","\n","    for (int c = 0; c < filter_count; c++) {\n","\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","\n","                int h_start = h_out * stride;\n","                int w_start = w_out * stride;\n","\n","                float max_val = -FLT_MAX;\n","                int max_input_idx = -1;\n","\n","                for (int fh = 0; fh < filter_height; fh++) {\n","                    for (int fw = 0; fw < filter_width; fw++) {\n","                        int h_in = h_start + fh;\n","                        int w_in = w_start + fw;\n","                        int input_idx = channel_offset_in + h_in * input_width + w_in;\n","                        float val = input[input_idx];\n","                        if (val > max_val) {\n","                            max_val = val;\n","                            max_input_idx = input_idx;\n","                        }\n","                    }\n","                }\n","                //Lấy gradient từ output\n","                if (max_input_idx != -1) {\n","                    int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                    d_input[max_input_idx] += d_output[output_idx];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void UpSample2D_Backward(float* d_output, int d_output_width, int d_output_height, int scale_factor, int filter_count,\n","    float* d_input, int d_input_height, int d_input_width) {\n","\n","    int plane_size_in = d_input_height * d_input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    int total_input_size = filter_count * plane_size_in;\n","    for (int i = 0; i < total_input_size; i++) {\n","        d_input[i] = 0.0f;\n","    }\n","    for (int c = 0; c < filter_count; c++) {\n","        int channel_offset_in = c * plane_size_in;\n","        int channel_offset_out = c * plane_size_out;\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < d_input_height; h_in++) {\n","            for (int w_in = 0; w_in < d_input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                int h_start_out = h_in * scale_factor;\n","                int w_start_out = w_in * scale_factor;\n","                for (int sh = 0; sh < scale_factor; sh++) {\n","                    for (int sw = 0; sw < scale_factor; sw++) {\n","                        int h_out = h_start_out + sh;\n","                        int w_out = w_start_out + sw;\n","                        if (h_out < d_output_height && w_out < d_output_width) {\n","                            int output_idx = channel_offset_out + h_out * d_output_width + w_out;\n","                            sum_gradient += d_output[output_idx];\n","                        }\n","                    }\n","                }\n","                int input_idx = channel_offset_in + h_in * d_input_width + w_in;\n","                d_input[input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Input(float* d_output, int d_output_width, int d_output_height, float* kernel, int kernel_width, int kernel_height,\n","    int input_width, int input_height, int input_channels, int padding, int stride, int filter_count, float* d_input) {\n","    // Thực hiện tích chập giữa dE/dO và kernel (xoay 180 độ) để tính dE/dI\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh input (kênh output gradient)\n","    for (int c_in = 0; c_in < input_channels; c_in++) {\n","        // Lặp qua input grid (d_input)\n","        for (int h_in = 0; h_in < input_height; h_in++) {\n","            for (int w_in = 0; w_in < input_width; w_in++) {\n","                float sum_gradient = 0.0f;\n","                // Lặp qua kênh output (số lượng filter)\n","                for (int c_out = 0; c_out < filter_count; c_out++) {\n","                    // Lặp qua kernel (xoay 180 độ)\n","                    for (int kh = 0; kh < kernel_height; kh++) {\n","                        for (int kw = 0; kw < kernel_width; kw++) {\n","                            int h_out = h_in - kh + padding;\n","                            int w_out = w_in - kw + padding;\n","                            float d_output_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_out >= 0 && h_out < d_output_height && w_out >= 0 && w_out < d_output_width) {\n","                                int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                                d_output_val = d_output[d_output_idx];\n","                            }\n","                            // Tính chỉ số kernel (xoay 180 độ)\n","                            int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                            (kernel_height - 1 - kh) * kernel_width + (kernel_width - 1 - kw);\n","\n","                            sum_gradient += d_output_val * kernel[kernel_idx];\n","                        }\n","                    }\n","                }\n","                int d_input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                d_input[d_input_idx] = sum_gradient;\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Kernel(float* d_output, int d_output_width, int d_output_height, float* input,\n","int input_width, int input_height, int input_channels, int kernel_width, int kernel_height, int padding, int stride, int filter_count, float* d_weights) {\n","    // Thực hiện tích chập giữa dE/dO và input để tính dE/dW\n","    int plane_size_in = input_height * input_width;\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua kênh đầu ra (filter)\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        // Lặp qua kênh đầu vào\n","        for (int c_in = 0; c_in < input_channels; c_in++) {\n","            // Lặp qua kích thước kernel\n","            for (int k_h = 0; k_h < kernel_height; k_h++) {\n","                for (int k_w = 0; k_w < kernel_width; k_w++) {\n","                    float sum_gradient = 0.0f;\n","                    // Lặp qua output grid (d_output) để tích lũy\n","                    for (int h_out = 0; h_out < d_output_height; h_out++) {\n","                        for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                            int h_in = h_out * stride + k_h - padding;\n","                            int w_in = w_out * stride + k_w - padding;\n","                            float input_val = 0.0f;\n","                            // Kiểm tra padding\n","                            if (h_in >= 0 && h_in < input_height && w_in >= 0 && w_in < input_width) {\n","                                int input_idx = c_in * plane_size_in + h_in * input_width + w_in;\n","                                input_val = input[input_idx];\n","                            }\n","                            int d_output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                            float d_output_val = d_output[d_output_idx];\n","                            // Tính gradient\n","                            sum_gradient += input_val * d_output_val;\n","                        }\n","                    }\n","                    int kernel_idx = c_out * input_channels * kernel_height * kernel_width + c_in * kernel_height * kernel_width +\n","                                    k_h * kernel_width + k_w;\n","                    d_weights[kernel_idx] += sum_gradient;\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","void Conv2D_Backward_Biases(float* d_output, int d_output_width, int d_output_height,\n","    int filter_count, float* d_biases) {\n","    int plane_size_out = d_output_height * d_output_width;\n","    // Lặp qua từng filter\n","    for (int c_out = 0; c_out < filter_count; c_out++) {\n","        float sum_gradient = 0.0f;\n","        // Lặp qua từng vị trí trong output\n","        for (int h_out = 0; h_out < d_output_height; h_out++) {\n","            for (int w_out = 0; w_out < d_output_width; w_out++) {\n","                // Tính chỉ số trong mảng d_output\n","                int output_idx = c_out * plane_size_out + h_out * d_output_width + w_out;\n","                // Cộng dồn gradient từ d_output\n","                sum_gradient += d_output[output_idx];\n","            }\n","        }\n","        d_biases[c_out] += sum_gradient;\n","    }\n","}\n","\n","void SGD_Update(float* weights, float* d_weights, double learning_rate, int N_params) {\n","    for (int i = 0; i < N_params; i++) {\n","        weights[i] -= (learning_rate * d_weights[i]);\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bcq8BOjpb98L","executionInfo":{"status":"ok","timestamp":1765960827471,"user_tz":-420,"elapsed":47,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"b396e194-c795-4db2-9d67-630287984c5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_layers.c\n"]}]},{"cell_type":"code","source":["%%writefile cpu_autoencoder.h\n","#pragma once\n","#include \"cpu_layers.h\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include <math.h>\n","#include <stdint.h>\n","#include <time.h>\n","#include <string.h>\n","// Định nghĩa kích thước Kernel/Stride/Padding\n","#define KERNEL_SIZE 3\n","#define POOL_SIZE 2\n","#define UPSAMPLE_SIZE 2\n","#define CONV_PADDING 1\n","#define CONV_STRIDE 1\n","#define POOL_STRIDE 2\n","\n","typedef struct {\n","    int batch_size;\n","    double learning_rate;\n","    // kích thước input\n","    int input_height;       // 32\n","    int input_width;        // 32\n","    int input_channels;     // 3\n","    // weight, bias và gradient của từng lớp Conv2D\n","    float* w1; float* b1; float* d_w1; float* d_b1;\n","    float* w2; float* b2; float* d_w2; float* d_b2;\n","    float* w3; float* b3; float* d_w3; float* d_b3;\n","    float* w4; float* b4; float* d_w4; float* d_b4;\n","    float* w5; float* b5; float* d_w5; float* d_b5;\n","\n","    float* batch_input;\n","    float* final_output;\n","    float* loss_gradient;\n","    // ouput và gradient của từng lớp Conv2D/MaxPool/UpSample\n","    float* conv1_output;   float* d_conv1_output;\n","    float* pool1_output;   float* d_pool1_output;\n","    float* conv2_output;   float* d_conv2_output;\n","    float* pool2_output;   float* d_pool2_output; // LATENT SPACE\n","    float* conv3_output;   float* d_conv3_output;\n","    float* upsample1_output; float* d_upsample1_output;\n","    float* conv4_output;   float* d_conv4_output;\n","    float* upsample2_output; float* d_upsample2_output;\n","} CPUAutoEncoder;\n","\n","void random_initialize(float* array, int size, float min, float max);\n","void initialize_autoencoder(CPUAutoEncoder* autoencoder, int batch_size, double learning_rate);\n","void free_autoencoder(CPUAutoEncoder* autoencoder);\n","void forward_autoencoder(CPUAutoEncoder* autoencoder);\n","void backward_autoencoder(CPUAutoEncoder* autoencoder);\n","void update_autoencoder_parameters(CPUAutoEncoder* autoencoder);\n","void save_weights(CPUAutoEncoder* autoencoder, const char* filename);\n","void cpu_extract_features(CPUAutoEncoder* autoencoder, float* input_data, int num_images, float* features_output);\n","void cpu_load_weights(CPUAutoEncoder* autoencoder, const char* filename);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pj4a1YG9b9up","executionInfo":{"status":"ok","timestamp":1765960828369,"user_tz":-420,"elapsed":10,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"d916ef81-006a-4ee1-bb88-881a6c581e43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_autoencoder.h\n"]}]},{"cell_type":"code","source":["%%writefile cpu_autoencoder.c\n","#include \"cpu_autoencoder.h\"\n","\n","// Hàm khởi tạo mảng trọng số với giá trị ngẫu nhiên trong khoảng [min, max]\n","void random_initialize(float* array, int size, float min, float max) {\n","    for (int i = 0; i < size; i++) {\n","        float scale = (float)rand() / (float)RAND_MAX;\n","        array[i] = min + scale * (max - min);\n","    }\n","}\n","\n","void zero_initialize(float* array, int size) {\n","    for (int i = 0; i < size; i++) {\n","        array[i] = 0.0f;\n","    }\n","}\n","\n","\n","void initialize_conv_layer(float** w, float** b, float** dw, float** db, int C_in, int C_out) {\n","    int size_W = C_out * C_in * KERNEL_SIZE * KERNEL_SIZE;\n","    *w = (float*)malloc(size_W * sizeof(float));\n","    *b = (float*)malloc(C_out * sizeof(float));\n","    *dw = (float*)malloc(size_W * sizeof(float));\n","    *db = (float*)malloc(C_out * sizeof(float));\n","\n","    random_initialize(*w, size_W, -0.05f, 0.05f);\n","    random_initialize(*b, C_out, -0.05f, 0.05f);\n","    zero_initialize(*dw, size_W);\n","    zero_initialize(*db, C_out);\n","}\n","\n","float* allocate_buffer(int batch_size, int H, int W, int C) {\n","    int size = batch_size * H * W * C;\n","    return (float*)malloc(size * sizeof(float));\n","}\n","\n","\n","void initialize_autoencoder(CPUAutoEncoder* autoencoder, int batch_size, double learning_rate) {\n","    // Tham số chung\n","    autoencoder->batch_size = batch_size;\n","    autoencoder->learning_rate = learning_rate;\n","    autoencoder->input_height = 32;\n","    autoencoder->input_width = 32;\n","    autoencoder->input_channels = 3;\n","\n","    // Output channels của các lớp\n","    int C_in = 3, C1 = 256, C2 = 128, C3 = 128, C4 = 256, C5 = 3;\n","    // Kích thước không gian (Pixel/kênh)\n","    int P1 = 32 * 32, P2 = 16 * 16, P3 = 8 * 8;\n","    // Khởi tạo trọng số, bias và gradient cho từng lớp Conv2D\n","    initialize_conv_layer(&autoencoder->w1, &autoencoder->b1, &autoencoder->d_w1, &autoencoder->d_b1, C_in, C1);\n","    initialize_conv_layer(&autoencoder->w2, &autoencoder->b2, &autoencoder->d_w2, &autoencoder->d_b2, C1, C2);\n","    initialize_conv_layer(&autoencoder->w3, &autoencoder->b3, &autoencoder->d_w3, &autoencoder->d_b3, C2, C3);\n","    initialize_conv_layer(&autoencoder->w4, &autoencoder->b4, &autoencoder->d_w4, &autoencoder->d_b4, C3, C4);\n","    initialize_conv_layer(&autoencoder->w5, &autoencoder->b5, &autoencoder->d_w5, &autoencoder->d_b5, C4, C5);\n","    // Khởi tạo Buffers cho activations và gradients\n","    int input_height = 32, input_width = 32;\n","    autoencoder->batch_input = allocate_buffer(batch_size, input_height, input_width, C_in);\n","    autoencoder->final_output = allocate_buffer(batch_size, input_height, input_width, C5); // Output size (32x32x3)\n","    autoencoder->loss_gradient = allocate_buffer(batch_size, input_height, input_width, C5);\n","    // Layer 1 (Conv1): 32x32x256\n","    autoencoder->conv1_output = allocate_buffer(batch_size, input_height, input_width, C1);\n","    autoencoder->d_conv1_output = allocate_buffer(batch_size, input_height, input_width, C1);\n","    // Layer 2 (Pool1): 16x16x256\n","    int H2 = 16, W2 = 16;\n","    autoencoder->pool1_output = allocate_buffer(batch_size, H2, W2, C1);\n","    autoencoder->d_pool1_output = allocate_buffer(batch_size, H2, W2, C1);\n","    // Layer 3 (Conv2): 16x16x128\n","    autoencoder->conv2_output = allocate_buffer(batch_size, H2, W2, C2);\n","    autoencoder->d_conv2_output = allocate_buffer(batch_size, H2, W2, C2);\n","    // Layer 4 (Pool2 - Latent): 8x8x128\n","    int H3 = 8, W3 = 8;\n","    autoencoder->pool2_output = allocate_buffer(batch_size, H3, W3, C2);\n","    autoencoder->d_pool2_output = allocate_buffer(batch_size, H3, W3, C2);\n","    // Layer 5 (Conv3): 8x8x128\n","    autoencoder->conv3_output = allocate_buffer(batch_size, H3, W3, C3);\n","    autoencoder->d_conv3_output = allocate_buffer(batch_size, H3, W3, C3);\n","    // Layer 6 (UpSample1): 16x16x128\n","    autoencoder->upsample1_output = allocate_buffer(batch_size, H2, W2, C3);\n","    autoencoder->d_upsample1_output = allocate_buffer(batch_size, H2, W2, C3);\n","    // Layer 7 (Conv4): 16x16x256\n","    autoencoder->conv4_output = allocate_buffer(batch_size, H2, W2, C4);\n","    autoencoder->d_conv4_output = allocate_buffer(batch_size, H2, W2, C4);\n","    // Layer 8 (UpSample2): 32x32x256\n","    autoencoder->upsample2_output = allocate_buffer(batch_size, input_height, input_width, C4);\n","    autoencoder->d_upsample2_output = allocate_buffer(batch_size, input_height, input_width, C4);\n","}\n","\n","void free_autoencoder(CPUAutoEncoder* autoencoder) {\n","    // Giải phóng trọng số và gradient\n","    free(autoencoder->w1); free(autoencoder->b1); free(autoencoder->d_w1); free(autoencoder->d_b1);\n","    free(autoencoder->w2); free(autoencoder->b2); free(autoencoder->d_w2); free(autoencoder->d_b2);\n","    free(autoencoder->w3); free(autoencoder->b3); free(autoencoder->d_w3); free(autoencoder->d_b3);\n","    free(autoencoder->w4); free(autoencoder->b4); free(autoencoder->d_w4); free(autoencoder->d_b4);\n","    free(autoencoder->w5); free(autoencoder->b5); free(autoencoder->d_w5); free(autoencoder->d_b5);\n","\n","    // Giải phóng buffers activation/gradient\n","    free(autoencoder->batch_input);\n","    free(autoencoder->final_output);\n","    free(autoencoder->loss_gradient);\n","    free(autoencoder->conv1_output); free(autoencoder->d_conv1_output);\n","    free(autoencoder->pool1_output); free(autoencoder->d_pool1_output);\n","    free(autoencoder->conv2_output); free(autoencoder->d_conv2_output);\n","    free(autoencoder->pool2_output); free(autoencoder->d_pool2_output);\n","    free(autoencoder->conv3_output); free(autoencoder->d_conv3_output);\n","    free(autoencoder->upsample1_output); free(autoencoder->d_upsample1_output);\n","    free(autoencoder->conv4_output); free(autoencoder->d_conv4_output);\n","    free(autoencoder->upsample2_output); free(autoencoder->d_upsample2_output);\n","}\n","\n","// Forward\n","void forward_autoencoder(CPUAutoEncoder* autoencoder) {\n","    int bs = autoencoder->batch_size;\n","\n","    // Kích thước activation của 1 ảnh tại các lớp\n","    int size_input = 32 * 32 * 3;\n","    int size_L1 = 32 * 32 * 256;\n","    int size_L2 = 16 * 16 * 256;\n","    int size_L3 = 16 * 16 * 128;\n","    int size_L4 = 8 * 8 * 128; // Latent\n","    // Decoder sizes\n","    int size_L5 = 8 * 8 * 128;\n","    int size_L6 = 16 * 16 * 128;\n","    int size_L7 = 16 * 16 * 256;\n","    int size_L8 = 32 * 32 * 256;\n","    int size_Out = 32 * 32 * 3;\n","    for (int b = 0; b < bs; b++) {\n","        // Tính offset con trỏ cho ảnh thứ b\n","        float* ptr_input = autoencoder->batch_input + b * size_input;\n","        float* ptr_L1 = autoencoder->conv1_output + b * size_L1;\n","        float* ptr_L2 = autoencoder->pool1_output + b * size_L2;\n","        float* ptr_L3 = autoencoder->conv2_output + b * size_L3;\n","        float* ptr_L4 = autoencoder->pool2_output + b * size_L4;\n","        float* ptr_L5 = autoencoder->conv3_output + b * size_L5;\n","        float* ptr_L6 = autoencoder->upsample1_output + b * size_L6;\n","        float* ptr_L7 = autoencoder->conv4_output + b * size_L7;\n","        float* ptr_L8 = autoencoder->upsample2_output + b * size_L8;\n","        float* ptr_Out = autoencoder->final_output + b * size_Out;\n","        // --- ENCODER ---\n","        // L1: Conv1 + ReLU\n","        Conv2D_Forward(ptr_input, 32, 32, 3, autoencoder->w1, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b1, CONV_PADDING, CONV_STRIDE, 256, ptr_L1, 32, 32);\n","        Relu(ptr_L1, size_L1, ptr_L1);\n","\n","        // L2: Pool1\n","        MaxPool2D_Forward(ptr_L1, 32, 32, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 256, ptr_L2, 16, 16);\n","\n","        // L3: Conv2 + ReLU\n","        Conv2D_Forward(ptr_L2, 16, 16, 256, autoencoder->w2, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b2, CONV_PADDING, CONV_STRIDE, 128, ptr_L3, 16, 16);\n","        Relu(ptr_L3, size_L3, ptr_L3);\n","\n","        // L4: Pool2 (Latent)\n","        MaxPool2D_Forward(ptr_L3, 16, 16, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 128, ptr_L4, 8, 8);\n","\n","        // --- DECODER ---\n","        // L5: Conv3 + ReLU\n","        Conv2D_Forward(ptr_L4, 8, 8, 128, autoencoder->w3, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b3, CONV_PADDING, CONV_STRIDE, 128, ptr_L5, 8, 8);\n","        Relu(ptr_L5, size_L5, ptr_L5);\n","\n","        // L6: UpSample1\n","        UpSample2D_Forward(ptr_L5, 8, 8, UPSAMPLE_SIZE, 128, ptr_L6, 16, 16);\n","\n","        // L7: Conv4 + ReLU\n","        Conv2D_Forward(ptr_L6, 16, 16, 128, autoencoder->w4, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b4, CONV_PADDING, CONV_STRIDE, 256, ptr_L7, 16, 16);\n","        Relu(ptr_L7, size_L7, ptr_L7);\n","\n","        // L8: UpSample2\n","        UpSample2D_Forward(ptr_L7, 16, 16, UPSAMPLE_SIZE, 256, ptr_L8, 32, 32);\n","\n","        // L9: Conv5 (Output)\n","        Conv2D_Forward(ptr_L8, 32, 32, 256, autoencoder->w5, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b5, CONV_PADDING, CONV_STRIDE, 3, ptr_Out, 32, 32);\n","    }\n","}\n","\n","\n","// Backward\n","void backward_autoencoder(CPUAutoEncoder* autoencoder) {\n","    int bs = autoencoder->batch_size;\n","    int total_elements = bs * 32 * 32 * 3;\n","    MSE_Gradient(autoencoder->batch_input, autoencoder->final_output, total_elements, autoencoder->loss_gradient);\n","    // Khởi tạo gradient về 0 trước khi cộng dồn\n","    zero_initialize(autoencoder->d_w1, 256*3*3*3); zero_initialize(autoencoder->d_b1, 256);\n","    zero_initialize(autoencoder->d_w2, 128*256*3*3); zero_initialize(autoencoder->d_b2, 128);\n","    zero_initialize(autoencoder->d_w3, 128*128*3*3); zero_initialize(autoencoder->d_b3, 128);\n","    zero_initialize(autoencoder->d_w4, 256*128*3*3); zero_initialize(autoencoder->d_b4, 256);\n","    zero_initialize(autoencoder->d_w5, 3*256*3*3); zero_initialize(autoencoder->d_b5, 3);\n","    // Kích thước 1 ảnh tại các lớp (như Forward)\n","    int size_Out = 32*32*3;\n","    int size_L8 = 32*32*256;\n","    int size_L7 = 16*16*256;\n","    int size_L6 = 16*16*128;\n","    int size_L5 = 8*8*128;\n","    int size_L4 = 8*8*128;\n","    int size_L3 = 16*16*128;\n","    int size_L2 = 16*16*256;\n","    int size_L1 = 32*32*256;\n","    int size_In = 32*32*3;\n","\n","    for (int b = 0; b < bs; b++) {\n","        // Offset pointers\n","        float* ptr_dOut = autoencoder->loss_gradient + b * size_Out;\n","        float* ptr_Upsample2_Out = autoencoder->upsample2_output + b * size_L8;\n","        float* ptr_d_Upsample2_Out = autoencoder->d_upsample2_output + b * size_L8;\n","        float* ptr_d_Conv4_Out = autoencoder->d_conv4_output + b * size_L7;\n","        float* ptr_Upsample1_Out = autoencoder->upsample1_output + b * size_L6;\n","        float* ptr_d_Upsample1_Out = autoencoder->d_upsample1_output + b * size_L6;\n","        float* ptr_d_Conv3_Out = autoencoder->d_conv3_output + b * size_L5;\n","        float* ptr_Pool2_Out = autoencoder->pool2_output + b * size_L4;\n","        float* ptr_d_Pool2_Out = autoencoder->d_pool2_output + b * size_L4;\n","        float* ptr_Conv2_Out = autoencoder->conv2_output + b * size_L3;\n","        float* ptr_d_Conv2_Out = autoencoder->d_conv2_output + b * size_L3;\n","        float* ptr_Pool1_Out = autoencoder->pool1_output + b * size_L2;\n","        float* ptr_d_Pool1_Out = autoencoder->d_pool1_output + b * size_L2;\n","        float* ptr_Conv1_Out = autoencoder->conv1_output + b * size_L1;\n","        float* ptr_d_Conv1_Out = autoencoder->d_conv1_output + b * size_L1;\n","        float* ptr_Input = autoencoder->batch_input + b * size_In;\n","\n","        // === L9 (Conv5) ===\n","        // dW5, dB5\n","        Conv2D_Backward_Kernel(ptr_dOut, 32, 32, ptr_Upsample2_Out, 32, 32, 256, 3, 3, 1, 1, 3, autoencoder->d_w5);\n","        Conv2D_Backward_Biases(ptr_dOut, 32, 32, 3, autoencoder->d_b5);\n","        // dInput cho L8\n","        Conv2D_Backward_Input(ptr_dOut, 32, 32, autoencoder->w5, 3, 3, 32, 32, 256, 1, 1, 3, ptr_d_Upsample2_Out);\n","\n","        // === L8 (Upsample2) ===\n","        UpSample2D_Backward(ptr_d_Upsample2_Out, 32, 32, UPSAMPLE_SIZE, 256, autoencoder->d_conv4_output + b * size_L7, 16, 16);\n","\n","        // === L7 (Conv4) ===\n","        // ReLU Backward\n","        Relu_Backward(ptr_d_Conv4_Out, autoencoder->conv4_output + b * size_L7, 16*16*256);\n","        Conv2D_Backward_Kernel(ptr_d_Conv4_Out, 16, 16, ptr_Upsample1_Out, 16, 16, 128, 3, 3, 1, 1, 256, autoencoder->d_w4);\n","        Conv2D_Backward_Biases(ptr_d_Conv4_Out, 16, 16, 256, autoencoder->d_b4);\n","        Conv2D_Backward_Input(ptr_d_Conv4_Out, 16, 16, autoencoder->w4, 3, 3, 16, 16, 128, 1, 1, 256, ptr_d_Upsample1_Out);\n","\n","        // === L6 (Upsample1) ===\n","        UpSample2D_Backward(ptr_d_Upsample1_Out, 16, 16, UPSAMPLE_SIZE, 128, ptr_d_Conv3_Out, 8, 8);\n","\n","        // === L5 (Conv3) ===\n","        Relu_Backward(ptr_d_Conv3_Out, autoencoder->conv3_output + b * size_L5, 8*8*128);\n","        Conv2D_Backward_Kernel(ptr_d_Conv3_Out, 8, 8, ptr_Pool2_Out, 8, 8, 128, 3, 3, 1, 1, 128, autoencoder->d_w3);\n","        Conv2D_Backward_Biases(ptr_d_Conv3_Out, 8, 8, 128, autoencoder->d_b3);\n","        Conv2D_Backward_Input(ptr_d_Conv3_Out, 8, 8, autoencoder->w3, 3, 3, 8, 8, 128, 1, 1, 128, ptr_d_Pool2_Out);\n","\n","        // === L4 (Pool2) ===\n","        MaxPool2D_Backward(ptr_d_Pool2_Out, 8, 8, ptr_Conv2_Out, 16, 16, 2, 2, 2, 128, ptr_d_Conv2_Out);\n","\n","        // === L3 (Conv2) ===\n","        Relu_Backward(ptr_d_Conv2_Out, ptr_Conv2_Out, 16*16*128);\n","        Conv2D_Backward_Kernel(ptr_d_Conv2_Out, 16, 16, ptr_Pool1_Out, 16, 16, 256, 3, 3, 1, 1, 128, autoencoder->d_w2);\n","        Conv2D_Backward_Biases(ptr_d_Conv2_Out, 16, 16, 128, autoencoder->d_b2);\n","        Conv2D_Backward_Input(ptr_d_Conv2_Out, 16, 16, autoencoder->w2, 3, 3, 16, 16, 256, 1, 1, 128, ptr_d_Pool1_Out);\n","\n","        // === L2 (Pool1) ===\n","        MaxPool2D_Backward(ptr_d_Pool1_Out, 16, 16, ptr_Conv1_Out, 32, 32, 2, 2, 2, 256, ptr_d_Conv1_Out);\n","\n","        // === L1 (Conv1) ===\n","        Relu_Backward(ptr_d_Conv1_Out, ptr_Conv1_Out, 32*32*256);\n","        Conv2D_Backward_Kernel(ptr_d_Conv1_Out, 32, 32, ptr_Input, 32, 32, 3, 3, 3, 1, 1, 256, autoencoder->d_w1);\n","        Conv2D_Backward_Biases(ptr_d_Conv1_Out, 32, 32, 256, autoencoder->d_b1);\n","    }\n","}\n","\n","void update_autoencoder_parameters(CPUAutoEncoder* autoencoder) {\n","    // Cập nhật tất cả 5 lớp Conv: W += -learning_rate * dW\n","    int size_W1 = 256 * 3 * 3 * 3;\n","    SGD_Update(autoencoder->w1, autoencoder->d_w1, autoencoder->learning_rate, size_W1);\n","    SGD_Update(autoencoder->b1, autoencoder->d_b1, autoencoder->learning_rate, 256);\n","    int size_W2 = 128 * 256 * 3 * 3;\n","    SGD_Update(autoencoder->w2, autoencoder->d_w2, autoencoder->learning_rate, size_W2);\n","    SGD_Update(autoencoder->b2, autoencoder->d_b2, autoencoder->learning_rate, 128);\n","    int size_W3  = 128 * 128 * 3 * 3;\n","    SGD_Update(autoencoder->w3, autoencoder->d_w3, autoencoder->learning_rate, size_W3);\n","    SGD_Update(autoencoder->b3, autoencoder->d_b3, autoencoder->learning_rate, 128);\n","    int size_W4 = 256 * 128 * 3 * 3;\n","    SGD_Update(autoencoder->w4, autoencoder->d_w4, autoencoder->learning_rate, size_W4);\n","    SGD_Update(autoencoder->b4, autoencoder->d_b4, autoencoder->learning_rate, 256);\n","    int size_W5 = 3 * 256 * 3 * 3;\n","    SGD_Update(autoencoder->w5, autoencoder->d_w5, autoencoder->learning_rate, size_W5);\n","    SGD_Update(autoencoder->b5, autoencoder->d_b5, autoencoder->learning_rate, 3);\n","}\n","\n","void save_weights(CPUAutoEncoder* autoencoder, const char* filename) {\n","    FILE* file = fopen(filename, \"wb\");\n","    if (file == NULL) {\n","        printf(\"Error opening file for writing weights.\\n\");\n","        return;\n","    }\n","    // Lưu trọng số và bias của từng lớp Conv2D\n","    fwrite(autoencoder->w1, sizeof(float), 256*3*3*3, file);\n","    fwrite(autoencoder->b1, sizeof(float), 256, file);\n","    fwrite(autoencoder->w2, sizeof(float), 128*256*3*3, file);\n","    fwrite(autoencoder->b2, sizeof(float), 128, file);\n","    fwrite(autoencoder->w3, sizeof(float), 128*128*3*3, file);\n","    fwrite(autoencoder->b3, sizeof(float), 128, file);\n","    fwrite(autoencoder->w4, sizeof(float), 256*128*3*3, file);\n","    fwrite(autoencoder->b4, sizeof(float), 256, file);\n","    fwrite(autoencoder->w5, sizeof(float), 3*256*3*3, file);\n","    fwrite(autoencoder->b5, sizeof(float), 3, file);\n","    fclose(file);\n","}\n","\n","\n","void cpu_extract_features(CPUAutoEncoder* autoencoder, float* input_data, int num_images, float* features_output) {\n","    // Chỉ có forward pass của encoder\n","    // Kích thước các lớp\n","    int size_input = 32 * 32 * 3;\n","    int size_L1 = 32 * 32 * 256;\n","    int size_L2 = 16 * 16 * 256;\n","    int size_L3 = 16 * 16 * 128;\n","    int size_L4 = 8 * 8 * 128;\n","    for (int i = 0; i < num_images; i++) {\n","        float* ptr_input = input_data + i * size_input;\n","        float* ptr_feature_dst = features_output + i * size_L4;\n","        float* ptr_L1 = autoencoder->conv1_output;\n","        float* ptr_L2 = autoencoder->pool1_output;\n","        float* ptr_L3 = autoencoder->conv2_output;\n","        float* ptr_L4 = autoencoder->pool2_output;\n","        //  ENCODER FORWARD PASS\n","        // L1: Conv1 + ReLU\n","        Conv2D_Forward(ptr_input, 32, 32, 3, autoencoder->w1, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b1, CONV_PADDING, CONV_STRIDE, 256, ptr_L1, 32, 32);\n","        Relu(ptr_L1, size_L1, ptr_L1);\n","        // L2: Pool1\n","        MaxPool2D_Forward(ptr_L1, 32, 32, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 256, ptr_L2, 16, 16);\n","        // L3: Conv2 + ReLU\n","        Conv2D_Forward(ptr_L2, 16, 16, 256, autoencoder->w2, KERNEL_SIZE, KERNEL_SIZE, autoencoder->b2, CONV_PADDING, CONV_STRIDE, 128, ptr_L3, 16, 16);\n","        Relu(ptr_L3, size_L3, ptr_L3);\n","        // L4: Pool2 (Latent Space)\n","        MaxPool2D_Forward(ptr_L3, 16, 16, POOL_SIZE, POOL_SIZE, POOL_STRIDE, 128, ptr_L4, 8, 8);\n","\n","        // Sao chép kết quả vào mảng output tổng\n","        memcpy(ptr_feature_dst, ptr_L4, size_L4 * sizeof(float));\n","    }\n","}\n","\n","void cpu_load_weights(CPUAutoEncoder* autoencoder, const char* filename) {\n","    FILE* file = fopen(filename, \"rb\");\n","    if (file == NULL) {\n","        printf(\"Error opening file for reading weights.\\n\");\n","        return;\n","    }\n","    // Đọc trọng số và bias của từng lớp Conv2D\n","    fread(autoencoder->w1, sizeof(float), 256*3*3*3, file);\n","    fread(autoencoder->b1, sizeof(float), 256, file);\n","    fread(autoencoder->w2, sizeof(float), 128*256*3*3, file);\n","    fread(autoencoder->b2, sizeof(float), 128, file);\n","    fread(autoencoder->w3, sizeof(float), 128*128*3*3, file);\n","    fread(autoencoder->b3, sizeof(float), 128, file);\n","    fread(autoencoder->w4, sizeof(float), 256*128*3*3, file);\n","    fread(autoencoder->b4, sizeof(float), 256, file);\n","    fread(autoencoder->w5, sizeof(float), 3*256*3*3, file);\n","    fread(autoencoder->b5, sizeof(float), 3, file);\n","    fclose(file);\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0cGwPrlb9lJ","executionInfo":{"status":"ok","timestamp":1765960829283,"user_tz":-420,"elapsed":28,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"0991529f-c342-4eda-be59-19b51b3166b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing cpu_autoencoder.c\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","\n","__global__ void conv2d_forward_naive(\n","    const float* __restrict__ input,    // [N, C_in, H, W]\n","    const float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    const float* __restrict__ bias,     // [C_out]\n","    float* __restrict__ output,         // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    const float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    const float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    const float* __restrict__ x,       // forward output/input to ReLU\n","    const float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    const float* __restrict__ input,\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_backward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input_naive(\n","    const float* __restrict__ dY,\n","    const float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias_naive(\n","    const float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    const float* __restrict__ grad,\n","    int size,\n","    float lr);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOWjZlNncz8o","executionInfo":{"status":"ok","timestamp":1765960830096,"user_tz":-420,"elapsed":24,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"0b5b80bc-4b81-4b14-9fb6-fb34f9907eab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers.cu\n","#include \"gpu_layers.h\"\n","\n","// --------------- Conv2D forward ------------------\n","__global__ void conv2d_forward_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ weight,\n","    const float* __restrict__ bias,\n","    float* __restrict__ output,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n      = nc / C_out;\n","    int c_out  = nc % C_out;\n","    if (n >= N) return;\n","\n","    float sum = bias ? bias[c_out] : 0.0f;\n","\n","    for (int c_in = 0; c_in < C_in; ++c_in) {\n","        for (int kh = 0; kh < K; ++kh) {\n","            for (int kw = 0; kw < K; ++kw) {\n","                int h_in = h_out * stride + kh - pad;\n","                int w_in = w_out * stride + kw - pad;\n","                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n","                    continue;\n","\n","                int in_idx = ((n * C_in + c_in) * H + h_in) * W + w_in;\n","                int w_idx = (((c_out * C_in + c_in) * K) + kh) * K + kw;\n","                sum += weight[w_idx] * input[in_idx];\n","            }\n","        }\n","    }\n","    int out_idx = ((n * C_out + c_out) * H_out + h_out) * W_out + w_out;\n","    output[out_idx] = sum;\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = v > 0.0f ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 ------------------\n","__global__ void maxpool2x2_forward(\n","    const float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 ------------------\n","__global__ void upsample2x2_forward(\n","    const float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in  = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // reduce trong block\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    const float* __restrict__ x,\n","    const float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        grad_x[i] = (v > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    const float* __restrict__ input,\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc    = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    // Tìm max\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    // Chỉ ghi vào vị trí max (grad_in đã được zero trước đó)\n","    // Mỗi pooling window độc lập, không có conflict\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    const float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc   = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    const float* __restrict__ output,\n","    const float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input_naive(\n","    const float* __restrict__ dY,\n","    const float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int w = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w >= W || h >= H) return;\n","\n","    int n = nc / C_in;\n","    int c_in = nc % C_in;\n","    if (n >= N) return;\n","\n","    float sum = 0.0f;\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        for (int kh = 0; kh < K; ++kh) {\n","            for (int kw = 0; kw < K; ++kw) {\n","                int h_out = h + pad - kh;\n","                int w_out = w + pad - kw;\n","\n","                if (h_out % stride != 0 || w_out % stride != 0) continue;\n","\n","                h_out /= stride;\n","                w_out /= stride;\n","\n","                if (h_out < 0 || h_out >= H_out ||\n","                    w_out < 0 || w_out >= W_out)\n","                    continue;\n","\n","                int dy_idx = idx4(n, c_out, h_out, w_out,\n","                                  C_out, H_out, W_out);\n","\n","                int kh_flip = K - 1 - kh;\n","                int kw_flip = K - 1 - kw;\n","                int w_idx = (((c_out * C_in + c_in) * K) + kh_flip) * K + kw_flip;\n","                sum += dY[dy_idx] * weight[w_idx];\n","            }\n","        }\n","    }\n","\n","    int dx_idx = idx4(n, c_in, h, w, C_in, H, W);\n","    dX[dx_idx] = sum;\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","// Mỗi thread tính toàn bộ gradient cho 1 weight element\n","// Không có conflict vì mỗi thread ghi vào vị trí riêng biệt\n","__global__ void conv2d_backward_weight_naive(\n","    const float* __restrict__ input,\n","    const float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int K, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    int total = C_out * C_in * K * K;\n","    if (idx >= total) return;\n","\n","    int kw = idx % K;\n","    int tmp = idx / K;\n","    int kh = tmp % K;\n","    tmp /= K;\n","    int c_in = tmp % C_in;\n","    int c_out = tmp / C_in;\n","\n","    float sum = 0.0f;\n","    for (int n = 0; n < N; ++n) {\n","        for (int h_out = 0; h_out < H_out; ++h_out) {\n","            for (int w_out = 0; w_out < W_out; ++w_out) {\n","                int h_in = h_out * stride + kh - pad;\n","                int w_in = w_out * stride + kw - pad;\n","                if (h_in < 0 || h_in >= H || w_in < 0 || w_in >= W)\n","                    continue;\n","\n","                int in_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                int dy_idx = idx4(n, c_out, h_out, w_out,\n","                                  C_out, H_out, W_out);\n","                sum += dY[dy_idx] * input[in_idx];\n","            }\n","        }\n","    }\n","    dW[idx] += sum;\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","// Mỗi thread tính gradient cho 1 bias element\n","__global__ void conv2d_backward_bias_naive(\n","    const float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int c_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (c_out >= C_out) return;\n","\n","    float sum = 0.0f;\n","    for (int n = 0; n < N; ++n) {\n","        for (int h = 0; h < H_out; ++h) {\n","            for (int w = 0; w < W_out; ++w) {\n","                int idx = idx4(n, c_out, h, w, C_out, H_out, W_out);\n","                sum += dY[idx];\n","            }\n","        }\n","    }\n","    dB[c_out] += sum;\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    const float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAayqSphc3Rk","executionInfo":{"status":"ok","timestamp":1765960830926,"user_tz":-420,"elapsed":25,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"e53c17cd-e00e-4e64-fae5-26efb2864315"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers.h\"\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0;\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMwY-2xVc6lX","executionInfo":{"status":"ok","timestamp":1765960831725,"user_tz":-420,"elapsed":8,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"0880d82f-94ab-4287-9ecd-3651a7f856dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder.cu\n","// your GPUAutoencoder implementation\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","#include \"gpu_layers.h\"\n","#include \"gpu_autoencoder.h\"\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","    const int K = 3;\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","    // find max weight bytes\n","    size_t max_w_bytes = w1_bytes;\n","    if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","    if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","    if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","    if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","    // find max bias bytes\n","    size_t max_b_bytes = b1_bytes;\n","    if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","    if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","    if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","    if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","    float *h_w = (float*)malloc(max_w_bytes);\n","    float *h_b = (float*)malloc(max_b_bytes);\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ----------\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int K = 3;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ------------- copy input to device -------------\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ========= ENCODER =========\n","    // conv1: 3 -> 256, same 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_b1, ae->d_h1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16, then pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_b2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT is ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","    // conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_b3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv4: 128 -> 256, 16x16, then upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_b4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv5: 256 -> 3, 32x32 (no activation, usually MSE on raw output)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_b5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, K, pad, stride);\n","    }\n","\n","    // ------------- (optional) compute MSE loss -------------\n","    float loss_value = 0.0f;\n","        if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        // kernel giờ trả về SUM(diff^2) vào d_loss\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;  // MSE = sum / size\n","    }\n","\n","\n","    // ------------- copy output back to host -------------\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H; // 32\n","    const int W0 = ae->W; // 32\n","    const int K = 3;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Zero all gradient buffers\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","        CHECK_CUDA(cudaDeviceSynchronize());\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_u2, ae->d_gout, ae->d_gw5,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gout, ae->d_gb5,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int size = N * 256 * 16 * 16;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_u1, ae->d_gh4, ae->d_gw4,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh4, ae->d_gb4,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int size = N * 128 * 8 * 8;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_p2, ae->d_gh3, ae->d_gw3,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh3, ae->d_gb3,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward: P2 <- H2 =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16;\n","\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int size = N * 128 * 16 * 16;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_p1, ae->d_gh2, ae->d_gw2,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh2, ae->d_gb2,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward: P1 <- H1 =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32;\n","\n","        // Zero gradient buffer\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int size = N * 256 * 32 * 32;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input_naive<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int b = (num_w + t - 1) / t;\n","        conv2d_backward_weight_naive<<<b, t>>>(\n","            ae->d_x0, ae->d_gh1, ae->d_gw1,\n","            N, C_in, H, W, C_out, K, pad, stride);\n","\n","        int tb = 256;\n","        int bb = (C_out + tb - 1) / tb;\n","        conv2d_backward_bias_naive<<<bb, tb>>>(\n","            ae->d_gh1, ae->d_gb1,\n","            N, C_out, H, W);\n","\n","        sgd_update<<<b, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    const int K = 3;\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int K = 3;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Copy input [N_batch, 3, 32, 32] to GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_b1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, K, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        conv2d_forward_naive<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_b2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, K, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // FIX: Copy latent [N_batch, 128, 8, 8] correctly\n","    size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5Og-aYXc9UB","executionInfo":{"status":"ok","timestamp":1765960832605,"user_tz":-420,"elapsed":29,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"2a4e82d2-2314-48ad-8cc1-e7287db79483"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder.cu\n"]}]},{"cell_type":"code","source":["%%writefile verify_cpu_gpu_output.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","extern \"C\" {\n","    #include \"cpu_autoencoder.h\"\n","}\n","\n","#include \"gpu_autoencoder.h\"\n","\n","// Utility function to compare arrays\n","void compare_arrays(const char* name, const float* a, const float* b, int n) {\n","    double max_abs = 0.0;\n","    double sum_abs = 0.0;\n","    double sum_sq  = 0.0;\n","    int max_idx = 0;\n","\n","    for (int i = 0; i < n; ++i) {\n","        double diff = (double)a[i] - (double)b[i];\n","        double ad   = fabs(diff);\n","\n","        if (ad > max_abs) {\n","            max_abs = ad;\n","            max_idx = i;\n","        }\n","        sum_abs += ad;\n","        sum_sq  += diff * diff;\n","    }\n","\n","    double mean_abs = sum_abs / n;\n","    double rmse     = sqrt(sum_sq / n);\n","\n","    printf(\"[%s]\\n\", name);\n","    printf(\"  max|diff| = %.6e (at index %d: CPU=%.6f, GPU=%.6f)\\n\",\n","           max_abs, max_idx, a[max_idx], b[max_idx]);\n","    printf(\"  mean|diff| = %.6e\\n\", mean_abs);\n","    printf(\"  RMSE = %.6e\\n\", rmse);\n","\n","    // Check if differences are acceptable\n","    if (max_abs < 1e-4 && rmse < 1e-5) {\n","        printf(\"  PASSED (excellent match)\\n\");\n","    } else if (max_abs < 1e-3 && rmse < 1e-4) {\n","        printf(\"  PASSED (good match)\\n\");\n","    } else if (max_abs < 1e-2 && rmse < 1e-3) {\n","        printf(\"  ACCEPTABLE (minor differences)\\n\");\n","    } else {\n","        printf(\"  FAILED (significant differences)\\n\");\n","    }\n","}\n","\n","void copy_weights_cpu_to_gpu(CPUAutoEncoder* cpu_ae, GPUAutoencoder* gpu_ae) {\n","    // Helper function to copy weights from CPU to GPU autoencoder\n","\n","    // Get sizes\n","    int w1_size = 256 * 3 * 3 * 3;\n","    int w2_size = 128 * 256 * 3 * 3;\n","    int w3_size = 128 * 128 * 3 * 3;\n","    int w4_size = 256 * 128 * 3 * 3;\n","    int w5_size = 3 * 256 * 3 * 3;\n","\n","    // Copy to GPU\n","    gpu_autoencoder_copy_weights_to_device(gpu_ae,\n","        cpu_ae->w1, cpu_ae->b1,\n","        cpu_ae->w2, cpu_ae->b2,\n","        cpu_ae->w3, cpu_ae->b3,\n","        cpu_ae->w4, cpu_ae->b4,\n","        cpu_ae->w5, cpu_ae->b5);\n","}\n","\n","void verify_autoencoder_output() {\n","    printf(\"==================== Verify Autoencoder CPU vs GPU ====================\\n\\n\");\n","\n","    int batch_size = 4;\n","    double learning_rate = 0.001;\n","    int input_size = batch_size * 3 * 32 * 32;\n","\n","    // Initialize random input\n","    printf(\"Initializing input data...\\n\");\n","    float *h_input = (float*)malloc(input_size * sizeof(float));\n","    for (int i = 0; i < input_size; ++i) {\n","        h_input[i] = ((float)(i % 101) - 50.0f) / 100.0f;  // [-0.5, 0.5]\n","    }\n","\n","    // ===== Initialize CPU Autoencoder =====\n","    printf(\"Initializing CPU autoencoder...\\n\");\n","    CPUAutoEncoder cpu_ae;\n","    initialize_autoencoder(&cpu_ae, batch_size, learning_rate);\n","\n","    // Copy input to CPU autoencoder\n","    memcpy(cpu_ae.batch_input, h_input, input_size * sizeof(float));\n","\n","    // ===== Initialize GPU Autoencoder =====\n","    printf(\"Initializing GPU autoencoder...\\n\");\n","    GPUAutoencoder gpu_ae;\n","    gpu_autoencoder_init(&gpu_ae, batch_size);\n","\n","    // Copy weights from CPU to GPU (để đảm bảo cùng weights)\n","    printf(\"Copying weights from CPU to GPU...\\n\");\n","    copy_weights_cpu_to_gpu(&cpu_ae, &gpu_ae);\n","\n","    // ===== CPU Forward Pass =====\n","    printf(\"\\nRunning CPU forward pass...\\n\");\n","    forward_autoencoder(&cpu_ae);\n","\n","    // Calculate CPU loss\n","    float loss_cpu = MSE(cpu_ae.batch_input, cpu_ae.final_output, input_size);\n","    printf(\"CPU Loss: %.6f\\n\", loss_cpu);\n","\n","    // ===== GPU Forward Pass =====\n","    printf(\"\\nRunning GPU forward pass...\\n\");\n","    float *h_output_gpu = (float*)malloc(input_size * sizeof(float));\n","    float loss_gpu = gpu_autoencoder_forward(&gpu_ae, h_input, h_output_gpu, true);\n","    printf(\"GPU Loss: %.6f\\n\", loss_gpu);\n","\n","    // ===== Compare Results =====\n","    printf(\"\\n==================== Comparison Results ====================\\n\\n\");\n","\n","    printf(\"Loss Comparison:\\n\");\n","    printf(\"  CPU Loss: %.6f\\n\", loss_cpu);\n","    printf(\"  GPU Loss: %.6f\\n\", loss_gpu);\n","    printf(\"  Abs Diff: %.6e\\n\", fabs(loss_cpu - loss_gpu));\n","    printf(\"  Rel Diff: %.6e%%\\n\\n\", fabs(loss_cpu - loss_gpu) / loss_cpu * 100.0);\n","\n","    // Compare final output\n","    compare_arrays(\"Final Output (Reconstructed Image)\",\n","                   cpu_ae.final_output, h_output_gpu, input_size);\n","\n","    // ===== Compare Intermediate Activations =====\n","    printf(\"\\n--- Intermediate Activation Comparisons ---\\n\\n\");\n","\n","    // 1. Conv1 output: [batch, 256, 32, 32]\n","    int conv1_size = batch_size * 256 * 32 * 32;\n","    float *h_conv1_gpu = (float*)malloc(conv1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv1_gpu, gpu_ae.d_h1, conv1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv1 Output (h1)\", cpu_ae.conv1_output, h_conv1_gpu, conv1_size);\n","\n","    // 2. Pool1 output: [batch, 256, 16, 16]\n","    int pool1_size = batch_size * 256 * 16 * 16;\n","    float *h_pool1_gpu = (float*)malloc(pool1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_pool1_gpu, gpu_ae.d_p1, pool1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Pool1 Output (p1)\", cpu_ae.pool1_output, h_pool1_gpu, pool1_size);\n","\n","    // 3. Conv2 output: [batch, 128, 16, 16]\n","    int conv2_size = batch_size * 128 * 16 * 16;\n","    float *h_conv2_gpu = (float*)malloc(conv2_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv2_gpu, gpu_ae.d_h2, conv2_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv2 Output (h2)\", cpu_ae.conv2_output, h_conv2_gpu, conv2_size);\n","\n","    // 4. LATENT: Pool2 output: [batch, 128, 8, 8]\n","    int latent_size = batch_size * 128 * 8 * 8;\n","    float *h_latent_gpu = (float*)malloc(latent_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_latent_gpu, gpu_ae.d_p2, latent_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"LATENT (Pool2/p2)\", cpu_ae.pool2_output, h_latent_gpu, latent_size);\n","\n","    // 5. Conv3 output: [batch, 128, 8, 8]\n","    int conv3_size = batch_size * 128 * 8 * 8;\n","    float *h_conv3_gpu = (float*)malloc(conv3_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv3_gpu, gpu_ae.d_h3, conv3_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv3 Output (h3)\", cpu_ae.conv3_output, h_conv3_gpu, conv3_size);\n","\n","    // 6. Upsample1 output: [batch, 128, 16, 16]\n","    int up1_size = batch_size * 128 * 16 * 16;\n","    float *h_up1_gpu = (float*)malloc(up1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_up1_gpu, gpu_ae.d_u1, up1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Upsample1 Output (u1)\", cpu_ae.upsample1_output, h_up1_gpu, up1_size);\n","\n","    // 7. Conv4 output: [batch, 256, 16, 16]\n","    int conv4_size = batch_size * 256 * 16 * 16;\n","    float *h_conv4_gpu = (float*)malloc(conv4_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv4_gpu, gpu_ae.d_h4, conv4_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv4 Output (h4)\", cpu_ae.conv4_output, h_conv4_gpu, conv4_size);\n","\n","    // 8. Upsample2 output: [batch, 256, 32, 32]\n","    int up2_size = batch_size * 256 * 32 * 32;\n","    float *h_up2_gpu = (float*)malloc(up2_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_up2_gpu, gpu_ae.d_u2, up2_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Upsample2 Output (u2)\", cpu_ae.upsample2_output, h_up2_gpu, up2_size);\n","\n","    // ===== Summary =====\n","    printf(\"\\n==================== Summary ====================\\n\");\n","    printf(\"Architecture verified:\\n\");\n","    printf(\"  Input:      [%d, 3, 32, 32]\\n\", batch_size);\n","    printf(\"  Conv1:      [%d, 256, 32, 32]\\n\", batch_size);\n","    printf(\"  Pool1:      [%d, 256, 16, 16]\\n\", batch_size);\n","    printf(\"  Conv2:      [%d, 128, 16, 16]\\n\", batch_size);\n","    printf(\"  LATENT:     [%d, 128, 8, 8]  ← bottleneck\\n\", batch_size);\n","    printf(\"  Conv3:      [%d, 128, 8, 8]\\n\", batch_size);\n","    printf(\"  Upsample1:  [%d, 128, 16, 16]\\n\", batch_size);\n","    printf(\"  Conv4:      [%d, 256, 16, 16]\\n\", batch_size);\n","    printf(\"  Upsample2:  [%d, 256, 32, 32]\\n\", batch_size);\n","    printf(\"  Output:     [%d, 3, 32, 32]\\n\", batch_size);\n","\n","    // Cleanup\n","    free(h_input);\n","    free(h_output_gpu);\n","    free(h_conv1_gpu);\n","    free(h_pool1_gpu);\n","    free(h_conv2_gpu);\n","    free(h_latent_gpu);\n","    free(h_conv3_gpu);\n","    free(h_up1_gpu);\n","    free(h_conv4_gpu);\n","    free(h_up2_gpu);\n","\n","    free_autoencoder(&cpu_ae);\n","    gpu_autoencoder_free(&gpu_ae);\n","\n","    printf(\"\\n==================== Verification Complete ====================\\n\");\n","}\n","\n","int main() {\n","    verify_autoencoder_output();\n","    CHECK_CUDA(cudaDeviceReset());\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hQfA6j4b6a1","executionInfo":{"status":"ok","timestamp":1765960850248,"user_tz":-420,"elapsed":16,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"2ce63a94-f862-43e6-c34c-295475b2761f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing verify_cpu_gpu_output.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 \\\n","  verify_cpu_gpu_output.cu gpu_layers.cu cpu_layers.c gpu_autoencoder.cu cpu_autoencoder.c \\\n","  -o verify_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46ALeceidWxr","executionInfo":{"status":"ok","timestamp":1765960857502,"user_tz":-420,"elapsed":5323,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"6db272c5-bcd7-4aad-d0a0-25360a477bdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mverify_cpu_gpu_output.cu(56)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"w1_size\"\u001b[0m was declared but never referenced\n","      int w1_size = 256 * 3 * 3 * 3;\n","          ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mverify_cpu_gpu_output.cu(57)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"w2_size\"\u001b[0m was declared but never referenced\n","      int w2_size = 128 * 256 * 3 * 3;\n","          ^\n","\n","\u001b[01m\u001b[0m\u001b[01mverify_cpu_gpu_output.cu(58)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"w3_size\"\u001b[0m was declared but never referenced\n","      int w3_size = 128 * 128 * 3 * 3;\n","          ^\n","\n","\u001b[01m\u001b[0m\u001b[01mverify_cpu_gpu_output.cu(59)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"w4_size\"\u001b[0m was declared but never referenced\n","      int w4_size = 256 * 128 * 3 * 3;\n","          ^\n","\n","\u001b[01m\u001b[0m\u001b[01mverify_cpu_gpu_output.cu(60)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"w5_size\"\u001b[0m was declared but never referenced\n","      int w5_size = 3 * 256 * 3 * 3;\n","          ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(21)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(22)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(460)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder.cu(461)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[Kcpu_autoencoder.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcpu_load_weights\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kcpu_autoencoder.c:340:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  340 |     \u001b[01;35m\u001b[Kfread(autoencoder->w1, sizeof(float), 256*3*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:341:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  341 |     \u001b[01;35m\u001b[Kfread(autoencoder->b1, sizeof(float), 256, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:342:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  342 |     \u001b[01;35m\u001b[Kfread(autoencoder->w2, sizeof(float), 128*256*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:343:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  343 |     \u001b[01;35m\u001b[Kfread(autoencoder->b2, sizeof(float), 128, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:344:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  344 |     \u001b[01;35m\u001b[Kfread(autoencoder->w3, sizeof(float), 128*128*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:345:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  345 |     \u001b[01;35m\u001b[Kfread(autoencoder->b3, sizeof(float), 128, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:346:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  346 |     \u001b[01;35m\u001b[Kfread(autoencoder->w4, sizeof(float), 256*128*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:347:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  347 |     \u001b[01;35m\u001b[Kfread(autoencoder->b4, sizeof(float), 256, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:348:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  348 |     \u001b[01;35m\u001b[Kfread(autoencoder->w5, sizeof(float), 3*256*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:349:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  349 |     \u001b[01;35m\u001b[Kfread(autoencoder->b5, sizeof(float), 3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"]}]},{"cell_type":"code","source":["!./verify_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cO98tafqdsRF","executionInfo":{"status":"ok","timestamp":1765960875290,"user_tz":-420,"elapsed":2400,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"ca400395-d7c4-4e67-c2cf-5848ae312d84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================== Verify Autoencoder CPU vs GPU ====================\n","\n","Initializing input data...\n","Initializing CPU autoencoder...\n","Initializing GPU autoencoder...\n","Copying weights from CPU to GPU...\n","\n","Running CPU forward pass...\n","CPU Loss: 0.088634\n","\n","Running GPU forward pass...\n","GPU Loss: 0.088634\n","\n","==================== Comparison Results ====================\n","\n","Loss Comparison:\n","  CPU Loss: 0.088634\n","  GPU Loss: 0.088634\n","  Abs Diff: 1.192093e-07\n","  Rel Diff: 1.344958e-04%\n","\n","[Final Output (Reconstructed Image)]\n","  max|diff| = 2.905726e-07 (at index 670: CPU=-0.104752, GPU=-0.104752)\n","  mean|diff| = 5.541650e-08\n","  RMSE = 7.080682e-08\n","  ✓ PASSED (excellent match)\n","\n","--- Intermediate Activation Comparisons ---\n","\n","[Conv1 Output (h1)]\n","  max|diff| = 5.960464e-08 (at index 180328: CPU=0.149954, GPU=0.149954)\n","  mean|diff| = 2.246039e-09\n","  RMSE = 4.587140e-09\n","  ✓ PASSED (excellent match)\n","[Pool1 Output (p1)]\n","  max|diff| = 5.960464e-08 (at index 45076: CPU=0.149954, GPU=0.149954)\n","  mean|diff| = 3.714884e-09\n","  RMSE = 6.136012e-09\n","  ✓ PASSED (excellent match)\n","[Conv2 Output (h2)]\n","  max|diff| = 5.140901e-07 (at index 93201: CPU=0.076877, GPU=0.076876)\n","  mean|diff| = 2.312060e-08\n","  RMSE = 4.644612e-08\n","  ✓ PASSED (excellent match)\n","[LATENT (Pool2/p2)]\n","  max|diff| = 4.619360e-07 (at index 9706: CPU=0.226227, GPU=0.226227)\n","  mean|diff| = 3.283412e-08\n","  RMSE = 5.668881e-08\n","  ✓ PASSED (excellent match)\n","[Conv3 Output (h3)]\n","  max|diff| = 4.470348e-07 (at index 25121: CPU=0.187459, GPU=0.187460)\n","  mean|diff| = 2.454062e-08\n","  RMSE = 4.628772e-08\n","  ✓ PASSED (excellent match)\n","[Upsample1 Output (u1)]\n","  max|diff| = 4.470348e-07 (at index 100482: CPU=0.187459, GPU=0.187460)\n","  mean|diff| = 2.454062e-08\n","  RMSE = 4.628772e-08\n","  ✓ PASSED (excellent match)\n","[Conv4 Output (h4)]\n","  max|diff| = 3.874302e-07 (at index 91577: CPU=0.111106, GPU=0.111106)\n","  mean|diff| = 2.186754e-08\n","  RMSE = 3.906520e-08\n","  ✓ PASSED (excellent match)\n","[Upsample2 Output (u2)]\n","  max|diff| = 3.874302e-07 (at index 366290: CPU=0.111106, GPU=0.111106)\n","  mean|diff| = 2.186754e-08\n","  RMSE = 3.906520e-08\n","  ✓ PASSED (excellent match)\n","\n","==================== Summary ====================\n","Architecture verified:\n","  Input:      [4, 3, 32, 32]\n","  Conv1:      [4, 256, 32, 32]\n","  Pool1:      [4, 256, 16, 16]\n","  Conv2:      [4, 128, 16, 16]\n","  LATENT:     [4, 128, 8, 8]  ← bottleneck\n","  Conv3:      [4, 128, 8, 8]\n","  Upsample1:  [4, 128, 16, 16]\n","  Conv4:      [4, 256, 16, 16]\n","  Upsample2:  [4, 256, 32, 32]\n","  Output:     [4, 3, 32, 32]\n","\n","==================== Verification Complete ====================\n"]}]},{"cell_type":"markdown","source":["## 2) Verify CPU and GPU optimization version 1 outputs"],"metadata":{"id":"uHUipNYrLsYN"}},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fBA9MO7frkr","executionInfo":{"status":"ok","timestamp":1765961516351,"user_tz":-420,"elapsed":47,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"e44da5fb-fe3b-491d-97ab-6f78246fcf72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt1.cu\n","#include \"gpu_layers_opt1.h\"\n","\n","// --------------- Conv2D forward (optimization 1) ------------------\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","__global__ void conv2d_forward(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // Load input tile into shared memory\n","        for (int i = ty; i < BLOCK_H; i += blockDim.y) {\n","            for (int j = tx; j < BLOCK_W; j += blockDim.x) {\n","                int h_in = row_start + i;\n","                int w_in = col_start + j;\n","\n","                if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                    size_t input_idx = idx4(n, c_in, h_in, w_in, C_in, H, W);\n","                    smem[i][j] = input[input_idx];\n","                } else {\n","                    smem[i][j] = 0.0f;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","\n","        // Compute convolution\n","        for (int i = 0; i < K; ++i) {\n","            for (int j = 0; j < K; ++j) {\n","                size_t weight_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","                value += smem[ty + i][tx + j] * weight[weight_idx];\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- Conv2D backward: dX ------------------\n","__global__ void conv2d_backward_input(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    // Shared Memory chứa dY\n","    __shared__ float s_dY[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    // Tọa độ dX (output của kernel này)\n","    int h_out = blockIdx.y * TILE_H + ty;\n","    int w_out = blockIdx.x * TILE_W + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n = nc / C_in;\n","    //if (h_out >= H || w_out >= W || n >= N) return;\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // Tọa độ dY cần tải\n","        int h_global = h_out;\n","        int w_global = w_out;\n","\n","        // Tải main tile\n","        if (h_global >= 0 && h_global < H_out && w_global >= 0 && w_global < W_out) {\n","            s_dY[ty + pad][tx + pad] = dY[idx4(n, c_out, h_global, w_global, C_out, H_out, W_out)];\n","        } else {\n","            s_dY[ty + pad][tx + pad] = 0.0f;\n","        }\n","\n","        // Tải biên trên/dưới\n","        if (ty < pad) {\n","            // Biên trên\n","            int h_top = blockIdx.y * TILE_H + ty - pad;\n","            if (h_top >= 0 && h_top < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[ty][tx + pad] = dY[idx4(n, c_out, h_top, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty][tx + pad] = 0.0f;\n","            }\n","\n","            // Biên dưới\n","            int h_bottom = blockIdx.y * TILE_H + TILE_H + pad - 1 - ty;\n","            if (h_bottom >= 0 && h_bottom < H_out && w_global >= 0 && w_global < W_out) {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = dY[idx4(n, c_out, h_bottom, w_global, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","            }\n","        }\n","\n","        // Tải biên trái/phải\n","        if (tx < pad) {\n","            // Biên trái\n","            int w_left = blockIdx.x * TILE_W + tx - pad;\n","            if (w_left >= 0 && w_left < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][tx] = dY[idx4(n, c_out, h_global, w_left, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][tx] = 0.0f;\n","            }\n","\n","            // Biên phải\n","            int w_right = blockIdx.x * TILE_W + TILE_W + pad - 1 - tx;\n","            if (w_right >= 0 && w_right < W_out && h_global >= 0 && h_global < H_out) {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = dY[idx4(n, c_out, h_global, w_right, C_out, H_out, W_out)];\n","            } else {\n","                s_dY[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","            }\n","        }\n","\n","        // Tải 4 góc (Thread (0,0) tải)\n","        if (tx == 0 && ty == 0) {\n","            // Góc trên trái [0][0]\n","            int h_c = blockIdx.y * TILE_H - pad;\n","            int w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[0][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc trên phải [0][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới trái [17][0]\n","            h_c = blockIdx.y * TILE_H + TILE_H + pad - 1;\n","            w_c = blockIdx.x * TILE_W - pad;\n","            s_dY[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","\n","            // Góc dưới phải [17][17]\n","            w_c = blockIdx.x * TILE_W + TILE_W + pad - 1;\n","            s_dY[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H_out && w_c >= 0 && w_c < W_out) ? dY[idx4(n, c_out, h_c, w_c, C_out, H_out, W_out)] : 0.0f;\n","        }\n","\n","        __syncthreads();\n","\n","        // Tính convolution\n","            for (int kh = 0; kh < K; ++kh) {\n","                for (int kw = 0; kw < K; ++kw) {\n","                    int smem_y = ty + 2 * pad - kh;\n","                    int smem_x = tx + 2* pad - kw;\n","\n","                    size_t w_idx = idx4(c_out, c_in, K - 1 - kh, K - 1 - kw, C_in, K, K);\n","                    value += s_dY[smem_y][smem_x] * weight[w_idx];\n","                }\n","            }\n","        __syncthreads();\n","    }\n","\n","    if (h_out < H && w_out < W && n < N) {\n","        dX[idx4(n, c_in, h_out, w_out, C_in, H, W)] = value;\n","    }\n","}\n","\n","// --------------- Conv2D backward: dW ------------------\n","__global__ void conv2d_backward_weight(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    float dw[K][K];\n","    for (int i = 0; i < K; ++i)\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // Load s_dY\n","                int h_out = block_h * TILE_H + ty;\n","                int w_out = block_w * TILE_W + tx;\n","\n","                if (h_out < H_out && w_out < W_out) {\n","                    s_dY[ty][tx] = dY[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)];\n","                } else {\n","                    s_dY[ty][tx] = 0.0f;\n","                }\n","\n","                // Load s_in\n","                int h_in_base = block_h * TILE_H + ty;\n","                int w_in_base = block_w * TILE_W + tx;\n","\n","                // Main tile\n","                if (h_in_base >= 0 && h_in_base < H && w_in_base >= 0 && w_in_base < W) {\n","                    s_in[ty + pad][tx + pad] = input[idx4(n, c_in, h_in_base, w_in_base, C_in, H, W)];\n","                } else {\n","                    s_in[ty + pad][tx + pad] = 0.0f;\n","                }\n","\n","                // Top/bottom borders\n","                if (ty < pad) {\n","                    int h_top = block_h * TILE_H - pad + ty;\n","                    if (h_top >= 0 && h_top < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[ty][tx + pad] = input[idx4(n, c_in, h_top, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[ty][tx + pad] = 0.0f;\n","                    }\n","\n","                    int h_bottom = block_h * TILE_H + TILE_H + pad - 1 - ty;\n","                    if (h_bottom >= 0 && h_bottom < H && w_in_base >= 0 && w_in_base < W) {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = input[idx4(n, c_in, h_bottom, w_in_base, C_in, H, W)];\n","                    } else {\n","                        s_in[BLOCK_H - 1 - ty][tx + pad] = 0.0f;\n","                    }\n","                }\n","\n","                // Left/right borders\n","                if (tx < pad) {\n","                    int w_left = block_w * TILE_W - pad + tx;\n","                    if (w_left >= 0 && w_left < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][tx] = input[idx4(n, c_in, h_in_base, w_left, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][tx] = 0.0f;\n","                    }\n","\n","                    int w_right = block_w * TILE_W + TILE_W + pad - 1 - tx;\n","                    if (w_right >= 0 && w_right < W && h_in_base >= 0 && h_in_base < H) {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = input[idx4(n, c_in, h_in_base, w_right, C_in, H, W)];\n","                    } else {\n","                        s_in[ty + pad][BLOCK_W - 1 - tx] = 0.0f;\n","                    }\n","                }\n","\n","                // Thread (0,0) loads 4 corners\n","                if (tx == 0 && ty == 0) {\n","                    int h_c = block_h * TILE_H - pad;\n","                    int w_c = block_w * TILE_W - pad;\n","                    s_in[0][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[0][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    h_c = block_h * TILE_H + TILE_H + pad - 1;\n","                    w_c = block_w * TILE_W - pad;\n","                    s_in[BLOCK_H - 1][0] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","\n","                    w_c = block_w * TILE_W + TILE_W + pad - 1;\n","                    s_in[BLOCK_H - 1][BLOCK_W - 1] = (h_c >= 0 && h_c < H && w_c >= 0 && w_c < W)\n","                        ? input[idx4(n, c_in, h_c, w_c, C_in, H, W)] : 0.0f;\n","                }\n","\n","                __syncthreads();\n","\n","                // Compute dW: X * dY\n","                float val_dy = s_dY[ty][tx];\n","                for (int kh = 0; kh < K; ++kh) {\n","                    for (int kw = 0; kw < K; ++kw) {\n","                        dw[kh][kw] += s_in[ty + kh][tx + kw] * val_dy;\n","                    }\n","                }\n","\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    for (int i = 0; i < K; ++i) {\n","        for (int j = 0; j < K; ++j) {\n","            // Tính index global của dW[i][j] cho cặp filter (c_out, c_in)\n","            size_t dw_idx = idx4(c_out, c_in, i, j, C_in, K, K);\n","            // Cộng giá trị dw[i][j] của thread hiện tại vào bộ nhớ global\n","            atomicAdd(&dW[dw_idx], dw[i][j]);\n","        }\n","    }\n","}\n","\n","// --------------- Conv2D backward: dB ------------------\n","__global__ void conv2d_backward_bias(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","    extern __shared__ float sdata[];\n","    int tid = threadIdx.x;\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","    float sum = 0.0f;\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","    sdata[tid] = sum;\n","    __syncthreads();\n","\n","    // Reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        dB[c] = sdata[0];\n","    }\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwtKEs2zf8SG","executionInfo":{"status":"ok","timestamp":1765961517510,"user_tz":-420,"elapsed":25,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"89aa704d-4a29-47ef-9d32-0fb6496067a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt1.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers_opt1.h\"\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// This autoencoder matches the project architecture exactly.\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0; ///\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights, biases ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7QaMX64f8_Y","executionInfo":{"status":"ok","timestamp":1765961518367,"user_tz":-420,"elapsed":25,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"066f9451-2395-490d-8fc1-091be0aa8cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt1.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt1.cu\n","#include \"gpu_autoencoder_opt1.h\"\n","#include <cmath>\n","\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","    // find max weight bytes\n","    size_t max_w_bytes = w1_bytes;\n","    if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","    if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","    if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","    if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","    // find max bias bytes\n","    size_t max_b_bytes = b1_bytes;\n","    if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","    if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","    if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","    if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","    float *h_w = (float*)malloc(max_w_bytes);\n","    float *h_b = (float*)malloc(max_b_bytes);\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ---------- [batch_size, channels, height, width]\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // ------------- copy input to device -------------\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ========= ENCODER =========\n","    // conv1: 3 -> 256, same 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16, then pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT is ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","    // conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b3, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv4: 128 -> 256, 16x16, then upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b4, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // conv5: 256 -> 3, 32x32 (no activation, usually MSE on raw output)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b5, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","    }\n","\n","    // ------------- (optional) compute MSE loss -------------\n","    float loss_value = 0.0f;\n","        if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        // kernel giờ trả về SUM(diff^2) vào d_loss\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;  // MSE = sum / size\n","    }\n","\n","\n","    // ------------- copy output back to host -------------\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H; // 32\n","    const int W0 = ae->W; // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Zero all gradient buffers\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        // dU2\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW5\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_u2, ae->d_gout, ae->d_gw5, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB5\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gout, ae->d_gb5, N, C_out, H, W);\n","\n","        // SGD update W5,B5\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward: U2 <- H4 =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;   // H4 size\n","        int Hu = 32, Wu = 32; // U2 size\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int C = 256, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward: 128->256, 16x16 (input U1, output H4) =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        // dU1\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW4\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_u1, ae->d_gh4, ae->d_gw4, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB4\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh4, ae->d_gb4, N, C_out, H, W);\n","\n","        // SGD W4,B4\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward: U1 <- H3 =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;   // H3 size\n","        int Hu = 16, Wu = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int C = 128, H = 8, W = 8;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward: 128->128, 8x8 (input P2, output H3) =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        // dP2\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW3\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        CHECK_CUDA(cudaMemset(ae->d_gw3, 0, C_out * C_in * K * K * sizeof(float)));\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_p2, ae->d_gh3, ae->d_gw3, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB3\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh3, ae->d_gb3, N, C_out, H, W);\n","\n","        // SGD W3,B3\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward: P2 <- H2 =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16; // H2 size\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int C = 128, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward: 256->128, 16x16 (input P1, output H2) =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        // dP1\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW2\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        CHECK_CUDA(cudaMemset(ae->d_gw2, 0, C_out * C_in * K * K * sizeof(float)));\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_p1, ae->d_gh2, ae->d_gw2, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB2\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh2, ae->d_gb2, N, C_out, H, W);\n","\n","        // SGD W2,B2\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward: P1 <- H1 =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32; // H1 size\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1,\n","            N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int C = 256, H = 32, W = 32;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward: 3->256, 32x32 (input X0, output H1) =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        // dX0 (không dùng tiếp nhưng tính cho đủ)\n","        dim3 gridIn(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C_in);\n","\n","        conv2d_backward_input<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW1\n","        dim3 blockW(16, 16);\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight<<<gridW, blockW>>>(ae->d_x0, ae->d_gh1, ae->d_gw1, N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB1\n","        dim3 blockB(256);\n","        dim3 gridB(C_out);\n","        size_t shmem_size = 256 * sizeof(float);\n","        conv2d_backward_bias<<<gridB, blockB, shmem_size>>>(ae->d_gh1, ae->d_gb1, N, C_out, H, W);\n","\n","        // SGD W1,B1\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // copy input [N_batch, 3, 32, 32] lên GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, rồi ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, rồi ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","\n","         cudaError_t err = cudaGetLastError();\n","   if (err != cudaSuccess) {\n","       fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n","   }\n","   cudaDeviceSynchronize();\n","    }\n","    cudaDeviceSynchronize();\n","\n","size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-_KJplmf9P4","executionInfo":{"status":"ok","timestamp":1765961519320,"user_tz":-420,"elapsed":26,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"d2706746-d6a6-44c5-acc8-f5634f655035"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt1.cu\n"]}]},{"cell_type":"code","source":["%%writefile verify_cpu_gpuOpt1_output.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","extern \"C\" {\n","    #include \"cpu_autoencoder.h\"\n","}\n","\n","#include \"gpu_autoencoder_opt1.h\"\n","\n","// Utility function to compare arrays\n","void compare_arrays(const char* name, const float* a, const float* b, int n) {\n","    double max_abs = 0.0;\n","    double sum_abs = 0.0;\n","    double sum_sq  = 0.0;\n","    int max_idx = 0;\n","\n","    for (int i = 0; i < n; ++i) {\n","        double diff = (double)a[i] - (double)b[i];\n","        double ad   = fabs(diff);\n","\n","        if (ad > max_abs) {\n","            max_abs = ad;\n","            max_idx = i;\n","        }\n","        sum_abs += ad;\n","        sum_sq  += diff * diff;\n","    }\n","\n","    double mean_abs = sum_abs / n;\n","    double rmse     = sqrt(sum_sq / n);\n","\n","    printf(\"[%s]\\n\", name);\n","    printf(\"  max|diff| = %.6e (at index %d: CPU=%.6f, GPU_Opt1=%.6f)\\n\",\n","           max_abs, max_idx, a[max_idx], b[max_idx]);\n","    printf(\"  mean|diff| = %.6e\\n\", mean_abs);\n","    printf(\"  RMSE = %.6e\\n\", rmse);\n","\n","    // Check if differences are acceptable\n","    if (max_abs < 1e-4 && rmse < 1e-5) {\n","        printf(\"  ✓ PASSED (excellent match)\\n\");\n","    } else if (max_abs < 1e-3 && rmse < 1e-4) {\n","        printf(\"  ✓ PASSED (good match)\\n\");\n","    } else if (max_abs < 1e-2 && rmse < 1e-3) {\n","        printf(\"  ~ ACCEPTABLE (minor differences)\\n\");\n","    } else {\n","        printf(\"  ✗ FAILED (significant differences)\\n\");\n","    }\n","}\n","\n","void copy_weights_cpu_to_gpu(CPUAutoEncoder* cpu_ae, GPUAutoencoder* gpu_ae) {\n","    // Helper function to copy weights from CPU to GPU_Opt1 autoencoder\n","    gpu_autoencoder_copy_weights_to_device(gpu_ae,\n","        cpu_ae->w1, cpu_ae->b1,\n","        cpu_ae->w2, cpu_ae->b2,\n","        cpu_ae->w3, cpu_ae->b3,\n","        cpu_ae->w4, cpu_ae->b4,\n","        cpu_ae->w5, cpu_ae->b5);\n","}\n","\n","void verify_autoencoder_output_opt1() {\n","    printf(\"==================== Verify Autoencoder CPU vs GPU_Opt1 ====================\\n\\n\");\n","\n","    int batch_size = 4;\n","    double learning_rate = 0.001;\n","    int input_size = batch_size * 3 * 32 * 32;\n","\n","    // Initialize random input\n","    printf(\"Initializing input data...\\n\");\n","    float *h_input = (float*)malloc(input_size * sizeof(float));\n","    for (int i = 0; i < input_size; ++i) {\n","        h_input[i] = ((float)(i % 101) - 50.0f) / 100.0f;  // [-0.5, 0.5]\n","    }\n","\n","    // ===== Initialize CPU Autoencoder =====\n","    printf(\"Initializing CPU autoencoder...\\n\");\n","    CPUAutoEncoder cpu_ae;\n","    initialize_autoencoder(&cpu_ae, batch_size, learning_rate);\n","\n","    // Copy input to CPU autoencoder\n","    memcpy(cpu_ae.batch_input, h_input, input_size * sizeof(float));\n","\n","    // ===== Initialize GPU_Opt1 Autoencoder =====\n","    printf(\"Initializing GPU_Opt1 autoencoder...\\n\");\n","    GPUAutoencoder gpu_ae;\n","    gpu_autoencoder_init(&gpu_ae, batch_size);\n","\n","    // Copy weights from CPU to GPU_Opt1 (để đảm bảo cùng weights)\n","    printf(\"Copying weights from CPU to GPU_Opt1...\\n\");\n","    copy_weights_cpu_to_gpu(&cpu_ae, &gpu_ae);\n","\n","    // ===== CPU Forward Pass =====\n","    printf(\"\\nRunning CPU forward pass...\\n\");\n","    forward_autoencoder(&cpu_ae);\n","\n","    // Calculate CPU loss\n","    float loss_cpu = MSE(cpu_ae.batch_input, cpu_ae.final_output, input_size);\n","    printf(\"CPU Loss: %.6f\\n\", loss_cpu);\n","\n","    // ===== GPU_Opt1 Forward Pass =====\n","    printf(\"\\nRunning GPU_Opt1 forward pass...\\n\");\n","    float *h_output_gpu = (float*)malloc(input_size * sizeof(float));\n","    float loss_gpu = gpu_autoencoder_forward(&gpu_ae, h_input, h_output_gpu, true);\n","    printf(\"GPU_Opt1 Loss: %.6f\\n\", loss_gpu);\n","\n","    // ===== Compare Results =====\n","    printf(\"\\n==================== Comparison Results ====================\\n\\n\");\n","\n","    printf(\"Loss Comparison:\\n\");\n","    printf(\"  CPU Loss:      %.6f\\n\", loss_cpu);\n","    printf(\"  GPU_Opt1 Loss: %.6f\\n\", loss_gpu);\n","    printf(\"  Abs Diff:      %.6e\\n\", fabs(loss_cpu - loss_gpu));\n","    printf(\"  Rel Diff:      %.6e%%\\n\\n\", fabs(loss_cpu - loss_gpu) / loss_cpu * 100.0);\n","\n","    // Compare final output\n","    compare_arrays(\"Final Output (Reconstructed Image)\",\n","                   cpu_ae.final_output, h_output_gpu, input_size);\n","\n","    // ===== Compare Intermediate Activations =====\n","    printf(\"\\n--- Intermediate Activation Comparisons ---\\n\\n\");\n","\n","    // 1. Conv1 output: [batch, 256, 32, 32]\n","    int conv1_size = batch_size * 256 * 32 * 32;\n","    float *h_conv1_gpu = (float*)malloc(conv1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv1_gpu, gpu_ae.d_h1, conv1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv1 Output (h1)\", cpu_ae.conv1_output, h_conv1_gpu, conv1_size);\n","\n","    printf(\"\\n\");\n","\n","    // 2. Pool1 output: [batch, 256, 16, 16]\n","    int pool1_size = batch_size * 256 * 16 * 16;\n","    float *h_pool1_gpu = (float*)malloc(pool1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_pool1_gpu, gpu_ae.d_p1, pool1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Pool1 Output (p1)\", cpu_ae.pool1_output, h_pool1_gpu, pool1_size);\n","\n","    printf(\"\\n\");\n","\n","    // 3. Conv2 output: [batch, 128, 16, 16]\n","    int conv2_size = batch_size * 128 * 16 * 16;\n","    float *h_conv2_gpu = (float*)malloc(conv2_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv2_gpu, gpu_ae.d_h2, conv2_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv2 Output (h2)\", cpu_ae.conv2_output, h_conv2_gpu, conv2_size);\n","\n","    printf(\"\\n\");\n","\n","    // 4. LATENT: Pool2 output: [batch, 128, 8, 8]\n","    int latent_size = batch_size * 128 * 8 * 8;\n","    float *h_latent_gpu = (float*)malloc(latent_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_latent_gpu, gpu_ae.d_p2, latent_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"LATENT (Pool2/p2)\", cpu_ae.pool2_output, h_latent_gpu, latent_size);\n","\n","    printf(\"\\n\");\n","\n","    // 5. Conv3 output: [batch, 128, 8, 8]\n","    int conv3_size = batch_size * 128 * 8 * 8;\n","    float *h_conv3_gpu = (float*)malloc(conv3_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv3_gpu, gpu_ae.d_h3, conv3_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv3 Output (h3)\", cpu_ae.conv3_output, h_conv3_gpu, conv3_size);\n","\n","    printf(\"\\n\");\n","\n","    // 6. Upsample1 output: [batch, 128, 16, 16]\n","    int up1_size = batch_size * 128 * 16 * 16;\n","    float *h_up1_gpu = (float*)malloc(up1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_up1_gpu, gpu_ae.d_u1, up1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Upsample1 Output (u1)\", cpu_ae.upsample1_output, h_up1_gpu, up1_size);\n","\n","    printf(\"\\n\");\n","\n","    // 7. Conv4 output: [batch, 256, 16, 16]\n","    int conv4_size = batch_size * 256 * 16 * 16;\n","    float *h_conv4_gpu = (float*)malloc(conv4_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv4_gpu, gpu_ae.d_h4, conv4_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv4 Output (h4)\", cpu_ae.conv4_output, h_conv4_gpu, conv4_size);\n","\n","    printf(\"\\n\");\n","\n","    // 8. Upsample2 output: [batch, 256, 32, 32]\n","    int up2_size = batch_size * 256 * 32 * 32;\n","    float *h_up2_gpu = (float*)malloc(up2_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_up2_gpu, gpu_ae.d_u2, up2_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Upsample2 Output (u2)\", cpu_ae.upsample2_output, h_up2_gpu, up2_size);\n","\n","    // ===== Summary =====\n","    printf(\"\\n==================== Summary ====================\\n\");\n","    printf(\"Architecture verified (GPU_Opt1):\\n\");\n","    printf(\"  Input:      [%d, 3, 32, 32]\\n\", batch_size);\n","    printf(\"  Conv1:      [%d, 256, 32, 32] (Opt1: shared memory)\\n\", batch_size);\n","    printf(\"  Pool1:      [%d, 256, 16, 16]\\n\", batch_size);\n","    printf(\"  Conv2:      [%d, 128, 16, 16] (Opt1: shared memory)\\n\", batch_size);\n","    printf(\"  LATENT:     [%d, 128, 8, 8]  ← bottleneck\\n\", batch_size);\n","    printf(\"  Conv3:      [%d, 128, 8, 8]  (Opt1: shared memory)\\n\", batch_size);\n","    printf(\"  Upsample1:  [%d, 128, 16, 16]\\n\", batch_size);\n","    printf(\"  Conv4:      [%d, 256, 16, 16] (Opt1: shared memory)\\n\", batch_size);\n","    printf(\"  Upsample2:  [%d, 256, 32, 32]\\n\", batch_size);\n","    printf(\"  Output:     [%d, 3, 32, 32]  (Opt1: shared memory)\\n\", batch_size);\n","    printf(\"\\n\");\n","    printf(\"Optimizations in Opt1:\\n\");\n","    printf(\"  - Shared memory for convolution input tiles\\n\");\n","    printf(\"  - Reduced global memory accesses\\n\");\n","    printf(\"  - Better memory coalescing\\n\");\n","\n","    // Cleanup\n","    free(h_input);\n","    free(h_output_gpu);\n","    free(h_conv1_gpu);\n","    free(h_pool1_gpu);\n","    free(h_conv2_gpu);\n","    free(h_latent_gpu);\n","    free(h_conv3_gpu);\n","    free(h_up1_gpu);\n","    free(h_conv4_gpu);\n","    free(h_up2_gpu);\n","\n","    free_autoencoder(&cpu_ae);\n","    gpu_autoencoder_free(&gpu_ae);\n","\n","    printf(\"\\n==================== Verification Complete ====================\\n\");\n","}\n","\n","int main() {\n","    verify_autoencoder_output_opt1();\n","    CHECK_CUDA(cudaDeviceReset());\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Urb2ed-fpAf","executionInfo":{"status":"ok","timestamp":1765961766413,"user_tz":-420,"elapsed":50,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"b82f82ce-948a-46da-8f00-177b93e47688"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting verify_cpu_gpuOpt1_output.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 \\\n","  verify_cpu_gpuOpt1_output.cu gpu_layers_opt1.cu cpu_layers.c gpu_autoencoder_opt1.cu cpu_autoencoder.c \\\n","  -o verify_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1O9i1T0f38O","executionInfo":{"status":"ok","timestamp":1765961773630,"user_tz":-420,"elapsed":5262,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"f01970e5-a376-4f29-b5a8-dd44eb722ba9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(530)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(530)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(592)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(592)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(462)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[Kcpu_autoencoder.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcpu_load_weights\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kcpu_autoencoder.c:340:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  340 |     \u001b[01;35m\u001b[Kfread(autoencoder->w1, sizeof(float), 256*3*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:341:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  341 |     \u001b[01;35m\u001b[Kfread(autoencoder->b1, sizeof(float), 256, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:342:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  342 |     \u001b[01;35m\u001b[Kfread(autoencoder->w2, sizeof(float), 128*256*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:343:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  343 |     \u001b[01;35m\u001b[Kfread(autoencoder->b2, sizeof(float), 128, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:344:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  344 |     \u001b[01;35m\u001b[Kfread(autoencoder->w3, sizeof(float), 128*128*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:345:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  345 |     \u001b[01;35m\u001b[Kfread(autoencoder->b3, sizeof(float), 128, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:346:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  346 |     \u001b[01;35m\u001b[Kfread(autoencoder->w4, sizeof(float), 256*128*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:347:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  347 |     \u001b[01;35m\u001b[Kfread(autoencoder->b4, sizeof(float), 256, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:348:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  348 |     \u001b[01;35m\u001b[Kfread(autoencoder->w5, sizeof(float), 3*256*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:349:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  349 |     \u001b[01;35m\u001b[Kfread(autoencoder->b5, sizeof(float), 3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"]}]},{"cell_type":"code","source":["!./verify_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpyM1OjDf5JB","executionInfo":{"status":"ok","timestamp":1765961782593,"user_tz":-420,"elapsed":1513,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"8224f908-f500-4f0b-c79d-b9f47b2a9239"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================== Verify Autoencoder CPU vs GPU_Opt1 ====================\n","\n","Initializing input data...\n","Initializing CPU autoencoder...\n","Initializing GPU_Opt1 autoencoder...\n","Copying weights from CPU to GPU_Opt1...\n","\n","Running CPU forward pass...\n","CPU Loss: 0.088634\n","\n","Running GPU_Opt1 forward pass...\n","GPU_Opt1 Loss: 0.088634\n","\n","==================== Comparison Results ====================\n","\n","Loss Comparison:\n","  CPU Loss:      0.088634\n","  GPU_Opt1 Loss: 0.088634\n","  Abs Diff:      1.266599e-07\n","  Rel Diff:      1.429018e-04%\n","\n","[Final Output (Reconstructed Image)]\n","  max|diff| = 1.937151e-07 (at index 4818: CPU=0.063446, GPU_Opt1=0.063446)\n","  mean|diff| = 3.140174e-08\n","  RMSE = 4.040118e-08\n","  ✓ PASSED (excellent match)\n","\n","--- Intermediate Activation Comparisons ---\n","\n","[Conv1 Output (h1)]\n","  max|diff| = 2.980232e-08 (at index 21609: CPU=0.091961, GPU_Opt1=0.091961)\n","  mean|diff| = 1.203794e-09\n","  RMSE = 2.941030e-09\n","  ✓ PASSED (excellent match)\n","\n","[Pool1 Output (p1)]\n","  max|diff| = 2.980232e-08 (at index 5396: CPU=0.091961, GPU_Opt1=0.091961)\n","  mean|diff| = 1.985855e-09\n","  RMSE = 4.022049e-09\n","  ✓ PASSED (excellent match)\n","\n","[Conv2 Output (h2)]\n","  max|diff| = 1.192093e-07 (at index 16718: CPU=0.139577, GPU_Opt1=0.139577)\n","  mean|diff| = 8.508680e-09\n","  RMSE = 1.616637e-08\n","  ✓ PASSED (excellent match)\n","\n","[LATENT (Pool2/p2)]\n","  max|diff| = 1.192093e-07 (at index 17872: CPU=0.175958, GPU_Opt1=0.175958)\n","  mean|diff| = 1.228130e-08\n","  RMSE = 2.008743e-08\n","  ✓ PASSED (excellent match)\n","\n","[Conv3 Output (h3)]\n","  max|diff| = 2.235174e-07 (at index 2851: CPU=0.109921, GPU_Opt1=0.109921)\n","  mean|diff| = 1.141401e-08\n","  RMSE = 2.211730e-08\n","  ✓ PASSED (excellent match)\n","\n","[Upsample1 Output (u1)]\n","  max|diff| = 2.235174e-07 (at index 11398: CPU=0.109921, GPU_Opt1=0.109921)\n","  mean|diff| = 1.141401e-08\n","  RMSE = 2.211730e-08\n","  ✓ PASSED (excellent match)\n","\n","[Conv4 Output (h4)]\n","  max|diff| = 1.788139e-07 (at index 157018: CPU=0.107238, GPU_Opt1=0.107238)\n","  mean|diff| = 1.168476e-08\n","  RMSE = 2.119412e-08\n","  ✓ PASSED (excellent match)\n","\n","[Upsample2 Output (u2)]\n","  max|diff| = 1.788139e-07 (at index 628052: CPU=0.107238, GPU_Opt1=0.107238)\n","  mean|diff| = 1.168476e-08\n","  RMSE = 2.119412e-08\n","  ✓ PASSED (excellent match)\n","\n","==================== Summary ====================\n","Architecture verified (GPU_Opt1):\n","  Input:      [4, 3, 32, 32]\n","  Conv1:      [4, 256, 32, 32] (Opt1: shared memory)\n","  Pool1:      [4, 256, 16, 16]\n","  Conv2:      [4, 128, 16, 16] (Opt1: shared memory)\n","  LATENT:     [4, 128, 8, 8]  ← bottleneck\n","  Conv3:      [4, 128, 8, 8]  (Opt1: shared memory)\n","  Upsample1:  [4, 128, 16, 16]\n","  Conv4:      [4, 256, 16, 16] (Opt1: shared memory)\n","  Upsample2:  [4, 256, 32, 32]\n","  Output:     [4, 3, 32, 32]  (Opt1: shared memory)\n","\n","Optimizations in Opt1:\n","  - Shared memory for convolution input tiles\n","  - Reduced global memory accesses\n","  - Better memory coalescing\n","\n","==================== Verification Complete ====================\n"]}]},{"cell_type":"markdown","source":["## 3) Verify CPU and GPU optimization version 2 outputs"],"metadata":{"id":"epR2T0HJL2TA"}},{"cell_type":"code","source":["%%writefile gpu_layers_opt2.h\n","#pragma once\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","#define TILE_W 16\n","#define TILE_H 16\n","// Kích thước Kernel\n","#define K 3\n","#define R (K/2) // Radius = 1\n","#define BLOCK_W (TILE_W + 2 * R)\n","#define BLOCK_H (TILE_H + 2 * R)\n","__constant__ float dc_bias[256];\n","\n","\n","#define CHECK_CUDA(call)                                                \\\n","    do {                                                                \\\n","        cudaError_t err = call;                                         \\\n","        if (err != cudaSuccess) {                                       \\\n","            fprintf(stderr, \"CUDA Error %s:%d: %s\\n\",                   \\\n","                    __FILE__, __LINE__, cudaGetErrorString(err));       \\\n","            exit(EXIT_FAILURE);                                         \\\n","        }                                                               \\\n","    } while (0)\n","\n","// NCHW layout: [N, C, H, W]\n","__device__ __host__ inline int idx4(int n, int c, int h, int w,\n","                                    int C, int H, int W) {\n","    return ((n * C + c) * H + h) * W + w;\n","}\n","\n","// ==== KERNEL DECLARATIONS ====\n","void update_dc_bias(float* d_bias_ptr, int count);\n","__global__ void conv2d_forward_opt2(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    // float* bias,     // [C_out]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void relu_forward(float* x, int size);\n","\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, H/2, W/2]\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,  // [N, C, H, W]\n","    float* __restrict__ output,       // [N, C, 2H, 2W]\n","    int N, int C, int H, int W);\n","\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,   // single float on device\n","    int size);\n","\n","__global__ void relu_backward(\n","    float* __restrict__ x,       // forward output/input to ReLU\n","    float* __restrict__ grad_y,  // dL/dy\n","    float* __restrict__ grad_x,        // dL/dx\n","    int size);\n","\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W);\n","\n","\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size);\n","\n","__global__ void conv2d_backward_input_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ weight,\n","    float* __restrict__ dX,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_weight_opt2(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride);\n","\n","__global__ void conv2d_backward_bias_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out);\n","\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeeCZwlViBOn","executionInfo":{"status":"ok","timestamp":1765962532420,"user_tz":-420,"elapsed":45,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"ddb1dde1-4868-4e46-c4cd-88efb9b826b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt2.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_layers_opt2.cu\n","\n","#include <cuda_runtime.h>\n","#include \"gpu_layers_opt2.h\"\n","\n","// Optimal configurations\n","#define TILE_W 16\n","#define TILE_H 16\n","#define K 3\n","\n","void update_dc_bias(float* d_bias_ptr, int count) {\n","    CHECK_CUDA(cudaMemcpyToSymbol(dc_bias, d_bias_ptr, count * sizeof(float),\n","                                   0, cudaMemcpyDeviceToDevice));\n","}\n","\n","\n","__global__ void conv2d_forward_opt2(\n","    float* __restrict__ input,    // [N, C_in, H, W]\n","    float* __restrict__ weight,   // [C_out, C_in, K, K]\n","    float* __restrict__ output,   // [N, C_out, H_out, W_out]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float smem[BLOCK_H][BLOCK_W];\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","    int tid = ty * blockDim.x + tx;\n","    int num_threads = blockDim.x * blockDim.y;  // 256\n","\n","    int w_out = blockIdx.x * TILE_W + tx;\n","    int h_out = blockIdx.y * TILE_H + ty;\n","\n","    int nc = blockIdx.z;\n","    int c_out = nc % C_out;\n","    int n = nc / C_out;\n","\n","    float value = 0.0f;\n","\n","    // Base position for loading (với padding)\n","    int row_start = blockIdx.y * TILE_H * stride - pad;\n","    int col_start = blockIdx.x * TILE_W * stride - pad;\n","\n","    // Loop over input channels\n","    for (int c_in = 0; c_in < C_in; c_in++) {\n","        // OPTIMIZED: Cooperative loading\n","        // Total elements = 18x18 = 324\n","        // 256 threads → mỗi thread load 1-2 elements\n","        int total_elements = BLOCK_H * BLOCK_W;\n","\n","        for (int idx = tid; idx < total_elements; idx += num_threads) {\n","            int i = idx / BLOCK_W;  // row\n","            int j = idx % BLOCK_W;  // col\n","\n","            int h_in = row_start + i;\n","            int w_in = col_start + j;\n","\n","            // Load with bounds checking\n","            if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {\n","                smem[i][j] = input[idx4(n, c_in, h_in, w_in, C_in, H, W)];\n","            } else {\n","                smem[i][j] = 0.0f;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // OPTIMIZED: Compute convolution with unrolling\n","        if (h_out < H_out && w_out < W_out && n < N) {\n","            int smem_row = ty * stride;\n","            int smem_col = tx * stride;\n","\n","            #pragma unroll\n","            for (int i = 0; i < K; ++i) {\n","                #pragma unroll\n","                for (int j = 0; j < K; ++j) {\n","                    float in_val = smem[smem_row + i][smem_col + j];\n","                    float w_val = weight[idx4(c_out, c_in, i, j, C_in, K, K)];\n","                    value += in_val * w_val;\n","                }\n","            }\n","        }\n","        __syncthreads();\n","    }\n","\n","    // Write output with bias from constant memory\n","    if (w_out < W_out && h_out < H_out && n < N) {\n","        value += dc_bias[c_out];\n","        output[idx4(n, c_out, h_out, w_out, C_out, H_out, W_out)] = value;\n","    }\n","}\n","\n","\n","__global__ void conv2d_backward_input_opt2(\n","    float* __restrict__ dY,     // [N, C_out, H, W]\n","    float* __restrict__ weight, // [C_out, C_in, K, K]\n","    float* __restrict__ dX,     // [N, C_in, H, W]\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    // Only support K=3, pad=1, stride=1\n","    if (stride != 1 || pad != 1 || K != 3) return;\n","\n","    const int H_out = H;\n","    const int W_out = W;\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","\n","    int base_h = blockIdx.y * TILE_H;\n","    int base_w = blockIdx.x * TILE_W;\n","\n","    int h_in = base_h + ty;\n","    int w_in = base_w + tx;\n","\n","    int nc = blockIdx.z;\n","    int c_in = nc % C_in;\n","    int n    = nc / C_in;\n","\n","    if (n >= N || c_in >= C_in) return;\n","\n","    // Shared memory for tile + halo\n","    __shared__ float s_dY[TILE_H + 2][TILE_W + 2];\n","\n","    float value = 0.0f;\n","\n","    for (int c_out = 0; c_out < C_out; ++c_out) {\n","        // ---- Load dY tile (n, c_out, :, :) into shared memory ----\n","        for (int sy = ty; sy < TILE_H + 2; sy += blockDim.y) {\n","            for (int sx = tx; sx < TILE_W + 2; sx += blockDim.x) {\n","                int ho = base_h + sy - 1;  // -pad\n","                int wo = base_w + sx - 1;  // -pad\n","\n","                float v = 0.0f;\n","                if (ho >= 0 && ho < H_out && wo >= 0 && wo < W_out) {\n","                    v = dY[idx4(n, c_out, ho, wo, C_out, H_out, W_out)];\n","                }\n","                s_dY[sy][sx] = v;\n","            }\n","        }\n","        __syncthreads();\n","\n","        // ---- Compute gradient for (n, c_in, h_in, w_in) ----\n","        if (h_in < H && w_in < W) {\n","            #pragma unroll\n","            for (int kh = 0; kh < K; ++kh) {\n","                #pragma unroll\n","                for (int kw = 0; kw < K; ++kw) {\n","                    // Shared memory indices\n","                    int sy = ty + 2 - kh;\n","                    int sx = tx + 2 - kw;\n","\n","                    float dy = s_dY[sy][sx];\n","\n","                    // Flip the kernel indices\n","                    int kh_flip = K - 1 - kh;  // 2, 1, 0\n","                    int kw_flip = K - 1 - kw;  // 2, 1, 0\n","\n","                    // Weight layout [C_out, C_in, K, K]\n","                    float w = weight[idx4(c_out, c_in, kh_flip, kw_flip, C_in, K, K)];\n","                    value += dy * w;\n","                }\n","            }\n","        }\n","\n","        __syncthreads();\n","    }\n","\n","    if (h_in < H && w_in < W) {\n","        dX[idx4(n, c_in, h_in, w_in, C_in, H, W)] = value;\n","    }\n","}\n","\n","__global__ void conv2d_backward_weight_opt2(\n","    float* __restrict__ input,\n","    float* __restrict__ dY,\n","    float* __restrict__ dW,\n","    int N, int C_in, int H, int W,\n","    int C_out, int pad, int stride)\n","{\n","    int H_out = (H + 2 * pad - K) / stride + 1;\n","    int W_out = (W + 2 * pad - K) / stride + 1;\n","\n","    __shared__ float s_in[BLOCK_H][BLOCK_W];\n","    __shared__ float s_dY[TILE_H][TILE_W];\n","    __shared__ float s_dw[K * K][256];  // Reduction buffer\n","\n","    int tx = threadIdx.x;\n","    int ty = threadIdx.y;\n","    int tid = ty * blockDim.x + tx;\n","    int num_threads = blockDim.x * blockDim.y;\n","\n","    int index = blockIdx.z;\n","    int c_in = index % C_in;\n","    int c_out = index / C_in;\n","\n","    // Local accumulator\n","    float dw[K][K];\n","    #pragma unroll\n","    for (int i = 0; i < K; ++i)\n","        #pragma unroll\n","        for (int j = 0; j < K; ++j)\n","            dw[i][j] = 0.0f;\n","\n","    for (int n = 0; n < N; ++n) {\n","        int num_blocks_h = (H_out + TILE_H - 1) / TILE_H;\n","        int num_blocks_w = (W_out + TILE_W - 1) / TILE_W;\n","\n","        for (int block_h = 0; block_h < num_blocks_h; ++block_h) {\n","            for (int block_w = 0; block_w < num_blocks_w; ++block_w) {\n","\n","                // Cooperative loading for dY\n","                int h_out = block_h * TILE_H;\n","                int w_out = block_w * TILE_W;\n","\n","                for (int idx = tid; idx < TILE_H * TILE_W; idx += num_threads) {\n","                    int i = idx / TILE_W;\n","                    int j = idx % TILE_W;\n","                    int h = h_out + i;\n","                    int w = w_out + j;\n","\n","                    if (h < H_out && w < W_out) {\n","                        s_dY[i][j] = dY[idx4(n, c_out, h, w, C_out, H_out, W_out)];\n","                    } else {\n","                        s_dY[i][j] = 0.0f;\n","                    }\n","                }\n","\n","                // Cooperative loading for input with halo\n","                int h_in_base = block_h * TILE_H - pad;\n","                int w_in_base = block_w * TILE_W - pad;\n","\n","                for (int idx = tid; idx < BLOCK_H * BLOCK_W; idx += num_threads) {\n","                    int i = idx / BLOCK_W;\n","                    int j = idx % BLOCK_W;\n","                    int h = h_in_base + i;\n","                    int w = w_in_base + j;\n","\n","                    if (h >= 0 && h < H && w >= 0 && w < W) {\n","                        s_in[i][j] = input[idx4(n, c_in, h, w, C_in, H, W)];\n","                    } else {\n","                        s_in[i][j] = 0.0f;\n","                    }\n","                }\n","                __syncthreads();\n","\n","                // Compute local dW\n","                if (ty < TILE_H && tx < TILE_W) {\n","                    float val_dy = s_dY[ty][tx];\n","                    #pragma unroll\n","                    for (int kh = 0; kh < K; ++kh) {\n","                        #pragma unroll\n","                        for (int kw = 0; kw < K; ++kw) {\n","                            dw[kh][kw] += s_in[ty * stride + kh][tx * stride + kw] * val_dy;\n","                        }\n","                    }\n","                }\n","                __syncthreads();\n","            }\n","        }\n","    }\n","\n","    // Block-level reduction BEFORE atomic\n","    // Each kernel element has its own reduction\n","    #pragma unroll\n","    for (int kh = 0; kh < K; ++kh) {\n","        #pragma unroll\n","        for (int kw = 0; kw < K; ++kw) {\n","            int k_idx = kh * K + kw;\n","\n","            // Store to shared memory\n","            s_dw[k_idx][tid] = dw[kh][kw];\n","            __syncthreads();\n","\n","            // Tree reduction\n","            for (int s = num_threads / 2; s > 0; s >>= 1) {\n","                if (tid < s) {\n","                    s_dw[k_idx][tid] += s_dw[k_idx][tid + s];\n","                }\n","                __syncthreads();\n","            }\n","\n","            // ONLY thread 0 does atomic\n","            if (tid == 0) {\n","                size_t dw_idx = idx4(c_out, c_in, kh, kw, C_in, K, K);\n","                atomicAdd(&dW[dw_idx], s_dw[k_idx][0]);\n","            }\n","        }\n","    }\n","}\n","\n","\n","__global__ void conv2d_backward_bias_opt2(\n","    float* __restrict__ dY,\n","    float* __restrict__ dB,\n","    int N, int C_out, int H_out, int W_out)\n","{\n","    int c = blockIdx.x;\n","    if (c >= C_out) return;\n","\n","    int spatial_size = H_out * W_out;\n","    int channel_size = N * spatial_size;\n","\n","    int tid = threadIdx.x;\n","    int lane = tid % 32;\n","\n","    float sum = 0.0f;\n","\n","    // Grid-stride loop\n","    for (int i = tid; i < channel_size; i += blockDim.x) {\n","        int n = i / spatial_size;\n","        int rem = i % spatial_size;\n","        int global_idx = n * (C_out * spatial_size) + c * spatial_size + rem;\n","        sum += dY[global_idx];\n","    }\n","\n","    // Warp-level reduction\n","    #pragma unroll\n","    for (int offset = 16; offset > 0; offset >>= 1) {\n","        sum += __shfl_down_sync(0xffffffff, sum, offset);\n","    }\n","\n","    // First thread in each warp writes to shared memory\n","    __shared__ float warp_sums[32];\n","    int warp_id = tid / 32;\n","\n","    if (lane == 0) {\n","        warp_sums[warp_id] = sum;\n","    }\n","    __syncthreads();\n","\n","    // Final reduction by first warp\n","    if (warp_id == 0) {\n","        sum = (lane < (blockDim.x / 32)) ? warp_sums[lane] : 0.0f;\n","\n","        #pragma unroll\n","        for (int offset = 16; offset > 0; offset >>= 1) {\n","            sum += __shfl_down_sync(0xffffffff, sum, offset);\n","        }\n","\n","        if (lane == 0) {\n","            dB[c] = sum;\n","        }\n","    }\n","}\n","\n","// --------------- ReLU ------------------\n","__global__ void relu_forward(float* __restrict__ x, int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        float v = x[i];\n","        x[i] = (v > 0.0f) ? v : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 (stride 2) ------------------\n","__global__ void maxpool2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    float m = -1e30f;\n","    for (int dh = 0; dh < 2; dh++) {\n","        for (int dw = 0; dw < 2; dw++) {\n","            int h_in = h_in0 + dh;\n","            int w_in = w_in0 + dw;\n","            int idx = idx4(n, c, h_in, w_in, C, H, W);\n","            float v = input[idx];\n","            if (v > m) m = v;\n","        }\n","    }\n","\n","    int out_idx = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[out_idx] = m;\n","}\n","\n","// --------------- UpSample 2x2 (nearest) ------------------\n","__global__ void upsample2x2_forward(\n","    float* __restrict__ input,\n","    float* __restrict__ output,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in = h_out / 2;\n","    int w_in = w_out / 2;\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    int idx_out = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","    output[idx_out] = input[idx_in];\n","}\n","\n","// --------------- MSE loss ------------------\n","__global__ void mse_loss_forward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ loss,\n","    int size)\n","{\n","    extern __shared__ float sdata[];\n","\n","    int tid = threadIdx.x;\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    float val = 0.0f;\n","    if (idx < size) {\n","        float diff = output[idx] - target[idx];\n","        val = diff * diff;\n","    }\n","    sdata[tid] = val;\n","    __syncthreads();\n","\n","    // Parallel reduction\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        atomicAdd(loss, sdata[0]);\n","    }\n","}\n","\n","// --------------- ReLU backward ------------------\n","__global__ void relu_backward(\n","    float* __restrict__ x,\n","    float* __restrict__ grad_y,\n","    float* __restrict__ grad_x,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        grad_x[i] = (x[i] > 0.0f) ? grad_y[i] : 0.0f;\n","    }\n","}\n","\n","// --------------- MaxPool 2x2 backward ------------------\n","__global__ void maxpool2x2_backward(\n","    float* __restrict__ input,\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H / 2;\n","    int W_out = W / 2;\n","\n","    int w_out = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_out = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_out >= W_out || h_out >= H_out) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_in0 = h_out * 2;\n","    int w_in0 = w_out * 2;\n","\n","    int idx00 = idx4(n, c, h_in0 + 0, w_in0 + 0, C, H, W);\n","    int idx01 = idx4(n, c, h_in0 + 0, w_in0 + 1, C, H, W);\n","    int idx10 = idx4(n, c, h_in0 + 1, w_in0 + 0, C, H, W);\n","    int idx11 = idx4(n, c, h_in0 + 1, w_in0 + 1, C, H, W);\n","\n","    float v00 = input[idx00];\n","    float v01 = input[idx01];\n","    float v10 = input[idx10];\n","    float v11 = input[idx11];\n","\n","    float g = grad_out[idx4(n, c, h_out, w_out, C, H_out, W_out)];\n","\n","    float m = v00;\n","    int max_idx = 0;\n","    if (v01 > m) { m = v01; max_idx = 1; }\n","    if (v10 > m) { m = v10; max_idx = 2; }\n","    if (v11 > m) { m = v11; max_idx = 3; }\n","\n","    if (max_idx == 0) grad_in[idx00] = g;\n","    else if (max_idx == 1) grad_in[idx01] = g;\n","    else if (max_idx == 2) grad_in[idx10] = g;\n","    else grad_in[idx11] = g;\n","}\n","\n","// --------------- UpSample 2x2 backward ------------------\n","__global__ void upsample2x2_backward(\n","    float* __restrict__ grad_out,\n","    float* __restrict__ grad_in,\n","    int N, int C, int H, int W)\n","{\n","    int H_out = H * 2;\n","    int W_out = W * 2;\n","\n","    int w_in = blockIdx.x * blockDim.x + threadIdx.x;\n","    int h_in = blockIdx.y * blockDim.y + threadIdx.y;\n","    int nc = blockIdx.z;\n","\n","    if (w_in >= W || h_in >= H) return;\n","\n","    int n = nc / C;\n","    int c = nc % C;\n","    if (n >= N) return;\n","\n","    int h_out0 = h_in * 2;\n","    int w_out0 = w_in * 2;\n","\n","    float sum = 0.0f;\n","    for (int dh = 0; dh < 2; ++dh) {\n","        for (int dw = 0; dw < 2; ++dw) {\n","            int h_out = h_out0 + dh;\n","            int w_out = w_out0 + dw;\n","            int idx_o = idx4(n, c, h_out, w_out, C, H_out, W_out);\n","            sum += grad_out[idx_o];\n","        }\n","    }\n","\n","    int idx_in = idx4(n, c, h_in, w_in, C, H, W);\n","    grad_in[idx_in] = sum;\n","}\n","\n","// --------------- MSE loss backward ------------------\n","__global__ void mse_loss_backward(\n","    float* __restrict__ output,\n","    float* __restrict__ target,\n","    float* __restrict__ grad_out,\n","    int size)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i >= size) return;\n","    grad_out[i] = 2.0f * (output[i] - target[i]) / size;\n","}\n","\n","// --------------- SGD update ------------------\n","__global__ void sgd_update(\n","    float* __restrict__ param,\n","    float* __restrict__ grad,\n","    int size,\n","    float lr)\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < size) {\n","        param[i] -= lr * grad[i];\n","    }\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTjDF9y4iBF4","executionInfo":{"status":"ok","timestamp":1765962533344,"user_tz":-420,"elapsed":28,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"530e249a-2556-4955-f9ae-c87a4de8bf1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_layers_opt2.cu\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt2.h\n","// header for GPUAutoencoder (the struct + declarations)\n","#pragma once\n","#include \"gpu_layers_opt2.h\"\n","#include <cstdlib>\n","#include <cstdio>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","// latent 128 x 8 x 8\n","static const int AE_LATENT_C   = 128;\n","static const int AE_LATENT_H   = 8;\n","static const int AE_LATENT_W   = 8;\n","static const int AE_LATENT_DIM = AE_LATENT_C * AE_LATENT_H * AE_LATENT_W;\n","\n","// Layout: NCHW [batch, channels, height, width]\n","struct GPUAutoencoder {\n","    int N;   // batch size\n","    int H;   // 32\n","    int W;   // 32;\n","\n","    // --- Conv layer parameters ---\n","    // conv1: 3 -> 256 (3x3)\n","    float *d_w1, *d_b1;\n","    // conv2: 256 -> 128\n","    float *d_w2, *d_b2;\n","    // conv3: 128 -> 128\n","    float *d_w3, *d_b3;\n","    // conv4: 128 -> 256\n","    float *d_w4, *d_b4;\n","    // conv5: 256 -> 3\n","    float *d_w5, *d_b5;\n","\n","    // --- Activations ---\n","    // Input batch\n","    float *d_x0;   // [N, 3, 32, 32]\n","\n","    // Encoder\n","    float *d_h1;   // conv1 out: [N, 256, 32, 32]\n","    float *d_p1;   // pool1   : [N, 256, 16, 16]\n","    float *d_h2;   // conv2   : [N, 128, 16, 16]\n","    float *d_p2;   // pool2   : [N, 128,  8,  8]   (latent)\n","\n","    // Decoder\n","    float *d_h3;   // conv3   : [N, 128,  8,  8]\n","    float *d_u1;   // up1     : [N, 128, 16, 16]\n","    float *d_h4;   // conv4   : [N, 256, 16, 16]\n","    float *d_u2;   // up2     : [N, 256, 32, 32]\n","    float *d_out;  // conv5   : [N,   3, 32, 32]\n","\n","    // Loss buffer\n","    float *d_loss; // single float on device\n","\n","    // ---- gradients for activations ----\n","    float *d_gx0; ///\n","    float *d_gh1;\n","    float *d_gp1;\n","    float *d_gh2;\n","    float *d_gp2;\n","    float *d_gh3;\n","    float *d_gu1;\n","    float *d_gh4;\n","    float *d_gu2;\n","    float *d_gout;\n","\n","    // ---- gradients for weights, biases ----\n","    float *d_gw1, *d_gb1;\n","    float *d_gw2, *d_gb2;\n","    float *d_gw3, *d_gb3;\n","    float *d_gw4, *d_gb4;\n","    float *d_gw5, *d_gb5;\n","};\n","\n","// API\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size);\n","void gpu_autoencoder_free(GPUAutoencoder *ae);\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5);\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5);\n","\n","\n","// Forward on GPU:\n","//   h_input  : host pointer [N * 3 * 32 * 32]\n","//   h_output : host pointer [N * 3 * 32 * 32]\n","//   returns loss value (MSE(x_hat, x)) if compute_loss=true;\n","//   otherwise returns 0.0f.\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss = true);\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr);\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename);\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename);\n","\n","// encode only: lấy latent [N_batch, 128, 8, 8] -> h_latent [N_batch, AE_LATENT_DIM]\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twqOq9XKiA-2","executionInfo":{"status":"ok","timestamp":1765962534148,"user_tz":-420,"elapsed":6,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"cb9bdb48-f8b0-4a74-cd26-39a7f2824160"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt2.h\n"]}]},{"cell_type":"code","source":["%%writefile gpu_autoencoder_opt2.cu\n","#include \"gpu_autoencoder_opt2.h\"\n","#include <cmath>\n","\n","static inline float rand_uniform(float min_val, float max_val) {\n","    float r = (float)rand() / (float)RAND_MAX;\n","    return min_val + r * (max_val - min_val);\n","}\n","\n","void gpu_autoencoder_init(GPUAutoencoder *ae, int batch_size) {\n","    ae->N = batch_size;\n","    ae->H = 32;\n","    ae->W = 32;\n","\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","\n","    // ---------- allocate weights ----------\n","\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_w1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_w5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_b5, b5_bytes));\n","\n","    // init weights on host\n","    // find max weight bytes\n","    size_t max_w_bytes = w1_bytes;\n","    if (w2_bytes > max_w_bytes) max_w_bytes = w2_bytes;\n","    if (w3_bytes > max_w_bytes) max_w_bytes = w3_bytes;\n","    if (w4_bytes > max_w_bytes) max_w_bytes = w4_bytes;\n","    if (w5_bytes > max_w_bytes) max_w_bytes = w5_bytes;\n","\n","    // find max bias bytes\n","    size_t max_b_bytes = b1_bytes;\n","    if (b2_bytes > max_b_bytes) max_b_bytes = b2_bytes;\n","    if (b3_bytes > max_b_bytes) max_b_bytes = b3_bytes;\n","    if (b4_bytes > max_b_bytes) max_b_bytes = b4_bytes;\n","    if (b5_bytes > max_b_bytes) max_b_bytes = b5_bytes;\n","\n","    float *h_w = (float*)malloc(max_w_bytes);\n","    float *h_b = (float*)malloc(max_b_bytes);\n","\n","    auto init_wb = [&](float *d_w, size_t w_bytes, float *d_b, size_t b_bytes) {\n","        size_t w_cnt = w_bytes / sizeof(float);\n","        size_t b_cnt = b_bytes / sizeof(float);\n","        for (size_t i = 0; i < w_cnt; ++i) h_w[i] = rand_uniform(-0.05f, 0.05f);\n","        for (size_t i = 0; i < b_cnt; ++i) h_b[i] = rand_uniform(-0.05f, 0.05f);\n","        CHECK_CUDA(cudaMemcpy(d_w, h_w, w_bytes, cudaMemcpyHostToDevice));\n","        CHECK_CUDA(cudaMemcpy(d_b, h_b, b_bytes, cudaMemcpyHostToDevice));\n","    };\n","\n","    init_wb(ae->d_w1, w1_bytes, ae->d_b1, b1_bytes);\n","    init_wb(ae->d_w2, w2_bytes, ae->d_b2, b2_bytes);\n","    init_wb(ae->d_w3, w3_bytes, ae->d_b3, b3_bytes);\n","    init_wb(ae->d_w4, w4_bytes, ae->d_b4, b4_bytes);\n","    init_wb(ae->d_w5, w5_bytes, ae->d_b5, b5_bytes);\n","\n","    free(h_w);\n","    free(h_b);\n","\n","    // ---------- allocate activations ---------- [batch_size, channels, height, width]\n","    size_t bytes_x0  = N * 3   * 32 * 32 * sizeof(float);\n","    size_t bytes_h1  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_p1  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_h2  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_p2  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_h3  = N * 128 *  8 *  8 * sizeof(float);\n","    size_t bytes_u1  = N * 128 * 16 * 16 * sizeof(float);\n","    size_t bytes_h4  = N * 256 * 16 * 16 * sizeof(float);\n","    size_t bytes_u2  = N * 256 * 32 * 32 * sizeof(float);\n","    size_t bytes_out = N * 3   * 32 * 32 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMalloc(&ae->d_x0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_p2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_h4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_u2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_out, bytes_out));\n","\n","    // loss buffer\n","    CHECK_CUDA(cudaMalloc(&ae->d_loss, sizeof(float)));\n","\n","    // ---------- allocate activation gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gx0,  bytes_x0));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh1,  bytes_h1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp1,  bytes_p1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh2,  bytes_h2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gp2,  bytes_p2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh3,  bytes_h3));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu1,  bytes_u1));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gh4,  bytes_h4));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gu2,  bytes_u2));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gout, bytes_out));\n","\n","    // ---------- allocate weight gradients ----------\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw1, w1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb1, b1_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw2, w2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb2, b2_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw3, w3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb3, b3_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw4, w4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb4, b4_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gw5, w5_bytes));\n","    CHECK_CUDA(cudaMalloc(&ae->d_gb5, b5_bytes));\n","}\n","\n","void gpu_autoencoder_free(GPUAutoencoder *ae) {\n","    // weights\n","    cudaFree(ae->d_w1); cudaFree(ae->d_b1);\n","    cudaFree(ae->d_w2); cudaFree(ae->d_b2);\n","    cudaFree(ae->d_w3); cudaFree(ae->d_b3);\n","    cudaFree(ae->d_w4); cudaFree(ae->d_b4);\n","    cudaFree(ae->d_w5); cudaFree(ae->d_b5);\n","\n","    // activations\n","    cudaFree(ae->d_x0);\n","    cudaFree(ae->d_h1);\n","    cudaFree(ae->d_p1);\n","    cudaFree(ae->d_h2);\n","    cudaFree(ae->d_p2);\n","    cudaFree(ae->d_h3);\n","    cudaFree(ae->d_u1);\n","    cudaFree(ae->d_h4);\n","    cudaFree(ae->d_u2);\n","    cudaFree(ae->d_out);\n","\n","    cudaFree(ae->d_loss);\n","\n","    // activation gradients\n","    cudaFree(ae->d_gx0);\n","    cudaFree(ae->d_gh1);\n","    cudaFree(ae->d_gp1);\n","    cudaFree(ae->d_gh2);\n","    cudaFree(ae->d_gp2);\n","    cudaFree(ae->d_gh3);\n","    cudaFree(ae->d_gu1);\n","    cudaFree(ae->d_gh4);\n","    cudaFree(ae->d_gu2);\n","    cudaFree(ae->d_gout);\n","\n","    // weight gradients\n","    cudaFree(ae->d_gw1); cudaFree(ae->d_gb1);\n","    cudaFree(ae->d_gw2); cudaFree(ae->d_gb2);\n","    cudaFree(ae->d_gw3); cudaFree(ae->d_gb3);\n","    cudaFree(ae->d_gw4); cudaFree(ae->d_gb4);\n","    cudaFree(ae->d_gw5); cudaFree(ae->d_gb5);\n","}\n","\n","void gpu_autoencoder_copy_weights_to_host(\n","    GPUAutoencoder *ae,\n","    float *h_w1, float *h_b1,\n","    float *h_w2, float *h_b2,\n","    float *h_w3, float *h_b3,\n","    float *h_w4, float *h_b4,\n","    float *h_w5, float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(h_w1, ae->d_w1, w1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b1, ae->d_b1, b1_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w2, ae->d_w2, w2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b2, ae->d_b2, b2_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w3, ae->d_w3, w3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b3, ae->d_b3, b3_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w4, ae->d_w4, w4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b4, ae->d_b4, b4_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_w5, ae->d_w5, w5_bytes, cudaMemcpyDeviceToHost));\n","    CHECK_CUDA(cudaMemcpy(h_b5, ae->d_b5, b5_bytes, cudaMemcpyDeviceToHost));\n","}\n","\n","void gpu_autoencoder_copy_weights_to_device(\n","    GPUAutoencoder *ae,\n","    const float *h_w1, const float *h_b1,\n","    const float *h_w2, const float *h_b2,\n","    const float *h_w3, const float *h_b3,\n","    const float *h_w4, const float *h_b4,\n","    const float *h_w5, const float *h_b5)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_bytes = C_out1 * C_in1 * K * K * sizeof(float);\n","    size_t b1_bytes = C_out1 * sizeof(float);\n","    size_t w2_bytes = C_out2 * C_in2 * K * K * sizeof(float);\n","    size_t b2_bytes = C_out2 * sizeof(float);\n","    size_t w3_bytes = C_out3 * C_in3 * K * K * sizeof(float);\n","    size_t b3_bytes = C_out3 * sizeof(float);\n","    size_t w4_bytes = C_out4 * C_in4 * K * K * sizeof(float);\n","    size_t b4_bytes = C_out4 * sizeof(float);\n","    size_t w5_bytes = C_out5 * C_in5 * K * K * sizeof(float);\n","    size_t b5_bytes = C_out5 * sizeof(float);\n","\n","    CHECK_CUDA(cudaMemcpy(ae->d_w1, h_w1, w1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b1, h_b1, b1_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w2, h_w2, w2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b2, h_b2, b2_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w3, h_w3, w3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b3, h_b3, b3_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w4, h_w4, w4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b4, h_b4, b4_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_w5, h_w5, w5_bytes, cudaMemcpyHostToDevice));\n","    CHECK_CUDA(cudaMemcpy(ae->d_b5, h_b5, b5_bytes, cudaMemcpyHostToDevice));\n","}\n","\n","float gpu_autoencoder_forward(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_output,\n","    bool compute_loss)\n","{\n","    const int N = ae->N;\n","    const int H = ae->H;\n","    const int W = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Copy input to device\n","    size_t in_bytes = N * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);  // 256 threads per block\n","\n","    // ========= ENCODER =========\n","\n","    // Layer 1: conv1: 3 -> 256, 32x32\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        // Grid configuration cho optimized kernel\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,  // Ceiling division\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        // Update constant memory bias\n","        update_dc_bias(ae->d_b1, C_out);\n","\n","        // Launch optimized kernel\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // ReLU\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        // MaxPool 2x2 -> 16x16\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // Layer 2: conv2: 256 -> 128, 16x16, pool -> 8x8\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        // Pool -> 8x8\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N, C_out, H_out, W_out);\n","    }\n","\n","    // LATENT: ae->d_p2: [N, 128, 8, 8]\n","\n","    // ========= DECODER =========\n","\n","    // Layer 3: conv3: 128 -> 128, 8x8\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H_in = 8, W_in = 8;\n","        int H_out = 8, W_out = 8;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b3, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p2, ae->d_w3, ae->d_h3,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h3, size);\n","\n","        // Upsample 8x8 -> 16x16\n","        int Hu = 16, Wu = 16;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h3, ae->d_u1,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // Layer 4: conv4: 128 -> 256, 16x16, upsample 16->32\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b4, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_u1, ae->d_w4, ae->d_h4,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h4, size);\n","\n","        // Upsample 16x16 -> 32x32\n","        int Hu = 32, Wu = 32;\n","        dim3 gridUp(\n","            (Wu + block2d.x - 1) / block2d.x,\n","            (Hu + block2d.y - 1) / block2d.y,\n","            N * C_out);\n","\n","        upsample2x2_forward<<<gridUp, block2d>>>(\n","            ae->d_h4, ae->d_u2,\n","            N, C_out, H_in, W_in);\n","    }\n","\n","    // Layer 5: conv5: 256 -> 3, 32x32 (output layer)\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H_in = 32, W_in = 32;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + TILE_W - 1) / TILE_W,\n","            (H_out + TILE_H - 1) / TILE_H,\n","            N * C_out);\n","\n","        update_dc_bias(ae->d_b5, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_u2, ae->d_w5, ae->d_out,\n","            N, C_in, H_in, W_in, C_out, pad, stride);\n","    }\n","\n","    // ========= COMPUTE LOSS (optional) =========\n","    float loss_value = 0.0f;\n","    if (compute_loss) {\n","        int size = N * 3 * 32 * 32;\n","        CHECK_CUDA(cudaMemset(ae->d_loss, 0, sizeof(float)));\n","\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        size_t shmem_bytes = t * sizeof(float);\n","\n","        mse_loss_forward<<<b, t, shmem_bytes>>>(\n","            ae->d_out, ae->d_x0, ae->d_loss, size);\n","\n","        float loss_sum = 0.0f;\n","        CHECK_CUDA(cudaMemcpy(&loss_sum, ae->d_loss,\n","                              sizeof(float),\n","                              cudaMemcpyDeviceToHost));\n","\n","        loss_value = loss_sum / size;\n","    }\n","\n","    // Copy output back to host\n","    size_t out_bytes = N * 3 * 32 * 32 * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(h_output, ae->d_out,\n","                          out_bytes,\n","                          cudaMemcpyDeviceToHost));\n","\n","    return loss_value;\n","}\n","\n","void gpu_autoencoder_backward(GPUAutoencoder *ae, float lr)\n","{\n","    const int N = ae->N;\n","    const int H0 = ae->H;\n","    const int W0 = ae->W;\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // Zero all gradient buffers\n","    CHECK_CUDA(cudaMemset(ae->d_gw1, 0, 256 * 3 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb1, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw2, 0, 128 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb2, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw3, 0, 128 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb3, 0, 128 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw4, 0, 256 * 128 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb4, 0, 256 * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gw5, 0, 3 * 256 * K * K * sizeof(float)));\n","    CHECK_CUDA(cudaMemset(ae->d_gb5, 0, 3 * sizeof(float)));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== 1. dL/dout (MSE) =====\n","    int size_out = N * 3 * 32 * 32;\n","    {\n","        int t = 256;\n","        int b = (size_out + t - 1) / t;\n","        mse_loss_backward<<<b, t>>>(\n","            ae->d_out, ae->d_x0, ae->d_gout, size_out);\n","    }\n","\n","    // ===== 2. Backward conv5: 256->3, 32x32 =====\n","    {\n","        int C_in = 256, C_out = 3;\n","        int H = 32, W = 32;\n","\n","        // dU2 - OPTIMIZED KERNEL\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,  // Use TILE_W instead of block2d.x\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gout, ae->d_w5, ae->d_gu2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dW5 - OPTIMIZED KERNEL\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_u2, ae->d_gout, ae->d_gw5,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // dB5 - OPTIMIZED KERNEL\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gout, ae->d_gb5, N, C_out, H, W);\n","\n","        // SGD update\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w5, ae->d_gw5, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b5, ae->d_gb5, C_out, lr);\n","    }\n","\n","    // ===== 3. UpSample2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 16, W = 16;\n","        int Hu = 32, Wu = 32;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu2, ae->d_gh4, N, C, H, W);\n","    }\n","\n","    // ===== 4. ReLU backward h4 =====\n","    {\n","        int C = 256, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h4, ae->d_gh4, ae->d_gh4, size);\n","    }\n","\n","    // ===== 5. conv4 backward: 128->256, 16x16 =====\n","    {\n","        int C_in = 128, C_out = 256;\n","        int H = 16, W = 16;\n","\n","        // OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh4, ae->d_w4, ae->d_gu1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_u1, ae->d_gh4, ae->d_gw4,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh4, ae->d_gb4, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w4, ae->d_gw4, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b4, ae->d_gb4, C_out, lr);\n","    }\n","\n","    // ===== 6. UpSample2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 8, W = 8;\n","        int Hu = 16, Wu = 16;\n","\n","        dim3 grid(\n","            (W + block2d.x - 1) / block2d.x,\n","            (H + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        upsample2x2_backward<<<grid, block2d>>>(\n","            ae->d_gu1, ae->d_gh3, N, C, H, W);\n","    }\n","\n","    // ===== 7. ReLU backward h3 =====\n","    {\n","        int C = 128, H = 8, W = 8;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h3, ae->d_gh3, ae->d_gh3, size);\n","    }\n","\n","    // ===== 8. conv3 backward: 128->128, 8x8 =====\n","    {\n","        int C_in = 128, C_out = 128;\n","        int H = 8, W = 8;\n","\n","        // OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh3, ae->d_w3, ae->d_gp2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_p2, ae->d_gh3, ae->d_gw3,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh3, ae->d_gb3, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w3, ae->d_gw3, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b3, ae->d_gb3, C_out, lr);\n","    }\n","\n","    // ===== 9. MaxPool2x2 backward =====\n","    {\n","        int C = 128;\n","        int H = 16, W = 16;\n","        CHECK_CUDA(cudaMemset(ae->d_gh2, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h2, ae->d_gp2, ae->d_gh2, N, C, H, W);\n","    }\n","\n","    // ===== 10. ReLU backward h2 =====\n","    {\n","        int C = 128, H = 16, W = 16;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h2, ae->d_gh2, ae->d_gh2, size);\n","    }\n","\n","    // ===== 11. conv2 backward: 256->128, 16x16 =====\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H = 16, W = 16;\n","\n","        // OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh2, ae->d_w2, ae->d_gp1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_p1, ae->d_gh2, ae->d_gw2,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh2, ae->d_gb2, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w2, ae->d_gw2, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b2, ae->d_gb2, C_out, lr);\n","    }\n","\n","    // ===== 12. MaxPool2x2 backward =====\n","    {\n","        int C = 256;\n","        int H = 32, W = 32;\n","        CHECK_CUDA(cudaMemset(ae->d_gh1, 0, N * C * H * W * sizeof(float)));\n","\n","        dim3 grid(\n","            (W/2 + block2d.x - 1) / block2d.x,\n","            (H/2 + block2d.y - 1) / block2d.y,\n","            N * C);\n","\n","        maxpool2x2_backward<<<grid, block2d>>>(\n","            ae->d_h1, ae->d_gp1, ae->d_gh1, N, C, H, W);\n","    }\n","\n","    // ===== 13. ReLU backward h1 =====\n","    {\n","        int C = 256, H = 32, W = 32;\n","        int size = N * C * H * W;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_backward<<<b, t>>>(\n","            ae->d_h1, ae->d_gh1, ae->d_gh1, size);\n","    }\n","\n","    // ===== 14. conv1 backward: 3->256, 32x32 =====\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H = 32, W = 32;\n","\n","        // OPTIMIZED\n","        dim3 gridIn(\n","            (W + TILE_W - 1) / TILE_W,\n","            (H + TILE_H - 1) / TILE_H,\n","            N * C_in);\n","\n","        conv2d_backward_input_opt2<<<gridIn, block2d>>>(\n","            ae->d_gh1, ae->d_w1, ae->d_gx0,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridW(1, 1, C_out * C_in);\n","        conv2d_backward_weight_opt2<<<gridW, block2d>>>(\n","            ae->d_x0, ae->d_gh1, ae->d_gw1,\n","            N, C_in, H, W, C_out, pad, stride);\n","\n","        // OPTIMIZED\n","        dim3 gridB(C_out);\n","        dim3 blockB(256);\n","        conv2d_backward_bias_opt2<<<gridB, blockB>>>(\n","            ae->d_gh1, ae->d_gb1, N, C_out, H, W);\n","\n","        // SGD\n","        int num_w = C_out * C_in * K * K;\n","        int t = 256;\n","        int bw = (num_w + t - 1) / t;\n","        sgd_update<<<bw, t>>>(ae->d_w1, ae->d_gw1, num_w, lr);\n","\n","        int bbp = (C_out + t - 1) / t;\n","        sgd_update<<<bbp, t>>>(ae->d_b1, ae->d_gb1, C_out, lr);\n","    }\n","}\n","\n","void gpu_autoencoder_save_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    gpu_autoencoder_copy_weights_to_host(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5);\n","\n","    FILE *f = fopen(filename, \"wb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for writing\\n\", filename);\n","    } else {\n","        fwrite(h_w1, sizeof(float), w1_cnt, f);\n","        fwrite(h_b1, sizeof(float), b1_cnt, f);\n","        fwrite(h_w2, sizeof(float), w2_cnt, f);\n","        fwrite(h_b2, sizeof(float), b2_cnt, f);\n","        fwrite(h_w3, sizeof(float), w3_cnt, f);\n","        fwrite(h_b3, sizeof(float), b3_cnt, f);\n","        fwrite(h_w4, sizeof(float), w4_cnt, f);\n","        fwrite(h_b4, sizeof(float), b4_cnt, f);\n","        fwrite(h_w5, sizeof(float), w5_cnt, f);\n","        fwrite(h_b5, sizeof(float), b5_cnt, f);\n","        fclose(f);\n","        printf(\"Saved weights to %s\\n\", filename);\n","    }\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","}\n","\n","void gpu_autoencoder_load_weights(GPUAutoencoder *ae, const char *filename)\n","{\n","    int C_in1 = 3,   C_out1 = 256;\n","    int C_in2 = 256, C_out2 = 128;\n","    int C_in3 = 128, C_out3 = 128;\n","    int C_in4 = 128, C_out4 = 256;\n","    int C_in5 = 256, C_out5 = 3;\n","\n","    size_t w1_cnt = C_out1 * C_in1 * K * K;\n","    size_t b1_cnt = C_out1;\n","    size_t w2_cnt = C_out2 * C_in2 * K * K;\n","    size_t b2_cnt = C_out2;\n","    size_t w3_cnt = C_out3 * C_in3 * K * K;\n","    size_t b3_cnt = C_out3;\n","    size_t w4_cnt = C_out4 * C_in4 * K * K;\n","    size_t b4_cnt = C_out4;\n","    size_t w5_cnt = C_out5 * C_in5 * K * K;\n","    size_t b5_cnt = C_out5;\n","\n","    float *h_w1 = (float*)malloc(w1_cnt * sizeof(float));\n","    float *h_b1 = (float*)malloc(b1_cnt * sizeof(float));\n","    float *h_w2 = (float*)malloc(w2_cnt * sizeof(float));\n","    float *h_b2 = (float*)malloc(b2_cnt * sizeof(float));\n","    float *h_w3 = (float*)malloc(w3_cnt * sizeof(float));\n","    float *h_b3 = (float*)malloc(b3_cnt * sizeof(float));\n","    float *h_w4 = (float*)malloc(w4_cnt * sizeof(float));\n","    float *h_b4 = (float*)malloc(b4_cnt * sizeof(float));\n","    float *h_w5 = (float*)malloc(w5_cnt * sizeof(float));\n","    float *h_b5 = (float*)malloc(b5_cnt * sizeof(float));\n","\n","    FILE *f = fopen(filename, \"rb\");\n","    if (!f) {\n","        fprintf(stderr, \"Cannot open %s for reading\\n\", filename);\n","        exit(1);\n","    }\n","\n","    size_t r1 = fread(h_w1, sizeof(float), w1_cnt, f);\n","    size_t r2 = fread(h_b1, sizeof(float), b1_cnt, f);\n","    size_t r3 = fread(h_w2, sizeof(float), w2_cnt, f);\n","    size_t r4 = fread(h_b2, sizeof(float), b2_cnt, f);\n","    size_t r5 = fread(h_w3, sizeof(float), w3_cnt, f);\n","    size_t r6 = fread(h_b3, sizeof(float), b3_cnt, f);\n","    size_t r7 = fread(h_w4, sizeof(float), w4_cnt, f);\n","    size_t r8 = fread(h_b4, sizeof(float), b4_cnt, f);\n","    size_t r9 = fread(h_w5, sizeof(float), w5_cnt, f);\n","    size_t r10 = fread(h_b5, sizeof(float), b5_cnt, f);\n","    fclose(f);\n","\n","    if (r1 != w1_cnt || r2 != b1_cnt ||\n","        r3 != w2_cnt || r4 != b2_cnt ||\n","        r5 != w3_cnt || r6 != b3_cnt ||\n","        r7 != w4_cnt || r8 != b4_cnt ||\n","        r9 != w5_cnt || r10 != b5_cnt)\n","    {\n","        fprintf(stderr, \"Error reading weights from %s\\n\", filename);\n","        exit(1);\n","    }\n","\n","    gpu_autoencoder_copy_weights_to_device(\n","        ae,\n","        h_w1, h_b1,\n","        h_w2, h_b2,\n","        h_w3, h_b3,\n","        h_w4, h_b4,\n","        h_w5, h_b5\n","    );\n","\n","    free(h_w1); free(h_b1);\n","    free(h_w2); free(h_b2);\n","    free(h_w3); free(h_b3);\n","    free(h_w4); free(h_b4);\n","    free(h_w5); free(h_b5);\n","\n","    printf(\"Loaded weights from %s\\n\", filename);\n","}\n","\n","void gpu_autoencoder_encode_batch(\n","    GPUAutoencoder *ae,\n","    const float *h_input,\n","    float *h_latent,\n","    int N_batch)\n","{\n","    const int H = ae->H;    // 32\n","    const int W = ae->W;    // 32\n","    const int pad = 1;\n","    const int stride = 1;\n","\n","    // copy input [N_batch, 3, 32, 32] lên GPU\n","    size_t in_bytes = (size_t)N_batch * 3 * H * W * sizeof(float);\n","    CHECK_CUDA(cudaMemcpy(ae->d_x0, h_input, in_bytes, cudaMemcpyHostToDevice));\n","\n","    dim3 block2d(16, 16);\n","\n","    // ===== ENCODER =====\n","    // conv1: 3 -> 256, 32x32 -> h1, rồi ReLU + MaxPool -> p1 (16x16)\n","    {\n","        int C_in = 3, C_out = 256;\n","        int H_out = 32, W_out = 32;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b1, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_x0, ae->d_w1, ae->d_h1,\n","            N_batch, C_in, H, W, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h1, size);\n","\n","        int Hp = 16, Wp = 16;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h1, ae->d_p1,\n","            N_batch, C_out, H_out, W_out);\n","    }\n","\n","    // conv2: 256 -> 128, 16x16 -> h2, rồi ReLU + MaxPool -> p2 (8x8)\n","    {\n","        int C_in = 256, C_out = 128;\n","        int H_in = 16, W_in = 16;\n","        int H_out = 16, W_out = 16;\n","\n","        dim3 gridConv(\n","            (W_out + block2d.x - 1) / block2d.x,\n","            (H_out + block2d.y - 1) / block2d.y,\n","            N_batch * C_out\n","        );\n","\n","        // copy bias to constant memory\n","        update_dc_bias(ae->d_b2, C_out);\n","        conv2d_forward_opt2<<<gridConv, block2d>>>(\n","            ae->d_p1, ae->d_w2, ae->d_h2,\n","            N_batch, C_in, H_in, W_in, C_out, pad, stride);\n","\n","        int size = N_batch * C_out * H_out * W_out;\n","        int t = 256;\n","        int b = (size + t - 1) / t;\n","        relu_forward<<<b, t>>>(ae->d_h2, size);\n","\n","        int Hp = 8, Wp = 8;\n","        dim3 gridPool(\n","            (Wp + block2d.x - 1) / block2d.x,\n","            (Hp + block2d.y - 1) / block2d.y,\n","            N_batch * C_out);\n","\n","        maxpool2x2_forward<<<gridPool, block2d>>>(\n","            ae->d_h2, ae->d_p2,\n","            N_batch, C_out, H_out, W_out);\n","\n","         cudaError_t err = cudaGetLastError();\n","   if (err != cudaSuccess) {\n","       fprintf(stderr, \"Kernel launch error: %s\\n\", cudaGetErrorString(err));\n","   }\n","   cudaDeviceSynchronize();\n","    }\n","    cudaDeviceSynchronize();\n","\n","size_t latent_bytes = (size_t)N_batch * 128 * 8 * 8 * sizeof(float);\n","CHECK_CUDA(cudaMemcpy(h_latent, ae->d_p2, latent_bytes, cudaMemcpyDeviceToHost));\n","\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbFIxhvkiA52","executionInfo":{"status":"ok","timestamp":1765962535000,"user_tz":-420,"elapsed":19,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"543e39cb-7cab-4678-9e4f-136e42ce5ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing gpu_autoencoder_opt2.cu\n"]}]},{"cell_type":"code","source":["%%writefile verify_cpu_gpuOpt2_output.cu\n","#include <cstdio>\n","#include <cstdlib>\n","#include <cmath>\n","#include <cuda_runtime.h>\n","\n","extern \"C\" {\n","    #include \"cpu_autoencoder.h\"\n","}\n","\n","#include \"gpu_autoencoder_opt2.h\"\n","\n","// Utility function to compare arrays\n","void compare_arrays(const char* name, const float* a, const float* b, int n) {\n","    double max_abs = 0.0;\n","    double sum_abs = 0.0;\n","    double sum_sq  = 0.0;\n","    int max_idx = 0;\n","\n","    for (int i = 0; i < n; ++i) {\n","        double diff = (double)a[i] - (double)b[i];\n","        double ad   = fabs(diff);\n","\n","        if (ad > max_abs) {\n","            max_abs = ad;\n","            max_idx = i;\n","        }\n","        sum_abs += ad;\n","        sum_sq  += diff * diff;\n","    }\n","\n","    double mean_abs = sum_abs / n;\n","    double rmse     = sqrt(sum_sq / n);\n","\n","    printf(\"[%s]\\n\", name);\n","    printf(\"  max|diff| = %.6e (at index %d: CPU=%.6f, GPU_Opt2=%.6f)\\n\",\n","           max_abs, max_idx, a[max_idx], b[max_idx]);\n","    printf(\"  mean|diff| = %.6e\\n\", mean_abs);\n","    printf(\"  RMSE = %.6e\\n\", rmse);\n","\n","    // Check if differences are acceptable\n","    if (max_abs < 1e-4 && rmse < 1e-5) {\n","        printf(\"  ✓ PASSED (excellent match)\\n\");\n","    } else if (max_abs < 1e-3 && rmse < 1e-4) {\n","        printf(\"  ✓ PASSED (good match)\\n\");\n","    } else if (max_abs < 1e-2 && rmse < 1e-3) {\n","        printf(\"  ~ ACCEPTABLE (minor differences)\\n\");\n","    } else {\n","        printf(\"  ✗ FAILED (significant differences)\\n\");\n","    }\n","}\n","\n","void copy_weights_cpu_to_gpu(CPUAutoEncoder* cpu_ae, GPUAutoencoder* gpu_ae) {\n","    // Helper function to copy weights from CPU to GPU_Opt2 autoencoder\n","    gpu_autoencoder_copy_weights_to_device(gpu_ae,\n","        cpu_ae->w1, cpu_ae->b1,\n","        cpu_ae->w2, cpu_ae->b2,\n","        cpu_ae->w3, cpu_ae->b3,\n","        cpu_ae->w4, cpu_ae->b4,\n","        cpu_ae->w5, cpu_ae->b5);\n","}\n","\n","void verify_autoencoder_output_opt2() {\n","    printf(\"==================== Verify Autoencoder CPU vs GPU_Opt2 ====================\\n\\n\");\n","\n","    int batch_size = 4;\n","    double learning_rate = 0.001;\n","    int input_size = batch_size * 3 * 32 * 32;\n","\n","    // Initialize random input\n","    printf(\"Initializing input data...\\n\");\n","    float *h_input = (float*)malloc(input_size * sizeof(float));\n","    for (int i = 0; i < input_size; ++i) {\n","        h_input[i] = ((float)(i % 101) - 50.0f) / 100.0f;  // [-0.5, 0.5]\n","    }\n","\n","    // ===== Initialize CPU Autoencoder =====\n","    printf(\"Initializing CPU autoencoder...\\n\");\n","    CPUAutoEncoder cpu_ae;\n","    initialize_autoencoder(&cpu_ae, batch_size, learning_rate);\n","\n","    // Copy input to CPU autoencoder\n","    memcpy(cpu_ae.batch_input, h_input, input_size * sizeof(float));\n","\n","    // ===== Initialize GPU_Opt2 Autoencoder =====\n","    printf(\"Initializing GPU_Opt2 autoencoder...\\n\");\n","    GPUAutoencoder gpu_ae;\n","    gpu_autoencoder_init(&gpu_ae, batch_size);\n","\n","    // Copy weights from CPU to GPU_Opt2 (để đảm bảo cùng weights)\n","    printf(\"Copying weights from CPU to GPU_Opt2...\\n\");\n","    copy_weights_cpu_to_gpu(&cpu_ae, &gpu_ae);\n","\n","    // ===== CPU Forward Pass =====\n","    printf(\"\\nRunning CPU forward pass...\\n\");\n","    forward_autoencoder(&cpu_ae);\n","\n","    // Calculate CPU loss\n","    float loss_cpu = MSE(cpu_ae.batch_input, cpu_ae.final_output, input_size);\n","    printf(\"CPU Loss: %.6f\\n\", loss_cpu);\n","\n","    // ===== GPU_Opt1 Forward Pass =====\n","    printf(\"\\nRunning GPU_Opt2 forward pass...\\n\");\n","    float *h_output_gpu = (float*)malloc(input_size * sizeof(float));\n","    float loss_gpu = gpu_autoencoder_forward(&gpu_ae, h_input, h_output_gpu, true);\n","    printf(\"GPU_Opt1 Loss: %.6f\\n\", loss_gpu);\n","\n","    // ===== Compare Results =====\n","    printf(\"\\n==================== Comparison Results ====================\\n\\n\");\n","\n","    printf(\"Loss Comparison:\\n\");\n","    printf(\"  CPU Loss:      %.6f\\n\", loss_cpu);\n","    printf(\"  GPU_Opt1 Loss: %.6f\\n\", loss_gpu);\n","    printf(\"  Abs Diff:      %.6e\\n\", fabs(loss_cpu - loss_gpu));\n","    printf(\"  Rel Diff:      %.6e%%\\n\\n\", fabs(loss_cpu - loss_gpu) / loss_cpu * 100.0);\n","\n","    // Compare final output\n","    compare_arrays(\"Final Output (Reconstructed Image)\",\n","                   cpu_ae.final_output, h_output_gpu, input_size);\n","\n","    // ===== Compare Intermediate Activations =====\n","    printf(\"\\n--- Intermediate Activation Comparisons ---\\n\\n\");\n","\n","    // 1. Conv1 output: [batch, 256, 32, 32]\n","    int conv1_size = batch_size * 256 * 32 * 32;\n","    float *h_conv1_gpu = (float*)malloc(conv1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv1_gpu, gpu_ae.d_h1, conv1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv1 Output (h1)\", cpu_ae.conv1_output, h_conv1_gpu, conv1_size);\n","\n","    printf(\"\\n\");\n","\n","    // 2. Pool1 output: [batch, 256, 16, 16]\n","    int pool1_size = batch_size * 256 * 16 * 16;\n","    float *h_pool1_gpu = (float*)malloc(pool1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_pool1_gpu, gpu_ae.d_p1, pool1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Pool1 Output (p1)\", cpu_ae.pool1_output, h_pool1_gpu, pool1_size);\n","\n","    printf(\"\\n\");\n","\n","    // 3. Conv2 output: [batch, 128, 16, 16]\n","    int conv2_size = batch_size * 128 * 16 * 16;\n","    float *h_conv2_gpu = (float*)malloc(conv2_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv2_gpu, gpu_ae.d_h2, conv2_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv2 Output (h2)\", cpu_ae.conv2_output, h_conv2_gpu, conv2_size);\n","\n","    printf(\"\\n\");\n","\n","    // 4. LATENT: Pool2 output: [batch, 128, 8, 8]\n","    int latent_size = batch_size * 128 * 8 * 8;\n","    float *h_latent_gpu = (float*)malloc(latent_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_latent_gpu, gpu_ae.d_p2, latent_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"LATENT (Pool2/p2)\", cpu_ae.pool2_output, h_latent_gpu, latent_size);\n","\n","    printf(\"\\n\");\n","\n","    // 5. Conv3 output: [batch, 128, 8, 8]\n","    int conv3_size = batch_size * 128 * 8 * 8;\n","    float *h_conv3_gpu = (float*)malloc(conv3_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv3_gpu, gpu_ae.d_h3, conv3_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv3 Output (h3)\", cpu_ae.conv3_output, h_conv3_gpu, conv3_size);\n","\n","    printf(\"\\n\");\n","\n","    // 6. Upsample1 output: [batch, 128, 16, 16]\n","    int up1_size = batch_size * 128 * 16 * 16;\n","    float *h_up1_gpu = (float*)malloc(up1_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_up1_gpu, gpu_ae.d_u1, up1_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Upsample1 Output (u1)\", cpu_ae.upsample1_output, h_up1_gpu, up1_size);\n","\n","    printf(\"\\n\");\n","\n","    // 7. Conv4 output: [batch, 256, 16, 16]\n","    int conv4_size = batch_size * 256 * 16 * 16;\n","    float *h_conv4_gpu = (float*)malloc(conv4_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_conv4_gpu, gpu_ae.d_h4, conv4_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Conv4 Output (h4)\", cpu_ae.conv4_output, h_conv4_gpu, conv4_size);\n","\n","    printf(\"\\n\");\n","\n","    // 8. Upsample2 output: [batch, 256, 32, 32]\n","    int up2_size = batch_size * 256 * 32 * 32;\n","    float *h_up2_gpu = (float*)malloc(up2_size * sizeof(float));\n","    CHECK_CUDA(cudaMemcpy(h_up2_gpu, gpu_ae.d_u2, up2_size * sizeof(float),\n","                          cudaMemcpyDeviceToHost));\n","    compare_arrays(\"Upsample2 Output (u2)\", cpu_ae.upsample2_output, h_up2_gpu, up2_size);\n","\n","    // ===== Summary =====\n","    printf(\"\\n==================== Summary ====================\\n\");\n","    printf(\"Architecture verified (GPU_Opt1):\\n\");\n","    printf(\"  Input:      [%d, 3, 32, 32]\\n\", batch_size);\n","    printf(\"  Conv1:      [%d, 256, 32, 32] \\n\", batch_size);\n","    printf(\"  Pool1:      [%d, 256, 16, 16]\\n\", batch_size);\n","    printf(\"  Conv2:      [%d, 128, 16, 16] \\n\", batch_size);\n","    printf(\"  LATENT:     [%d, 128, 8, 8]  ← bottleneck\\n\", batch_size);\n","    printf(\"  Conv3:      [%d, 128, 8, 8]  \\n\", batch_size);\n","    printf(\"  Upsample1:  [%d, 128, 16, 16]\\n\", batch_size);\n","    printf(\"  Conv4:      [%d, 256, 16, 16] \\n\", batch_size);\n","    printf(\"  Upsample2:  [%d, 256, 32, 32]\\n\", batch_size);\n","    printf(\"  Output:     [%d, 3, 32, 32] \\n\", batch_size);\n","    printf(\"\\n\");\n","    printf(\"Optimizations in Opt1:\\n\");\n","    printf(\"  - Shared memory for convolution input tiles\\n\");\n","    printf(\"  - Reduced global memory accesses\\n\");\n","    printf(\"  - Better memory coalescing\\n\");\n","\n","    // Cleanup\n","    free(h_input);\n","    free(h_output_gpu);\n","    free(h_conv1_gpu);\n","    free(h_pool1_gpu);\n","    free(h_conv2_gpu);\n","    free(h_latent_gpu);\n","    free(h_conv3_gpu);\n","    free(h_up1_gpu);\n","    free(h_conv4_gpu);\n","    free(h_up2_gpu);\n","\n","    free_autoencoder(&cpu_ae);\n","    gpu_autoencoder_free(&gpu_ae);\n","\n","    printf(\"\\n==================== Verification Complete ====================\\n\");\n","}\n","\n","int main() {\n","    verify_autoencoder_output_opt2();\n","    CHECK_CUDA(cudaDeviceReset());\n","    return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pCvDXPOiCFO","executionInfo":{"status":"ok","timestamp":1765962580565,"user_tz":-420,"elapsed":20,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"b0d3a440-6513-46d8-d505-7fb62d3c9337"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting verify_cpu_gpuOpt2_output.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -O2 \\\n","  verify_cpu_gpuOpt2_output.cu gpu_layers_opt1.cu cpu_layers.c gpu_autoencoder_opt1.cu cpu_autoencoder.c \\\n","  -o verify_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-KcvhMHiFmD","executionInfo":{"status":"ok","timestamp":1765962588727,"user_tz":-420,"elapsed":4527,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"c12da529-c756-40a1-f0bd-7200fdd5b3b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(16)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H\"\u001b[0m was declared but never referenced\n","      const int H = ae->H;\n","                ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(17)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W\"\u001b[0m was declared but never referenced\n","      const int W = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(530)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(530)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 32, Wu = 32;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(592)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Hu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(592)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"Wu\"\u001b[0m was declared but never referenced\n","          int Hu = 16, Wu = 16;\n","                       ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(462)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"H0\"\u001b[0m was declared but never referenced\n","      const int H0 = ae->H;\n","                ^\n","\n","\u001b[01m\u001b[0m\u001b[01mgpu_autoencoder_opt1.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"W0\"\u001b[0m was declared but never referenced\n","      const int W0 = ae->W;\n","                ^\n","\n","\u001b[01m\u001b[Kcpu_autoencoder.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcpu_load_weights\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kcpu_autoencoder.c:340:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  340 |     \u001b[01;35m\u001b[Kfread(autoencoder->w1, sizeof(float), 256*3*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:341:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  341 |     \u001b[01;35m\u001b[Kfread(autoencoder->b1, sizeof(float), 256, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:342:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  342 |     \u001b[01;35m\u001b[Kfread(autoencoder->w2, sizeof(float), 128*256*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:343:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  343 |     \u001b[01;35m\u001b[Kfread(autoencoder->b2, sizeof(float), 128, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:344:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  344 |     \u001b[01;35m\u001b[Kfread(autoencoder->w3, sizeof(float), 128*128*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:345:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  345 |     \u001b[01;35m\u001b[Kfread(autoencoder->b3, sizeof(float), 128, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:346:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  346 |     \u001b[01;35m\u001b[Kfread(autoencoder->w4, sizeof(float), 256*128*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:347:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  347 |     \u001b[01;35m\u001b[Kfread(autoencoder->b4, sizeof(float), 256, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:348:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  348 |     \u001b[01;35m\u001b[Kfread(autoencoder->w5, sizeof(float), 3*256*3*3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kcpu_autoencoder.c:349:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n","  349 |     \u001b[01;35m\u001b[Kfread(autoencoder->b5, sizeof(float), 3, file)\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"]}]},{"cell_type":"code","source":["!./verify_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fop84rsIiFcy","executionInfo":{"status":"ok","timestamp":1765962625514,"user_tz":-420,"elapsed":2762,"user":{"displayName":"Minh Anh","userId":"16931774503843092106"}},"outputId":"7e2eb83f-5591-4c8d-9259-df9155401070"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================== Verify Autoencoder CPU vs GPU_Opt2 ====================\n","\n","Initializing input data...\n","Initializing CPU autoencoder...\n","Initializing GPU_Opt2 autoencoder...\n","Copying weights from CPU to GPU_Opt2...\n","\n","Running CPU forward pass...\n","CPU Loss: 0.088634\n","\n","Running GPU_Opt2 forward pass...\n","GPU_Opt1 Loss: 0.088634\n","\n","==================== Comparison Results ====================\n","\n","Loss Comparison:\n","  CPU Loss:      0.088634\n","  GPU_Opt1 Loss: 0.088634\n","  Abs Diff:      1.192093e-07\n","  Rel Diff:      1.344958e-04%\n","\n","[Final Output (Reconstructed Image)]\n","  max|diff| = 1.937151e-07 (at index 4818: CPU=0.063446, GPU_Opt2=0.063446)\n","  mean|diff| = 3.140174e-08\n","  RMSE = 4.040118e-08\n","  ✓ PASSED (excellent match)\n","\n","--- Intermediate Activation Comparisons ---\n","\n","[Conv1 Output (h1)]\n","  max|diff| = 2.980232e-08 (at index 21609: CPU=0.091961, GPU_Opt2=0.091961)\n","  mean|diff| = 1.203794e-09\n","  RMSE = 2.941030e-09\n","  ✓ PASSED (excellent match)\n","\n","[Pool1 Output (p1)]\n","  max|diff| = 2.980232e-08 (at index 5396: CPU=0.091961, GPU_Opt2=0.091961)\n","  mean|diff| = 1.985855e-09\n","  RMSE = 4.022049e-09\n","  ✓ PASSED (excellent match)\n","\n","[Conv2 Output (h2)]\n","  max|diff| = 1.192093e-07 (at index 16718: CPU=0.139577, GPU_Opt2=0.139577)\n","  mean|diff| = 8.508680e-09\n","  RMSE = 1.616637e-08\n","  ✓ PASSED (excellent match)\n","\n","[LATENT (Pool2/p2)]\n","  max|diff| = 1.192093e-07 (at index 17872: CPU=0.175958, GPU_Opt2=0.175958)\n","  mean|diff| = 1.228130e-08\n","  RMSE = 2.008743e-08\n","  ✓ PASSED (excellent match)\n","\n","[Conv3 Output (h3)]\n","  max|diff| = 2.235174e-07 (at index 2851: CPU=0.109921, GPU_Opt2=0.109921)\n","  mean|diff| = 1.141401e-08\n","  RMSE = 2.211730e-08\n","  ✓ PASSED (excellent match)\n","\n","[Upsample1 Output (u1)]\n","  max|diff| = 2.235174e-07 (at index 11398: CPU=0.109921, GPU_Opt2=0.109921)\n","  mean|diff| = 1.141401e-08\n","  RMSE = 2.211730e-08\n","  ✓ PASSED (excellent match)\n","\n","[Conv4 Output (h4)]\n","  max|diff| = 1.788139e-07 (at index 157018: CPU=0.107238, GPU_Opt2=0.107238)\n","  mean|diff| = 1.168476e-08\n","  RMSE = 2.119412e-08\n","  ✓ PASSED (excellent match)\n","\n","[Upsample2 Output (u2)]\n","  max|diff| = 1.788139e-07 (at index 628052: CPU=0.107238, GPU_Opt2=0.107238)\n","  mean|diff| = 1.168476e-08\n","  RMSE = 2.119412e-08\n","  ✓ PASSED (excellent match)\n","\n","==================== Summary ====================\n","Architecture verified (GPU_Opt1):\n","  Input:      [4, 3, 32, 32]\n","  Conv1:      [4, 256, 32, 32] \n","  Pool1:      [4, 256, 16, 16]\n","  Conv2:      [4, 128, 16, 16] \n","  LATENT:     [4, 128, 8, 8]  ← bottleneck\n","  Conv3:      [4, 128, 8, 8]  \n","  Upsample1:  [4, 128, 16, 16]\n","  Conv4:      [4, 256, 16, 16] \n","  Upsample2:  [4, 256, 32, 32]\n","  Output:     [4, 3, 32, 32] \n","\n","Optimizations in Opt1:\n","  - Shared memory for convolution input tiles\n","  - Reduced global memory accesses\n","  - Better memory coalescing\n","\n","==================== Verification Complete ====================\n"]}]}]}