{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeblyKIuybIm"
   },
   "source": [
    "# Báo cáo Đồ án Cuối kì - Lập trình Song song\n",
    "##### Nhóm 9:\n",
    "##### Đoàn Thị Minh Anh - 22120213\n",
    "##### Trần Hoàng Kim Ngân - 22120224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwJ5HgSPzBZU"
   },
   "source": [
    "## 1. Mô tả bài toán:\n",
    "### 1.1. Phát biểu bài toán:\n",
    "- Xác định bài toán phân loại ảnh:  \n",
    "  - Bài toán: Học đặc trưng không giám sát từ bộ dữ liệu ảnh CIFAR-10 bằng Autoencoder rồi dùng ác đặc trưng đã học để huấn luyện một bộ phân loại SVM cho nhiệm vụ phân loại 10 lớp.\n",
    "  - Quy trình:\n",
    "    - Huấn luyện Autoencoder trên toàn bộ ảnh huấn luyện không dùng nhãn để tối thiểu hóa loss MSE. Encoder xuất ra vector đặc trưng có kích thước 8192 cho mỗi ảnh.\n",
    "    - Dùng encoder đã huấn luyện để trích xuất đặc trưng cho tập huấn luyện và tập kiểm tra, rồi huấn luyện SVM sử dụng các nhãn lớp.\n",
    "    - Đánh giá hiệu năng SVM.  \n",
    "  - Mục tiêu: Chứng minh rằng đặc trưng học được từ Autoencoder giúp phân loại hiệu quả và triển khai CUDA để chạy autoencoder trên GPU hỗ trợ tăng tốc đáng kể so với chạy bằng CPU.\n",
    "- Mô tả động lực tăng tốc GPU:\n",
    "  - Bài toán đòi hỏi xử lý một lượng lớn phép tính số học, đặc biệt trong các tầng tích chập (Convolution), phép nhân ma trận và lan truyền ngược. Việc triển khai các phép tính này trên CPU theo tuần tự thường rất mất thời gian và kém hiệu quả. Trong khi đó, các phép tính này đều có khả năng song song hóa cao, hoàn toàn có thể triển khai một cách hợp lý để ứng dụng GPU.\n",
    "  - Bộ dữ liệu CIFAR-10 với 50,000 ảnh train và 10,000 ảnh test đòi hỏi khả năng truy xuất bộ nhớ mạnh mẽ. Tận dụng GPU tốt sẽ giúp hệ thống xử lý dữ liệu hiệu quả hơn, đạt tốc độ huấn luyện nhanh hơn và cải thiện chất lượng đặc trưng thu được cho nhiệm vụ phân loại hình ảnh.\n",
    "### 1.2. Tổng quan về bộ dữ liệu CIFAR-10:\n",
    "- Đặc tả tập dữ liệu (kích thước, lớp, phân chia):\n",
    "  - Kích thước ảnh: 32x32 pixels (RGB).\n",
    "  - 10 lớp gồm: airplane,automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
    "  - Training set: 50,000 ảnh (5,000 ảnh/lớp).\n",
    "  - Test set: 10,000 ảnh (1,000 ảnh/lớp).\n",
    "  - Tổng số ảnh: 60,000 ảnh\n",
    "  - Format: File nhị phân với giá trị pixel uint8.\n",
    "- Hiển thị hình ảnh mẫu từ mỗi lớp (visualization):\n",
    "![Hình minh họa CIFAR-10](https://drive.google.com/uc?export=view&id=1Gy6BtsEWLgvlRHhyh5M7H5PCLy76H9Ss)\n",
    "\n",
    "- Giải thích các bước tiền xử lý dữ liệu (normalization, format):\n",
    "  - Chuyển đổi kiểu dữ liệu: Dữ liệu pixel gốc trong file ảnh là uint8, là số nguyên 8-bit không dấu nằm trong khoảng [0,255]. Để tiện cho việc chuẩn hoá các giá trị về khoảng [0,1], ở đây chúng em sẽ chuyển chúng về kiểu float.\n",
    "  - Chuyển về [0,1]: Sau khi chuyển sang kiểu số thực, các giá trị pixel sẽ được chuẩn hoá về khoảng [0,1]. Việc này giúp mạng nơ-ron hội tụ nhanh hơn và hoạt động ổn định hơn.\n",
    "### 1.3. Kiến trúc Autoencoder:\n",
    "- Mô tả kiến ​​trúc mạng bằng sơ đồ\n",
    "\n",
    "![Hình sơ đồ kiến trúc mạng](https://drive.google.com/uc?export=view&id=1TZZe0q-VkPQI0rTmbzO1prhDyyeocfeE)\n",
    "- Xác định kích thước và phép biến đổi lớp\n",
    "| Khối | Lớp | Tham số chính | Input shape | Output shape |\n",
    "|---|---|---|---|---|\n",
    "| Input | - | - | (32, 32, 3) | (32, 32, 3) |\n",
    "| Encoder | Conv2D + ReLU | 256 filters, K=3, pad=1, stride=1 | (32, 32, 3) | (32, 32, 256) |\n",
    "| Encoder | MaxPool2D | 2×2, stride=2 | (32, 32, 256) | (16, 16, 256) |\n",
    "| Encoder | Conv2D + ReLU | 128 filters, K=3, pad=1, stride=1 | (16, 16, 256) | (16, 16, 128) |\n",
    "| Encoder | MaxPool2D | 2×2, stride=2 | (16, 16, 128) | **(8, 8, 128)** |\n",
    "| Latent | Flatten (khi trích đặc trưng) | 8×8×128 = 8192 | (8, 8, 128) | **(8192, )** |\n",
    "| Decoder | Conv2D + ReLU | 128 filters, K=3, pad=1, stride=1 | (8, 8, 128) | (8, 8, 128) |\n",
    "| Decoder | UpSample2D | scale=2 | (8, 8, 128) | (16, 16, 128) |\n",
    "| Decoder | Conv2D + ReLU | 256 filters, K=3, pad=1, stride=1 | (16, 16, 128) | (16, 16, 256) |\n",
    "| Decoder | UpSample2D | scale=2 | (16, 16, 256) | (32, 32, 256) |\n",
    "| Output | Conv2D | 3 filters, K=3, pad=1, stride=1 | (32, 32, 256) | (32, 32, 3) |\n",
    "\n",
    "- Giải thích cấu trúc bộ mã hóa-giải mã và biểu diễn latent\n",
    "  - Encoder (downsampling path): Hai lớp Conv+ReLU giúp học đặc trưng cục bộ (biên, texture) còn MaxPool giúp nèn thông tin và tăng tính bất biến theo dịch chuyển. Kết quả cuối encoder là tensor (8, 8, 128). Khi trích đặc trưng để phân loại, tensor này được flatten thành vector 8192 chiều.\n",
    "  - Latent space: là nút thắt cổ chai chưa thông tin quan trọng nhất để tái tạo ảnh. Latent càng tốt thì ảnh reconstruct càng gần ảnh gốc.\n",
    "  - Decoder (upsampling path): đối xứng với encoder, dùng UpSample để đảo quá trình downsample và Conv nhằm tái tạo ảnh về (32, 32, 3). Lớp cuối không dùng activation, thườngđể mô hình tự học giá trị tái tạo.\n",
    "### 1.4. Mục tiêu đồ án:\n",
    "- Mục tiêu hiệu suất (thời gian luyện tập, mục tiêu\n",
    "tăng tốc, độ chính xác): Đồ án hướng đến xây dựng một hệ thống trích chọn đặc trưng và phân loại ảnh hiệu quả trên tập dữ liệu CIFAR-10. Các mục tiêu hiệu suất chính gồm:\n",
    "  - Giảm thời gian huấn luyện bằng cách tận dụng GPU và tối ưu các bước tính toán quan trọng trong Autoencoder.\n",
    "  - Đạt được tốc độ tăng tốc (speedup) đáng kể so với phiên bản chạy trên CPU.\n",
    "  - Đạt độ chính xác phân loại thỏa yêu cầu tối hiểu (khoảng 55-65%).\n",
    "- Mục tiêu học tập kỹ thuật: Đồ án giúp nhóm phát triển kiến thức, kỹ năng liên quan đến học máy và kỹ năng lập trình song song trên GPU, cụ thể:\n",
    "  - Hiểu và triển khai được kiến trúc Autoencoder cho bài toán học đặc trưng không giám sát.\n",
    "  - Nắm vững quy trình huấn luyện mô hình học sâu (forward, backward, tối ưu, đánh giá).\n",
    "  - Thực hành lập trình song song trên GPU bằng CUDA.\n",
    "  - Tối ưu hóa các tác vụ tính toán như biến đổi tensor, tích chập, nhân ma trận nhằm khai thác tối đa khả năng song song của GPU.\n",
    "- Tiêu chí thành công\n",
    "  - Chức năng đúng: Autoencoder huấn luyện thành công, tái tạo ảnh hợp lý và bộ phân loại hoạt động chính xác.\n",
    "  - Trích chọn đặc trưng hiệu quả: đặc trưng nén từ Autoencoder giúp cải thiện độ chính xác phân loại so với baseline.\n",
    "  - Tăng tốc rõ rệt khi chuyển từ CPU sang GPU, với báo cáo định lượng (speedup, thời gian huấn luyện).\n",
    "  - Kết quả thực nghiệm đầy đủ bao gồm biểu đồ loss, accuracy, tốc độ huấn luyện, hình ảnh tái tạo, confusion matrix.\n",
    "  - Báo cáo hoàn chỉnh, phân tích hợp lý và giải thích được tác động của GPU trong toàn hệ thống."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FIBQ2thzfXk"
   },
   "source": [
    "## 2. Các giai đoạn triển khai:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj9C96f-zsMQ"
   },
   "source": [
    "### 2.1. CPU cơ bản\n",
    "#### Mục tiêu:\n",
    "- Mục tiêu nhóm muốn đạt được trong giai đoạn này:\n",
    "  - Xây dựng một phiên bản hoàn chỉnh của autoencoder trên CPU để làm baseline tham chiếu cho các phiên bản GPU sau này.\n",
    "  - Triển khai đầy đủ các layer cần thiết (Convolution, ReLU, MaxPooling, Upsampling, MSE Loss) cùng forward và backward pass trên CPU để đảm bảo mô hình có thể train được và đạt được reconstruction loss hợp lý.\n",
    "  - Đo lường thời gian huấn luyện và hiệu suất ban đầu trên CPU, từ đó xác định rõ các điểm nghẽn hiệu năng (bottleneck) để làm cơ sở cho việc tối ưu hóa trên GPU ở các phase tiếp theo.\n",
    "  - Có thể tạo ra một phiên bản đúng và có thể chạy được để dễ dàng so sánh kết quả với các phiên bản GPU khác và làm nền tảng cho việc chuyển đổi sang code CUDA.\n",
    "- Tại sao giai đoạn này là cần thiết:\n",
    "  - Kiểm chứng độ chính xác: Lập trình song song trên GPU vốn phức tạp với các vấn đề về đồng bộ luồng và quản lý bộ nhớ. Khi kết quả đầu ra bị sai lệch, rất khó để phân định nguyên nhân đến từ lỗi logic thuật toán hay lỗi kỹ thuật song song. Phiên bản CPU hoạt động như một chân lý (Ground Truth) tin cậy. Trong các giai đoạn sau, kết quả từ GPU sẽ được so sánh trực tiếp với CPU để đảm bảo tính đúng đắn trước khi tính đến hiệu năng.\n",
    "  - Nhận diện điểm nghẽn: Việc chạy trên CPU giúp xác định lớp nào (Convolution, Pooling hay Upsampling) chiếm nhiều thời gian xử lý nhất. Đây là chỉ dẫn quan trọng để biết nên dồn lực tối ưu vào lớp nào khi chuyển sang GPU.\n",
    "  - Thấu hiểu sâu sắc kiến trúc giải thuật: Việc chuyển đổi trực tiếp một công thức toán học sang CUDA kernel là rất rủi ro nếu chưa hiểu rõ luồng dữ liệu. Việc tự tay cài đặt các lớp phức tạp như Convolution hay MaxPooling bằng các vòng lặp tường minh giúp lập trình viên nắm bắt tường tận cơ chế truy cập bộ nhớ, cách xử lý biên và sải bước (stride).\n",
    "#### Chi tiết triển khai:\n",
    "- Cách tải và tiền xử lý dữ liệu CIFAR-10:\n",
    "\n",
    "  Chúng em tải bộ dữ liệu CIFAR-10 dạng binary nguyên bản từ trang web chính thức của tác giả.\n",
    "\n",
    "  Bộ dữ liệu gồm 5 file training batch (`data_batch_1.bin` đến `data_batch_5.bin`) và 1 file test batch (`test_batch.bin`).\n",
    "\n",
    "  Mỗi bản ghi trong các file có đúng *3073* bytes, được sắp xếp như sau:  \n",
    "  - Byte 0: nhãn lớp (1 byte, giá trị 0-9).\n",
    "  - Byte 1-1024: toàn bộ kênh màu đỏ (1024 pixel).\n",
    "  - Byte 1025-2048: toàn bộ kênh màu xanh lá (1024 pixel).\n",
    "  - Byte 2049-3072: toàn bộ kênh màu xanh lam (1024 pixel).\n",
    "\n",
    "  Quy trình đọc và tiền xử lý dữ liệu được thực hiện như sau:\n",
    "\n",
    "  - Mở và đọc lần lượt từng file binary theo đúng thứ tự.\n",
    "  - Với mỗi bản ghi 3073 bytes, tách riêng 1 byte nhãn và 3072 byte pixel.\n",
    "  - Chuyển ngay 3072 byte pixel từ kiểu `uint8` sang `float` trong quá trình đọc, đồng thời giữ nguyên thứ tự kênh R, G và B như file gốc.\n",
    "  - Lưu toàn bộ dữ liệu ảnh train (50.000 ảnh) và test (10.000 ảnh) vào các mảng float liên tục trong bộ nhớ host với kích thước là *N * 3072*.\n",
    "  - Sau khi tải xong, thực hiện chuẩn hóa một lần duy nhất bằng cách chia toàn bộ giá trị pixel cho 255.0 để đưa về khoảng [0.0, 1.0].\n",
    "  - Để hỗ trợ mini-batch training, chúng em duy trì một mảng chỉ số `train_indices[50000]`. Ở đầu mỗi epoch, hàm `shuffle_cifar10()` sử dụng thuật toán Fisher-Yates shuffle để xáo trộn ngẫu nhiên thứ tự ảnh.\n",
    "  - Hàm `get_next_batch()` sao chép nhanh dữ liệu batch từ `train_images` vào bộ nhớ đích bằng `memcpy`, đảm bảo hiệu suất cao và dễ dàng chuyển đổi sang GPU ở các phase sau.\n",
    "- Mô tả cách triển khai các lớp (Convolution, ReLU, MaxPool, UpSample):\n",
    "  1) Convolution:\n",
    "  - Forward: Hàm `Conv2D_Forward` thực hiện phép tích chập chuẩn với padding = 1, stride = 1, kernel 3x3. Duyệt theo thứ tự NCHW, hỗ trợ nhiều input và output channels. Bias được cộng trực tiếp sau tích chập.\n",
    "  - Backward: Tương tự với phiên bản forward, backward cũng thực hiện duyệt mảng theo thứ tự NCHW.\n",
    "    - Input: Hàm `Conv2D_Backward_Input` thực hiện full convolution giữa gradient từ lớp sau (d_output) với kernel đã được xoay 180°. Kết quả được cộng dồn vào đúng vị trí tương ứng trên input gradient.\n",
    "    - Weight: Hàm `Conv2D_Backward_Kernel` thực hiện tích chập giữa input của lớp hiện tại và gradient từ lớp sau (d_output), sau đó cộng dồn vào ma trận gradient trọng số.\n",
    "    - Bias: Hàm `Conv2D_Backward_Biases` thực hiện với mỗi filter, gradient của bias bằng tổng tất cả các giá trị gradient tương ứng trên toàn bộ feature map đầu ra (d_output).\n",
    "  2) ReLU:\n",
    "  - Forward: Với từng phần tử, thực hiện so sánh giá trị input[i], với i là index hiện tại trong mảng đầu vào, với 0.0f nhằm đảm bảo đầu ra luôn dương.\n",
    "  - Backward: Hàm `Relu_Backward` nhân gradient từ lớp sau với 1 nếu input > 0, ngược lại bằng 0, đảm bảo gradient chỉ lan truyền qua các neuron dương.\n",
    "  3) MaxPool:\n",
    "  - Forward: Hàm `MaxPool2D_Forward` thực hiện pooling 2x2, stride = 2, giảm kích thước không gian xuống một nửa. Với mỗi vị trí output, tìm giá trị lớn nhất trong vùng 2x2 của từng kênh.\n",
    "  - Backward: Hàm `MaxPool2D_Backward` chỉ truyền gradient về đúng vị trí có giá trị max trong forward pass và các vị trí còn lại nhận giá trị 0.\n",
    "  4) UpSample:\n",
    "  - Forward: Hàm `UpSample2D_Forward` thực hiện nearest-neighbor upsampling với scale_factor = 2. Mỗi pixel đầu vào được sao chép vào khối 2x2 tương ứng trên output.\n",
    "  - Backward: Hàm `UpSample2D_Backward` cộng dồn gradient từ toàn bộ khối 2x2 của output về đúng pixel gốc trên input.\n",
    "- Vòng lặp huấn luyện được cấu trúc như sau:\n",
    "  1) Lặp qua các epoch: Chúng em huấn luyện trong 20 epoch.\n",
    "  2) Xáo trộn dữ liệu: Ở đầu mỗi epoch, hàm `shuffle_cifar10(&data)` được gọi để áp dụng thuật toán Fisher-Yates shuffle trên mảng `train_indices`, đảm bảo thứ tự các ảnh được thay đổi ngẫu nhiên, giúp mô hình học tốt hơn và tránh bias theo thứ tự file.\n",
    "  3) Xử lý từng mini-batch:\n",
    "  - Số batch mỗi epoch được tính tự động bằng công thức: `num_batches = train_subset_size / batch_size`, ở đây do có những giới hạn về bộ nhớ, nên chúng em chỉ gán `train_subset_size = 1000` (chỉ train với 1000 ảnh). Ngoài ra, nhóm áp dụng `batch_size = 32`.\n",
    "  - Với mỗi batch_id, hàm `get_next_batch()` sao chép nhanh dữ liệu từ `train_images` (theo thứ tự đã shuffle) vào bộ nhớ tạm `batch_images` bằng `memcpy`.\n",
    "  - Dữ liệu batch được copy vào batch_input của struct autoencoder (buffer đã được cấp sẵn trong struct CPUAutoEncoder) để các hàm forward/backward có thể truy cập trực tiếp.\n",
    "  4) Quy trình huấn luyện một batch:\n",
    "  - Forward pass: Gọi hàm dành cho forward để tính lần lượt output của tất cả các layer từ input đến reconstructed output.\n",
    "  - Tính loss: Sử dụng hàm để tính Mean Squared Error giữa ảnh gốc (batch_input) và ảnh tái tạo (final_output). Loss của batch hiện tại được hiển thị trên terminal.\n",
    "  - Backward pass: Gọi hàm dành cho backward để tính gradient cho tất cả các layer theo thứ tự ngược lại.\n",
    "  - Cập nhật tham số: Gọi hàm cập nhật để áp dụng Gradient Descent với `learning rate = 0.001`, cập nhật trọng số và bias của tất cả các lớp Convolution.\n",
    "- Một vài đoạn mã chính:\n",
    "  1) Cấu trúc hàm Conv2D():\n",
    "  ```c\n",
    "  void Conv2D_Forward(float* input, int input_width, int input_height, int input_channels,\n",
    "    float* kernel, int kernel_width, int kernel_height,\n",
    "    float* biases, int padding, int stride, int filter_count,\n",
    "    float* output, int output_height, int output_width) \n",
    "  {\n",
    "      // Lặp qua kênh đầu ra (filter)\n",
    "      for (int c_out = 0; c_out < filter_count; c_out++) {  \n",
    "          // Lặp qua chiều cao output\n",
    "          for (int h_out = 0; h_out < output_height; h_out++) {\n",
    "              // Lặp qua chiều rộng output\n",
    "              for (int w_out = 0; w_out < output_width; w_out++) {\n",
    "                  // Lặp qua kênh đầu vào (c_in)\n",
    "                  for (int c_in = 0; c_in < input_channels; c_in++) {\n",
    "                      // Lặp qua kernel height\n",
    "                      for (int k_h = 0; k_h < kernel_height; k_h++) {\n",
    "                          // Lặp qua kernel width\n",
    "                          for (int k_w = 0; k_w < kernel_width; k_w++) {\n",
    "                              // Tính toán vị trí input tương ứng. \n",
    "                              // Kiểm tra zero padding.\n",
    "                              // Tính toán vị trí trọng số.\n",
    "                              // Thực hiện phép tích chập.\n",
    "                              // Với val là giá trị trong mảng input (có thể là 0.0f do thuộc phần padding), kernel: mảng trọng số, \n",
    "                              weight_idx: vị trí giá trị trọng số trong mảng kernel.\n",
    "                              sum += val * kernel[weight_idx];\n",
    "                          }\n",
    "                      }\n",
    "                  } \n",
    "                  sum += biases[c_out];  // Thêm bias\n",
    "                  // Gán sum cho giá trị pixel đầu ra.\n",
    "                  output[output_idx] = sum;\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "  ```\n",
    "  2) Cấu trúc vòng lặp chính:\n",
    "  ```c\n",
    "  // Cấu trúc vòng lặp huấn luyện chính\n",
    "  for (int epoch = 0; epoch < num_epochs; epoch++) {\n",
    "      float total_loss = 0.0f;\n",
    "      clock_t start_time = clock(); // Bắt đầu đo giờ\n",
    "\n",
    "      // 1. Xáo trộn dữ liệu (Data shuffling)\n",
    "      shuffle_indices(train_indices, num_train_images);\n",
    "\n",
    "      for (int batch_id = 0; batch_id < num_batches; batch_id++) {\n",
    "          // 2. Chuẩn bị dữ liệu batch\n",
    "          load_batch(autoencoder.input, train_images, train_indices, batch_id, batch_size);\n",
    "\n",
    "          // 3. Lan truyền xuôi (Forward pass)\n",
    "          // Dữ liệu đi qua: Conv -> ReLU -> MaxPool -> ... -> Decoder\n",
    "          forward_autoencoder(&autoencoder);\n",
    "\n",
    "          // 4. Tính toán Loss (MSE)\n",
    "          float batch_loss = MSE(autoencoder.input, autoencoder.output, input_size);\n",
    "          total_loss += batch_loss;\n",
    "\n",
    "          // 5. Lan truyền ngược (Backward pass)\n",
    "          // Tính gradient từ output ngược về input\n",
    "          backward_autoencoder(&autoencoder);\n",
    "\n",
    "          // 6. Cập nhật tham số (Update weights)\n",
    "          // W = W - learning_rate * gradient\n",
    "          update_autoencoder_parameters(&autoencoder, learning_rate);\n",
    "      }\n",
    "\n",
    "      // Kết thúc epoch: Tính thời gian thực thi epoch.\n",
    "      clock_t end_time = clock();\n",
    "      double epoch_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n",
    "  }\n",
    "  ```\n",
    "#### Kết quả:\n",
    "- Thời gian huấn luyện cho mỗi epoch và tổng thời gian huấn luyện:\n",
    "  - Với mỗi epoch, phiên CPU sẽ thực thi trong khoảng 489 giây.\n",
    "  - Tổng thời gian thực thi 20 epoch là khoảng 9780 giây (tương đương gần với 2 giờ 43 phút).\n",
    "- Final reconstruction loss: 0.054788.\n",
    "- Một số ảnh gốc và ảnh tái tạo:\n",
    "![Ảnh mẫu reconstructed bởi CPU](https://drive.google.com/uc?export=view&id=1HPJctlPGdm7VdnxSwX7QfXSTkSVR500y)\n",
    "- Memory usage\n",
    "#### Những điểm chính:\n",
    "1) Bài học rút ra từ thuật toán:\n",
    "- Qua quá trình tự cài đặt thủ công các lớp mạng nơ-ron và thuật toán lan truyền ngược (Backpropagation) trên CPU, chúng em đã rút ra những bài học quan trọng về bản chất của thuật toán:\n",
    "  - Cường độ tính toán cực lớn: Khi trực tiếp cài đặt thuật toán Convolution, chúng em nhận thấy rằng đây không chỉ là phép nhân ma trận đơn thuần mà là một chuỗi các phép tính tích vô hướng (dot product) trượt qua không gian đầu vào. Với mỗi lớp Conv2D, việc áp dụng các bộ lọc (filters) kích thước 3×3 lên toàn bộ chiều sâu của ảnh đầu vào tạo ra một khối lượng tính toán khổng lồ.\n",
    "  - Cơ chế nén và tái tạo thông tin: Việc cài đặt tuần tự các lớp của Autoencoder giúp nhóm có thể hình dung rõ cách Autoencoder học các đặc trưng.\n",
    "  - Kiểm soát biên và bộ nhớ: Thao tác trực tiếp với Padding và Stride cho thấy tầm quan trọng của việc tính toán chỉ số (indexing) chính xác. Một sai sót nhỏ tại biên không chỉ làm lệch kích thước Tensor mà còn gây lỗi truy cập bộ nhớ nghiêm trọng.\n",
    "2) Những hiểu biết sâu sắc đã hướng dẫn việc triển khai GPU:\n",
    "- Khai thác tính độc lập dữ liệu cho mô hình SIMT: Thông qua việc cài đặt thủ công các lớp Convolution, MaxPool và UpSample, chúng em nhận thấy các phép toán tại mỗi vị trí đầu ra (h,w,c) hoàn toàn độc lập. Giá trị của một pixel trong Feature Map đích không phụ thuộc vào việc tính toán các pixel lân cận. \n",
    "  - Định hướng GPU: Đặc tính này hoàn toàn phù hợp với kiến trúc SIMT (Single Instruction, Multiple Threads) của CUDA. Thay vì sử dụng vòng lặp tuần tự, chúng em sẽ thiết kế kernel sao cho mỗi luồng (thread) hoặc một nhóm luồng sẽ chịu trách nhiệm tính toán song song cho một điểm ảnh cụ thể, tận dụng tối đa số lượng nhân CUDA khổng lồ.\n",
    "- Chiến lược giải quyết nút thắt cổ chai tính toán: Việc quan sát mã nguồn CPU cho thấy lớp Conv2D sở hữu độ phức tạp tính toán lớn nhất với 6 vòng lặp lồng nhau. Đây chính là điểm nghẽn hiệu năng chính, chiếm phần lớn thời gian huấn luyện.\n",
    "  - Định hướng GPU: Để giải quyết vấn đề này, chiến lược song song hóa cần tập trung vào việc trải phẳng (flatten) các chiều của vòng lặp và ánh xạ chúng vào không gian lưới (Grid/Block) của GPU.\n",
    "- Tối ưu hóa mô hình truy cập bộ nhớ: Phân tích cơ chế \"cửa sổ trượt\" (sliding window) của phép tích chập cho thấy một lượng lớn dữ liệu đầu vào bị đọc lặp lại dư thừa. Cụ thể, với kernel 3×3, một pixel đầu vào được truy xuất để tính toán cho 9 pixel đầu ra khác nhau. Trên CPU, việc truy cập bộ nhớ liên tục nhưng tại các địa chỉ rời rạc gây lãng phí băng thông nghiêm trọng.\n",
    "  - Định hướng GPU: Việc chỉ dựa vào Global Memory trên GPU sẽ tạo ra độ trễ lớn. Nhận định này dẫn dắt nhóm đến việc ứng dụng Shared Memory Tiling. Bằng cách tải các khối dữ liệu (tiles) vào bộ nhớ chia sẻ tốc độ cao, các luồng trong cùng một block có thể tái sử dụng dữ liệu hiệu quả, giảm thiểu tối đa lưu lượng truy cập xuống Global Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDIWzpuE0GLd"
   },
   "source": [
    "### 2.2. GPU cơ bản\n",
    "#### Mục tiêu:\n",
    "- Chuyển việc triển khai trên CPU sang trên GPU bằng song song hóa cơ bản cho các phép toán chính là Con2D, MaxPool2D, ReLU và UpSample2D.\n",
    "- So sánh output GPU với CPU để đảm bảo kết quả đúng so với CPU.\n",
    "- Thiết lập baseline hiệu suất GPU: đo thời gian thực thi theo kernel và tổng thời gian để làm mốc cho các version tối ưu phía sau.\n",
    "#### Chi tiết cài đặt:\n",
    "- Chiến lược song song hóa:\n",
    "  - Layout dữ liệu: Tensor được lưu dạng 1D liên tục theo NCHW với công thức tỉ số là\n",
    "  $$idx(n,c,h,w) = ((n \\cdot C + c)\\cdot H +h)\\cdot W +w$$\n",
    "  - Các kernel được thiết kế theo nguyên tắc:\n",
    "    - Các phép toán tạo output 4D như Conv/Pool/Upsample: 1 thread xử lý 1 phần tử output.\n",
    "    - Các phép toán \"element-wise\" như ReLU: 1 thread xử lý 1 phần tử.\n",
    "  - Cách ánh xạ thread:\n",
    "    - `blockIdx.x` và `blockIdx.y` quét theo không gian `w` và `h`.\n",
    "    - `blockIdx.z` gộp `(N,C)` hoặc `(N, C_out)` bằng chỉ số `nc`.\n",
    "- Thiết kế kernel:\n",
    "\n",
    "  **1. Convolution:**\n",
    "    - Forward: Mỗi thread tính 1 phần tử output tại tọa độ `(n, c_out, h_out, w_out)`. Lưới tính toán được ánh xạ bằng grid 3D với các trục x-y tương ứng với vị trí không gian của output và trục z mã hóa batch và kênh đầu ra. Mỗi thread duyệt toàn bộ kênh vào và kernel `K × K`, tự tính chỉ số input/weight theo định dạng NCHW để thực hiện pháp nhân-cộng. Padding được xử lý bằng kiểm tra biên trực tiếp và kết quả cuối cùng được ghi vào bộ nhớ output theo chỉ số tương ứng (cộng bias nếu có).\n",
    "    - Backward:\n",
    "      - Input: Hàm được thiết kế để tính gradient theo input dX, với mỗi thread phụ trách một phần tử `(n, c_in, h, w)` của tensor đầu vào. Thread duyệt toàn bộ `c_out`, kernel `K × K` và kiểm tra điều kiện stride để truy ngược vị trí tương ứng trên dY. Kernel có được xoay 180 độ theo đúng quy tắc convolution gradient. Chỉ số được tính theo định dạng NCHW và kết quả ghi trực tiếp vào dX.\n",
    "      - Weight: Mỗi thread chịu trách nhiệm tính gradient cho đúng 1 phần tử weight tại vị trí `(c_out, c_in, kh, kw)`. Thread duyệt toàn bộ batch và toàn bộ vị trí `h_out`, `w_out` của output, nhân dY với phần tử input tương ứng rồi cộng dồn vào một biến sum cục bộ.\n",
    "      - Bias: Hàm được thiết kế theo hướng mỗi thread tính gradient cho một phần tử bias tương ứng c_out. Thread duyệt toàn bộ batch và toàn bộ spatial vị trí của dY, cộng dồn toàn bộ giá trị gradient để tạo ra `dB[c_out]`. \n",
    "\n",
    "  **2. MaxPool:**\n",
    "  - Forward: Kernel `maxpool2x2_forward` thực hiện max pooling 2×2 với `stride = 2`, giảm kích thước từ `H×W` xuống `H/2 × W/2`. Duyệt theo layout NCHW, trong đó mỗi thread phụ trách một phần tử output `(n, c, h_out, w_out)`. Thread ánh xạ về cửa sổ input 2×2 tương ứng `(h_in0=2*h_out, w_in0=2*w_out)`, lấy giá trị lớn nhất trong 4 phần tử và ghi ra output.\n",
    "  - Backward: Kernel `maxpool2x2_backward` lan truyền gradient qua max pooling bằng cách chỉ chuyển gradient về đúng phần tử đạt max trong cửa sổ 2×2. Với mỗi output `(n, c, h_out, w_out)`, thread đọc 4 giá trị input, xác định `max_idx`, lấy `g = grad_out[...]` rồi gán `grad_in[...] = g` tại vị trí max (các vị trí còn lại giữ 0). Thiết kế giả định `grad_in` đã được zero trước khi chạy.\n",
    "  \n",
    "  **3. UpSample:**\n",
    "  - Forward: Kernel `upsample2x2_forward` thực hiện nearest-neighbor upsampling ×2 theo cả chiều cao và chiều rộng, tăng kích thước từ `H×W` lên `2H×2W`. Duyệt theo layout NCHW, mỗi thread tính một phần tử output `(n, c, h_out, w_out)` và ánh xạ ngược về input bằng phép chia nguyên `h_in = h_out/2, w_in = w_out/2`, sau đó copy giá trị input sang output tương ứng.\n",
    "  - Backward: Kernel `upsample2x2_backward` lan truyền gradient qua nearest-neighbor upsampling bằng cách cộng dồn gradient từ 4 pixel output (khối 2×2) quay về một pixel input. Mỗi thread phụ trách một phần tử input (n, c, h_in, w_in), lấy tổng grad_out tại các vị trí `(2h_in+dh, 2w_in+dw)` với `dh,dw∈{0,1}`, rồi gán vào `grad_in[idx_in]`.\n",
    "\n",
    "  **4. ReLU:**\n",
    "  - Forward: Kernel `relu_forward` thực hiện hàm kích hoạt ReLU theo kiểu element-wise trên một vector/ tensor đã được làm phẳng. Mỗi thread xử lý một phần tử `x[i]`, áp dụng phép cắt ngưỡng `x[i] = max(x[i], 0)` và ghi in-place để giảm bộ nhớ trung gian. Có kiểm tra biên `i < size` để đảm bảo an toàn truy cập.\n",
    "  - Backward: Kernel `relu_backward` tính gradient của ReLU theo từng phần tử: với mỗi `i`, nếu `x[i] > 0` thì truyền gradient `grad_x[i] = grad_y[i]`, ngược lại gán 0. Thiết kế dùng các con trỏ `__restrict__` để giúp tối ưu truy cập bộ nhớ, và mỗi thread ghi đúng một phần tử `grad_x` nên không có xung đột ghi (race condition).\n",
    "\n",
    "  **5. MSE loss:**\n",
    "  - Forward: Kernel `mse_loss_forward` tính tổng sai số bình phương giữa output và target trên toàn bộ tensor. Mỗi thread xử lý một phần tử idx, tính `diff²` và ghi vào shared memory sdata, sau đó thực hiện reduction trong block để cộng dồn. Kết quả mỗi block được cộng vào biến loss toàn cục bằng atomicAdd, nên cần đảm bảo loss đã được khởi tạo về 0 trước khi launch. (cần chia cho size ở ngoài để thu được MSE)\n",
    "  - Backward: Kernel `mse_loss_backward` tính gradient theo từng phần tử cho MSE: $∂L/∂output[i]=2(output[i]−target[i])/size$. Mỗi thread ghi độc lập `grad_out[i]`, có kiểm tra biên `i < size`, không phát sinh race condition và phù hợp cho lan truyền ngược trong autoencoder.\n",
    "- Chiến lược quản lý bộ nhớ device: Sử dụng chiến lược cấp phát trước (pre-allocation/static). Toàn bộ bộ đệm trên device cho tham số mô hình (weights/biases), các tensor trung gian (activations) và gradient tương ứng được `cudaMalloc` một lần duy nhất trong hàm `gpu_autoencoder_init()` trước khi bắt đầu huấn luyện. Trong quá trình train, mỗi iteration chỉ cần copy input batch từ host sang device và tái sử dụng các buffer đã cấp phát cho các bước forward/backward, nhờ đó tránh việc cấp phát/giải phóng lặp lại trong vòng lặp huấn luyện.\n",
    "- Vòng lặp huấn luyện được thiết kế như sau:\n",
    "  1. Lặp qua các epoch: Mô hình được huấn luyện trong 20 epoch với batch_size là 64 và learning rate là 0.001.\n",
    "  2. Xáo trộn dữ liệu: Ở đầu mỗi epoch, hàm `shuffle_cifar10(&data)` được gọi để xáo trộn thứ tự ảnh huấn luyện.\n",
    "  3. Xử lý từng mini-batch:\n",
    "  - Số batch mỗi epoch được tính theo công thức `num_batches = TRAIN_NUM/batch_size`.\n",
    "  - Với mỗi `batch_id`, hàm `get_next_batch()` lấy dữ liệu batch hiện tại từ tập train và đưa vào buffer host `h_batch`.\n",
    "  - Batch trên host sau đó được truyền vào `gpu_autoencoder_forward()`, bên trong sẽ copy H2D vào buffer input đã cấp phát sẵn.\n",
    "  4. Quy trình huấn luyện một batch:\n",
    "  - Forward pass: Gọi `gou_autoencoder_forward()` để tính lần lượt output qua toàn bộ các layer và tạo reconstructed output.\n",
    "  - Loss tái tạo (MSE) được tính giữa ảnh gốc và ảnh tái tạo, giá trị loss của batch được dùng để tích lũy `epoch_loss` và được in log mỗi 100 batch.\n",
    "  - Backward pass và cập nhật tham số: Gọi `gpu_autoencoder_backward()` để tính gradient theo thứ tự ngược và cập nhật weights/bias bằng Gradient Descent với lr = 0.001. \n",
    "#### Results:\n",
    "- Thời gian huấn luyện mỗi epoch: trung bình khoảng 304.57s (khoảng 5 phút 4 giây)\n",
    "- Thời gian huấn luyện tổng cộng: 6091.42s (khoảng 101 phút 31 giây)\n",
    "- Tăng tốc so với CPU baseline: GPU nhanh hơn khoảng 80 lần.\n",
    "- Một số ảnh gốc và ảnh tái tạo:\n",
    "![Ảnh mẫu reconstructed bởi GPU naive](https://drive.google.com/uc?export=view&id=1Phzzp3KKKRkMTYVaK5ykaM0lLNFVn5-o)\n",
    "- Kiểm chứng kết quả của GPU so với CPU: Để kiểm chứng kết quả của các kernel chạy trên GPU, chúng em chạy CPU (baseline) và GPU trên cùng một bộ dữ liệu đầu vào/gradient được khởi tạo trước, sau đó so sánh bằng cách tính loss.\n",
    "  - Cách làm:\n",
    "    - Khởi tạo `h_x, h_w, h_b,...` theo công thức cố định để thu được kết quả.\n",
    "    - Chạy CPU để lấy `cpu_out`.\n",
    "    - Chạy GPU kernel, copy kết quả về host để lấy `gpu_out`.\n",
    "    - So sánh `cpu_out` và `gpu_out` bằng hàm `compare_arrays()`.\n",
    "  - Các thông số tính toán:\n",
    "    - **max|diff|**: sai khác tuyệt đối lớn nhất.\n",
    "    - **mean|diff|**: sai khác tuyệt đối trung bình.\n",
    "    - **RMSE**: căn trung bình bình phương sai khác.\n",
    "  - Các kernel cần verify: Conv2D bao gồm Forward và Backward (dX, dW, dB), MaxPool2D gồm Forward và Backward và UpSample2D bao gồm Forward, Backward.\n",
    "  - Kết quả thu được:\n",
    "  ![Kết quả verify GPU](https://drive.google.com/uc?export=view&id=1XpzRLBKkGTfdEMN3Ehp63HNm6pH0Cu03)\n",
    "#### Profiling Analysis:\n",
    "- Basic profiling results:  \n",
    "![Basic profiling results](https://drive.google.com/uc?export=view&id=1-rGNA7ARLzKWIRgPJbxNEbBjtacYX7LE)\n",
    "  - `conv2d_backward_input_naive` chiếm 49.49% tổng thời gian GPU, 78,100 lần gọi, 39.882 ms/lần (min 6.93 ms, max 100.45 ms).\n",
    "  - `conv2d_backward_weight_naive` chiếm 32.11%, 78,100 lần gọi, 25.876 ms/lần (min 3.74 ms, max 54.964 ms).\n",
    "  - `conv2d_forward_naive` chiếm 11.54% (~726.525 s), 78,100 lần gọi, 9.3025 ms/lần.\n",
    "  - `conv2d_backward_bias_naive` chiếm 5.76% (~362.393 s), 78,100 lần gọi, 4.6401 ms/lần.\n",
    "  - Các kernel còn lại rất nhỏ: ReLU/MaxPool/Upsample mỗi cái < 0.3%; `cudaMemcpy HtoD/DtoH` tổng khoảng 0.04%; `sgd_update` và `mse_loss` gần như không đáng kể.\n",
    "- Nhận diện nút thắt ban đầu: \n",
    "  - Toàn bộ nhóm Conv2D (forward + backward) chiếm khoảng 98.9% thời gian GPU. Đây là bottleneck chính của chương trình.\n",
    "  - Thời gian `cudaMemcpy` rất thấp, chứng tỏ hệ thống không bị nghẽn truyền dữ liệu mà chủ yếu bị giới hạn bởi tính toán và truy cập bộ nhớ global của các kernel conv2d.\n",
    "  - Hướng tối ưu ưu tiên: thay các kernel conv2d bằng bản tối ưu hơn (sử dụng tiling shared memory,...)\n",
    "#### Key Takeaways:\n",
    "- Cái gì nhanh bất ngờ/chậm bất ngờ:\n",
    "  - Nhanh bất ngờ: Các bước phụ trợ như ReLU, MaxPool, Upsample, MSE loss, SGD update đều chiếm tỷ lệ rất nhỏ, điều này cho thấy chúng đang chạy rất hiệu quả.\n",
    "  - Chậm bất ngờ: Conv2D backward chiếm áp đảo tổng thời gian GPU, đặc biệt là hàm backward cho input và backward cho weight.\n",
    "- Cơ hội tối ưu: Ưu tiên tối ưu Conv2D backward: Có thể cân nhắc sử dụng tiling và shared memory, vectorized load/store (float4) nếu dữ liệu phù hợp, unroll kernel 3x3,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmlPUJnS03BH"
   },
   "source": [
    "### 2.3. Triển khai tối ưu hoá GPU - Phiên bản 1.\n",
    "#### Tập trung tối ưu hoá bộ nhớ.\n",
    "#### Mục tiêu:\n",
    "- Mục tiêu: Nhắm vào tối ưu hoá bộ nhớ bằng các kỹ thuật như Shared Memory Tiling.\n",
    "- Hiệu suất cải thiện mong đợi: Giảm thời gian huấn luyện autoencoder nhưng vẫn đảm bảo độ chính xác cao.\n",
    "#### Chi tiết triển khai:\n",
    "Ở phiên bản Naive, sau khi nhóm xem xét kết quả phân tích (Profiling result), nhận thấy rằng có bốn hàm conv2d_backward_input_naive(), conv2d_backward_weight_naive(), conv2d_forward_naive() và conv2d_backward_bias_naive() chiếm thời gian nhiều nhất (khoảng 98% tổng thời gian thực thi). Do đó, nhóm sẽ tập trung tối ưu hoá bốn hàm trên.\n",
    "- Các kỹ thuật tối ưu hoá được áp dụng:\n",
    "1) Kỹ thuật Tiling với Shared Memory (Shared Memory Tiling):\n",
    "  - Bối cảnh: Trong thuật toán Convolution ngây thơ (Naive implementation), mỗi luồng (thread) chịu trách nhiệm tính toán một điểm ảnh đầu ra. Với kernel kích thước 3x3, mỗi luồng phải thực hiện 9 lần đọc dữ liệu từ bộ nhớ toàn cục (Global Memory).\n",
    "  - Vấn đề: Các luồng lân cận tính toán các pixel đầu ra liền kề cũng cần đọc các pixel đầu vào giống nhau (do tính chất cửa sổ trượt). Việc mỗi luồng tự đọc lại dữ liệu từ Global Memory gây ra sự dư thừa băng thông.\n",
    "  - Lợi ích: Shared Memory là bộ nhớ nằm ngay trên chip (on-chip) với tốc độ truy xuất cực nhanh (tương đương L1 Cache) và độ trễ thấp hơn hàng trăm lần so với Global Memory.\n",
    "  - Hiệu quả: Bằng cách tải một khối dữ liệu đầu vào (Input Tile) vào Shared Memory một lần duy nhất, tất cả các luồng trong cùng một block có thể tái sử dụng dữ liệu này nhiều lần cho phép tính tích chập. Điều này làm giảm đáng kể áp lực lên băng thông bộ nhớ, yếu tố giới hạn hiệu năng chính của các mạng CNN.\n",
    "  - Cách triển khai trong hàm Conv2D():\n",
    "    - Bước 1: Xác định kích thước của output cần tính toán là 16x16.\n",
    "    - Bước 2: Cấp phát một mảng 2 chiều trong Shared Memory có kích thước 18x18 để chứa vùng ảnh đầu vào tương ứng với block đầu ra, cộng thêm vùng biên (halo regions) cần thiết cho kernel.\n",
    "    - Bước 3: Sử dụng `__syncthreads()` để đảm bảo toàn bộ tile đã được tải xong trước khi bất kỳ luồng nào bắt đầu tính toán. Nếu thiếu bước này, một luồng có thể đọc phải dữ liệu rác chưa kịp tải.\n",
    "    - Bước 4: Thực hiện phép tích chập trên mảng hai chiều đầu vào trong Shared Memory thay vì input.\n",
    "  - Kỹ thuật này cũng được áp dụng cho Backward pass, cho hai hàm tính gradient cho input và trọng số.\n",
    "  - Đối với việc tính toán gradient của bias, chúng em áp dụng kỹ thuật Parallel Reduction sử dụng Shared Memory. Kỹ thuật này cho phép mỗi block CUDA tính tổng song song các giá trị gradient cục bộ và chỉ thực hiện một thao tác ghi duy nhất xuống Global Memory, loại bỏ hoàn toàn hiện tượng xung đột nguyên tử (atomic contention) và tăng tốc độ hội tụ.\n",
    "2) Kỹ thuật Constant Memory:\n",
    "  - Bối cảnh: Trong lan truyền xuôi lớp Tích chập, mỗi luồng sẽ chịu trách nhiệm tính toán cho một đầu ra và bias được truy cập giống hệt nhau bởi tất cả các luồng trong cùng một thời điểm.\n",
    "  - Vấn đề: Nếu để bias trong Global Memory, khi hàng ngàn luồng cùng yêu cầu đọc một địa chỉ bộ nhớ duy nhất, GPU phải tuần tự hóa các yêu cầu này hoặc sử dụng băng thông Global Memory một cách lãng phí để phục vụ dữ liệu giống nhau cho nhiều luồng. Điều này gây ra độ trễ lớn, đặc biệt khi các giá trị trọng số này là bất biến trong suốt quá trình thực thi kernel.\n",
    "  - Lợi ích: Việc dùng Constant Memory mang lại hiệu vượt trội nhờ cơ chế phần cứng đặc biệt của GPU:\n",
    "    - Cơ chế broadcast: Khi tất cả các luồng trong một warp (nhóm 32 luồng) cùng đọc một địa chỉ từ Constant Memory, phần cứng GPU sẽ thực hiện một thao tác broadcast đơn lẻ. Thay vì 32 lần đọc bộ nhớ, dữ liệu được gửi đến cả 32 luồng chỉ trong một chu kỳ truy xuất.\n",
    "    - Bộ nhớ đệm chuyên dụng: Constant Memory có bộ nhớ đệm riêng tách biệt với L1 Data Cache hay Shared Memory. Vì kích thước kernel thường nhỏ và được truy cập lặp đi lặp lại, tỉ lệ cache hit gần như đạt 100%, giảm thiểu tối đa độ trễ truy cập.\n",
    "    - Giảm tải băng thông Global Memory: Bằng cách đưa bias vào Constant Memory, băng thông của Global Memory được giải phóng để tập trung phục vụ việc đọc/ghi dữ liệu ảnh, giúp tăng thông lượng tổng thể.\n",
    "  - Cách triển khai:\n",
    "    - Bước 1: Khai báo mảng bias trong phạm vi toàn cục (global scope). Do kích thước bộ nhớ hằng số có hạn (thường là 64KB), nó rất phù hợp để chứa các bias (có kích thước tối đa là 256 * 4 = 1 KB).\n",
    "    - Bước 2: Thực hiện sao chép từ Global Memory sang Constant Memory.\n",
    "    - Bước 3: Khi thực hiện phép tích chập, mỗi luồng sẽ không cần truy cập vào Global Memory để lấy số liệu tính toán nữa.\n",
    "  3) Kỹ thuật Memory Coalescing trong tải dữ liệu biên:\n",
    "  - Bối cảnh: Trong kỹ thuật tiling, ngoài việc tải vùng dữ liệu chính, các luồng cần tải thêm các vùng biên xung quanh để đảm bảo đủ dữ liệu cho cửa sổ trượt của kernel (ví dụ 3×3). \n",
    "  - Lợi ích: Memory coalescing xảy ra khi các luồng liền kề trong một warp (32 threads) truy cập vào các địa chỉ bộ nhớ liền kề nhau trong Global Memory. Điều này cho phép GPU gộp nhiều yêu cầu đọc nhỏ thành một giao dịch bộ nhớ lớn, tối ưu hóa băng thông.\n",
    "  - Cách triển khai: \n",
    "    - Tải biên Trái/Phải: Các thread thỏa mãn `threadIdx.x < pad` sẽ chịu trách nhiệm tải. Dù số lượng thread hoạt động ít, nhưng các thread này có chỉ số tx liền kề nhau và đọc các địa chỉ bộ nhớ liền kề (theo chiều ngang), đảm bảo Coalescing.\n",
    "    - Tải biên Trên/Dưới: Các thread thỏa mãn threadIdx.y < pad sẽ tải. Quan trọng nhất, bên trong cách triển khai, chỉ số truy cập mảng vẫn dựa trên threadIdx.x (để quét dọc theo hàng ngang của vùng biên), do đó vẫn duy trì được Coalescing.\n",
    "    - Xử lý góc: Riêng thread (0,0) sẽ tải 4 góc. Mặc dù không đạt Coalescing (do chỉ 1 thread chạy), nhưng vì số lượng dữ liệu cực nhỏ (4 phần tử) nên chi phí này không đáng kể.\n",
    "- Key Code Snippets: Show the optimized kernel or key changes\n",
    "#### Kết quả:\n",
    "- Thời gian thực thi: Khoảng 58 phút. Nhanh gấp 1.81 lần so với phiên bản Naive.\n",
    "- Performance metrics (bandwidth utilization, occupancy)\n",
    "- Một số ảnh gốc và ảnh tái tạo:\n",
    "![Ảnh mẫu reconstructed bởi GPU version 1](https://drive.google.com/uc?export=view&id=1HxMYgUUDyIy5BUthej5hNJIPSvblqDiS)\n",
    "- So sánh profiling:\n",
    "  - Phiên bản Naive:\n",
    "\n",
    "  |Tên hàm|Thời gian thực thi (s)|Phần trăm thời gian thực thi (%)|\n",
    "  |-------|-----------------------|--------------------------------|\n",
    "  |conv2d_backward_input_naive|3100|49.49|\n",
    "  |conv2d_backward_weight_naive|2000|32.11|\n",
    "  |conv2d_forward_naive|726.525|11.54|\n",
    "  |conv2d_backward_bias_naive|362.393|5.76|\n",
    "\n",
    "  - Phiên bản tối ưu hoá 1:\n",
    "\n",
    "  |Tên hàm|Thời gian thực thi (s)|Phần trăm thời gian thực thi (%)|\n",
    "  |-------|-----------------------|--------------------------------|\n",
    "  |conv2d_backward_weight|1300|37.76|\n",
    "  |conv2d_backward_input|1100|30.71|\n",
    "  |conv2d_forward|1000|29.27|\n",
    "  |conv2d_backward_bias|9.02314|0.26|\n",
    "\n",
    "  *Nhận xét:* Có thể thấy các hàm đã giảm thời gian thực thi đáng kể, đặc biệt là lan truyền ngược của bias. Ngoài ra, thời gian `conv2d_forward` tăng lên (~1.38 lần) cho thấy chi phí quản lý Shared Memory đang lớn hơn lợi ích thu được. Tuy nhiên, phiên bản này vẫn đạt được hiệu quả tốt cho các hàm còn lại.\n",
    "\n",
    "#### Analysis:\n",
    "- Why did this optimization work (or not work as expected)?\n",
    "- What did profiling reveal?\n",
    "- What's the next bottleneck?\n",
    "#### Key Takeaways:\n",
    "- Lessons learned from this optimization\n",
    "- Applicability to other problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guwkMoVg1o1l"
   },
   "source": [
    "### 2.4. GPU Optimized Implementation - Version 2\n",
    "#### Optimization Focus:\n",
    "#### Objectives:\n",
    "- What specific optimization(s) you targeted\n",
    "- Expected performance improvement\n",
    "#### Implementation Details:\n",
    "- Optimization Techniques Applied\n",
    "  - Detailed explanation of the optimization (e.g., shared memory tiling)- Why this optimization should help\n",
    "  - Implementation approach\n",
    "- Key Code Snippets: Show the optimized kernel or key changes\n",
    "#### Results:\n",
    "- Training time comparison with previous version\n",
    "- Speedup over previous phase (incremental and cumulative)\n",
    "- Performance metrics (bandwidth utilization, occupancy)\n",
    "- Sample reconstructed images\n",
    "![Ảnh mẫu reconstructed bởi GPU version 2](https://drive.google.com/uc?export=view&id=1CgsqohZwarREM4Fmc1W5ynOcQRsZEYL1)\n",
    "- Profiling comparison: before vs after\n",
    "#### Analysis:\n",
    "- Why did this optimization work (or not work as expected)?\n",
    "- What did profiling reveal?\n",
    "- What's the next bottleneck?\n",
    "#### Key Takeaways:\n",
    "- Lessons learned from this optimization\n",
    "- Applicability to other problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dTqqiJd1sdH"
   },
   "source": [
    "### 2.5. Tích hợp SVM:\n",
    "#### Chi tiết triển khai:\n",
    "- Trích xuất đặc trưng từ Encoder:\n",
    "    - Quá trình trích xuất đặc trưng gồm 2 bước:\n",
    "        - Bước 1: Chúng em tải lại trọng số của mô hình đã huấn luyện. Thực hiện lại phần kiến trúc Encoder (từ lớp Input đến lớp Latent Space) và loại bỏ hoàn toàn phần Decoder. Điều này đảm bảo mạng nơ-ron hoạt động như một bộ trích xuất đặc trưng thuần túy.\n",
    "        - Bước 2: Mỗi ảnh đầu vào x (kích thước 32×32×3) được đưa qua các lớp tích chập (Conv2D) và giảm mẫu (MaxPooling). Tại lớp cuối cùng của Encoder, dữ liệu được cô đọng lại thành một tensor 3 chiều có kích thước 8×8×128.\n",
    "- Tích hợp LIBSVM: \n",
    "    - Thay vì nhúng trực tiếp thư viện LIBSVM vào mã nguồn C có thể gây phức tạp trong việc biên dịch và quản lý bộ nhớ, nhóm thực hiện chiến lược tương tác gián tiếp thông qua tệp tin. Quy trình được chia thành hai bước:\n",
    "        - Bước 1: Sử dụng Encoder đã huấn luyện để trích xuất đặc trưng và ghi ra tệp văn bản theo định dạng chuẩn của LIBSVM.\n",
    "        - Bước 2: Sử dụng thư viện LIBSVM (thông qua CLI) để đọc tệp dữ liệu này và thực hiện huấn luyện.\n",
    "- Lựa chọn siêu tham số: \n",
    "    - Loại SVM: C-SVC (C-Support Vector Classification).\n",
    "    - Loại kernel: Linear kernel (Nhân tuyến tính).\n",
    "    - Tham số phạt: C = 1.0.\n",
    "- Các đoạn mã chính:\n",
    "    1) Trích xuất đặc trưng dùng để huấn luyện:\n",
    "    ```c\n",
    "    // Lặp qua từng batch\n",
    "    for (int b = 0; b < num_batches_train; ++b) {\n",
    "        // Copy ảnh vào bộ đệm\n",
    "        // Thực hiện encoder\n",
    "        cpu_extract_features(&autoencoder, h_batch, cur_bs, h_latent);\n",
    "        // Ghi file theo format LIBSVM\n",
    "        for (int i = 0; i < cur_bs; ++i) {\n",
    "            int idx = start + i;\n",
    "            int label = data.train_labels[idx];\n",
    "            const float* feat = h_latent + i * AE_LATENT_DIM;\n",
    "            write_svm_line(f_train, label, feat, AE_LATENT_DIM);\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "    2) Mã huấn luyện SVM: Phiên bản GPU optimization 2.\n",
    "    ```python\n",
    "    !./svm-train -s 0 -t 0 -c 1.0 \\\n",
    "    \"../opt2/train_svm.txt\" \\\n",
    "    ../opt2/model_ae_svm\n",
    "    ```\n",
    "#### Kết quả:\n",
    "- Thời gian trích xuất đặc trưng (50K train + 10K test):\n",
    "    - Phiên bản CPU: Khoảng 3 tiếng (cho 10K ảnh train và 2K ảnh test).\n",
    "    - Phiên bản GPU naive: Khoảng 4 phút 30 giây.\n",
    "    - Phiên bản tối ưu hoá GPU (Phiên bản 1): Khoảng 4 phút.\n",
    "    - Phiên bản tối ưu hoá GPU (Phiên bản 2): Khoảng 3 phút 40 giây.\n",
    "- Thời gian huấn luyện SVM:\n",
    "    - Phiên bản CPU huấn luyện khoảng 1 tiếng.\n",
    "    - Mỗi phiên bản GPU huấn luyện khoảng 7 tiếng.\n",
    "- Độ chính xác phân loại trên tập dữ liệu thử nghiệm: \n",
    "    - Phiên bản CPU: 56.65%, do nhóm em chỉ huấn luyện SVM với 10 ngàn ảnh nên sẽ không đạt được chính xác mong đợi là 60-65%.\n",
    "    - Phiên bản GPU naive: 63.63%.\n",
    "    - Phiên bản tối ưu hoá GPU (Phiên bản 1): 63.96%.\n",
    "    - Phiên bản tối ưu hoá GPU (Phiên bản 2): 64.25%.\n",
    "- Phân tích độ chính xác theo từng lớp:\n",
    "\n",
    "    1) Phiên bản CPU:\n",
    "\n",
    "    |ID lớp|Độ chính xác (%)|\n",
    "    |------|----------------|\n",
    "    |0|11.8|\n",
    "    |1|13.4|\n",
    "    |2|7.9|\n",
    "    |3|8.7|\n",
    "    |4|8.6|\n",
    "    |5|6.8|\n",
    "    |6|15.6|\n",
    "    |7|13|\n",
    "    |8|15.6|\n",
    "    |9|11.9|\n",
    "\n",
    "    2) Phiên bản GPU naive:\n",
    "\n",
    "    |ID lớp|Độ chính xác (%)|\n",
    "    |------|----------------|\n",
    "    |0|69.4|\n",
    "    |1|71.2|\n",
    "    |2|48.5|\n",
    "    |3|45.3|\n",
    "    |4|60.2|\n",
    "    |5|54.3|\n",
    "    |6|75.8|\n",
    "    |7|66.8|\n",
    "    |8|75|\n",
    "    |9|69.8|\n",
    "\n",
    "    3) Phiên bản tối ưu hoá GPU (Phiên bản 1):\n",
    "\n",
    "    |ID lớp|Độ chính xác (%)|\n",
    "    |------|----------------|\n",
    "    |0|67.3|\n",
    "    |1|71.7|\n",
    "    |2|51|\n",
    "    |3|45.2|\n",
    "    |4|60.2|\n",
    "    |5|55|\n",
    "    |6|74.1|\n",
    "    |7|69|\n",
    "    |8|76|\n",
    "    |9|70.1|\n",
    "\n",
    "    4) Phiên bản tối ưu hoá GPU (Phiên bản 2):\n",
    "\n",
    "    |ID lớp|Độ chính xác (%)|\n",
    "    |------|----------------|\n",
    "    |0|68.2|\n",
    "    |1|72.5|\n",
    "    |2|49.7|\n",
    "    |3|47|\n",
    "    |4|60.8|\n",
    "    |5|54.3|\n",
    "    |6|76|\n",
    "    |7|68.3|\n",
    "    |8|74.6|\n",
    "    |9|71.1|\n",
    "\n",
    "- Confusion matrix:\n",
    "1) Phiên bản CPU:\n",
    "\n",
    "![Confusion matrix cho CPU](https://drive.google.com/uc?export=view&id=1JXa1PKPK55oXImVMVrgwEAgf2hObp0Gb)\n",
    "\n",
    "2) Phiên bản GPU naive:\n",
    "\n",
    "![Confusion matrix cho GPU Naive](https://drive.google.com/uc?export=view&id=12FQrKBFRRpnVDVCJ71m5XZtWDsiOYmRm)\n",
    "\n",
    "3) Phiên bản tối ưu hoá GPU (Phiên bản 1):\n",
    "\n",
    "![Confusion matrix cho GPU V1](https://drive.google.com/uc?export=view&id=1YgcJr-rr7cHVnLTmdx84bUOEsOpu13de)\n",
    "\n",
    "4) Phiên bản tối ưu hoá GPU (Phiên bản 2):\n",
    "\n",
    "![Confusion matrix cho GPU V2](https://drive.google.com/uc?export=view&id=1EhF_rA_7LW-WbjapH_84-jbJ9u3mRrPK)\n",
    "\n",
    "#### Phân tích:\n",
    "- Những lớp dễ phân loại và khó phân loại nhất:\n",
    "    - Kết quả trên tập kiểm tra cho thấy mô hình hoạt động rất hiệu quả trên nhóm đối tượng nhân tạo (máy bay, ô tô) và các lớp có đặc trưng thị giác nổi bật như lớp 6 (ếch) và lớp 8 (tàu thủy). Sự vượt trội này xuất phát từ tính đặc thù của dữ liệu. Ví dụ, ảnh tàu thủy thường gắn liền với nền xanh đồng nhất (nước hoặc bầu trời), ít nhiễu nền, giúp Autoencoder dễ dàng cô lập đối tượng. Tương tự, ếch có hình thái cơ thể tròn trịa và tư thế ngồi đặc trưng, khác biệt hoàn toàn so với cấu trúc bốn chân của các loài thú khác, giúp mô hình dễ dàng phân tách . \n",
    "    - Tuy nhiên, độ chính xác bị giảm sút đáng kể ở các lớp động vật, đặc biệt là lớp 2 (chim) và lớp 3 (mèo) với độ chính xác chỉ đạt ngưỡng 50%. Cho thấy hai lớp này sẽ khó phân loại nhất do cấu trúc sinh học phức tạp của chúng có thể gây nhầm lẫn sang thực thể khác.\n",
    "    \n",
    "    **Kết luận:** Điều đó cho thấy, mặc dù SVM phân loại tốt các đối tượng có hình học cố định, nhưng việc trích xuất đặc trưng cho các đối tượng sinh học phức tạp vẫn là một thách thức, gợi ý rằng mô hình cần huấn luyện lâu hơn hoặc cần kiến trúc Autoencoder sâu hơn để bắt được các chi tiết tinh tế này.\n",
    "- Ma trận nhầm lẫn tiết lộ những sai lầm của mô hình SVM trong quá trình phân loại nhãn, cụ thể như sau:\n",
    "    - Phân tách rõ ràng giữa Động vật và Phương tiện: Ma trận cho thấy các lỗi phân loại sai không phân bố ngẫu nhiên mà có xu hướng co cụm. Mô hình hiếm khi nhầm lẫn giữa nhóm phương tiện (Máy bay, ô tô, tàu, xe tải) và nhóm động vật. Cho thấy Autoencoder đã học thành công các đặc trưng toàn cục, đủ để SVM có thể phân tách hai siêu lớp này.\n",
    "    - Sự nhập nhằng do tương đồng thị giác: Đa số các trường hợp dự đoán sai xảy ra giữa các cặp lớp có cấu trúc hình học tương tự nhau. Cụ thể:\n",
    "        - Ở các phiên bản, cho thấy rằng mô hình phần lớn nhầm lẫn giữa lớp chó và mèo. Vì chúng có các đặc điểm giống nhau, ví dụ như sự tương đồng về kích thước, cấu trúc 4 chân và kết cấu lông.\n",
    "        - Ngoài ra, mô hình cũng thường xuyên nhầm lẫn giữa lớp ô tô và lớp xe tải do chúng đều cùng có bánh xe và hình dáng hộp.\n",
    "        - Điều này chỉ ra giới hạn của mô hình là các đặc trưng trong không gian tiềm ẩn (Latent Space) chưa đủ độ chi tiết để phân biệt các khác biệt nhỏ về mặt cấu trúc sinh học.\n",
    "    **Kết luận:** Phân tích ma trận nhầm lẫn cho thấy mô hình đã đạt được sự trừu tượng hóa cấp cao, thể hiện qua khả năng phân tách rạch ròi giữa hai nhóm đối tượng lớn là 'Phương tiện' và 'Động vật' dựa trên các cấu trúc hình học tổng quát. Tuy nhiên, các biểu diễn trong không gian tiềm ẩn chưa đủ chi tiết để giải quyết bài toán phân biệt các đối tượng có độ tương đồng thị giác cao (như kết cấu lông hay hình dáng cơ thể giữa các loài thú), dẫn đến sự nhập nhằng trong các dự đoán nội bộ nhóm.\n",
    "- Độ chính xác so với kỳ vọng: Trong phần mô tả, nêu rõ là kỳ vọng độ chính xác 60-65%. Ở đây tất cả các phiên bản đều đạt được độ chính xác kỳ vọng là 63-64% (ngoại trừ CPU không đạt do huấn luyện dữ liệu ít hơn).\n",
    "#### Những điểm chính:\n",
    "- Chất lượng của các đặc trưng mô hình đã học: Kết quả đạt được từ việc huấn luyện mô hình cho thấy rằng những đặc trưng mà Autoencoder đã học được rất tốt. Từ đó giúp mô hình có khả năng phân loại tốt, và đạt được hiệu quả mong đợi.\n",
    "- Effectiveness of two-stage approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsPue2eb2Fkf"
   },
   "source": [
    "## 3. Comprehensive Performance Analysis\n",
    "### 3.1. Performance Comparison Across All Phases:\n",
    "#### Table Format:\n",
    "| Phase        | Training Time | Speedup (vs CPU) | Incremental Speedup | Memory Usage | Key Optimization        |\n",
    "|-------------|---------------|------------------|----------------------|-------------|-------------------------|\n",
    "| CPU Baseline| 1800s         | 1.0×             | -                    | -           | -                       |\n",
    "| GPU Basic   | 180s          | 10.0×            | 10.0×                | 2.1 GB      | Parallelization         |\n",
    "| GPU Opt v1  | 45s           | 40.0×            | 4.0×                 | 2.3 GB      | Shared memory           |\n",
    "| GPU Opt v2  | 25s           | 72.0×            | 1.8×                 | 2.5 GB      | Kernel Fusion + Streams |\n",
    "\n",
    "#### Visualizations:\n",
    "- Bar chart showing training time across phases\n",
    "- Line graph showing cumulative speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EttBWMjc2aAa"
   },
   "source": [
    "## 4. Bài học và khó khăn trong quá trình thực hiện:\n",
    "### 4.1. Những kỹ thuật quan trọng:\n",
    "- Lập trình CUDA:\n",
    "  - Hiểu rõ cách tổ chức kernel theo mô hình grid/block/thread, đặc biệt là chiến lược ánh xạ (x,y) cho không gian (h,w) và z cho (batch, channel) để xử lý tensor NCHW hiệu quả.\n",
    "  - Nắm vững các vấn đề khi làm việc với CUDA như: quản lý truy cập bộ nhớ, tránh ghi đè (race condition) và quản lý lỗi bằng CHECK_CUDA.\n",
    "  - Hiểu được tầm quan trọng của việc đồng bộ các hoạt động được thực thi trên GPU để đảm bảo thu được kết quả đúng và hiệu quả.\n",
    "- Deep Learning: \n",
    "  - Hiểu được kiến trúc của mô hình Autoencoder, cách mà mô hình hoạt động để học và trích xuất được đặc trưng không cần nhãn.\n",
    "  - Biết được các hoạt động của các lớp trong mô hình (Conv, Pool, Upsample), cách hoạt động của encoder-decoder.\n",
    "- Tối ưu hiệu suất:\n",
    "  - Thiết lập được baseline để tối ưu: Trước khi thực hiện tối ưu cần phải chính xác và có mốc đo để so sánh.\n",
    "  - Thấy rõ được bottleneck của baseline, từ đó đưa ra các chiến lược tối ưu thích hợp để đạt được hiệu suất tốt nhất.  \n",
    "### 4.2. Khó khăn chính và giải pháp:\n",
    "#### - Khó khăn 1: Chuyển code từ CPU sang GPU vẫn giữ đúng logic và độ chính xác\n",
    "- Vấn đề: Khi chuyển đổi từng layer sang GPU, rất dễ sai ở phần chỉ số (index), mapping thread với output, chỉ cần lệch 1 chỉ số là output sai toàn bộ.\n",
    "- Giải pháp: \n",
    "  - Có test để verify kết quả so với CPU theo từng kernel.\n",
    "  - Dùng một layout index thống nhất là idx4().\n",
    "- Bài học: Cần phải đảm bảo chính xác từng baseline viết bằng CPU, có test để verify kết quả của GPU và CPU, sau đó mới tập trung vào tối ưu.\n",
    "#### - Khó khăn 2: Train trên số ít epoch khiến mô hình hội tụ không tốt.\n",
    "- Vấn đề: Do hạn chế về mặt tài nguyên, nhóm chỉ có thể train trên số lượng ít epoch. Có trường hợp đạt loss thấp nhưng khi extract_features và đưa sang bộ phân loại SVM thì accuracy chỉ đạt quanh 10%.\n",
    "- Giải pháp:\n",
    "  - Xem xét lại phần khởi tạo trọng số, điều chỉnh khoảng khởi tạo tốt hơn.\n",
    "  - Tăng learning rate\n",
    "- Bài học:\n",
    "  - Reconstruction loss thấp không đảm bảo latent representation sẽ tốt, vẫn có trường hợp tệ là đặc trưng được trích xuất không mang nhiều thông tin hỗ trợ phân loại lớp.\n",
    "  - Khởi tạo trọng số và learning rate trong vài trường hợp có ảnh hưởng mạnh đến hiệu suất mô hình. Khởi tạo không hù hợp có thể khiến mô hình hội tụ kém, tạo ra latent không có ý nghĩa phân loại. Tương tự, learning rate quá nhỏ khiến mô hình học chậm, quá lớn lại đi lệch đi điểm tối ưu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMYtXPS12o8t"
   },
   "source": [
    "## 5. Conclusion and Future Work:\n",
    "### 5.1. Project Summary:\n",
    "- Recap of what was accomplished\n",
    "- Final performance metrics summary table\n",
    "- Achievement of original objectives\n",
    "### 5.2. Key Achievements:\n",
    "Highlight your best results:\n",
    "- Maximum speedup achieved\n",
    "- Classification accuracy\n",
    "- Most successful optimization\n",
    "- Technical skills mastered\n",
    "### 5.3. Limitations:\n",
    "Honestly discuss:\n",
    "- Current performance bottlenecks\n",
    "- Accuracy limitations\n",
    "- Implementation constraints\n",
    "### 5.4. Future Improvements:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
